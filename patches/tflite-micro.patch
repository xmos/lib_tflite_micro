diff --git a/python/tflite_micro/python_ops_resolver.cc b/python/tflite_micro/python_ops_resolver.cc
index ab67bd4a..127c4ee9 100644
--- a/python/tflite_micro/python_ops_resolver.cc
+++ b/python/tflite_micro/python_ops_resolver.cc
@@ -39,28 +39,28 @@ PythonOpsResolver::PythonOpsResolver() {
   AddConv2D();
   AddCos();
   AddCumSum();
-  AddDelay();
+  //AddDelay();
   AddDepthToSpace();
   AddDepthwiseConv2D();
   AddDequantize();
   AddDetectionPostprocess();
   AddDiv();
-  AddEnergy();
+  //AddEnergy();
   AddElu();
   AddEqual();
   AddEthosU();
   AddExp();
   AddExpandDims();
-  AddFftAutoScale();
+  //AddFftAutoScale();
   AddFill();
-  AddFilterBank();
-  AddFilterBankLog();
-  AddFilterBankSquareRoot();
-  AddFilterBankSpectralSubtraction();
+  // AddFilterBank();
+  // AddFilterBankLog();
+  // AddFilterBankSquareRoot();
+  // AddFilterBankSpectralSubtraction();
   AddFloor();
   AddFloorDiv();
   AddFloorMod();
-  AddFramer();
+  //AddFramer();
   AddFullyConnected();
   AddGather();
   AddGatherNd();
@@ -68,7 +68,7 @@ PythonOpsResolver::PythonOpsResolver() {
   AddGreaterEqual();
   AddHardSwish();
   AddIf();
-  AddIrfft();
+  //AddIrfft();
   AddL2Normalization();
   AddL2Pool2D();
   AddLeakyRelu();
@@ -88,11 +88,11 @@ PythonOpsResolver::PythonOpsResolver() {
   AddMul();
   AddNeg();
   AddNotEqual();
-  AddOverlapAdd();
+  //AddOverlapAdd();
   AddPack();
   AddPad();
   AddPadV2();
-  AddPCAN();
+  //AddPCAN();
   AddPrelu();
   AddQuantize();
   AddReadVariable();
@@ -102,7 +102,7 @@ PythonOpsResolver::PythonOpsResolver() {
   AddReshape();
   AddResizeBilinear();
   AddResizeNearestNeighbor();
-  AddRfft();
+  //AddRfft();
   AddRound();
   AddRsqrt();
   AddSelectV2();
@@ -119,7 +119,7 @@ PythonOpsResolver::PythonOpsResolver() {
   AddSquaredDifference();
   AddSqueeze();
   AddStridedSlice();
-  AddStacker();
+  //AddStacker();
   AddSub();
   AddSum();
   AddSvdf();
@@ -130,7 +130,7 @@ PythonOpsResolver::PythonOpsResolver() {
   AddUnpack();
   AddVarHandle();
   AddWhile();
-  AddWindow();
+  //AddWindow();
   AddZerosLike();
 }
 
diff --git a/python/tflite_micro/python_ops_resolver.h b/python/tflite_micro/python_ops_resolver.h
index ae0a7566..709e7e6f 100644
--- a/python/tflite_micro/python_ops_resolver.h
+++ b/python/tflite_micro/python_ops_resolver.h
@@ -23,7 +23,7 @@ namespace tflite {
 // PythonOpsResolver is used to register all the Ops for the TFLM Python
 // interpreter. This is ok since code size is not a concern from Python and
 // the goal is to be able to run any model supported by TFLM in a flexible way
-class PythonOpsResolver : public MicroMutableOpResolver<200> {
+class PythonOpsResolver : public tflite_micro::MicroMutableOpResolver<200> {
  public:
   PythonOpsResolver();
 
diff --git a/tensorflow/lite/array.cc b/tensorflow/lite/array.cc
index 1b1ff2e4..bea9fe5a 100644
--- a/tensorflow/lite/array.cc
+++ b/tensorflow/lite/array.cc
@@ -15,7 +15,7 @@ limitations under the License.
 
 #include "tensorflow/lite/array.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace array_internal {
 
 void TfLiteArrayDeleter::operator()(TfLiteIntArray* a) {
@@ -30,4 +30,4 @@ void TfLiteArrayDeleter::operator()(TfLiteFloatArray* a) {
 }
 
 }  // namespace array_internal
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/array.h b/tensorflow/lite/array.h
index 44550cd1..2612305b 100644
--- a/tensorflow/lite/array.h
+++ b/tensorflow/lite/array.h
@@ -23,7 +23,7 @@ limitations under the License.
 
 #include "tensorflow/lite/core/c/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 /// TfLite*Array helpers
 
@@ -151,6 +151,6 @@ inline FloatArrayUniquePtr BuildTfLiteArray(const TfLiteFloatArray& other) {
   return BuildTfLiteArray(other.size, other.data);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_ARRAY_H_
diff --git a/tensorflow/lite/context_util.h b/tensorflow/lite/context_util.h
index cbbe9f10..1698f281 100644
--- a/tensorflow/lite/context_util.h
+++ b/tensorflow/lite/context_util.h
@@ -23,7 +23,7 @@ limitations under the License.
 
 #include "tensorflow/lite/core/c/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 /// Provides a range iterable wrapper for TfLiteIntArray* (C lists) that TfLite
 /// C api uses.
@@ -49,6 +49,6 @@ class TfLiteIntArrayView {
   const TfLiteIntArray* int_array_;
 };
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_CONTEXT_UTIL_H_
diff --git a/tensorflow/lite/core/api/error_reporter.cc b/tensorflow/lite/core/api/error_reporter.cc
index 7070eaa5..c5e75d6e 100644
--- a/tensorflow/lite/core/api/error_reporter.cc
+++ b/tensorflow/lite/core/api/error_reporter.cc
@@ -15,7 +15,7 @@ limitations under the License.
 #include "tensorflow/lite/core/api/error_reporter.h"
 #include <cstdarg>
 
-namespace tflite {
+namespace tflite_micro {
 
 int ErrorReporter::Report(const char* format, ...) {
   va_list args;
@@ -35,4 +35,4 @@ int ErrorReporter::ReportError(void*, const char* format, ...) {
   return code;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/core/api/error_reporter.h b/tensorflow/lite/core/api/error_reporter.h
index 1e0ef7dc..7a71995e 100644
--- a/tensorflow/lite/core/api/error_reporter.h
+++ b/tensorflow/lite/core/api/error_reporter.h
@@ -17,7 +17,7 @@ limitations under the License.
 
 #include <cstdarg>
 
-namespace tflite {
+namespace tflite_micro {
 
 /// A functor that reports error to supporting system. Invoked similar to
 /// printf.
@@ -53,7 +53,7 @@ class ErrorReporter {
   int ReportError(void*, const char* format, ...);
 };
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 // You should not make bare calls to the error reporter, instead use the
 // TF_LITE_REPORT_ERROR macro, since this allows message strings to be
@@ -63,7 +63,7 @@ class ErrorReporter {
 #ifndef TF_LITE_STRIP_ERROR_STRINGS
 #define TF_LITE_REPORT_ERROR(reporter, ...)                               \
   do {                                                                    \
-    static_cast<::tflite::ErrorReporter*>(reporter)->Report(__VA_ARGS__); \
+    static_cast<::tflite_micro::ErrorReporter*>(reporter)->Report(__VA_ARGS__); \
   } while (false)
 #else  // TF_LITE_STRIP_ERROR_STRINGS
 #define TF_LITE_REPORT_ERROR(reporter, ...)
diff --git a/tensorflow/lite/core/api/flatbuffer_conversions.cc b/tensorflow/lite/core/api/flatbuffer_conversions.cc
index a7089182..da5b9989 100644
--- a/tensorflow/lite/core/api/flatbuffer_conversions.cc
+++ b/tensorflow/lite/core/api/flatbuffer_conversions.cc
@@ -27,7 +27,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/compatibility.h"
 #include "tensorflow/lite/schema/schema_generated.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -733,7 +733,7 @@ TfLiteStatus ParseOpDataTfLite(const Operator* op, BuiltinOperator op_type,
       const auto* unique_params = op->builtin_options_as_UniqueOptions();
       if (unique_params != nullptr) {
         params->index_out_type =
-            unique_params->idx_out_type() == tflite::TensorType_INT64
+            unique_params->idx_out_type() == tflite_micro::TensorType_INT64
                 ? TfLiteType::kTfLiteInt64
                 : TfLiteType::kTfLiteInt32;
       }
@@ -2751,4 +2751,4 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,
 #endif
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/core/api/flatbuffer_conversions.h b/tensorflow/lite/core/api/flatbuffer_conversions.h
index 9c895b2f..cee68ccd 100644
--- a/tensorflow/lite/core/api/flatbuffer_conversions.h
+++ b/tensorflow/lite/core/api/flatbuffer_conversions.h
@@ -27,7 +27,7 @@ limitations under the License.
 #include "tensorflow/lite/core/c/common.h"
 #include "tensorflow/lite/schema/schema_generated.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // Interface class for builtin data allocations.
 class BuiltinDataAllocator {
@@ -435,6 +435,6 @@ TfLiteStatus ParseStablehloGather(const Operator* op,
                                   BuiltinDataAllocator* allocator,
                                   void** builtin_data);
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_CORE_API_FLATBUFFER_CONVERSIONS_H_
diff --git a/tensorflow/lite/core/api/tensor_utils.cc b/tensorflow/lite/core/api/tensor_utils.cc
index 18a643c7..277808d9 100644
--- a/tensorflow/lite/core/api/tensor_utils.cc
+++ b/tensorflow/lite/core/api/tensor_utils.cc
@@ -19,7 +19,7 @@ limitations under the License.
 
 #include "tensorflow/lite/core/c/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 TfLiteStatus ResetVariableTensor(TfLiteTensor* tensor) {
   if (!tensor->is_variable) {
@@ -47,4 +47,4 @@ TfLiteStatus ResetVariableTensor(TfLiteTensor* tensor) {
   return kTfLiteOk;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/core/api/tensor_utils.h b/tensorflow/lite/core/api/tensor_utils.h
index 440da8ab..4527a1c4 100644
--- a/tensorflow/lite/core/api/tensor_utils.h
+++ b/tensorflow/lite/core/api/tensor_utils.h
@@ -18,11 +18,11 @@ limitations under the License.
 
 #include "tensorflow/lite/core/c/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // Resets a variable tensor to the default value.
 TfLiteStatus ResetVariableTensor(TfLiteTensor* tensor);
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_CORE_API_TENSOR_UTILS_H_
diff --git a/tensorflow/lite/core/c/c_api_types.h b/tensorflow/lite/core/c/c_api_types.h
index 3a6594da..a26f482d 100644
--- a/tensorflow/lite/core/c/c_api_types.h
+++ b/tensorflow/lite/core/c/c_api_types.h
@@ -76,15 +76,15 @@ typedef enum TfLiteStatus {
   kTfLiteApplicationError = 3,
 
   /// Generally referring to serialized delegate data not being found.
-  /// See tflite::delegates::Serialization.
+  /// See tflite_micro::delegates::Serialization.
   kTfLiteDelegateDataNotFound = 4,
 
   /// Generally referring to data-writing issues in delegate serialization.
-  /// See tflite::delegates::Serialization.
+  /// See tflite_micro::delegates::Serialization.
   kTfLiteDelegateDataWriteError = 5,
 
   /// Generally referring to data-reading issues in delegate serialization.
-  /// See tflite::delegates::Serialization.
+  /// See tflite_micro::delegates::Serialization.
   kTfLiteDelegateDataReadError = 6,
 
   /// Generally referring to issues when the TF Lite model has ops that cannot
diff --git a/tensorflow/lite/core/c/common.cc b/tensorflow/lite/core/c/common.cc
index a2046fc8..c88efed6 100644
--- a/tensorflow/lite/core/c/common.cc
+++ b/tensorflow/lite/core/c/common.cc
@@ -106,78 +106,78 @@ void TfLiteVarArrayFree(T* a) {
 
 extern "C" {
 
-size_t TfLiteIntArrayGetSizeInBytes(int size) {
+size_t TfliteMicroIntArrayGetSizeInBytes(int size) {
   return TfLiteVarArrayGetSizeInBytes<TfLiteIntArray>(size);
 }
 
-int TfLiteIntArrayEqual(const TfLiteIntArray* a, const TfLiteIntArray* b) {
+int TfliteMicroIntArrayEqual(const TfLiteIntArray* a, const TfLiteIntArray* b) {
   return TfLiteVarArrayEqual(a, b);
 }
 
-int TfLiteIntArrayEqualsArray(const TfLiteIntArray* a, int b_size,
+int TfliteMicroIntArrayEqualsArray(const TfLiteIntArray* a, int b_size,
                               const int b_data[]) {
   return TfLiteVarArrayEqualsArray(a, b_size, b_data);
 }
 
 #ifndef TF_LITE_STATIC_MEMORY
 
-TfLiteIntArray* TfLiteIntArrayCreate(int size) {
+TfLiteIntArray* TfliteMicroIntArrayCreate(int size) {
   return TfLiteVarArrayCreate<TfLiteIntArray>(size);
 }
 
-TfLiteIntArray* TfLiteIntArrayCopy(const TfLiteIntArray* src) {
+TfLiteIntArray* TfliteMicroIntArrayCopy(const TfLiteIntArray* src) {
   return TfLiteVarArrayCopy(src);
 }
 
-void TfLiteIntArrayFree(TfLiteIntArray* a) { TfLiteVarArrayFree(a); }
+void TfliteMicroIntArrayFree(TfLiteIntArray* a) { TfLiteVarArrayFree(a); }
 
 #endif  // TF_LITE_STATIC_MEMORY
 
-int TfLiteFloatArrayGetSizeInBytes(int size) {
+int TfliteMicroFloatArrayGetSizeInBytes(int size) {
   return TfLiteVarArrayGetSizeInBytes<TfLiteFloatArray>(size);
 }
 
 #ifndef TF_LITE_STATIC_MEMORY
 
-TfLiteFloatArray* TfLiteFloatArrayCreate(int size) {
+TfLiteFloatArray* TfliteMicroFloatArrayCreate(int size) {
   return TfLiteVarArrayCreate<TfLiteFloatArray>(size);
 }
 
-TfLiteFloatArray* TfLiteFloatArrayCopy(const TfLiteFloatArray* src) {
+TfLiteFloatArray* TfliteMicroFloatArrayCopy(const TfLiteFloatArray* src) {
   return TfLiteVarArrayCopy(src);
 }
 
-void TfLiteFloatArrayFree(TfLiteFloatArray* a) { TfLiteVarArrayFree(a); }
+void TfliteMicroFloatArrayFree(TfLiteFloatArray* a) { TfLiteVarArrayFree(a); }
 
-void TfLiteTensorDataFree(TfLiteTensor* t) {
+void TfliteMicroTensorDataFree(TfLiteTensor* t) {
   if (t->allocation_type == kTfLiteVariantObject && t->data.data) {
     delete static_cast<VariantData*>(t->data.data);
   } else if (t->allocation_type == kTfLiteDynamic ||
              t->allocation_type == kTfLitePersistentRo) {
     if (t->data.raw) {
 #ifdef TF_LITE_TENSORFLOW_PROFILER
-      tflite::PauseHeapMonitoring(/*pause=*/true);
-      tflite::OnTfLiteTensorDealloc(t);
+      tflite_micro::PauseHeapMonitoring(/*pause=*/true);
+      tflite_micro::OnTfLiteTensorDealloc(t);
 #endif
       free(t->data.raw);
 #ifdef TF_LITE_TENSORFLOW_PROFILER
-      tflite::PauseHeapMonitoring(/*pause=*/false);
+      tflite_micro::PauseHeapMonitoring(/*pause=*/false);
 #endif
     }
   }
   t->data.raw = nullptr;
 }
 
-void TfLiteQuantizationFree(TfLiteQuantization* quantization) {
+void TfLiteMicroQuantizationFree(TfLiteQuantization* quantization) {
   if (quantization->type == kTfLiteAffineQuantization) {
     TfLiteAffineQuantization* q_params =
         (TfLiteAffineQuantization*)(quantization->params);
     if (q_params->scale) {
-      TfLiteFloatArrayFree(q_params->scale);
+      TfliteMicroFloatArrayFree(q_params->scale);
       q_params->scale = nullptr;
     }
     if (q_params->zero_point) {
-      TfLiteIntArrayFree(q_params->zero_point);
+      TfliteMicroIntArrayFree(q_params->zero_point);
       q_params->zero_point = nullptr;
     }
     free(q_params);
@@ -186,18 +186,18 @@ void TfLiteQuantizationFree(TfLiteQuantization* quantization) {
   quantization->type = kTfLiteNoQuantization;
 }
 
-void TfLiteSparsityFree(TfLiteSparsity* sparsity) {
+void TfLiteMicroSparsityFree(TfLiteSparsity* sparsity) {
   if (sparsity == nullptr) {
     return;
   }
 
   if (sparsity->traversal_order) {
-    TfLiteIntArrayFree(sparsity->traversal_order);
+    TfliteMicroIntArrayFree(sparsity->traversal_order);
     sparsity->traversal_order = nullptr;
   }
 
   if (sparsity->block_map) {
-    TfLiteIntArrayFree(sparsity->block_map);
+    TfliteMicroIntArrayFree(sparsity->block_map);
     sparsity->block_map = nullptr;
   }
 
@@ -206,9 +206,9 @@ void TfLiteSparsityFree(TfLiteSparsity* sparsity) {
     for (; i < sparsity->dim_metadata_size; i++) {
       TfLiteDimensionMetadata metadata = sparsity->dim_metadata[i];
       if (metadata.format == kTfLiteDimSparseCSR) {
-        TfLiteIntArrayFree(metadata.array_segments);
+        TfliteMicroIntArrayFree(metadata.array_segments);
         metadata.array_segments = nullptr;
-        TfLiteIntArrayFree(metadata.array_indices);
+        TfliteMicroIntArrayFree(metadata.array_indices);
         metadata.array_indices = nullptr;
       }
     }
@@ -219,13 +219,13 @@ void TfLiteSparsityFree(TfLiteSparsity* sparsity) {
   free(sparsity);
 }
 
-void TfLiteTensorFree(TfLiteTensor* t) {
-  TfLiteTensorDataFree(t);
-  if (t->dims) TfLiteIntArrayFree(t->dims);
+void TfliteMicroTensorFree(TfLiteTensor* t) {
+  TfliteMicroTensorDataFree(t);
+  if (t->dims) TfliteMicroIntArrayFree(t->dims);
   t->dims = nullptr;
 
   if (t->dims_signature) {
-    TfLiteIntArrayFree((TfLiteIntArray*)t->dims_signature);
+    TfliteMicroIntArrayFree((TfLiteIntArray*)t->dims_signature);
   }
   t->dims_signature = nullptr;
 
@@ -234,12 +234,12 @@ void TfLiteTensorFree(TfLiteTensor* t) {
   t->sparsity = nullptr;
 }
 
-void TfLiteTensorReset(TfLiteType type, const char* name, TfLiteIntArray* dims,
+void TfliteMicroTensorReset(TfLiteType type, const char* name, TfLiteIntArray* dims,
                        TfLiteQuantizationParams quantization, char* buffer,
                        size_t size, TfLiteAllocationType allocation_type,
                        const void* allocation, bool is_variable,
                        TfLiteTensor* tensor) {
-  TfLiteTensorFree(tensor);
+  TfliteMicroTensorFree(tensor);
   tensor->type = type;
   tensor->name = name;
   tensor->dims = dims;
@@ -254,19 +254,19 @@ void TfLiteTensorReset(TfLiteType type, const char* name, TfLiteIntArray* dims,
   tensor->quantization.params = nullptr;
 }
 
-TfLiteStatus TfLiteTensorCopy(const TfLiteTensor* src, TfLiteTensor* dst) {
+TfLiteStatus TfliteMicroTensorCopy(const TfLiteTensor* src, TfLiteTensor* dst) {
   if (!src || !dst) return kTfLiteOk;
   if (src->bytes != dst->bytes) return kTfLiteError;
   if (src == dst) return kTfLiteOk;
   dst->type = src->type;
-  if (dst->dims) TfLiteIntArrayFree(dst->dims);
-  dst->dims = TfLiteIntArrayCopy(src->dims);
+  if (dst->dims) TfliteMicroIntArrayFree(dst->dims);
+  dst->dims = TfliteMicroIntArrayCopy(src->dims);
   if (src->allocation_type == kTfLiteVariantObject) {
     // An edge case exists in control flow ops when they copy inputs to outputs
     // before invoking any body, in this case the `dst` will not have its
     // `allocation_type` set properly, so we handle here for now.
     if (dst->allocation_type != kTfLiteVariantObject) {
-      TfLiteTensorDataFree(dst);
+      TfliteMicroTensorDataFree(dst);
       dst->allocation_type = kTfLiteVariantObject;
     }
     auto* dst_vd = static_cast<VariantData*>(dst->data.data);
@@ -286,14 +286,14 @@ TfLiteStatus TfLiteTensorCopy(const TfLiteTensor* src, TfLiteTensor* dst) {
   return kTfLiteOk;
 }
 
-TfLiteStatus TfLiteTensorResizeMaybeCopy(size_t num_bytes, TfLiteTensor* tensor,
+TfLiteStatus TfliteMicroTensorResizeMaybeCopy(size_t num_bytes, TfLiteTensor* tensor,
                                          bool preserve_data) {
   if (tensor->allocation_type != kTfLiteDynamic &&
       tensor->allocation_type != kTfLitePersistentRo) {
     return kTfLiteOk;
   }
 #ifdef TF_LITE_TENSORFLOW_PROFILER
-  tflite::PauseHeapMonitoring(/*pause=*/true);
+  tflite_micro::PauseHeapMonitoring(/*pause=*/true);
 #endif
   size_t alloc_bytes = num_bytes;
   // TODO(b/145340303): Tensor data should be aligned.
@@ -303,11 +303,11 @@ TfLiteStatus TfLiteTensorResizeMaybeCopy(size_t num_bytes, TfLiteTensor* tensor,
   if (!tensor->data.data) {
     tensor->data.data = (char*)malloc(alloc_bytes);
 #ifdef TF_LITE_TENSORFLOW_PROFILER
-    tflite::OnTfLiteTensorAlloc(tensor, alloc_bytes);
+    tflite_micro::OnTfLiteTensorAlloc(tensor, alloc_bytes);
 #endif
   } else if (num_bytes > tensor->bytes) {
 #ifdef TF_LITE_TENSORFLOW_PROFILER
-    tflite::OnTfLiteTensorDealloc(tensor);
+    tflite_micro::OnTfLiteTensorDealloc(tensor);
 #endif
     if (preserve_data) {
       tensor->data.data = (char*)realloc(tensor->data.data, alloc_bytes);
@@ -318,11 +318,11 @@ TfLiteStatus TfLiteTensorResizeMaybeCopy(size_t num_bytes, TfLiteTensor* tensor,
       tensor->data.data = (char*)malloc(alloc_bytes);
     }
 #ifdef TF_LITE_TENSORFLOW_PROFILER
-    tflite::OnTfLiteTensorAlloc(tensor, alloc_bytes);
+    tflite_micro::OnTfLiteTensorAlloc(tensor, alloc_bytes);
 #endif
   }
 #ifdef TF_LITE_TENSORFLOW_PROFILER
-  tflite::PauseHeapMonitoring(/*pause=*/false);
+  tflite_micro::PauseHeapMonitoring(/*pause=*/false);
 #endif
   tensor->bytes = num_bytes;
   if (tensor->data.data == nullptr && num_bytes != 0) {
@@ -333,12 +333,12 @@ TfLiteStatus TfLiteTensorResizeMaybeCopy(size_t num_bytes, TfLiteTensor* tensor,
   return kTfLiteOk;
 }
 
-TfLiteStatus TfLiteTensorRealloc(size_t num_bytes, TfLiteTensor* tensor) {
-  return TfLiteTensorResizeMaybeCopy(num_bytes, tensor, true);
+TfLiteStatus TfliteMicroTensorRealloc(size_t num_bytes, TfLiteTensor* tensor) {
+  return TfliteMicroTensorResizeMaybeCopy(num_bytes, tensor, true);
 }
 #endif  // TF_LITE_STATIC_MEMORY
 
-const char* TfLiteTypeGetName(TfLiteType type) {
+const char* TfLiteMicroTypeGetName(TfLiteType type) {
   switch (type) {
     case kTfLiteNoType:
       return "NOTYPE";
@@ -382,10 +382,10 @@ const char* TfLiteTypeGetName(TfLiteType type) {
   return "Unknown type";
 }
 
-TfLiteDelegate TfLiteDelegateCreate() { return TfLiteDelegate{}; }
+TfLiteDelegate TfLiteMicroDelegateCreate() { return TfLiteDelegate{}; }
 
 #ifndef TF_LITE_STATIC_MEMORY
-TfLiteOpaqueDelegate* TfLiteOpaqueDelegateCreate(
+TfLiteOpaqueDelegate* TfliteMicroOpaqueDelegateCreate(
     const TfLiteOpaqueDelegateBuilder* opaque_delegate_builder) {
   if (!opaque_delegate_builder) return nullptr;
 
@@ -396,7 +396,7 @@ TfLiteOpaqueDelegate* TfLiteOpaqueDelegateCreate(
   return reinterpret_cast<TfLiteOpaqueDelegate*>(result);
 }
 
-void TfLiteOpaqueDelegateDelete(TfLiteOpaqueDelegate* opaque_delegate) {
+void TfliteMicroOpaqueDelegateDelete(TfLiteOpaqueDelegate* opaque_delegate) {
   if (!opaque_delegate) return;
 
   const TfLiteDelegate* tflite_delegate =
@@ -406,7 +406,7 @@ void TfLiteOpaqueDelegateDelete(TfLiteOpaqueDelegate* opaque_delegate) {
 }
 #endif  // TF_LITE_STATIC_MEMORY
 
-void* TfLiteOpaqueDelegateGetData(const TfLiteOpaqueDelegate* delegate) {
+void* TfliteMicroOpaqueDelegateGetData(const TfLiteOpaqueDelegate* delegate) {
   if (!delegate) return nullptr;
 
   // The following cast is safe only because this code is part of the
@@ -421,7 +421,7 @@ void* TfLiteOpaqueDelegateGetData(const TfLiteOpaqueDelegate* delegate) {
 }
 
 // Returns a tensor data allocation strategy.
-TfLiteAllocationStrategy TfLiteTensorGetAllocationStrategy(
+TfLiteAllocationStrategy TfliteMicroTensorGetAllocationStrategy(
     const TfLiteTensor* const t) {
   switch (t->allocation_type) {
     case kTfLiteMemNone:
@@ -445,7 +445,7 @@ TfLiteAllocationStrategy TfLiteTensorGetAllocationStrategy(
 }
 
 // Returns how stable a tensor data buffer address is across runs.
-TfLiteRunStability TfLiteTensorGetBufferAddressStability(
+TfLiteRunStability TfliteMicroTensorGetBufferAddressStability(
     const TfLiteTensor* const t) {
   switch (t->allocation_type) {
     case kTfLiteMemNone:
@@ -469,7 +469,7 @@ TfLiteRunStability TfLiteTensorGetBufferAddressStability(
 }
 
 // Returns how stable a tensor data values are across runs.
-TfLiteRunStability TfLiteTensorGetDataStability(const TfLiteTensor* const t) {
+TfLiteRunStability TfliteMicroTensorGetDataStability(const TfLiteTensor* const t) {
   switch (t->allocation_type) {
     case kTfLiteMemNone:
       return kTfLiteRunStabilityAcrossRuns;
@@ -495,7 +495,7 @@ TfLiteRunStability TfLiteTensorGetDataStability(const TfLiteTensor* const t) {
 //
 // Some operations can precompute their results before the evaluation step. This
 // makes the data available earlier for subsequent operations.
-TfLiteRunStep TfLiteTensorGetDataKnownStep(const TfLiteTensor* t) {
+TfLiteRunStep TfliteMicroTensorGetDataKnownStep(const TfLiteTensor* t) {
   switch (t->allocation_type) {
     case kTfLiteMemNone:
       return kTfLiteRunStepInit;
@@ -522,7 +522,7 @@ TfLiteRunStep TfLiteTensorGetDataKnownStep(const TfLiteTensor* t) {
 // Some operations can precompute the shape of their results before the
 // evaluation step. This makes the shape available earlier for subsequent
 // operations.
-TfLiteRunStep TfLiteTensorGetShapeKnownStep(const TfLiteTensor* t) {
+TfLiteRunStep TfliteMicroTensorGetShapeKnownStep(const TfLiteTensor* t) {
   switch (t->allocation_type) {
     case kTfLiteMemNone:
       return kTfLiteRunStepInit;
diff --git a/tensorflow/lite/core/c/common.h b/tensorflow/lite/core/c/common.h
index d9abf9ea..d5e8d40f 100644
--- a/tensorflow/lite/core/c/common.h
+++ b/tensorflow/lite/core/c/common.h
@@ -472,7 +472,7 @@ typedef struct TfLiteTensor {
   // bytes = sizeof(float) * 3 * 2 = 4 * 3 * 2 = 24.
   size_t bytes;
 
-  // An opaque pointer to a tflite::MMapAllocation
+  // An opaque pointer to a tflite_micro::MMapAllocation
   const void* allocation;
 
   // Null-terminated name of this tensor.
@@ -570,13 +570,6 @@ typedef struct TfLiteNode {
 // - name
 // - sparsity
 typedef struct TfLiteTensor {
-  // TODO(b/155784997): Consider consolidating these quantization fields:
-  // Quantization information. Replaces params field above.
-  TfLiteQuantization quantization;
-
-  // Quantization information.
-  TfLiteQuantizationParams params;
-
   // A union of data pointers. The appropriate type should be used for a typed
   // tensor based on `type`.
   TfLitePtrUnion data;
@@ -586,16 +579,23 @@ typedef struct TfLiteTensor {
   // and the element datatype size should be equal to `bytes` below.
   TfLiteIntArray* dims;
 
+  // The data type specification for data stored in `data`. This affects
+  // what member of `data` union should be used.
+  TfLiteType type;
+
+  // TODO(b/155784997): Consider consolidating these quantization fields:
+  // Quantization information. Replaces params field above.
+  TfLiteQuantization quantization;
+
+  // Quantization information.
+  TfLiteQuantizationParams params;
+
   // The number of bytes required to store the data of this Tensor. I.e.
   // (bytes of each element) * dims[0] * ... * dims[n-1].  For example, if
   // type is kTfLiteFloat32 and dims = {3, 2} then
   // bytes = sizeof(float) * 3 * 2 = 4 * 3 * 2 = 24.
   size_t bytes;
 
-  // The data type specification for data stored in `data`. This affects
-  // what member of `data` union should be used.
-  TfLiteType type;
-
   // How memory is mapped
   //  kTfLiteMmapRo: Memory mapped read only.
   //  i.e. weights
@@ -621,10 +621,6 @@ typedef struct TfLiteNode {
   // Outputs to this node expressed as indices into the simulator's tensors.
   TfLiteIntArray* outputs;
 
-  // intermediate tensors to this node expressed as indices into the simulator's
-  // tensors.
-  TfLiteIntArray* intermediates;
-
   // Opaque data provided by the node implementer through `Registration.init`.
   void* user_data;
 
@@ -634,7 +630,6 @@ typedef struct TfLiteNode {
 
   // Custom initial data. This is the opaque data provided in the flatbuffer.
   // WARNING: This is an experimental interface that is subject to change.
-  const void* custom_initial_data;
   int custom_initial_data_size;
 } TfLiteNode;
 #endif  // TF_LITE_STATIC_MEMORY
@@ -1316,8 +1311,8 @@ typedef struct TfLiteOpaqueDelegateBuilder {
 // the objects pointed to by any of the fields within the
 // 'opaque_delegate_builder' must outlive the returned
 // 'TfLiteOpaqueDelegate' and any 'TfLiteInterpreter',
-// 'TfLiteInterpreterOptions', 'tflite::Interpreter', or
-// 'tflite::InterpreterBuilder' that the delegate is added to.  The returned
+// 'TfLiteInterpreterOptions', 'tflite_micro::Interpreter', or
+// 'tflite_micro::InterpreterBuilder' that the delegate is added to.  The returned
 // address should be passed to 'TfLiteOpaqueDelegateDelete' for deletion.  If
 // 'opaque_delegate_builder' is a null pointer, then a null pointer will be
 // returned.
diff --git a/tensorflow/lite/kernels/internal/common.cc b/tensorflow/lite/kernels/internal/common.cc
index 1654ab84..8cbbf0c7 100644
--- a/tensorflow/lite/kernels/internal/common.cc
+++ b/tensorflow/lite/kernels/internal/common.cc
@@ -15,7 +15,7 @@ limitations under the License.
 
 #include "tensorflow/lite/kernels/internal/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 int32_t MultiplyByQuantizedMultiplier(int32_t x, int32_t quantized_multiplier,
                                       int shift) {
@@ -52,4 +52,4 @@ int32_t MultiplyByQuantizedMultiplier(int64_t x, int32_t quantized_multiplier,
   return result;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/kernels/internal/common.h b/tensorflow/lite/kernels/internal/common.h
index 14d85991..747ae42d 100644
--- a/tensorflow/lite/kernels/internal/common.h
+++ b/tensorflow/lite/kernels/internal/common.h
@@ -35,7 +35,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/optimized/neon_check.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 constexpr int kReverseShift = -1;
 
@@ -1353,6 +1353,6 @@ void optimized_ops_prefetch_write_l1_keep(const T* ptr) {
 #endif
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_COMMON_H_
diff --git a/tensorflow/lite/kernels/internal/cppmath.h b/tensorflow/lite/kernels/internal/cppmath.h
index 67ab4610..ee891753 100644
--- a/tensorflow/lite/kernels/internal/cppmath.h
+++ b/tensorflow/lite/kernels/internal/cppmath.h
@@ -17,7 +17,7 @@ limitations under the License.
 
 #include <cmath>
 
-namespace tflite {
+namespace tflite_micro {
 
 #if defined(TF_LITE_USE_GLOBAL_CMATH_FUNCTIONS) || \
     (defined(__ANDROID__) && !defined(__NDK_MAJOR__)) || defined(__ZEPHYR__)
@@ -35,6 +35,6 @@ namespace tflite {
 DECLARE_STD_GLOBAL_SWITCH1(TfLiteRound, round)
 DECLARE_STD_GLOBAL_SWITCH1(TfLiteExpm1, expm1)
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_CPPMATH_H_
diff --git a/tensorflow/lite/kernels/internal/max.h b/tensorflow/lite/kernels/internal/max.h
index c1810027..b8d8fc60 100644
--- a/tensorflow/lite/kernels/internal/max.h
+++ b/tensorflow/lite/kernels/internal/max.h
@@ -17,7 +17,7 @@ limitations under the License.
 
 #include <cmath>
 
-namespace tflite {
+namespace tflite_micro {
 
 #if defined(TF_LITE_USE_GLOBAL_MAX) || defined(__ZEPHYR__)
 inline float TfLiteMax(const float& x, const float& y) {
@@ -30,6 +30,6 @@ inline T TfLiteMax(const T& x, const T& y) {
 }
 #endif
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_MAX_H_
diff --git a/tensorflow/lite/kernels/internal/min.h b/tensorflow/lite/kernels/internal/min.h
index 62035dcc..76353545 100644
--- a/tensorflow/lite/kernels/internal/min.h
+++ b/tensorflow/lite/kernels/internal/min.h
@@ -17,7 +17,7 @@ limitations under the License.
 
 #include <cmath>
 
-namespace tflite {
+namespace tflite_micro {
 
 #if defined(TF_LITE_USE_GLOBAL_MIN) || defined(__ZEPHYR__)
 inline float TfLiteMin(const float& x, const float& y) {
@@ -30,6 +30,6 @@ inline T TfLiteMin(const T& x, const T& y) {
 }
 #endif
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_MIN_H_
diff --git a/tensorflow/lite/kernels/internal/portable_tensor.h b/tensorflow/lite/kernels/internal/portable_tensor.h
index a9f9551a..12fdc732 100644
--- a/tensorflow/lite/kernels/internal/portable_tensor.h
+++ b/tensorflow/lite/kernels/internal/portable_tensor.h
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/tensor_ctypes.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // A list of tensors in a format that can be used by kernels like split and
 // concatenation.
@@ -136,6 +136,6 @@ class SequentialTensorWriter {
   T* output_ptr_;
 };
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_PORTABLE_TENSOR_H_
diff --git a/tensorflow/lite/kernels/internal/portable_tensor_utils.cc b/tensorflow/lite/kernels/internal/portable_tensor_utils.cc
index 024043d7..b1aa1fe4 100644
--- a/tensorflow/lite/kernels/internal/portable_tensor_utils.cc
+++ b/tensorflow/lite/kernels/internal/portable_tensor_utils.cc
@@ -27,7 +27,7 @@ limitations under the License.
 #define __restrict__ __restrict
 #endif
 
-namespace tflite {
+namespace tflite_micro {
 
 // Not all backends support CpuBackendContext usage, so forward declare to avoid
 // pulling in its implementation. Use of CpuBackendContext in method
@@ -87,6 +87,6 @@ void UnpackDenseInt4IntoInt8(const int8_t* src_buffer, int num_elements,
 }
 
 }  // namespace tensor_utils
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_TENSOR_UTILS_H_
diff --git a/tensorflow/lite/kernels/internal/portable_tensor_utils.h b/tensorflow/lite/kernels/internal/portable_tensor_utils.h
index c28892c1..4a27c146 100644
--- a/tensorflow/lite/kernels/internal/portable_tensor_utils.h
+++ b/tensorflow/lite/kernels/internal/portable_tensor_utils.h
@@ -27,7 +27,7 @@ limitations under the License.
 #define __restrict__ __restrict
 #endif
 
-namespace tflite {
+namespace tflite_micro {
 
 // Not all backends support CpuBackendContext usage, so forward declare to avoid
 // pulling in its implementation. Use of CpuBackendContext in method
@@ -618,6 +618,6 @@ void UnpackDenseInt4IntoInt8(const int8_t* src_buffer, int num_elements,
 
 }  // namespace tensor_utils
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_PORTABLE_TENSOR_UTILS_H_
diff --git a/tensorflow/lite/kernels/internal/quantization_util.cc b/tensorflow/lite/kernels/internal/quantization_util.cc
index 62045d67..7ed63b7b 100644
--- a/tensorflow/lite/kernels/internal/quantization_util.cc
+++ b/tensorflow/lite/kernels/internal/quantization_util.cc
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/compatibility.h"
 #include "tensorflow/lite/kernels/internal/cppmath.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 // These constants are used to manipulate the binary representation of doubles.
@@ -314,8 +314,13 @@ void PreprocessSoftmaxScaling(double beta, double input_scale,
                        max_real_multiplier);
 #endif  // TFLITE_EMULATE_FLOAT
 
-  QuantizeMultiplierGreaterThanOne(input_beta_real_multiplier,
-                                   quantized_multiplier, left_shift);
+  if(input_beta_real_multiplier > 1.) {
+    QuantizeMultiplierGreaterThanOne(input_beta_real_multiplier,
+                                    quantized_multiplier, left_shift);
+  } else {
+    QuantizeMultiplierSmallerThanOneExp(input_beta_real_multiplier,
+                                    quantized_multiplier, left_shift);
+  }
 }
 
 void PreprocessLogSoftmaxScalingExp(double beta, double input_scale,
@@ -330,7 +335,7 @@ void PreprocessLogSoftmaxScalingExp(double beta, double input_scale,
   // Also calculate what amounts to the inverse scaling factor for the input.
   const double real_reverse_scaling_divisor =
       (1 << (31 - *left_shift)) / static_cast<double>(*quantized_multiplier);
-  tflite::QuantizeMultiplierSmallerThanOneExp(real_reverse_scaling_divisor,
+  tflite_micro::QuantizeMultiplierSmallerThanOneExp(real_reverse_scaling_divisor,
                                               reverse_scaling_divisor,
                                               reverse_scaling_left_shift);
 }
@@ -413,4 +418,4 @@ void QuantizeMultiplierArray(const double* effective_scales, size_t size,
   }
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/kernels/internal/quantization_util.h b/tensorflow/lite/kernels/internal/quantization_util.h
index 0ee914b0..902efbc0 100644
--- a/tensorflow/lite/kernels/internal/quantization_util.h
+++ b/tensorflow/lite/kernels/internal/quantization_util.h
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/cppmath.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // Given the min and max values of a float array, return
 // reasonable quantization parameters to use for this array.
@@ -287,6 +287,6 @@ void QuantizeMultiplierArray(const double* effective_scales, size_t size,
                              int32_t* effective_scale_significand,
                              int* effective_shift);
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_QUANTIZATION_UTIL_H_
diff --git a/tensorflow/lite/kernels/internal/reference/add.h b/tensorflow/lite/kernels/internal/reference/add.h
index 5b520bd1..63a6c47b 100644
--- a/tensorflow/lite/kernels/internal/reference/add.h
+++ b/tensorflow/lite/kernels/internal/reference/add.h
@@ -24,7 +24,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/common.h"
 #include "tensorflow/lite/kernels/internal/compatibility.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace reference_ops {
 
@@ -479,7 +479,7 @@ inline void BroadcastAddFivefold(const ArithmeticParams& unswitched_params,
 
   const bool use_unswitched =
       unswitched_params.broadcast_category ==
-      tflite::BroadcastableOpCategory::kFirstInputBroadcastsFast;
+      tflite_micro::BroadcastableOpCategory::kFirstInputBroadcastsFast;
 
   const ArithmeticParams& params =
       use_unswitched ? unswitched_params : switched_params;
@@ -556,6 +556,6 @@ inline void BroadcastAddFivefold(const ArithmeticParams& unswitched_params,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_ADD_H_
diff --git a/tensorflow/lite/kernels/internal/reference/add_n.h b/tensorflow/lite/kernels/internal/reference/add_n.h
index b6b5882d..da40d766 100644
--- a/tensorflow/lite/kernels/internal/reference/add_n.h
+++ b/tensorflow/lite/kernels/internal/reference/add_n.h
@@ -20,7 +20,7 @@ limitations under the License.
 
 #include "tensorflow/lite/kernels/internal/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_ops {
 
 // T is expected to be either float or int.
@@ -81,6 +81,6 @@ inline void AddN(const ArithmeticParams& params,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_ADD_N_H_
diff --git a/tensorflow/lite/kernels/internal/reference/arg_min_max.h b/tensorflow/lite/kernels/internal/reference/arg_min_max.h
index 8154fbf7..beee76b4 100644
--- a/tensorflow/lite/kernels/internal/reference/arg_min_max.h
+++ b/tensorflow/lite/kernels/internal/reference/arg_min_max.h
@@ -19,7 +19,7 @@ limitations under the License.
 
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace reference_ops {
 
@@ -83,6 +83,6 @@ void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_ARG_MIN_MAX_H_
diff --git a/tensorflow/lite/kernels/internal/reference/batch_matmul.h b/tensorflow/lite/kernels/internal/reference/batch_matmul.h
index 767ad6ab..9b19f60c 100644
--- a/tensorflow/lite/kernels/internal/reference/batch_matmul.h
+++ b/tensorflow/lite/kernels/internal/reference/batch_matmul.h
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/portable_tensor_utils.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_ops {
 namespace batch_matmul {
 
@@ -270,6 +270,6 @@ inline void BatchMatMul(const FullyConnectedParams& params,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_BATCH_MATMUL_H_
diff --git a/tensorflow/lite/kernels/internal/reference/batch_to_space_nd.h b/tensorflow/lite/kernels/internal/reference/batch_to_space_nd.h
index cda46a26..0788e971 100644
--- a/tensorflow/lite/kernels/internal/reference/batch_to_space_nd.h
+++ b/tensorflow/lite/kernels/internal/reference/batch_to_space_nd.h
@@ -20,7 +20,7 @@ limitations under the License.
 #include "ruy/profiler/instrumentation.h"  // from @ruy
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_ops {
 
 // TODO(b/135760455): Move this method anonymous namespace in a cc file.
@@ -96,6 +96,6 @@ inline void BatchToSpaceND(const RuntimeShape& unextended_input1_shape,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_BATCH_TO_SPACE_ND_H_
diff --git a/tensorflow/lite/kernels/internal/reference/binary_function.h b/tensorflow/lite/kernels/internal/reference/binary_function.h
index 0b124af8..c66d39b8 100644
--- a/tensorflow/lite/kernels/internal/reference/binary_function.h
+++ b/tensorflow/lite/kernels/internal/reference/binary_function.h
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/compatibility.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace reference_ops {
 
@@ -86,6 +86,6 @@ inline void BinaryFunction(const RuntimeShape& input1_shape,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_BINARY_FUNCTION_H_
diff --git a/tensorflow/lite/kernels/internal/reference/broadcast_args.h b/tensorflow/lite/kernels/internal/reference/broadcast_args.h
index d93c316d..532aaf59 100644
--- a/tensorflow/lite/kernels/internal/reference/broadcast_args.h
+++ b/tensorflow/lite/kernels/internal/reference/broadcast_args.h
@@ -18,7 +18,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/compatibility.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_ops {
 
 template <typename T>
@@ -51,6 +51,6 @@ void BroadcastArgs(const RuntimeShape& input1_shape, const T* input1_data,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_BROADCAST_ARGS_H_
diff --git a/tensorflow/lite/kernels/internal/reference/broadcast_to.h b/tensorflow/lite/kernels/internal/reference/broadcast_to.h
index f106b2b5..60a4549e 100644
--- a/tensorflow/lite/kernels/internal/reference/broadcast_to.h
+++ b/tensorflow/lite/kernels/internal/reference/broadcast_to.h
@@ -18,7 +18,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/common.h"
 #include "tensorflow/lite/kernels/kernel_util.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_ops {
 template <int N>
 void BroadcastImpl(const NdArrayDesc<N>& input_desc, const char* input_data,
@@ -93,5 +93,5 @@ inline void BroadcastTo(const RuntimeShape& unextended_input_shape,
                    last_broadcast_dim, TfLiteTypeGetSize(data_type));
 }
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_BROADCAST_TO_H_
diff --git a/tensorflow/lite/kernels/internal/reference/ceil.h b/tensorflow/lite/kernels/internal/reference/ceil.h
index 66d1dc35..565f3584 100644
--- a/tensorflow/lite/kernels/internal/reference/ceil.h
+++ b/tensorflow/lite/kernels/internal/reference/ceil.h
@@ -19,7 +19,7 @@ limitations under the License.
 
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace reference_ops {
 
@@ -33,5 +33,5 @@ inline void Ceil(const RuntimeShape& input_shape, const float* input_data,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_CEIL_H_
diff --git a/tensorflow/lite/kernels/internal/reference/comparisons.cc b/tensorflow/lite/kernels/internal/reference/comparisons.cc
index 86b4a6af..d233af43 100644
--- a/tensorflow/lite/kernels/internal/reference/comparisons.cc
+++ b/tensorflow/lite/kernels/internal/reference/comparisons.cc
@@ -15,7 +15,7 @@ limitations under the License.
 
 #include "tensorflow/lite/kernels/internal/reference/comparisons.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_ops {
 
 BroadcastComparison4DSlowCommon BroadcastComparison4DSlowPreprocess(
@@ -34,4 +34,4 @@ BroadcastComparison4DSlowCommon BroadcastComparison4DSlowPreprocess(
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/kernels/internal/reference/comparisons.h b/tensorflow/lite/kernels/internal/reference/comparisons.h
index 366b378c..72b53bda 100644
--- a/tensorflow/lite/kernels/internal/reference/comparisons.h
+++ b/tensorflow/lite/kernels/internal/reference/comparisons.h
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/common.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace reference_ops {
 
@@ -266,6 +266,6 @@ TFLITE_COMPARISON_OP(LessEqual)
 #undef TFLITE_COMPARISON_OP
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_COMPARISONS_H_
diff --git a/tensorflow/lite/kernels/internal/reference/concatenation.h b/tensorflow/lite/kernels/internal/reference/concatenation.h
index 9d2ecbec..8f062f86 100644
--- a/tensorflow/lite/kernels/internal/reference/concatenation.h
+++ b/tensorflow/lite/kernels/internal/reference/concatenation.h
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/cppmath.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_ops {
 
 template <typename Scalar>
@@ -123,7 +123,7 @@ inline void ConcatenationWithScaling(const ConcatenationParams& params,
         const float scale = input_scale[i] * inverse_output_scale;
         const float bias = -input_zeropoint[i] * scale;
         for (int j = 0; j < copy_size; ++j) {
-          const int32_t value = static_cast<int32_t>(tflite::TfLiteRound(
+          const int32_t value = static_cast<int32_t>(tflite_micro::TfLiteRound(
                                     input_ptr[j] * scale + bias)) +
                                 output_zeropoint;
           output_ptr[j] = static_cast<uint8_t>(
@@ -136,6 +136,6 @@ inline void ConcatenationWithScaling(const ConcatenationParams& params,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_CONCATENATION_H_
diff --git a/tensorflow/lite/kernels/internal/reference/conv.h b/tensorflow/lite/kernels/internal/reference/conv.h
index 3c9f9fc7..38443c3b 100644
--- a/tensorflow/lite/kernels/internal/reference/conv.h
+++ b/tensorflow/lite/kernels/internal/reference/conv.h
@@ -20,7 +20,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/common.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace reference_ops {
 
@@ -284,6 +284,6 @@ inline void HybridConvPerChannel(
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_CONV_H_
diff --git a/tensorflow/lite/kernels/internal/reference/cumsum.h b/tensorflow/lite/kernels/internal/reference/cumsum.h
index 7cbc87c0..880a2bc3 100644
--- a/tensorflow/lite/kernels/internal/reference/cumsum.h
+++ b/tensorflow/lite/kernels/internal/reference/cumsum.h
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/common.h"
 #include "tensorflow/lite/kernels/internal/compatibility.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_ops {
 
 template <typename T>
@@ -170,6 +170,6 @@ inline void CumSum(const ArithmeticParams& params, const int8_t* input_data,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_CUMSUM_H_
diff --git a/tensorflow/lite/kernels/internal/reference/depth_to_space.h b/tensorflow/lite/kernels/internal/reference/depth_to_space.h
index 23cff285..dcc9e5fa 100644
--- a/tensorflow/lite/kernels/internal/reference/depth_to_space.h
+++ b/tensorflow/lite/kernels/internal/reference/depth_to_space.h
@@ -17,11 +17,11 @@ limitations under the License.
 
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_ops {
 
 template <typename T>
-inline void DepthToSpace(const tflite::DepthToSpaceParams& op_params,
+inline void DepthToSpace(const tflite_micro::DepthToSpaceParams& op_params,
                          const RuntimeShape& unextended_input_shape,
                          const T* input_data,
                          const RuntimeShape& unextended_output_shape,
@@ -74,6 +74,6 @@ inline void DepthToSpace(const tflite::DepthToSpaceParams& op_params,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_DEPTH_TO_SPACE_H_
diff --git a/tensorflow/lite/kernels/internal/reference/depthwiseconv_float.h b/tensorflow/lite/kernels/internal/reference/depthwiseconv_float.h
index 0cecb16b..b50e7bec 100644
--- a/tensorflow/lite/kernels/internal/reference/depthwiseconv_float.h
+++ b/tensorflow/lite/kernels/internal/reference/depthwiseconv_float.h
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/compatibility.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_ops {
 
 inline void DepthwiseConv(
diff --git a/tensorflow/lite/kernels/internal/reference/depthwiseconv_uint8.h b/tensorflow/lite/kernels/internal/reference/depthwiseconv_uint8.h
index d4fba139..aef25b7b 100644
--- a/tensorflow/lite/kernels/internal/reference/depthwiseconv_uint8.h
+++ b/tensorflow/lite/kernels/internal/reference/depthwiseconv_uint8.h
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/compatibility.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // Used in tests and template parameters to control which version of depthwise
 // convolution is called. Primarily for reference code, and specializations
diff --git a/tensorflow/lite/kernels/internal/reference/dequantize.h b/tensorflow/lite/kernels/internal/reference/dequantize.h
index b90951f9..6e1a1cab 100644
--- a/tensorflow/lite/kernels/internal/reference/dequantize.h
+++ b/tensorflow/lite/kernels/internal/reference/dequantize.h
@@ -22,13 +22,13 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/common.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace reference_ops {
 
 // Dequantizes into a float without rounding.
 template <typename InputT, typename OutputT>
-inline void Dequantize(const tflite::DequantizationParams& op_params,
+inline void Dequantize(const tflite_micro::DequantizationParams& op_params,
                        const RuntimeShape& input_shape,
                        const InputT* input_data,
                        const RuntimeShape& output_shape, OutputT* output_data) {
@@ -46,7 +46,7 @@ inline void Dequantize(const tflite::DequantizationParams& op_params,
 // Dequantizes per-channel quantized tensor to float.
 template <typename T>
 inline void PerChannelDequantize(
-    const tflite::PerChannelDequantizationParams& op_params,
+    const tflite_micro::PerChannelDequantizationParams& op_params,
     const RuntimeShape& input_shape, const T* input_data,
     const RuntimeShape& output_shape, float* output_data) {
   // Ensure flat size is same.
@@ -74,5 +74,5 @@ inline void PerChannelDequantize(
 
 }  // namespace reference_ops
 
-}  // namespace tflite
+}  // namespace tflite_micro
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_DEQUANTIZE_H_
diff --git a/tensorflow/lite/kernels/internal/reference/div.h b/tensorflow/lite/kernels/internal/reference/div.h
index df8da1b1..330b88b5 100644
--- a/tensorflow/lite/kernels/internal/reference/div.h
+++ b/tensorflow/lite/kernels/internal/reference/div.h
@@ -19,7 +19,7 @@ limitations under the License.
 
 #include "tensorflow/lite/kernels/internal/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace reference_ops {
 
@@ -242,6 +242,6 @@ inline void Div(const ArithmeticParams& params,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_DIV_H_
diff --git a/tensorflow/lite/kernels/internal/reference/elu.h b/tensorflow/lite/kernels/internal/reference/elu.h
index 3dc93589..20ef92ee 100644
--- a/tensorflow/lite/kernels/internal/reference/elu.h
+++ b/tensorflow/lite/kernels/internal/reference/elu.h
@@ -18,7 +18,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/cppmath.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace reference_ops {
 
@@ -32,6 +32,6 @@ inline void Elu(const RuntimeShape& input_shape, const float* input_data,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_ELU_H_
diff --git a/tensorflow/lite/kernels/internal/reference/exp.h b/tensorflow/lite/kernels/internal/reference/exp.h
index 134ee13f..bd1cadc5 100644
--- a/tensorflow/lite/kernels/internal/reference/exp.h
+++ b/tensorflow/lite/kernels/internal/reference/exp.h
@@ -20,7 +20,7 @@ limitations under the License.
 #include "ruy/profiler/instrumentation.h"  // from @ruy
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_ops {
 
 template <typename T>
@@ -33,6 +33,6 @@ inline void Exp(const T* input_data, const size_t num_elements,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_EXP_H_
diff --git a/tensorflow/lite/kernels/internal/reference/fill.h b/tensorflow/lite/kernels/internal/reference/fill.h
index 16630e61..aa412959 100644
--- a/tensorflow/lite/kernels/internal/reference/fill.h
+++ b/tensorflow/lite/kernels/internal/reference/fill.h
@@ -19,7 +19,7 @@ limitations under the License.
 
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_ops {
 
 template <typename T>
@@ -33,6 +33,6 @@ void Fill(const RuntimeShape& value_shape, const T* value_data,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_FILL_H_
diff --git a/tensorflow/lite/kernels/internal/reference/floor.h b/tensorflow/lite/kernels/internal/reference/floor.h
index 0693fd42..28fe190e 100644
--- a/tensorflow/lite/kernels/internal/reference/floor.h
+++ b/tensorflow/lite/kernels/internal/reference/floor.h
@@ -19,7 +19,7 @@ limitations under the License.
 
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace reference_ops {
 
@@ -34,6 +34,6 @@ inline void Floor(const RuntimeShape& input_shape, const float* input_data,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_FLOOR_H_
diff --git a/tensorflow/lite/kernels/internal/reference/floor_div.h b/tensorflow/lite/kernels/internal/reference/floor_div.h
index e75d473c..3b56e06a 100644
--- a/tensorflow/lite/kernels/internal/reference/floor_div.h
+++ b/tensorflow/lite/kernels/internal/reference/floor_div.h
@@ -20,7 +20,7 @@ limitations under the License.
 
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_ops {
 
 template <typename T>
@@ -30,6 +30,6 @@ T FloorDiv(T input1, T input2) {
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_FLOOR_DIV_H_
diff --git a/tensorflow/lite/kernels/internal/reference/floor_mod.h b/tensorflow/lite/kernels/internal/reference/floor_mod.h
index 20ce18b7..9a4c7139 100644
--- a/tensorflow/lite/kernels/internal/reference/floor_mod.h
+++ b/tensorflow/lite/kernels/internal/reference/floor_mod.h
@@ -18,7 +18,7 @@ limitations under the License.
 #include <cmath>
 #include <functional>
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace reference_ops {
 
@@ -39,6 +39,6 @@ T FloorMod(T input1, T input2) {
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_FLOOR_MOD_H_
diff --git a/tensorflow/lite/kernels/internal/reference/fully_connected.h b/tensorflow/lite/kernels/internal/reference/fully_connected.h
index ba51cbcf..9ede8d9b 100644
--- a/tensorflow/lite/kernels/internal/reference/fully_connected.h
+++ b/tensorflow/lite/kernels/internal/reference/fully_connected.h
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/quantization_util.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_ops {
 
 inline void FullyConnected(
@@ -318,6 +318,6 @@ inline void ShuffledFullyConnected(
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_FULLY_CONNECTED_H_
diff --git a/tensorflow/lite/kernels/internal/reference/hard_swish.h b/tensorflow/lite/kernels/internal/reference/hard_swish.h
index 81fcd63e..28867479 100644
--- a/tensorflow/lite/kernels/internal/reference/hard_swish.h
+++ b/tensorflow/lite/kernels/internal/reference/hard_swish.h
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/common.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_ops {
 
 inline int16_t SaturatingLeftShift(int16_t value, int amount) {
@@ -163,6 +163,6 @@ inline void HardSwish(const HardSwishParams& params,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_HARD_SWISH_H_
diff --git a/tensorflow/lite/kernels/internal/reference/integer_ops/add.h b/tensorflow/lite/kernels/internal/reference/integer_ops/add.h
index c2a0e0f0..113d7941 100644
--- a/tensorflow/lite/kernels/internal/reference/integer_ops/add.h
+++ b/tensorflow/lite/kernels/internal/reference/integer_ops/add.h
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/common.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_integer_ops {
 
 inline void CheckArithmeticParams(const ArithmeticParams& params) {
@@ -245,6 +245,6 @@ inline void BroadcastAdd4DSlow(const ArithmeticParams& params,
 }
 
 }  // namespace reference_integer_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_INTEGER_OPS_ADD_H_
diff --git a/tensorflow/lite/kernels/internal/reference/integer_ops/conv.h b/tensorflow/lite/kernels/internal/reference/integer_ops/conv.h
index eac00576..1bce235d 100644
--- a/tensorflow/lite/kernels/internal/reference/integer_ops/conv.h
+++ b/tensorflow/lite/kernels/internal/reference/integer_ops/conv.h
@@ -19,7 +19,7 @@ limitations under the License.
 
 #include "tensorflow/lite/kernels/internal/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_integer_ops {
 
 // Fixed-point per-channel-quantization convolution reference kernel.
@@ -236,6 +236,6 @@ inline void ConvPerChannel(
 }
 
 }  // namespace reference_integer_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_INTEGER_OPS_CONV_H_
diff --git a/tensorflow/lite/kernels/internal/reference/integer_ops/depthwise_conv.h b/tensorflow/lite/kernels/internal/reference/integer_ops/depthwise_conv.h
index 7676fce0..b0afaeef 100644
--- a/tensorflow/lite/kernels/internal/reference/integer_ops/depthwise_conv.h
+++ b/tensorflow/lite/kernels/internal/reference/integer_ops/depthwise_conv.h
@@ -19,7 +19,7 @@ limitations under the License.
 
 #include "tensorflow/lite/kernels/internal/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_integer_ops {
 inline void DepthwiseConvPerChannel(
     const DepthwiseParams& params, const int32_t* output_multiplier,
@@ -286,6 +286,6 @@ inline void DepthwiseConvHybridPerChannel(
 }
 
 }  // namespace reference_integer_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_INTEGER_OPS_DEPTHWISE_CONV_H_
diff --git a/tensorflow/lite/kernels/internal/reference/integer_ops/fully_connected.h b/tensorflow/lite/kernels/internal/reference/integer_ops/fully_connected.h
index 3a74402e..0421d531 100644
--- a/tensorflow/lite/kernels/internal/reference/integer_ops/fully_connected.h
+++ b/tensorflow/lite/kernels/internal/reference/integer_ops/fully_connected.h
@@ -19,7 +19,7 @@ limitations under the License.
 
 #include "tensorflow/lite/kernels/internal/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_integer_ops {
 
 // For per-channel functions, since it is defined in quantization spec that
@@ -121,6 +121,6 @@ void FullyConnected(const FullyConnectedParams& params,
 }
 
 }  // namespace reference_integer_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_INTEGER_OPS_FULLY_CONNECTED_H_
diff --git a/tensorflow/lite/kernels/internal/reference/integer_ops/l2normalization.h b/tensorflow/lite/kernels/internal/reference/integer_ops/l2normalization.h
index 164a8367..adf2aacc 100644
--- a/tensorflow/lite/kernels/internal/reference/integer_ops/l2normalization.h
+++ b/tensorflow/lite/kernels/internal/reference/integer_ops/l2normalization.h
@@ -19,7 +19,7 @@ limitations under the License.
 
 #include "tensorflow/lite/kernels/internal/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_integer_ops {
 
 inline void L2Normalization(int32_t input_zero_point, int32_t outer_size,
@@ -62,6 +62,6 @@ inline void L2Normalization(int32_t input_zero_point, int32_t outer_size,
   }
 }
 }  // namespace reference_integer_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_INTEGER_OPS_L2NORMALIZATION_H_
diff --git a/tensorflow/lite/kernels/internal/reference/integer_ops/logistic.h b/tensorflow/lite/kernels/internal/reference/integer_ops/logistic.h
index 16eff133..be5fb90e 100644
--- a/tensorflow/lite/kernels/internal/reference/integer_ops/logistic.h
+++ b/tensorflow/lite/kernels/internal/reference/integer_ops/logistic.h
@@ -20,7 +20,7 @@ limitations under the License.
 
 #include "tensorflow/lite/kernels/internal/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_integer_ops {
 
 inline void Logistic(int32_t input_zero_point, int32_t input_range_radius,
@@ -116,6 +116,6 @@ inline void Logistic(int32_t input_multiplier, int32_t input_left_shift,
 }
 
 }  // namespace reference_integer_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_INTEGER_OPS_LOGISTIC_H_
diff --git a/tensorflow/lite/kernels/internal/reference/integer_ops/mul.h b/tensorflow/lite/kernels/internal/reference/integer_ops/mul.h
index a57056d5..40516f58 100644
--- a/tensorflow/lite/kernels/internal/reference/integer_ops/mul.h
+++ b/tensorflow/lite/kernels/internal/reference/integer_ops/mul.h
@@ -21,7 +21,7 @@ limitations under the License.
 #include "ruy/profiler/instrumentation.h"  // from @ruy
 #include "tensorflow/lite/kernels/internal/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_integer_ops {
 
 // Maximum dimension supported by the broadcast mul operation.
@@ -190,5 +190,5 @@ inline void BroadcastMul4DSlow(
 }
 
 }  // namespace reference_integer_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_INTEGER_OPS_MUL_H_
diff --git a/tensorflow/lite/kernels/internal/reference/integer_ops/pooling.h b/tensorflow/lite/kernels/internal/reference/integer_ops/pooling.h
index 4dc31d9e..b1d58176 100644
--- a/tensorflow/lite/kernels/internal/reference/integer_ops/pooling.h
+++ b/tensorflow/lite/kernels/internal/reference/integer_ops/pooling.h
@@ -20,7 +20,7 @@ limitations under the License.
 
 #include "tensorflow/lite/kernels/internal/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_integer_ops {
 
 inline bool AveragePool(const PoolParams& params,
@@ -259,6 +259,6 @@ inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,
 }
 
 }  // namespace reference_integer_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_INTEGER_OPS_POOLING_H_
diff --git a/tensorflow/lite/kernels/internal/reference/integer_ops/tanh.h b/tensorflow/lite/kernels/internal/reference/integer_ops/tanh.h
index 7b1e003b..25d81fa1 100644
--- a/tensorflow/lite/kernels/internal/reference/integer_ops/tanh.h
+++ b/tensorflow/lite/kernels/internal/reference/integer_ops/tanh.h
@@ -21,7 +21,7 @@ limitations under the License.
 #include "fixedpoint/fixedpoint.h"
 #include "tensorflow/lite/kernels/internal/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_integer_ops {
 
 inline void Tanh(int32_t input_zero_point, int32_t input_range_radius,
@@ -112,6 +112,6 @@ inline void Tanh(int32_t input_multiplier, int32_t input_left_shift,
 }
 
 }  // namespace reference_integer_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_INTEGER_OPS_TANH_H_
diff --git a/tensorflow/lite/kernels/internal/reference/integer_ops/transpose_conv.h b/tensorflow/lite/kernels/internal/reference/integer_ops/transpose_conv.h
index 40f99cee..7434b2a7 100644
--- a/tensorflow/lite/kernels/internal/reference/integer_ops/transpose_conv.h
+++ b/tensorflow/lite/kernels/internal/reference/integer_ops/transpose_conv.h
@@ -19,7 +19,7 @@ limitations under the License.
 
 #include "tensorflow/lite/kernels/internal/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_integer_ops {
 
 // Fixed-point per-channel-quantization transpose convolution reference kernel.
@@ -219,6 +219,6 @@ inline void TransposeConv(
 }
 
 }  // namespace reference_integer_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_INTEGER_OPS_TRANSPOSE_CONV_H_
diff --git a/tensorflow/lite/kernels/internal/reference/l2normalization.h b/tensorflow/lite/kernels/internal/reference/l2normalization.h
index e5c91bf5..2744366a 100644
--- a/tensorflow/lite/kernels/internal/reference/l2normalization.h
+++ b/tensorflow/lite/kernels/internal/reference/l2normalization.h
@@ -22,11 +22,11 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/common.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace reference_ops {
 
-inline void L2Normalization(const tflite::L2NormalizationParams& op_params,
+inline void L2Normalization(const tflite_micro::L2NormalizationParams& op_params,
                             const RuntimeShape& input_shape,
                             const float* input_data,
                             const RuntimeShape& output_shape,
@@ -50,7 +50,7 @@ inline void L2Normalization(const tflite::L2NormalizationParams& op_params,
   }
 }
 
-inline void L2Normalization(const tflite::L2NormalizationParams& op_params,
+inline void L2Normalization(const tflite_micro::L2NormalizationParams& op_params,
                             const RuntimeShape& input_shape,
                             const uint8_t* input_data,
                             const RuntimeShape& output_shape,
@@ -86,5 +86,5 @@ inline void L2Normalization(const tflite::L2NormalizationParams& op_params,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_L2NORMALIZATION_H_
diff --git a/tensorflow/lite/kernels/internal/reference/leaky_relu.h b/tensorflow/lite/kernels/internal/reference/leaky_relu.h
index 06f691ab..06e3c155 100644
--- a/tensorflow/lite/kernels/internal/reference/leaky_relu.h
+++ b/tensorflow/lite/kernels/internal/reference/leaky_relu.h
@@ -20,10 +20,10 @@ limitations under the License.
 
 #include "tensorflow/lite/kernels/internal/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_ops {
 
-inline void LeakyRelu(const tflite::LeakyReluParams& params,
+inline void LeakyRelu(const tflite_micro::LeakyReluParams& params,
                       const RuntimeShape& input_shape, const float* input_data,
                       const RuntimeShape& output_shape, float* output_data) {
   const int flat_size = MatchingFlatSize(input_shape, output_shape);
@@ -64,6 +64,6 @@ inline void QuantizeLeakyRelu(const LeakyReluParams& params,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_LEAKY_RELU_H_
diff --git a/tensorflow/lite/kernels/internal/reference/log_softmax.h b/tensorflow/lite/kernels/internal/reference/log_softmax.h
index 394dd3a9..c99f3a20 100644
--- a/tensorflow/lite/kernels/internal/reference/log_softmax.h
+++ b/tensorflow/lite/kernels/internal/reference/log_softmax.h
@@ -22,7 +22,7 @@ limitations under the License.
 #include "fixedpoint/fixedpoint.h"
 #include "tensorflow/lite/kernels/internal/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_ops {
 
 inline void LogSoftmax(const SoftmaxParams& params,
@@ -251,6 +251,6 @@ inline void LogSoftmax(const SoftmaxParams& params, const size_t outer_size,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_LOG_SOFTMAX_H_
diff --git a/tensorflow/lite/kernels/internal/reference/logistic.h b/tensorflow/lite/kernels/internal/reference/logistic.h
index 64b7133b..6b2dcf0e 100644
--- a/tensorflow/lite/kernels/internal/reference/logistic.h
+++ b/tensorflow/lite/kernels/internal/reference/logistic.h
@@ -24,7 +24,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/types.h"
 #include "tensorflow/lite/kernels/op_macros.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_ops {
 
 inline void Logistic(const RuntimeShape& input_shape, const float* input_data,
@@ -127,6 +127,6 @@ inline void Logistic(const RuntimeShape& input_shape, const int8_t* input_data,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_LOGISTIC_H_
diff --git a/tensorflow/lite/kernels/internal/reference/lstm_cell.h b/tensorflow/lite/kernels/internal/reference/lstm_cell.h
index 17b113eb..b580eb2d 100644
--- a/tensorflow/lite/kernels/internal/reference/lstm_cell.h
+++ b/tensorflow/lite/kernels/internal/reference/lstm_cell.h
@@ -24,7 +24,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/reference/fully_connected.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_ops {
 
 inline void LstmCell(
@@ -94,14 +94,14 @@ inline void LstmCell(
   float const* concat_input_arrays_data[2] = {input_data, prev_activ_data};
   const RuntimeShape* concat_input_arrays_shapes[2] = {&input_shape,
                                                        &prev_activ_shape};
-  tflite::ConcatenationParams concat_params;
+  tflite_micro::ConcatenationParams concat_params;
   concat_params.axis = 3;
   concat_params.inputs_count = 2;
   Concatenation(concat_params, concat_input_arrays_shapes,
                 concat_input_arrays_data, concat_temp_shape, concat_temp_data);
 
   // Fully connected
-  tflite::FullyConnectedParams fc_params;
+  tflite_micro::FullyConnectedParams fc_params;
   fc_params.float_activation_min = std::numeric_limits<float>::lowest();
   fc_params.float_activation_max = std::numeric_limits<float>::max();
   FullyConnected(fc_params, concat_temp_shape, concat_temp_data, weights_shape,
@@ -305,7 +305,7 @@ inline void LstmCell(const LstmCellParams& params,
                                                 prev_activ_data_uint8};
   const RuntimeShape* concat_input_arrays_shapes[2] = {&input_shape,
                                                        &prev_activ_shape};
-  tflite::ConcatenationParams concat_params;
+  tflite_micro::ConcatenationParams concat_params;
   concat_params.axis = 3;
   concat_params.inputs_count = 2;
   Concatenation(concat_params, concat_input_arrays_shapes,
@@ -418,5 +418,5 @@ inline void LstmCell(const LstmCellParams& params,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_LSTM_CELL_H_
diff --git a/tensorflow/lite/kernels/internal/reference/maximum_minimum.h b/tensorflow/lite/kernels/internal/reference/maximum_minimum.h
index cd11b419..52d17105 100644
--- a/tensorflow/lite/kernels/internal/reference/maximum_minimum.h
+++ b/tensorflow/lite/kernels/internal/reference/maximum_minimum.h
@@ -18,7 +18,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/common.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_ops {
 
 template <typename T, typename Op, int N = 5>
@@ -59,6 +59,6 @@ void MaximumMinimumBroadcastSlow(const RuntimeShape& unextended_input1_shape,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_MAXIMUM_MINIMUM_H_
diff --git a/tensorflow/lite/kernels/internal/reference/mul.h b/tensorflow/lite/kernels/internal/reference/mul.h
index fca74a32..a53edf66 100644
--- a/tensorflow/lite/kernels/internal/reference/mul.h
+++ b/tensorflow/lite/kernels/internal/reference/mul.h
@@ -20,7 +20,7 @@ limitations under the License.
 
 #include "tensorflow/lite/kernels/internal/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace reference_ops {
 
@@ -262,6 +262,6 @@ inline void BroadcastMul4DSlow(
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_MUL_H_
diff --git a/tensorflow/lite/kernels/internal/reference/neg.h b/tensorflow/lite/kernels/internal/reference/neg.h
index e127883f..18c291c4 100644
--- a/tensorflow/lite/kernels/internal/reference/neg.h
+++ b/tensorflow/lite/kernels/internal/reference/neg.h
@@ -17,7 +17,7 @@ limitations under the License.
 
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace reference_ops {
 
@@ -32,6 +32,6 @@ inline void Negate(const RuntimeShape& input_shape, const T* input_data,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_NEG_H_
diff --git a/tensorflow/lite/kernels/internal/reference/pad.h b/tensorflow/lite/kernels/internal/reference/pad.h
index 27589445..feb3f3a7 100644
--- a/tensorflow/lite/kernels/internal/reference/pad.h
+++ b/tensorflow/lite/kernels/internal/reference/pad.h
@@ -20,7 +20,7 @@ limitations under the License.
 
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace reference_ops {
 
@@ -35,7 +35,7 @@ constexpr int PadKernelMaxDimensionCount() { return 5; }
 // Note that two typenames are required, so that T=P=int32_t is considered a
 // specialization distinct from P=int32_t.
 template <typename T, typename P>
-inline void PadImpl(const tflite::PadParams& op_params,
+inline void PadImpl(const tflite_micro::PadParams& op_params,
                     const RuntimeShape& input_shape, const T* input_data,
                     const P* pad_value_ptr, const RuntimeShape& output_shape,
                     T* output_data) {
@@ -115,7 +115,7 @@ inline void PadImpl(const tflite::PadParams& op_params,
 }
 
 template <typename T, typename P>
-inline void Pad(const tflite::PadParams& op_params,
+inline void Pad(const tflite_micro::PadParams& op_params,
                 const RuntimeShape& input_shape, const T* input_data,
                 const P* pad_value_ptr, const RuntimeShape& output_shape,
                 T* output_data) {
@@ -125,7 +125,7 @@ inline void Pad(const tflite::PadParams& op_params,
 
 // The second (pad-value) input can be int32_t when, say, the first is uint8_t.
 template <typename T>
-inline void Pad(const tflite::PadParams& op_params,
+inline void Pad(const tflite_micro::PadParams& op_params,
                 const RuntimeShape& input_shape, const T* input_data,
                 const int32_t* pad_value_ptr, const RuntimeShape& output_shape,
                 T* output_data) {
@@ -136,7 +136,7 @@ inline void Pad(const tflite::PadParams& op_params,
 
 // This version avoids conflicting template matching.
 template <>
-inline void Pad(const tflite::PadParams& op_params,
+inline void Pad(const tflite_micro::PadParams& op_params,
                 const RuntimeShape& input_shape, const int32_t* input_data,
                 const int32_t* pad_value_ptr, const RuntimeShape& output_shape,
                 int32_t* output_data) {
@@ -145,7 +145,7 @@ inline void Pad(const tflite::PadParams& op_params,
 }
 
 template <typename T, typename P>
-inline void PadImageStyle(const tflite::PadParams& op_params,
+inline void PadImageStyle(const tflite_micro::PadParams& op_params,
                           const RuntimeShape& input_shape, const T* input_data,
                           const P* pad_value_ptr,
                           const RuntimeShape& output_shape, T* output_data) {
@@ -154,7 +154,7 @@ inline void PadImageStyle(const tflite::PadParams& op_params,
 }
 
 template <typename P>
-inline void PadImageStyle(const tflite::PadParams& op_params,
+inline void PadImageStyle(const tflite_micro::PadParams& op_params,
                           const RuntimeShape& input_shape,
                           const float* input_data, const P* pad_value_ptr,
                           const RuntimeShape& output_shape,
@@ -164,6 +164,6 @@ inline void PadImageStyle(const tflite::PadParams& op_params,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_PAD_H_
diff --git a/tensorflow/lite/kernels/internal/reference/pooling.h b/tensorflow/lite/kernels/internal/reference/pooling.h
index fe17484c..c4294803 100644
--- a/tensorflow/lite/kernels/internal/reference/pooling.h
+++ b/tensorflow/lite/kernels/internal/reference/pooling.h
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/quantization_util.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_ops {
 
 inline bool AveragePool(const PoolParams& params,
@@ -298,6 +298,6 @@ inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,
   }
 }
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_POOLING_H_
diff --git a/tensorflow/lite/kernels/internal/reference/portable_tensor_utils.cc b/tensorflow/lite/kernels/internal/reference/portable_tensor_utils.cc
index d386203e..17b67500 100644
--- a/tensorflow/lite/kernels/internal/reference/portable_tensor_utils.cc
+++ b/tensorflow/lite/kernels/internal/reference/portable_tensor_utils.cc
@@ -29,7 +29,7 @@ limitations under the License.
 #define __restrict__ __restrict
 #endif
 
-namespace tflite {
+namespace tflite_micro {
 namespace tensor_utils {
 
 namespace {
@@ -806,4 +806,4 @@ void PortableTwoGateSaturatingAdd(const int8_t* input, int8_t input_zp,
 }
 
 }  // namespace tensor_utils
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/kernels/internal/reference/portable_tensor_utils.h b/tensorflow/lite/kernels/internal/reference/portable_tensor_utils.h
index 0416db09..edcaf642 100644
--- a/tensorflow/lite/kernels/internal/reference/portable_tensor_utils.h
+++ b/tensorflow/lite/kernels/internal/reference/portable_tensor_utils.h
@@ -21,7 +21,7 @@ limitations under the License.
 #define __restrict__ __restrict
 #endif
 
-namespace tflite {
+namespace tflite_micro {
 namespace tensor_utils {
 
 // Check if all entries of a vector are zero for float.
@@ -328,6 +328,6 @@ void TwoGateSaturatingAdd(const int8_t* input, int8_t input_zp,
 }
 
 }  // namespace tensor_utils
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_PORTABLE_TENSOR_UTILS_H_
diff --git a/tensorflow/lite/kernels/internal/reference/portable_tensor_utils_impl.h b/tensorflow/lite/kernels/internal/reference/portable_tensor_utils_impl.h
index 6c404d5e..88d9c640 100644
--- a/tensorflow/lite/kernels/internal/reference/portable_tensor_utils_impl.h
+++ b/tensorflow/lite/kernels/internal/reference/portable_tensor_utils_impl.h
@@ -22,7 +22,7 @@ limitations under the License.
 #define __restrict__ __restrict
 #endif
 
-namespace tflite {
+namespace tflite_micro {
 
 // Not all backends support CpuBackendContext usage, so forward declare to avoid
 // pulling in its implementation.
@@ -239,6 +239,6 @@ void PortableTwoGateSaturatingAdd(const int8_t* input, int8_t input_zp,
                                   int16_t* output);
 
 }  // namespace tensor_utils
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_PORTABLE_TENSOR_UTILS_IMPL_H_
diff --git a/tensorflow/lite/kernels/internal/reference/prelu.h b/tensorflow/lite/kernels/internal/reference/prelu.h
index aa9901d6..89d4be10 100644
--- a/tensorflow/lite/kernels/internal/reference/prelu.h
+++ b/tensorflow/lite/kernels/internal/reference/prelu.h
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/compatibility.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace reference_ops {
 
@@ -106,6 +106,6 @@ inline void Prelu(const PreluParams& params, const RuntimeShape& input_shape,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_PRELU_H_
diff --git a/tensorflow/lite/kernels/internal/reference/process_broadcast_shapes.h b/tensorflow/lite/kernels/internal/reference/process_broadcast_shapes.h
index bda27693..0cae0eb3 100644
--- a/tensorflow/lite/kernels/internal/reference/process_broadcast_shapes.h
+++ b/tensorflow/lite/kernels/internal/reference/process_broadcast_shapes.h
@@ -19,7 +19,7 @@ limitations under the License.
 
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace reference_ops {
 
@@ -39,7 +39,7 @@ namespace reference_ops {
 // patterns and falling back to generic broadcast.
 inline bool ProcessBroadcastShapes(const RuntimeShape& shape0,
                                    const RuntimeShape& shape1,
-                                   tflite::ArithmeticParams* params) {
+                                   tflite_micro::ArithmeticParams* params) {
   const int dims_count =
       std::max(shape0.DimensionsCount(), shape1.DimensionsCount());
 
@@ -135,6 +135,6 @@ inline bool ProcessBroadcastShapes(const RuntimeShape& shape0,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_PROCESS_BROADCAST_SHAPES_H_
diff --git a/tensorflow/lite/kernels/internal/reference/quantize.h b/tensorflow/lite/kernels/internal/reference/quantize.h
index f304b641..05af1959 100644
--- a/tensorflow/lite/kernels/internal/reference/quantize.h
+++ b/tensorflow/lite/kernels/internal/reference/quantize.h
@@ -24,12 +24,12 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/cppmath.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace reference_ops {
 
 template <typename InputT, typename OutputT>
-inline void AffineQuantize(const tflite::QuantizationParams& op_params,
+inline void AffineQuantize(const tflite_micro::QuantizationParams& op_params,
                            const RuntimeShape& input_shape,
                            const InputT* input_data,
                            const RuntimeShape& output_shape,
@@ -53,7 +53,7 @@ inline void AffineQuantize(const tflite::QuantizationParams& op_params,
 // Quantizes per-channel.
 template <typename InputT, typename OutputT>
 inline void PerChannelQuantize(
-    const tflite::PerChannelQuantizationParams& op_params,
+    const tflite_micro::PerChannelQuantizationParams& op_params,
     const RuntimeShape& input_shape, const InputT* input_data,
     const RuntimeShape& output_shape, OutputT* output_data) {
   // Ensure flat size is same.
@@ -85,5 +85,5 @@ inline void PerChannelQuantize(
 
 }  // namespace reference_ops
 
-}  // namespace tflite
+}  // namespace tflite_micro
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_QUANTIZE_H_
diff --git a/tensorflow/lite/kernels/internal/reference/reduce.h b/tensorflow/lite/kernels/internal/reference/reduce.h
index 5b795ea8..6016a8df 100644
--- a/tensorflow/lite/kernels/internal/reference/reduce.h
+++ b/tensorflow/lite/kernels/internal/reference/reduce.h
@@ -44,7 +44,7 @@ inline bool IsFirstReduction(const int* index, const int num_axis,
   return true;
 }
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace reference_ops {
 
@@ -268,7 +268,7 @@ inline bool Mean(const T* input_data, const int* input_dims,
   return true;
 }
 
-inline void Mean(const tflite::MeanParams& op_params,
+inline void Mean(const tflite_micro::MeanParams& op_params,
                  const RuntimeShape& unextended_input_shape,
                  const float* input_data,
                  const RuntimeShape& unextended_output_shape,
@@ -486,6 +486,6 @@ inline bool QuantizedReduceProd(const T* input_data, int32_t input_zero_point,
 
 }  // namespace reference_ops
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_REDUCE_H_
diff --git a/tensorflow/lite/kernels/internal/reference/requantize.h b/tensorflow/lite/kernels/internal/reference/requantize.h
index f35f6fc8..db89dc14 100644
--- a/tensorflow/lite/kernels/internal/reference/requantize.h
+++ b/tensorflow/lite/kernels/internal/reference/requantize.h
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/common.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_ops {
 
 template <typename input_type, typename output_type>
@@ -65,6 +65,6 @@ inline void Requantize(const input_type* input_data, int32_t size,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_REQUANTIZE_H_
diff --git a/tensorflow/lite/kernels/internal/reference/resize_bilinear.h b/tensorflow/lite/kernels/internal/reference/resize_bilinear.h
index bf9a88af..73108fe7 100644
--- a/tensorflow/lite/kernels/internal/reference/resize_bilinear.h
+++ b/tensorflow/lite/kernels/internal/reference/resize_bilinear.h
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/cppmath.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_ops {
 
 inline void ComputeInterpolationValues(const float value, const float scale,
@@ -44,7 +44,7 @@ inline void ComputeInterpolationValues(const float value, const float scale,
 }
 
 template <typename T>
-inline void ResizeBilinear(const tflite::ResizeBilinearParams& op_params,
+inline void ResizeBilinear(const tflite_micro::ResizeBilinearParams& op_params,
                            const RuntimeShape& unextended_input_shape,
                            const T* input_data,
                            const RuntimeShape& unextended_output_size_shape,
@@ -134,7 +134,7 @@ inline void ComputeInterpolationValuesInteger(
 // Same as above but doesn't use any floating-point for the resize
 template <typename T>
 inline void ResizeBilinearInteger(
-    const tflite::ResizeBilinearParams& op_params,
+    const tflite_micro::ResizeBilinearParams& op_params,
     const RuntimeShape& unextended_input_shape, const T* input_data,
     const RuntimeShape& unextended_output_size_shape,
     const int32_t* output_size_data,
@@ -228,6 +228,6 @@ inline void ResizeBilinearInteger(
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_RESIZE_BILINEAR_H_
diff --git a/tensorflow/lite/kernels/internal/reference/resize_nearest_neighbor.h b/tensorflow/lite/kernels/internal/reference/resize_nearest_neighbor.h
index bf0b757e..2860df3e 100644
--- a/tensorflow/lite/kernels/internal/reference/resize_nearest_neighbor.h
+++ b/tensorflow/lite/kernels/internal/reference/resize_nearest_neighbor.h
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/cppmath.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace reference_ops {
 
@@ -48,7 +48,7 @@ inline int32_t GetNearestNeighbor(const int input_value,
 
 template <typename T>
 inline void ResizeNearestNeighbor(
-    const tflite::ResizeNearestNeighborParams& op_params,
+    const tflite_micro::ResizeNearestNeighborParams& op_params,
     const RuntimeShape& unextended_input_shape, const T* input_data,
     const RuntimeShape& output_size_shape, const int32_t* output_size_data,
     const RuntimeShape& unextended_output_shape, T* output_data) {
@@ -97,6 +97,6 @@ inline void ResizeNearestNeighbor(
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_RESIZE_NEAREST_NEIGHBOR_H_
diff --git a/tensorflow/lite/kernels/internal/reference/round.h b/tensorflow/lite/kernels/internal/reference/round.h
index 9bd8f3f2..c8b4552c 100644
--- a/tensorflow/lite/kernels/internal/reference/round.h
+++ b/tensorflow/lite/kernels/internal/reference/round.h
@@ -19,7 +19,7 @@ limitations under the License.
 
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace reference_ops {
 
@@ -47,5 +47,5 @@ inline void Round(const RuntimeShape& input_shape, const float* input_data,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_ROUND_H_
diff --git a/tensorflow/lite/kernels/internal/reference/select.h b/tensorflow/lite/kernels/internal/reference/select.h
index 82b6097c..e4347246 100644
--- a/tensorflow/lite/kernels/internal/reference/select.h
+++ b/tensorflow/lite/kernels/internal/reference/select.h
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/common.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_ops {
 
 template <typename D, typename T>
@@ -146,6 +146,6 @@ void BroadcastSelect5DSlow(const RuntimeShape& input_condition_shape,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_SELECT_H_
diff --git a/tensorflow/lite/kernels/internal/reference/slice.h b/tensorflow/lite/kernels/internal/reference/slice.h
index cb73ea0d..277d25ff 100644
--- a/tensorflow/lite/kernels/internal/reference/slice.h
+++ b/tensorflow/lite/kernels/internal/reference/slice.h
@@ -18,12 +18,12 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/portable_tensor.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace reference_ops {
 
 template <typename T>
-inline void Slice(const tflite::SliceParams& op_params,
+inline void Slice(const tflite_micro::SliceParams& op_params,
                   const RuntimeShape& input_shape,
                   const RuntimeShape& output_shape,
                   SequentialTensorWriter<T>* writer) {
@@ -59,7 +59,7 @@ inline void Slice(const tflite::SliceParams& op_params,
 }
 
 template <typename T>
-inline void Slice(const tflite::SliceParams& op_params,
+inline void Slice(const tflite_micro::SliceParams& op_params,
                   const RuntimeShape& input_shape, const T* input_data,
                   const RuntimeShape& output_shape, T* output_data) {
   SequentialTensorWriter<T> writer(input_data, output_data);
@@ -67,7 +67,7 @@ inline void Slice(const tflite::SliceParams& op_params,
 }
 
 template <typename T>
-inline void Slice(const tflite::SliceParams& op_params,
+inline void Slice(const tflite_micro::SliceParams& op_params,
                   const RuntimeShape& input_shape, const TfLiteTensor* input,
                   const RuntimeShape& output_shape, TfLiteTensor* output) {
   SequentialTensorWriter<T> writer(input, output);
@@ -75,6 +75,6 @@ inline void Slice(const tflite::SliceParams& op_params,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_SLICE_H_
diff --git a/tensorflow/lite/kernels/internal/reference/softmax.h b/tensorflow/lite/kernels/internal/reference/softmax.h
index c09a7eae..81274043 100644
--- a/tensorflow/lite/kernels/internal/reference/softmax.h
+++ b/tensorflow/lite/kernels/internal/reference/softmax.h
@@ -25,7 +25,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/types.h"
 #include "tensorflow/lite/kernels/op_macros.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_ops {
 
 inline void Softmax(const SoftmaxParams& params,
@@ -228,6 +228,6 @@ inline void SoftmaxInt16(const SoftmaxParams& params,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_SOFTMAX_H_
diff --git a/tensorflow/lite/kernels/internal/reference/space_to_batch_nd.h b/tensorflow/lite/kernels/internal/reference/space_to_batch_nd.h
index 7f844152..77d5bd5b 100644
--- a/tensorflow/lite/kernels/internal/reference/space_to_batch_nd.h
+++ b/tensorflow/lite/kernels/internal/reference/space_to_batch_nd.h
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/common.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_ops {
 
 // TODO(b/135760455): Move this method anonymous namespace in a cc file.
@@ -104,6 +104,6 @@ inline void SpaceToBatchND(const SpaceToBatchParams& params,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_SPACE_TO_BATCH_ND_H_
diff --git a/tensorflow/lite/kernels/internal/reference/space_to_depth.h b/tensorflow/lite/kernels/internal/reference/space_to_depth.h
index 7ad46549..c576e3db 100644
--- a/tensorflow/lite/kernels/internal/reference/space_to_depth.h
+++ b/tensorflow/lite/kernels/internal/reference/space_to_depth.h
@@ -19,11 +19,11 @@ limitations under the License.
 
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_ops {
 
 template <typename T>
-inline void SpaceToDepth(const tflite::SpaceToDepthParams& op_params,
+inline void SpaceToDepth(const tflite_micro::SpaceToDepthParams& op_params,
                          const RuntimeShape& unextended_input_shape,
                          const T* input_data,
                          const RuntimeShape& unextended_output_shape,
@@ -75,6 +75,6 @@ inline void SpaceToDepth(const tflite::SpaceToDepthParams& op_params,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_SPACE_TO_DEPTH_H_
diff --git a/tensorflow/lite/kernels/internal/reference/strided_slice.h b/tensorflow/lite/kernels/internal/reference/strided_slice.h
index b76baaad..4eefdd19 100644
--- a/tensorflow/lite/kernels/internal/reference/strided_slice.h
+++ b/tensorflow/lite/kernels/internal/reference/strided_slice.h
@@ -22,19 +22,19 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/strided_slice_logic.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace reference_ops {
 
 template <typename T>
-inline void StridedSlice(const tflite::StridedSliceParams& op_params,
+inline void StridedSlice(const tflite_micro::StridedSliceParams& op_params,
                          const RuntimeShape& unextended_input_shape,
                          const RuntimeShape& unextended_output_shape,
                          SequentialTensorWriter<T>* writer) {
   ruy::profiler::ScopeLabel label("StridedSlice");
 
   // Note that the output_shape is not used herein.
-  tflite::StridedSliceParams params_copy = op_params;
+  tflite_micro::StridedSliceParams params_copy = op_params;
 
   TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 5);
   TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 5);
@@ -120,7 +120,7 @@ inline void StridedSlice(const tflite::StridedSliceParams& op_params,
 }
 
 template <typename T>
-inline void StridedSlice(const tflite::StridedSliceParams& op_params,
+inline void StridedSlice(const tflite_micro::StridedSliceParams& op_params,
                          const RuntimeShape& unextended_input_shape,
                          const T* input_data,
                          const RuntimeShape& unextended_output_shape,
@@ -131,7 +131,7 @@ inline void StridedSlice(const tflite::StridedSliceParams& op_params,
 }
 
 template <typename T>
-inline void StridedSlice(const tflite::StridedSliceParams& op_params,
+inline void StridedSlice(const tflite_micro::StridedSliceParams& op_params,
                          const RuntimeShape& unextended_input_shape,
                          const TfLiteTensor* input,
                          const RuntimeShape& unextended_output_shape,
@@ -142,6 +142,6 @@ inline void StridedSlice(const tflite::StridedSliceParams& op_params,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_STRIDED_SLICE_H_
diff --git a/tensorflow/lite/kernels/internal/reference/sub.h b/tensorflow/lite/kernels/internal/reference/sub.h
index 1a74aebe..e80b3c27 100644
--- a/tensorflow/lite/kernels/internal/reference/sub.h
+++ b/tensorflow/lite/kernels/internal/reference/sub.h
@@ -26,7 +26,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/compatibility.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace reference_ops {
 
@@ -460,6 +460,6 @@ inline void SubWithActivation(
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_SUB_H_
diff --git a/tensorflow/lite/kernels/internal/reference/tanh.h b/tensorflow/lite/kernels/internal/reference/tanh.h
index 3a05c474..578a1a90 100644
--- a/tensorflow/lite/kernels/internal/reference/tanh.h
+++ b/tensorflow/lite/kernels/internal/reference/tanh.h
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/types.h"
 #include "tensorflow/lite/kernels/op_macros.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace reference_ops {
 
 inline void Tanh(const RuntimeShape& input_shape, const float* input_data,
@@ -124,6 +124,6 @@ inline void Tanh(const TanhParams& params, const RuntimeShape& input_shape,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_TANH_H_
diff --git a/tensorflow/lite/kernels/internal/reference/transpose.h b/tensorflow/lite/kernels/internal/reference/transpose.h
index 7e2bf7b2..195202e6 100644
--- a/tensorflow/lite/kernels/internal/reference/transpose.h
+++ b/tensorflow/lite/kernels/internal/reference/transpose.h
@@ -19,7 +19,7 @@ limitations under the License.
 
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace reference_ops {
 
@@ -198,6 +198,6 @@ void Transpose(const TransposeParams& params, const RuntimeShape& input_shape,
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_TRANSPOSE_H_
diff --git a/tensorflow/lite/kernels/internal/reference/transpose_conv.h b/tensorflow/lite/kernels/internal/reference/transpose_conv.h
index 8a51e0fa..4a588a34 100644
--- a/tensorflow/lite/kernels/internal/reference/transpose_conv.h
+++ b/tensorflow/lite/kernels/internal/reference/transpose_conv.h
@@ -20,7 +20,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/common.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace reference_ops {
 
@@ -220,6 +220,6 @@ inline void TransposeConv(
 }
 
 }  // namespace reference_ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_TRANSPOSE_CONV_H_
diff --git a/tensorflow/lite/kernels/internal/runtime_shape.h b/tensorflow/lite/kernels/internal/runtime_shape.h
index bc786bdb..28fbfe27 100644
--- a/tensorflow/lite/kernels/internal/runtime_shape.h
+++ b/tensorflow/lite/kernels/internal/runtime_shape.h
@@ -19,7 +19,7 @@ limitations under the License.
 
 #include "tensorflow/lite/kernels/internal/compatibility.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 template <int N>
 struct Dims {
@@ -163,6 +163,6 @@ inline int Offset(const RuntimeShape& shape, int i0, int i1, int i2, int i3,
          i4;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_RUNTIME_SHAPE_H_
diff --git a/tensorflow/lite/kernels/internal/strided_slice_logic.h b/tensorflow/lite/kernels/internal/strided_slice_logic.h
index 449cac04..85849365 100644
--- a/tensorflow/lite/kernels/internal/strided_slice_logic.h
+++ b/tensorflow/lite/kernels/internal/strided_slice_logic.h
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/compatibility.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace strided_slice {
 
 // Use until std::clamp() is available from C++17.
@@ -33,7 +33,7 @@ inline int Clamp(const int v, const int lo, const int hi) {
   return v;
 }
 
-inline void StridedSlicePadIndices(tflite::StridedSliceParams* p,
+inline void StridedSlicePadIndices(tflite_micro::StridedSliceParams* p,
                                    int dim_count) {
   // Add indices and mask bits to fully include extra dimensions
   TFLITE_CHECK_LE(dim_count, 5);
@@ -72,7 +72,7 @@ inline void StridedSlicePadIndices(tflite::StridedSliceParams* p,
 // Return the index for the first element along that axis. This index will be a
 // positive integer between [0, axis_size] (or [-1, axis_size -1] if stride < 0)
 // that can be used to index directly into the data.
-inline int StridedSliceStartForAxis(const tflite::StridedSliceParams& params,
+inline int StridedSliceStartForAxis(const tflite_micro::StridedSliceParams& params,
                                     const RuntimeShape& input_shape,
                                     int32_t axis) {
   const int32_t axis_size = input_shape.Dims(axis);
@@ -97,7 +97,7 @@ inline int StridedSliceStartForAxis(const tflite::StridedSliceParams& params,
   return start;
 }
 
-inline int StridedSliceEndForAxis(const tflite::StridedSliceParams& params,
+inline int StridedSliceEndForAxis(const tflite_micro::StridedSliceParams& params,
                                   const RuntimeShape& input_shape, int axis,
                                   int start) {
   const auto shrink_axis_mask = params.shrink_axis_mask;
@@ -139,7 +139,7 @@ inline int StridedSliceEndForAxis(const tflite::StridedSliceParams& params,
 // Return the index for the first element along that axis. This index will be a
 // positive integer between [0, axis_size] (or [-1, axis_size -1] if stride < 0)
 // that can be used to index directly into the data.
-inline int StartForAxis(const tflite::StridedSliceParams& params,
+inline int StartForAxis(const tflite_micro::StridedSliceParams& params,
                         const RuntimeShape& input_shape, int axis) {
   const auto begin_mask = params.begin_mask;
   const auto* start_indices = params.start_indices;
@@ -186,7 +186,7 @@ inline int StartForAxis(const tflite::StridedSliceParams& params,
 // element. ie. So if you were iterating through all elements of a 1D array of
 // size 4, this function would return 4 as the stop, because it is one past the
 // "real" indices of 0, 1, 2 & 3.
-inline int StopForAxis(const tflite::StridedSliceParams& params,
+inline int StopForAxis(const tflite_micro::StridedSliceParams& params,
                        const RuntimeShape& input_shape, int axis,
                        int start_for_axis) {
   const auto end_mask = params.end_mask;
@@ -246,11 +246,11 @@ inline bool LoopCondition(int index, int stop, int stride) {
   return stride > 0 ? index >= stop : index <= stop;
 }
 
-inline tflite::StridedSliceParams BuildStridedSliceParams(
+inline tflite_micro::StridedSliceParams BuildStridedSliceParams(
     int begin_mask, int end_mask, int shrink_axis_mask,
     const std::vector<int>& start_indices, const std::vector<int>& stop_indices,
     const std::vector<int>& strides) {
-  tflite::StridedSliceParams op_params{};
+  tflite_micro::StridedSliceParams op_params{};
   const int dims_count = start_indices.size();
 
   op_params.start_indices_count = dims_count;
@@ -273,6 +273,6 @@ inline tflite::StridedSliceParams BuildStridedSliceParams(
 
 }  // namespace strided_slice
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_STRIDED_SLICE_LOGIC_H_
diff --git a/tensorflow/lite/kernels/internal/tensor_ctypes.cc b/tensorflow/lite/kernels/internal/tensor_ctypes.cc
index 6bd58fc1..83f828b2 100644
--- a/tensorflow/lite/kernels/internal/tensor_ctypes.cc
+++ b/tensorflow/lite/kernels/internal/tensor_ctypes.cc
@@ -17,7 +17,7 @@ limitations under the License.
 
 #include <vector>
 
-namespace tflite {
+namespace tflite_micro {
 
 RuntimeShape GetTensorShape(const TfLiteTensor* tensor) {
   if (tensor == nullptr) {
@@ -34,4 +34,4 @@ RuntimeShape GetTensorShape(std::vector<int32_t> data) {
   return RuntimeShape(data.size(), data.data());
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/kernels/internal/tensor_ctypes.h b/tensorflow/lite/kernels/internal/tensor_ctypes.h
index 9a7205c0..7cc05ff4 100644
--- a/tensorflow/lite/kernels/internal/tensor_ctypes.h
+++ b/tensorflow/lite/kernels/internal/tensor_ctypes.h
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/core/macros.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 template <typename T>
 inline T* GetTensorData(TfLiteTensor* tensor) {
@@ -37,6 +37,6 @@ inline const T* GetTensorData(const TfLiteTensor* tensor) {
 TFLITE_NOINLINE RuntimeShape GetTensorShape(const TfLiteTensor* tensor);
 RuntimeShape GetTensorShape(std::vector<int32_t> data);
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_TENSOR_CTYPES_H_
diff --git a/tensorflow/lite/kernels/internal/types.h b/tensorflow/lite/kernels/internal/types.h
index f2cc1603..bddd87b5 100644
--- a/tensorflow/lite/kernels/internal/types.h
+++ b/tensorflow/lite/kernels/internal/types.h
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/compatibility.h"
 #include "tensorflow/lite/kernels/internal/runtime_shape.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 enum class FusedActivationFunctionType : uint8_t {
   kNone,
@@ -1091,6 +1091,6 @@ struct is_int32_or_int64
                                               std::is_same<T, int64_t>::value> {
 };
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_TYPES_H_
diff --git a/tensorflow/lite/kernels/kernel_util.cc b/tensorflow/lite/kernels/kernel_util.cc
index 58fd99f8..ae3d4853 100644
--- a/tensorflow/lite/kernels/kernel_util.cc
+++ b/tensorflow/lite/kernels/kernel_util.cc
@@ -38,7 +38,7 @@ limitations under the License.
 #include "TargetConditionals.h"
 #endif
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -591,4 +591,4 @@ bool HasUnspecifiedDimension(const TfLiteTensor* tensor) {
   return false;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/kernels/kernel_util.h b/tensorflow/lite/kernels/kernel_util.h
index e318118f..89db28ae 100644
--- a/tensorflow/lite/kernels/kernel_util.h
+++ b/tensorflow/lite/kernels/kernel_util.h
@@ -28,7 +28,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/op_macros.h"
 #endif
 
-namespace tflite {
+namespace tflite_micro {
 
 // A fair number of functions in this header have historically been inline.
 // It is ok to change functions to not be inline if the latency with
@@ -336,6 +336,6 @@ bool IsMobilePlatform();
 // Returns whether there is unspecified dimension in the tensor's dim signature.
 bool HasUnspecifiedDimension(const TfLiteTensor* tensor);
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_KERNEL_UTIL_H_
diff --git a/tensorflow/lite/kernels/padding.h b/tensorflow/lite/kernels/padding.h
index cc9d596f..02437cb4 100644
--- a/tensorflow/lite/kernels/padding.h
+++ b/tensorflow/lite/kernels/padding.h
@@ -18,7 +18,7 @@ limitations under the License.
 #include "tensorflow/lite/core/c/builtin_op_data.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 inline int ComputePadding(int stride, int dilation_rate, int in_size,
                           int filter_size, int out_size) {
@@ -110,6 +110,6 @@ inline Padding3DValues ComputePadding3DValues(
   padding_values.width_offset = offset;
   return padding_values;
 }
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_KERNELS_PADDING_H_
diff --git a/tensorflow/lite/micro/arc_custom/micro_time.cc b/tensorflow/lite/micro/arc_custom/micro_time.cc
index 12e23f7e..6685130d 100644
--- a/tensorflow/lite/micro/arc_custom/micro_time.cc
+++ b/tensorflow/lite/micro/arc_custom/micro_time.cc
@@ -23,7 +23,7 @@ limitations under the License.
 
 #include <limits>
 
-namespace tflite {
+namespace tflite_micro {
 
 uint32_t ticks_per_second() {
   const unsigned long clocs_per_sec_real = _timer_clocks_per_sec();
@@ -39,4 +39,4 @@ uint32_t GetCurrentTimeTicks() {
   return ticks_real;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/arc_custom/system_setup.cc b/tensorflow/lite/micro/arc_custom/system_setup.cc
index c27c9554..9cf742f8 100644
--- a/tensorflow/lite/micro/arc_custom/system_setup.cc
+++ b/tensorflow/lite/micro/arc_custom/system_setup.cc
@@ -17,9 +17,9 @@ limitations under the License.
 
 #include <arc/arc_timer.h>
 
-namespace tflite {
+namespace tflite_micro {
 
 // Only the timer need to be reset for the custom arc platform
 void InitializeTarget() { _timer_default_reset(); }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/arena_allocator/ibuffer_allocator.h b/tensorflow/lite/micro/arena_allocator/ibuffer_allocator.h
index b92d6b2d..41e32c9a 100644
--- a/tensorflow/lite/micro/arena_allocator/ibuffer_allocator.h
+++ b/tensorflow/lite/micro/arena_allocator/ibuffer_allocator.h
@@ -20,7 +20,7 @@ limitations under the License.
 
 #include "tensorflow/lite/c/c_api_types.h"
 
-namespace tflite {
+namespace tflite_micro {
 // Interface classes that the TFLM framework relies on to get buffers it needs.
 // There are two types of buffers that the TFLM framework requires: persistent
 // and non-persistent. Persistent buffers, once allocated, are never freed by
@@ -95,6 +95,6 @@ class INonPersistentBufferAllocator {
   virtual size_t GetAvailableMemory(size_t alignment) const = 0;
 };
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_ARENA_ALLOCATOR_IBUFFER_ALLOCATOR_H_
diff --git a/tensorflow/lite/micro/arena_allocator/non_persistent_arena_buffer_allocator.cc b/tensorflow/lite/micro/arena_allocator/non_persistent_arena_buffer_allocator.cc
index a8f00ead..8fa132f8 100644
--- a/tensorflow/lite/micro/arena_allocator/non_persistent_arena_buffer_allocator.cc
+++ b/tensorflow/lite/micro/arena_allocator/non_persistent_arena_buffer_allocator.cc
@@ -17,7 +17,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/memory_helpers.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 NonPersistentArenaBufferAllocator::NonPersistentArenaBufferAllocator(
     uint8_t* buffer, size_t buffer_size)
@@ -167,4 +167,4 @@ size_t NonPersistentArenaBufferAllocator::GetAvailableMemory(
   return aligned_tail - aligned_temp;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/arena_allocator/non_persistent_arena_buffer_allocator.h b/tensorflow/lite/micro/arena_allocator/non_persistent_arena_buffer_allocator.h
index ebd37646..8707e5a4 100644
--- a/tensorflow/lite/micro/arena_allocator/non_persistent_arena_buffer_allocator.h
+++ b/tensorflow/lite/micro/arena_allocator/non_persistent_arena_buffer_allocator.h
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/arena_allocator/ibuffer_allocator.h"
 #include "tensorflow/lite/micro/compatibility.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // Implement INonPersistentBufferAllocator on an arena that is dedicated for
 // non-persistent buffers.
@@ -99,6 +99,6 @@ class NonPersistentArenaBufferAllocator : public INonPersistentBufferAllocator {
   bool resizable_buffer_allocated_ = false;
 };
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_ARENA_ALLOCATOR_NON_PERSISTENT_ARENA_BUFFER_ALLOCATOR_H_
diff --git a/tensorflow/lite/micro/arena_allocator/non_persistent_arena_buffer_allocator_test.cc b/tensorflow/lite/micro/arena_allocator/non_persistent_arena_buffer_allocator_test.cc
index e94cc8c7..ad8e2a86 100644
--- a/tensorflow/lite/micro/arena_allocator/non_persistent_arena_buffer_allocator_test.cc
+++ b/tensorflow/lite/micro/arena_allocator/non_persistent_arena_buffer_allocator_test.cc
@@ -26,7 +26,7 @@ TF_LITE_MICRO_TESTS_BEGIN
 TF_LITE_MICRO_TEST(TestResizableBuffer) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::NonPersistentArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::NonPersistentArenaBufferAllocator allocator(arena, arena_size);
 
   uint8_t* resizable_buf = allocator.AllocateResizableBuffer(10, 1);
   TF_LITE_MICRO_EXPECT(resizable_buf == arena);
@@ -54,7 +54,7 @@ TF_LITE_MICRO_TEST(TestResizableBuffer) {
 TF_LITE_MICRO_TEST(TestTempBuffer) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::NonPersistentArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::NonPersistentArenaBufferAllocator allocator(arena, arena_size);
 
   constexpr size_t allocation_size = 100;
   uint8_t* temp = allocator.AllocateTemp(/*size=*/allocation_size,
@@ -81,7 +81,7 @@ TF_LITE_MICRO_TEST(TestTempBuffer) {
 TF_LITE_MICRO_TEST(TestAllocateResizeFailIfTempStillExists) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::NonPersistentArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::NonPersistentArenaBufferAllocator allocator(arena, arena_size);
 
   constexpr size_t allocation_size = 100;
   uint8_t* temp = allocator.AllocateTemp(/*size=*/allocation_size,
@@ -97,7 +97,7 @@ TF_LITE_MICRO_TEST(TestAllocateResizeFailIfTempStillExists) {
 TF_LITE_MICRO_TEST(TestAllocateResizePassIfNoTemp) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::NonPersistentArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::NonPersistentArenaBufferAllocator allocator(arena, arena_size);
 
   constexpr size_t allocation_size = 100;
   uint8_t* temp = allocator.AllocateTemp(/*size=*/allocation_size,
@@ -114,7 +114,7 @@ TF_LITE_MICRO_TEST(TestAllocateResizePassIfNoTemp) {
 TF_LITE_MICRO_TEST(TestAllocateResizableFailIfResizableExists) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::NonPersistentArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::NonPersistentArenaBufferAllocator allocator(arena, arena_size);
 
   constexpr size_t allocation_size = 100;
   TF_LITE_MICRO_EXPECT(
@@ -130,7 +130,7 @@ TF_LITE_MICRO_TEST(TestAllocateResizableFailIfResizableExists) {
 TF_LITE_MICRO_TEST(TestResetTempFailIfTempStillExists) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::NonPersistentArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::NonPersistentArenaBufferAllocator allocator(arena, arena_size);
 
   constexpr size_t allocation_size = 100;
   allocator.AllocateTemp(/*size=*/allocation_size,
@@ -143,7 +143,7 @@ TF_LITE_MICRO_TEST(TestResetTempFailIfTempStillExists) {
 TF_LITE_MICRO_TEST(TestAllocateTempFailIfExceedAllowance) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::NonPersistentArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::NonPersistentArenaBufferAllocator allocator(arena, arena_size);
 
   TF_LITE_MICRO_EXPECT(allocator.AllocateTemp(/*size=*/arena_size + 1,
                                               /*alignment=*/1) == nullptr);
@@ -153,7 +153,7 @@ TF_LITE_MICRO_TEST(TestAllocateTempFailIfExceedAllowance) {
 TF_LITE_MICRO_TEST(TestAllocateTempFailIfExceedAllowance) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::NonPersistentArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::NonPersistentArenaBufferAllocator allocator(arena, arena_size);
 
   TF_LITE_MICRO_EXPECT(allocator.AllocateResizableBuffer(
                            /*size=*/arena_size + 1, /*alignment=*/1) ==
@@ -176,7 +176,7 @@ TF_LITE_MICRO_TEST(TestAllocateTempFailIfExceedAllowance) {
 TF_LITE_MICRO_TEST(TestGetNonPersistentUsedBytes) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::NonPersistentArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::NonPersistentArenaBufferAllocator allocator(arena, arena_size);
 
   constexpr size_t allocation_size = 100;
   TF_LITE_MICRO_EXPECT(
diff --git a/tensorflow/lite/micro/arena_allocator/persistent_arena_buffer_allocator.cc b/tensorflow/lite/micro/arena_allocator/persistent_arena_buffer_allocator.cc
index a770bc9d..c7479919 100644
--- a/tensorflow/lite/micro/arena_allocator/persistent_arena_buffer_allocator.cc
+++ b/tensorflow/lite/micro/arena_allocator/persistent_arena_buffer_allocator.cc
@@ -17,7 +17,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/memory_helpers.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 PersistentArenaBufferAllocator::PersistentArenaBufferAllocator(
     uint8_t* buffer, size_t buffer_size)
@@ -49,4 +49,4 @@ size_t PersistentArenaBufferAllocator::GetPersistentUsedBytes() const {
   return buffer_tail_ - tail_temp_;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/arena_allocator/persistent_arena_buffer_allocator.h b/tensorflow/lite/micro/arena_allocator/persistent_arena_buffer_allocator.h
index 2c8e3dca..a4ca701c 100644
--- a/tensorflow/lite/micro/arena_allocator/persistent_arena_buffer_allocator.h
+++ b/tensorflow/lite/micro/arena_allocator/persistent_arena_buffer_allocator.h
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/arena_allocator/ibuffer_allocator.h"
 #include "tensorflow/lite/micro/compatibility.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // PersistentArenaBufferAllocator is an implementatation of
 // IPersistentBufferAllocator interface on an arena that is dedicated for
@@ -53,6 +53,6 @@ class PersistentArenaBufferAllocator : public IPersistentBufferAllocator {
   uint8_t* tail_temp_;
 };
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_ARENA_ALLOCATOR_PERSISTENT_ARENA_BUFFER_ALLOCATOR_H_
diff --git a/tensorflow/lite/micro/arena_allocator/persistent_arena_buffer_allocator_test.cc b/tensorflow/lite/micro/arena_allocator/persistent_arena_buffer_allocator_test.cc
index 984b8a14..ff9561d7 100644
--- a/tensorflow/lite/micro/arena_allocator/persistent_arena_buffer_allocator_test.cc
+++ b/tensorflow/lite/micro/arena_allocator/persistent_arena_buffer_allocator_test.cc
@@ -25,7 +25,7 @@ TF_LITE_MICRO_TESTS_BEGIN
 TF_LITE_MICRO_TEST(TestGetPersistentUsedBytes) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::PersistentArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::PersistentArenaBufferAllocator allocator(arena, arena_size);
 
   const size_t size1 = 10;
   allocator.AllocatePersistentBuffer(size1, 1);
@@ -41,7 +41,7 @@ TF_LITE_MICRO_TEST(TestGetPersistentUsedBytes) {
 TF_LITE_MICRO_TEST(TestAllocatePersistBufferShallFailIfExceedLimit) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::PersistentArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::PersistentArenaBufferAllocator allocator(arena, arena_size);
 
   const size_t size1 = 10;
   uint8_t* persist1 = allocator.AllocatePersistentBuffer(size1, 1);
@@ -57,7 +57,7 @@ TF_LITE_MICRO_TEST(TestAllocatePersistBufferShallFailIfExceedLimit) {
 TF_LITE_MICRO_TEST(TestAllocatePersistBufferShallPassIfWithinLimit) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::PersistentArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::PersistentArenaBufferAllocator allocator(arena, arena_size);
 
   const size_t size1 = 10;
   uint8_t* persist1 = allocator.AllocatePersistentBuffer(size1, 1);
@@ -74,7 +74,7 @@ TF_LITE_MICRO_TEST(TestAllocatePersistBufferShallPassIfWithinLimit) {
 TF_LITE_MICRO_TEST(TestAllocatePersistBufferAligns) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::PersistentArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::PersistentArenaBufferAllocator allocator(arena, arena_size);
 
   const size_t size1 = 10;
   const size_t alignment = 16;
diff --git a/tensorflow/lite/micro/arena_allocator/recording_single_arena_buffer_allocator.cc b/tensorflow/lite/micro/arena_allocator/recording_single_arena_buffer_allocator.cc
index e21e3646..f9e94e0f 100644
--- a/tensorflow/lite/micro/arena_allocator/recording_single_arena_buffer_allocator.cc
+++ b/tensorflow/lite/micro/arena_allocator/recording_single_arena_buffer_allocator.cc
@@ -19,7 +19,7 @@ limitations under the License.
 
 #include "tensorflow/lite/kernels/internal/compatibility.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 RecordingSingleArenaBufferAllocator::RecordingSingleArenaBufferAllocator(
     uint8_t* buffer_head, size_t buffer_size)
@@ -82,4 +82,4 @@ uint8_t* RecordingSingleArenaBufferAllocator::AllocatePersistentBuffer(
   return result;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/arena_allocator/recording_single_arena_buffer_allocator.h b/tensorflow/lite/micro/arena_allocator/recording_single_arena_buffer_allocator.h
index 94e55a33..e24530e6 100644
--- a/tensorflow/lite/micro/arena_allocator/recording_single_arena_buffer_allocator.h
+++ b/tensorflow/lite/micro/arena_allocator/recording_single_arena_buffer_allocator.h
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/arena_allocator/single_arena_buffer_allocator.h"
 #include "tensorflow/lite/micro/compatibility.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // Utility class used to log allocations of a SingleArenaBufferAllocator. Should
 // only be used in debug/evaluation settings or unit tests to evaluate
@@ -58,6 +58,6 @@ class RecordingSingleArenaBufferAllocator : public SingleArenaBufferAllocator {
   TF_LITE_REMOVE_VIRTUAL_DELETE
 };
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_ARENA_ALLOCATOR_RECORDING_SINGLE_ARENA_BUFFER_ALLOCATOR_H_
diff --git a/tensorflow/lite/micro/arena_allocator/recording_single_arena_buffer_allocator_test.cc b/tensorflow/lite/micro/arena_allocator/recording_single_arena_buffer_allocator_test.cc
index a25ad506..75abaa6d 100644
--- a/tensorflow/lite/micro/arena_allocator/recording_single_arena_buffer_allocator_test.cc
+++ b/tensorflow/lite/micro/arena_allocator/recording_single_arena_buffer_allocator_test.cc
@@ -25,7 +25,7 @@ TF_LITE_MICRO_TESTS_BEGIN
 TF_LITE_MICRO_TEST(TestRecordsTailAllocations) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::RecordingSingleArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::RecordingSingleArenaBufferAllocator allocator(arena, arena_size);
 
   uint8_t* result =
       allocator.AllocatePersistentBuffer(/*size=*/10, /*alignment=*/1);
@@ -48,7 +48,7 @@ TF_LITE_MICRO_TEST(TestRecordsTailAllocations) {
 TF_LITE_MICRO_TEST(TestRecordsMisalignedTailAllocations) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::RecordingSingleArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::RecordingSingleArenaBufferAllocator allocator(arena, arena_size);
 
   uint8_t* result =
       allocator.AllocatePersistentBuffer(/*size=*/10, /*alignment=*/12);
@@ -65,7 +65,7 @@ TF_LITE_MICRO_TEST(TestRecordsMisalignedTailAllocations) {
 TF_LITE_MICRO_TEST(TestDoesNotRecordFailedTailAllocations) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::RecordingSingleArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::RecordingSingleArenaBufferAllocator allocator(arena, arena_size);
 
   uint8_t* result =
       allocator.AllocatePersistentBuffer(/*size=*/2048, /*alignment=*/1);
@@ -80,7 +80,7 @@ TF_LITE_MICRO_TEST(TestDoesNotRecordFailedTailAllocations) {
 TF_LITE_MICRO_TEST(TestRecordsHeadSizeAdjustment) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::RecordingSingleArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::RecordingSingleArenaBufferAllocator allocator(arena, arena_size);
 
   uint8_t* resizable_buf = allocator.AllocateResizableBuffer(0, 1);
   TF_LITE_MICRO_EXPECT(resizable_buf != nullptr);
@@ -108,7 +108,7 @@ TF_LITE_MICRO_TEST(TestRecordsHeadSizeAdjustment) {
 TF_LITE_MICRO_TEST(TestRecordsMisalignedHeadSizeAdjustments) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::RecordingSingleArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::RecordingSingleArenaBufferAllocator allocator(arena, arena_size);
   uint8_t* resizable_buf = allocator.AllocateResizableBuffer(0, 12);
   TF_LITE_MICRO_EXPECT(resizable_buf != nullptr);
 
@@ -128,7 +128,7 @@ TF_LITE_MICRO_TEST(TestRecordsMisalignedHeadSizeAdjustments) {
 TF_LITE_MICRO_TEST(TestDoesNotRecordFailedTailAllocations) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::RecordingSingleArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::RecordingSingleArenaBufferAllocator allocator(arena, arena_size);
 
   uint8_t* resizable_buf = allocator.AllocateResizableBuffer(0, 1);
   TF_LITE_MICRO_EXPECT(resizable_buf != nullptr);
diff --git a/tensorflow/lite/micro/arena_allocator/single_arena_buffer_allocator.cc b/tensorflow/lite/micro/arena_allocator/single_arena_buffer_allocator.cc
index 8655cfdc..0e48c98c 100644
--- a/tensorflow/lite/micro/arena_allocator/single_arena_buffer_allocator.cc
+++ b/tensorflow/lite/micro/arena_allocator/single_arena_buffer_allocator.cc
@@ -26,7 +26,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/memory_helpers.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 SingleArenaBufferAllocator::SingleArenaBufferAllocator(uint8_t* buffer_head,
                                                        uint8_t* buffer_tail)
@@ -196,4 +196,4 @@ uint8_t* SingleArenaBufferAllocator::head() const { return head_; }
 
 uint8_t* SingleArenaBufferAllocator::tail() const { return tail_; }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/arena_allocator/single_arena_buffer_allocator.h b/tensorflow/lite/micro/arena_allocator/single_arena_buffer_allocator.h
index a2e39588..91464c89 100644
--- a/tensorflow/lite/micro/arena_allocator/single_arena_buffer_allocator.h
+++ b/tensorflow/lite/micro/arena_allocator/single_arena_buffer_allocator.h
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/arena_allocator/ibuffer_allocator.h"
 #include "tensorflow/lite/micro/compatibility.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // TODO(petewarden): This allocator never frees up or reuses  any memory, even
 // though we have enough information about lifetimes of the tensors to do so.
@@ -139,6 +139,6 @@ class SingleArenaBufferAllocator : public INonPersistentBufferAllocator,
   int temp_buffer_count_ = 0;
 };
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_ARENA_ALLOCATOR_SINGLE_ARENA_BUFFER_ALLOCATOR_H_
diff --git a/tensorflow/lite/micro/arena_allocator/single_arena_buffer_allocator_test.cc b/tensorflow/lite/micro/arena_allocator/single_arena_buffer_allocator_test.cc
index 9779c4ef..7d12ac4e 100644
--- a/tensorflow/lite/micro/arena_allocator/single_arena_buffer_allocator_test.cc
+++ b/tensorflow/lite/micro/arena_allocator/single_arena_buffer_allocator_test.cc
@@ -25,7 +25,7 @@ TF_LITE_MICRO_TESTS_BEGIN
 TF_LITE_MICRO_TEST(TestEnsureHeadSizeSimpleAlignment) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::SingleArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::SingleArenaBufferAllocator allocator(arena, arena_size);
 
   uint8_t* resizable_buf = allocator.AllocateResizableBuffer(0, 1);
   TF_LITE_MICRO_EXPECT(resizable_buf != nullptr);
@@ -52,7 +52,7 @@ TF_LITE_MICRO_TEST(TestEnsureHeadSizeSimpleAlignment) {
 TF_LITE_MICRO_TEST(TestAdjustHeadSizeMisalignment) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::SingleArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::SingleArenaBufferAllocator allocator(arena, arena_size);
 
   uint8_t* resizable_buf = allocator.AllocateResizableBuffer(0, 12);
   TF_LITE_MICRO_EXPECT(resizable_buf != nullptr);
@@ -83,7 +83,7 @@ TF_LITE_MICRO_TEST(TestAdjustHeadSizeMisalignment) {
 TF_LITE_MICRO_TEST(TestAdjustHeadSizeMisalignedHandlesCorrectBytesAvailable) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::SingleArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::SingleArenaBufferAllocator allocator(arena, arena_size);
 
   uint8_t* resizable_buf = allocator.AllocateResizableBuffer(0, 12);
   TF_LITE_MICRO_EXPECT(resizable_buf != nullptr);
@@ -119,7 +119,7 @@ TF_LITE_MICRO_TEST(TestAdjustHeadSizeMisalignedHandlesCorrectBytesAvailable) {
 TF_LITE_MICRO_TEST(TestGetAvailableMemory) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::SingleArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::SingleArenaBufferAllocator allocator(arena, arena_size);
 
   uint8_t* resizable_buf = allocator.AllocateResizableBuffer(0, 1);
   TF_LITE_MICRO_EXPECT(resizable_buf != nullptr);
@@ -137,7 +137,7 @@ TF_LITE_MICRO_TEST(TestGetAvailableMemory) {
 TF_LITE_MICRO_TEST(TestGetAvailableMemoryWithTempAllocations) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::SingleArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::SingleArenaBufferAllocator allocator(arena, arena_size);
 
   constexpr size_t allocation_size = 100;
   uint8_t* temp = allocator.AllocateTemp(/*size=*/allocation_size,
@@ -158,7 +158,7 @@ TF_LITE_MICRO_TEST(TestGetAvailableMemoryWithTempAllocations) {
 TF_LITE_MICRO_TEST(TestGetUsedBytes) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::SingleArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::SingleArenaBufferAllocator allocator(arena, arena_size);
   TF_LITE_MICRO_EXPECT_EQ(allocator.GetUsedBytes(), static_cast<size_t>(0));
   uint8_t* resizable_buf = allocator.AllocateResizableBuffer(0, 1);
   TF_LITE_MICRO_EXPECT(resizable_buf != nullptr);
@@ -175,7 +175,7 @@ TF_LITE_MICRO_TEST(TestGetUsedBytes) {
 TF_LITE_MICRO_TEST(TestGetUsedBytesTempAllocations) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::SingleArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::SingleArenaBufferAllocator allocator(arena, arena_size);
 
   constexpr size_t allocation_size = 100;
   uint8_t* temp = allocator.AllocateTemp(/*size=*/allocation_size,
@@ -194,7 +194,7 @@ TF_LITE_MICRO_TEST(TestGetUsedBytesTempAllocations) {
 TF_LITE_MICRO_TEST(TestJustFits) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::SingleArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::SingleArenaBufferAllocator allocator(arena, arena_size);
 
   uint8_t* result = allocator.AllocatePersistentBuffer(arena_size, 1);
   TF_LITE_MICRO_EXPECT(nullptr != result);
@@ -203,7 +203,7 @@ TF_LITE_MICRO_TEST(TestJustFits) {
 TF_LITE_MICRO_TEST(TestAligned) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::SingleArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::SingleArenaBufferAllocator allocator(arena, arena_size);
 
   uint8_t* result = allocator.AllocatePersistentBuffer(1, 1);
   TF_LITE_MICRO_EXPECT(nullptr != result);
@@ -217,7 +217,7 @@ TF_LITE_MICRO_TEST(TestAligned) {
 TF_LITE_MICRO_TEST(TestMultipleTooLarge) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::SingleArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::SingleArenaBufferAllocator allocator(arena, arena_size);
 
   uint8_t* result = allocator.AllocatePersistentBuffer(768, 1);
   TF_LITE_MICRO_EXPECT(nullptr != result);
@@ -229,7 +229,7 @@ TF_LITE_MICRO_TEST(TestMultipleTooLarge) {
 TF_LITE_MICRO_TEST(TestTempAllocations) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::SingleArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::SingleArenaBufferAllocator allocator(arena, arena_size);
 
   uint8_t* temp1 = allocator.AllocateTemp(100, 1);
   TF_LITE_MICRO_EXPECT(nullptr != temp1);
@@ -244,7 +244,7 @@ TF_LITE_MICRO_TEST(TestTempAllocations) {
 TF_LITE_MICRO_TEST(TestResetTempAllocations) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::SingleArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::SingleArenaBufferAllocator allocator(arena, arena_size);
 
   uint8_t* temp1 = allocator.AllocateTemp(100, 1);
   TF_LITE_MICRO_EXPECT(nullptr != temp1);
@@ -262,7 +262,7 @@ TF_LITE_MICRO_TEST(TestResetTempAllocations) {
 TF_LITE_MICRO_TEST(TestEnsureHeadSizeWithoutResettingTemp) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::SingleArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::SingleArenaBufferAllocator allocator(arena, arena_size);
   uint8_t* resizable_buf = allocator.AllocateResizableBuffer(0, 1);
   TF_LITE_MICRO_EXPECT(resizable_buf != nullptr);
 
@@ -289,7 +289,7 @@ TF_LITE_MICRO_TEST(TestEnsureHeadSizeWithoutResettingTemp) {
 TF_LITE_MICRO_TEST(TestIsAllTempDeallocated) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::SingleArenaBufferAllocator allocator(arena, arena_size);
+  tflite_micro::SingleArenaBufferAllocator allocator(arena, arena_size);
 
   uint8_t* temp1 = allocator.AllocateTemp(100, 1);
   TF_LITE_MICRO_EXPECT(allocator.IsAllTempDeallocated() == false);
diff --git a/tensorflow/lite/micro/benchmarks/keyword_benchmark.cc b/tensorflow/lite/micro/benchmarks/keyword_benchmark.cc
index 3695c11f..c22b4e2b 100644
--- a/tensorflow/lite/micro/benchmarks/keyword_benchmark.cc
+++ b/tensorflow/lite/micro/benchmarks/keyword_benchmark.cc
@@ -33,7 +33,7 @@ limitations under the License.
  * weights and parameters are not representative of the original model.
  */
 
-namespace tflite {
+namespace tflite_micro {
 
 using KeywordBenchmarkRunner = MicroBenchmarkRunner<int16_t>;
 using KeywordOpResolver = MicroMutableOpResolver<6>;
@@ -53,10 +53,10 @@ KeywordBenchmarkRunner* CreateBenchmarkRunner(MicroProfiler* profiler) {
   // We allocate the KeywordOpResolver from a global buffer because the object's
   // lifetime must exceed that of the KeywordBenchmarkRunner object.
   KeywordOpResolver* op_resolver = new (op_resolver_buffer) KeywordOpResolver();
-  op_resolver->AddFullyConnected(tflite::Register_FULLY_CONNECTED_INT8());
+  op_resolver->AddFullyConnected(tflite_micro::Register_FULLY_CONNECTED_INT8());
   op_resolver->AddQuantize();
-  op_resolver->AddSoftmax(tflite::Register_SOFTMAX_INT8_INT16());
-  op_resolver->AddSvdf(tflite::Register_SVDF_INT8());
+  op_resolver->AddSoftmax(tflite_micro::Register_SOFTMAX_INT8_INT16());
+  op_resolver->AddSvdf(tflite_micro::Register_SVDF_INT8());
 
   return new (benchmark_runner_buffer)
       KeywordBenchmarkRunner(g_keyword_scrambled_model_data, op_resolver,
@@ -76,25 +76,25 @@ void KeywordRunNIerations(int iterations, const char* tag,
   MicroPrintf("%s took %d ticks (%d ms)", tag, ticks, TicksToMs(ticks));
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 int main(int argc, char** argv) {
-  tflite::InitializeTarget();
-  tflite::MicroProfiler profiler;
+  tflite_micro::InitializeTarget();
+  tflite_micro::MicroProfiler profiler;
 
   uint32_t event_handle = profiler.BeginEvent("InitializeKeywordRunner");
-  tflite::KeywordBenchmarkRunner* benchmark_runner =
+  tflite_micro::KeywordBenchmarkRunner* benchmark_runner =
       CreateBenchmarkRunner(&profiler);
   profiler.EndEvent(event_handle);
   profiler.Log();
   MicroPrintf("");  // null MicroPrintf serves as a newline.
 
-  tflite::KeywordRunNIerations(1, "KeywordRunNIerations(1)", *benchmark_runner,
+  tflite_micro::KeywordRunNIerations(1, "KeywordRunNIerations(1)", *benchmark_runner,
                                profiler);
   profiler.Log();
   MicroPrintf("");  // null MicroPrintf serves as a newline.
 
-  tflite::KeywordRunNIerations(10, "KeywordRunNIerations(10)",
+  tflite_micro::KeywordRunNIerations(10, "KeywordRunNIerations(10)",
                                *benchmark_runner, profiler);
   MicroPrintf("");  // null MicroPrintf serves as a newline.
 
diff --git a/tensorflow/lite/micro/benchmarks/keyword_benchmark_8bit.cc b/tensorflow/lite/micro/benchmarks/keyword_benchmark_8bit.cc
index e5928509..16e9388a 100644
--- a/tensorflow/lite/micro/benchmarks/keyword_benchmark_8bit.cc
+++ b/tensorflow/lite/micro/benchmarks/keyword_benchmark_8bit.cc
@@ -33,7 +33,7 @@ limitations under the License.
  * weights and parameters are not representative of the original model.
  */
 
-namespace tflite {
+namespace tflite_micro {
 
 using KeywordBenchmarkRunner = MicroBenchmarkRunner<int16_t>;
 using KeywordOpResolver = MicroMutableOpResolver<6>;
@@ -53,10 +53,10 @@ KeywordBenchmarkRunner* CreateBenchmarkRunner(MicroProfiler* profiler) {
   // We allocate the KeywordOpResolver from a global buffer because the object's
   // lifetime must exceed that of the KeywordBenchmarkRunner object.
   KeywordOpResolver* op_resolver = new (op_resolver_buffer) KeywordOpResolver();
-  op_resolver->AddFullyConnected(tflite::Register_FULLY_CONNECTED_INT8());
+  op_resolver->AddFullyConnected(tflite_micro::Register_FULLY_CONNECTED_INT8());
   op_resolver->AddQuantize();
-  op_resolver->AddSoftmax(tflite::Register_SOFTMAX_INT8_INT16());
-  op_resolver->AddSvdf(tflite::Register_SVDF_INT8());
+  op_resolver->AddSoftmax(tflite_micro::Register_SOFTMAX_INT8_INT16());
+  op_resolver->AddSvdf(tflite_micro::Register_SVDF_INT8());
 
   return new (benchmark_runner_buffer)
       KeywordBenchmarkRunner(g_keyword_scrambled_8bit_model_data, op_resolver,
@@ -76,25 +76,25 @@ void KeywordRunNIerations(int iterations, const char* tag,
   MicroPrintf("%s took %d ticks (%d ms)", tag, ticks, TicksToMs(ticks));
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 int main(int argc, char** argv) {
-  tflite::InitializeTarget();
-  tflite::MicroProfiler profiler;
+  tflite_micro::InitializeTarget();
+  tflite_micro::MicroProfiler profiler;
 
   uint32_t event_handle = profiler.BeginEvent("InitializeKeywordRunner");
-  tflite::KeywordBenchmarkRunner* benchmark_runner =
+  tflite_micro::KeywordBenchmarkRunner* benchmark_runner =
       CreateBenchmarkRunner(&profiler);
   profiler.EndEvent(event_handle);
   profiler.Log();
   MicroPrintf("");  // null MicroPrintf serves as a newline.
 
-  tflite::KeywordRunNIerations(1, "KeywordRunNIerations(1)", *benchmark_runner,
+  tflite_micro::KeywordRunNIerations(1, "KeywordRunNIerations(1)", *benchmark_runner,
                                profiler);
   profiler.Log();
   MicroPrintf("");  // null MicroPrintf serves as a newline.
 
-  tflite::KeywordRunNIerations(10, "KeywordRunNIerations(10)",
+  tflite_micro::KeywordRunNIerations(10, "KeywordRunNIerations(10)",
                                *benchmark_runner, profiler);
   MicroPrintf("");  // null MicroPrintf serves as a newline.
 
diff --git a/tensorflow/lite/micro/benchmarks/micro_benchmark.h b/tensorflow/lite/micro/benchmarks/micro_benchmark.h
index 6ade682d..01c06a4a 100644
--- a/tensorflow/lite/micro/benchmarks/micro_benchmark.h
+++ b/tensorflow/lite/micro/benchmarks/micro_benchmark.h
@@ -25,7 +25,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_time.h"
 #include "tensorflow/lite/micro/recording_micro_interpreter.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 template <typename inputT>
 class MicroBenchmarkRunner {
@@ -33,7 +33,7 @@ class MicroBenchmarkRunner {
   // The lifetimes of model, op_resolver, tensor_arena, profiler must exceed
   // that of the created MicroBenchmarkRunner object.
   MicroBenchmarkRunner(const uint8_t* model,
-                       const tflite::MicroOpResolver* op_resolver,
+                       const tflite_micro::MicroOpResolver* op_resolver,
                        uint8_t* tensor_arena, int tensor_arena_size,
                        MicroProfilerInterface* profiler,
                        int num_resource_variables = 0)
@@ -63,7 +63,7 @@ class MicroBenchmarkRunner {
 
     // Pre-populate input tensor with random values.
     int input_length = input->bytes / sizeof(inputT);
-    inputT* input_values = tflite::GetTensorData<inputT>(input);
+    inputT* input_values = tflite_micro::GetTensorData<inputT>(input);
     for (int i = 0; i < input_length; i++) {
       // Pre-populate input tensor with a random value based on a constant seed.
       input_values[i] = static_cast<inputT>(
@@ -74,7 +74,7 @@ class MicroBenchmarkRunner {
 
   void SetInput(const inputT* custom_input, int input_index = 0) {
     TfLiteTensor* input = interpreter_.input(input_index);
-    inputT* input_buffer = tflite::GetTensorData<inputT>(input);
+    inputT* input_buffer = tflite_micro::GetTensorData<inputT>(input);
     int input_length = input->bytes / sizeof(inputT);
     for (int i = 0; i < input_length; i++) {
       input_buffer[i] = custom_input[i];
@@ -86,10 +86,10 @@ class MicroBenchmarkRunner {
   }
 
  private:
-  tflite::RecordingMicroAllocator* allocator_;
-  tflite::RecordingMicroInterpreter interpreter_;
+  tflite_micro::RecordingMicroAllocator* allocator_;
+  tflite_micro::RecordingMicroInterpreter interpreter_;
 };
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_BENCHMARKS_MICRO_BENCHMARK_H_
diff --git a/tensorflow/lite/micro/benchmarks/person_detection_benchmark.cc b/tensorflow/lite/micro/benchmarks/person_detection_benchmark.cc
index e21789bb..b8b0605b 100644
--- a/tensorflow/lite/micro/benchmarks/person_detection_benchmark.cc
+++ b/tensorflow/lite/micro/benchmarks/person_detection_benchmark.cc
@@ -35,7 +35,7 @@ limitations under the License.
  * exmaples/person_detection.
  */
 
-namespace tflite {
+namespace tflite_micro {
 
 using PersonDetectionOpResolver = MicroMutableOpResolver<6>;
 using PersonDetectionBenchmarkRunner = MicroBenchmarkRunner<int8_t>;
@@ -57,11 +57,11 @@ PersonDetectionBenchmarkRunner* CreateBenchmarkRunner(MicroProfiler* profiler) {
   // PersonDetectionBenchmarkRunner object.
   PersonDetectionOpResolver* op_resolver =
       new (op_resolver_buffer) PersonDetectionOpResolver();
-  op_resolver->AddFullyConnected(tflite::Register_FULLY_CONNECTED_INT8());
-  op_resolver->AddConv2D(tflite::Register_CONV_2D_INT8REF());
+  op_resolver->AddFullyConnected(tflite_micro::Register_FULLY_CONNECTED_INT8());
+  op_resolver->AddConv2D(tflite_micro::Register_CONV_2D_INT8REF());
   op_resolver->AddDepthwiseConv2D();
   op_resolver->AddSoftmax();
-  op_resolver->AddAveragePool2D(tflite::Register_AVERAGE_POOL_2D_INT8());
+  op_resolver->AddAveragePool2D(tflite_micro::Register_AVERAGE_POOL_2D_INT8());
   op_resolver->AddReshape();
   return new (benchmark_runner_buffer)
       PersonDetectionBenchmarkRunner(g_person_detect_model_data, op_resolver,
@@ -82,38 +82,38 @@ void PersonDetectionNIerations(const int8_t* input, int iterations,
   MicroPrintf("%s took %u ticks (%u ms)", tag, ticks, TicksToMs(ticks));
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 int main(int argc, char** argv) {
-  tflite::InitializeTarget();
+  tflite_micro::InitializeTarget();
 
-  tflite::MicroProfiler profiler;
+  tflite_micro::MicroProfiler profiler;
 
   uint32_t event_handle = profiler.BeginEvent("InitializeBenchmarkRunner");
-  tflite::PersonDetectionBenchmarkRunner* benchmark_runner =
+  tflite_micro::PersonDetectionBenchmarkRunner* benchmark_runner =
       CreateBenchmarkRunner(&profiler);
   profiler.EndEvent(event_handle);
   profiler.Log();
   MicroPrintf("");  // null MicroPrintf serves as a newline.
 
-  tflite::PersonDetectionNIerations(
+  tflite_micro::PersonDetectionNIerations(
       reinterpret_cast<const int8_t*>(g_person_image_data), 1,
       "WithPersonDataIterations(1)", *benchmark_runner, profiler);
   profiler.Log();
   MicroPrintf("");  // null MicroPrintf serves as a newline.
 
-  tflite::PersonDetectionNIerations(
+  tflite_micro::PersonDetectionNIerations(
       reinterpret_cast<const int8_t*>(g_no_person_image_data), 1,
       "NoPersonDataIterations(1)", *benchmark_runner, profiler);
   profiler.Log();
   MicroPrintf("");  // null MicroPrintf serves as a newline.
 
-  tflite::PersonDetectionNIerations(
+  tflite_micro::PersonDetectionNIerations(
       reinterpret_cast<const int8_t*>(g_person_image_data), 10,
       "WithPersonDataIterations(10)", *benchmark_runner, profiler);
   MicroPrintf("");  // null MicroPrintf serves as a newline.
 
-  tflite::PersonDetectionNIerations(
+  tflite_micro::PersonDetectionNIerations(
       reinterpret_cast<const int8_t*>(g_no_person_image_data), 10,
       "NoPersonDataIterations(10)", *benchmark_runner, profiler);
   MicroPrintf("");  // null MicroPrintf serves as a newline.
diff --git a/tensorflow/lite/micro/ceva/system_setup.cc b/tensorflow/lite/micro/ceva/system_setup.cc
index f885b147..ad590a69 100644
--- a/tensorflow/lite/micro/ceva/system_setup.cc
+++ b/tensorflow/lite/micro/ceva/system_setup.cc
@@ -19,7 +19,7 @@ limitations under the License.
 
 #include "tensorflow/lite/micro/micro_time.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 uint32_t ticks_per_second() { return 100e6; }
 
@@ -31,4 +31,4 @@ void InitializeTarget() {
   start_clock();
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/cortex_m_corstone_300/system_setup.cc b/tensorflow/lite/micro/cortex_m_corstone_300/system_setup.cc
index 95a11b2e..ad3d0b86 100644
--- a/tensorflow/lite/micro/cortex_m_corstone_300/system_setup.cc
+++ b/tensorflow/lite/micro/cortex_m_corstone_300/system_setup.cc
@@ -26,7 +26,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_time.h"
 #include "tensorflow/lite/micro/system_setup.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 constexpr uint32_t kClocksPerSecond = 25e6;
@@ -100,4 +100,4 @@ void InitializeTarget() {
 #endif
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/cortex_m_generic/micro_time.cc b/tensorflow/lite/micro/cortex_m_generic/micro_time.cc
index 265bd349..5d39ad3e 100644
--- a/tensorflow/lite/micro/cortex_m_generic/micro_time.cc
+++ b/tensorflow/lite/micro/cortex_m_generic/micro_time.cc
@@ -21,7 +21,7 @@ limitations under the License.
 #include CMSIS_DEVICE_ARM_CORTEX_M_XX_HEADER_FILE
 #endif
 
-namespace tflite {
+namespace tflite_micro {
 
 #if defined(PROJECT_GENERATION)
 
@@ -78,4 +78,4 @@ uint32_t GetCurrentTimeTicks() {
 
 #endif  // defined(PROJECT_GENERATION)
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/docs/memory_management.md b/tensorflow/lite/micro/docs/memory_management.md
index 0a7fd675..1ab350a9 100644
--- a/tensorflow/lite/micro/docs/memory_management.md
+++ b/tensorflow/lite/micro/docs/memory_management.md
@@ -29,9 +29,9 @@ TFLM APIs for loading a model into a shared tensor arena.
 
 The main "working" space for TFLM allocations is inside a single `char` or
 `int8_t` buffer. This buffer can be managed by passing it directly into a
-`tflite::MicroInterpreter` constructor or through a `tflite::MicroAllocator`
-instance that can be passed into a `tflite::MicroInterpreter` constructor.
-Internally, the `tflite::MicroAllocator` classifies allocations into 3 different
+`tflite_micro::MicroInterpreter` constructor or through a `tflite_micro::MicroAllocator`
+instance that can be passed into a `tflite_micro::MicroInterpreter` constructor.
+Internally, the `tflite_micro::MicroAllocator` classifies allocations into 3 different
 sections:
 
 *   **Head** - non-persistent allocations.
@@ -56,10 +56,10 @@ does not allocate small iterative chunks, it can only be set by a specific
 length for the entire section.
 
 This allocation length of this section is managed by the
-`tflite::GreedyMemoryPlanner`. That memory planner looks at the entire graph of
+`tflite_micro::GreedyMemoryPlanner`. That memory planner looks at the entire graph of
 a model and tries to reuse as many buffers as possible to create the smallest
 length for the head. The Tensor buffers for this section can be accessed via a
-`TfLiteEvalTensor` or `TfLiteTensor` instance on the `tflite::MicroInterpreter`.
+`TfLiteEvalTensor` or `TfLiteTensor` instance on the `tflite_micro::MicroInterpreter`.
 
 #### Offline planned tensor allocations
 
@@ -70,7 +70,7 @@ allocation plan is added to model metadata. See format below.
 For each non-constant tensor in the `tensors:[Tensor]` list of the subgraph, a
 byte offset to the start of the head section of the memory arena is given. -1
 indicates that the tensor will be allocated at runtime by the
-`tflite::GreedyMemoryPlanner`. The offline plan is permitted to overlap buffers
+`tflite_micro::GreedyMemoryPlanner`. The offline plan is permitted to overlap buffers
 if it knows that the data will not be used at the same time.
 
 The offline tensor allocation plan will be encoded in the `metadata:[Metadata]`
@@ -99,7 +99,7 @@ ignored by the micro memory allocator. In case of multiple subgraphs, it assumes
 all tensors for all subgraphs are concatenated: all tensors for the first
 subgraph are first, followed by those of the second subgraph, etc.
 
-The `tflite::GreedyMemoryPlanner` treats the provided offline tensor allocation
+The `tflite_micro::GreedyMemoryPlanner` treats the provided offline tensor allocation
 plan as constant fixed offset to the start of the head section and will attempt
 to fit any other tensors (such as scratch tensors added a runtime using the
 `RequestScratchBufferInArena` API of `TfLiteContext`) around those fixed
@@ -140,8 +140,8 @@ size_t tensor_arena_size = 2048;
 uint8_t tensor_arena[tensor_arena_size];
 
 // Interpreter using the shared tensor arena above:
-tflite::MicroInterpreter interpreter(
-  tflite::GetModel(my_model_data), ops_resolver,
+tflite_micro::MicroInterpreter interpreter(
+  tflite_micro::GetModel(my_model_data), ops_resolver,
   tensor_arena, tensor_arena_size);
 
 // Invoke one time which will allocate internals:
@@ -151,8 +151,8 @@ if (interpreter.Invoke() != kTfLiteOk) {
 ```
 
 Recording API can simply be used by including the `RecordingMicroInterpreter`
-class (`recording_micro_interpreter.h`) and replace `tflite::MicroInterpreter`
-with `tflite::RecordingMicroInterpreter`. The same call to `invoke()` is
+class (`recording_micro_interpreter.h`) and replace `tflite_micro::MicroInterpreter`
+with `tflite_micro::RecordingMicroInterpreter`. The same call to `invoke()` is
 performed, but another call is made to `PrintAllocations()` which will output
 detailed allocation logging:
 
@@ -161,8 +161,8 @@ detailed allocation logging:
 #include "recording_micro_interpreter.h"
 
 // Simply change the class name from 'MicroInterpreter' to 'RecordingMicroInterpreter':
-tflite::RecordingMicroInterpreter interpreter(
-  tflite::GetModel(my_model_data), ops_resolver,
+tflite_micro::RecordingMicroInterpreter interpreter(
+  tflite_micro::GetModel(my_model_data), ops_resolver,
   tensor_arena, tensor_arena_size);
 
 // Invoke one time which will allocate internals:
@@ -200,12 +200,12 @@ More information about each recorded allocation section:
     *   C struct that holds more information than a `TfLiteEvalTensor` struct in
         the graph.
     *   Allocations in this bucket will only show up when accessing tensors from
-        the accessors on `tflite::MicroInterpreter`.
+        the accessors on `tflite_micro::MicroInterpreter`.
 *   'Persistent TfLiteTensor quantization data'
     *   Length of persistent quantization data assigned to persistent
         `TfLiteTensor` structs.
     *   Allocations in this bucket will only show up when accessing tensors from
-        the accessors on `tflite::MicroInterpreter`.
+        the accessors on `tflite_micro::MicroInterpreter`.
 *   'TfLiteTensor variable buffer data'
     *   Length of buffer data from a variable tensor (retains data throughout
         calls to `invoke()`).
diff --git a/tensorflow/lite/micro/docs/offline_memory_plan.md b/tensorflow/lite/micro/docs/offline_memory_plan.md
index 114ce5c6..793d785e 100644
--- a/tensorflow/lite/micro/docs/offline_memory_plan.md
+++ b/tensorflow/lite/micro/docs/offline_memory_plan.md
@@ -65,11 +65,11 @@ Interpreter such as below
 constexpr int kArenaSize = 2*1048;
 uint8_t tensor_arena[kArenaSize];
 
-tflite::NonPersistentMemoryPlannerShim planner(&kOfflineNonPersistentBufferPlan);
+tflite_micro::NonPersistentMemoryPlannerShim planner(&kOfflineNonPersistentBufferPlan);
 
-tflite::MicroAllocator * allocator = tflite::MicroAllocator::Create(
+tflite_micro::MicroAllocator * allocator = tflite_micro::MicroAllocator::Create(
   tensor_arena, arena_size, &planner);
 
-tflite::MicroInterpreter interpreter(model, op_resolver, allocator);
+tflite_micro::MicroInterpreter interpreter(model, op_resolver, allocator);
 ```
 
diff --git a/tensorflow/lite/micro/docs/porting_reference_ops.md b/tensorflow/lite/micro/docs/porting_reference_ops.md
index 1ffb3e3c..1d1f3b6f 100644
--- a/tensorflow/lite/micro/docs/porting_reference_ops.md
+++ b/tensorflow/lite/micro/docs/porting_reference_ops.md
@@ -287,7 +287,7 @@ platform/compiler dependent.
 ## How do I allocate persistent memory?
 Use `TfLiteContext::AllocatePersistentBuffer` to allocate persistent memory.
 Memory allocated by this method will remain valid throughout the lifetime of
-the `tflite::MicroInterpreter` instance.
+the `tflite_micro::MicroInterpreter` instance.
 
 An example code snippet looks like ([leaky_relu.cc](../kernels/leaky_relu.cc)):
 ```C++
@@ -339,8 +339,8 @@ the scope of your operator's `Invoke` method.
 ## Can I resize my input/output tensors?
 No.  The storage space for each input/output tensor is a fixed, calculated value
 determined at the time the TensorFlow Lite (TfLite) model converter is executed.
-During the `Init` phase of the `tflite::MicroInterpreter` all tensor storage is
-allocated by the `tflite::MicroInterpreter` instance, using the calculated values
+During the `Init` phase of the `tflite_micro::MicroInterpreter` all tensor storage is
+allocated by the `tflite_micro::MicroInterpreter` instance, using the calculated values
 of the model converter.
 For more information see: [Memory Allocation Overview](online_memory_allocation_overview.md)
 
@@ -348,15 +348,15 @@ For more information see: [Memory Allocation Overview](online_memory_allocation_
 Yes.  The new shape must not exceed the storage space indicated by the old shape.
 Because tensor shape values may live in memory that is not directly writable
 (ex. Flash, EEPROM, ROM), a special method must be called before modification
-is attempted.  The `tflite::micro::CreateWritableTensorDimsWithCopy` method will
+is attempted.  The `tflite_micro::micro::CreateWritableTensorDimsWithCopy` method will
 move the tensor shape values to guaranteed persistent writable memory.
 
 An example code snippet looks like ([l2_pool_2d.cc](../kernels/l2_pool_2d.cc)):
 ```C++
 // the output variable is a TfLiteTensor*
 TfLiteEvalTensor* output_eval =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
-TF_LITE_ENSURE_OK(context, tflite::micro::CreateWritableTensorDimsWithCopy(
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
+TF_LITE_ENSURE_OK(context, tflite_micro::micro::CreateWritableTensorDimsWithCopy(
                                context, output, output_eval));
 output->dims->data[kBatchRank] = batches;
 output->dims->data[kHeightRank] = out_height;
@@ -366,14 +366,14 @@ output->dims->data[kChannelRank] = channels_out;
 
 ## When can I change the shape of tensors in my operator code?
 Tensor shape values can be modified any time after the
-`tflite::micro::CreateWritableTensorDimsWithCopy` method has been called.
+`tflite_micro::micro::CreateWritableTensorDimsWithCopy` method has been called.
 This means that tensor shape values can be modified within the scope of
 your operator's `Prepare` or `Invoke` methods.
-The `tflite::micro::CreateWritableTensorDimsWithCopy` method may
+The `tflite_micro::micro::CreateWritableTensorDimsWithCopy` method may
 only be called within the scope of your operator's `Prepare` method.
 
 ## Can I modify a `TfLiteTensor` or `TfLiteEvalTensor`?
-No.  The `tflite::MicroInterpreter` is the owner and manipulator of these data
+No.  The `tflite_micro::MicroInterpreter` is the owner and manipulator of these data
 structures.  Your code should not modify these data structures.  The only
 directly allowed modification of tensors is to change their data values, or
 their shape values.
diff --git a/tensorflow/lite/micro/docs/profiling.md b/tensorflow/lite/micro/docs/profiling.md
index 16c6b519..d44a3fd4 100644
--- a/tensorflow/lite/micro/docs/profiling.md
+++ b/tensorflow/lite/micro/docs/profiling.md
@@ -24,7 +24,7 @@ from within operator kernels and other TFLite Micro routines.
 ## API
 
 The MicroInterpreter class constructor contains an optional profiler argument.
-This profiler must be an instance of the tflite::Profiler class, and should
+This profiler must be an instance of the tflite_micro::Profiler class, and should
 implement the BeginEvent and EndEvent methods. There is a default implementation
 in tensorflow/lite/micro/micro_profiler.cc which can be used for most purposes.
 
diff --git a/tensorflow/lite/micro/docs/rfc/002_16x8_quantization_port.md b/tensorflow/lite/micro/docs/rfc/002_16x8_quantization_port.md
index 4fd369b1..421806cc 100644
--- a/tensorflow/lite/micro/docs/rfc/002_16x8_quantization_port.md
+++ b/tensorflow/lite/micro/docs/rfc/002_16x8_quantization_port.md
@@ -87,11 +87,11 @@ For example, the following could be registered:
 
 ```
 // Support for all datatypes
-op_resolver->AddFullyConnected(tflite::Register_FULLY_CONNECTED);
+op_resolver->AddFullyConnected(tflite_micro::Register_FULLY_CONNECTED);
 // Support for 8 bit quantized models
-op_resolver->AddFullyConnected(tflite::Register_FULLY_CONNECTED_INT8);
+op_resolver->AddFullyConnected(tflite_micro::Register_FULLY_CONNECTED_INT8);
 // Support for 16x8 quantized models
-op_resolver->AddFullyConnected(tflite::Register_FULLY_CONNECTED_INT16X8());
+op_resolver->AddFullyConnected(tflite_micro::Register_FULLY_CONNECTED_INT16X8());
 ```
 
 This means that kernels not currently using this registration API will need to be refactored to use it. Currently only **FullyConnected** uses the API.
diff --git a/tensorflow/lite/micro/examples/hello_world/hello_world_test.cc b/tensorflow/lite/micro/examples/hello_world/hello_world_test.cc
index 1d8be4bc..a0d3449b 100644
--- a/tensorflow/lite/micro/examples/hello_world/hello_world_test.cc
+++ b/tensorflow/lite/micro/examples/hello_world/hello_world_test.cc
@@ -27,7 +27,7 @@ limitations under the License.
 #include "tensorflow/lite/schema/schema_generated.h"
 
 namespace {
-using HelloWorldOpResolver = tflite::MicroMutableOpResolver<1>;
+using HelloWorldOpResolver = tflite_micro::MicroMutableOpResolver<1>;
 
 TfLiteStatus RegisterOps(HelloWorldOpResolver& op_resolver) {
   TF_LITE_ENSURE_STATUS(op_resolver.AddFullyConnected());
@@ -36,7 +36,7 @@ TfLiteStatus RegisterOps(HelloWorldOpResolver& op_resolver) {
 }  // namespace
 
 TfLiteStatus ProfileMemoryAndLatency() {
-  tflite::MicroProfiler profiler;
+  tflite_micro::MicroProfiler profiler;
   HelloWorldOpResolver op_resolver;
   TF_LITE_ENSURE_STATUS(RegisterOps(op_resolver));
 
@@ -46,11 +46,11 @@ TfLiteStatus ProfileMemoryAndLatency() {
   uint8_t tensor_arena[kTensorArenaSize];
   constexpr int kNumResourceVariables = 24;
 
-  tflite::RecordingMicroAllocator* allocator(
-      tflite::RecordingMicroAllocator::Create(tensor_arena, kTensorArenaSize));
-  tflite::RecordingMicroInterpreter interpreter(
-      tflite::GetModel(g_hello_world_float_model_data), op_resolver, allocator,
-      tflite::MicroResourceVariables::Create(allocator, kNumResourceVariables),
+  tflite_micro::RecordingMicroAllocator* allocator(
+      tflite_micro::RecordingMicroAllocator::Create(tensor_arena, kTensorArenaSize));
+  tflite_micro::RecordingMicroInterpreter interpreter(
+      tflite_micro::GetModel(g_hello_world_float_model_data), op_resolver, allocator,
+      tflite_micro::MicroResourceVariables::Create(allocator, kNumResourceVariables),
       &profiler);
 
   TF_LITE_ENSURE_STATUS(interpreter.AllocateTensors());
@@ -67,8 +67,8 @@ TfLiteStatus ProfileMemoryAndLatency() {
 }
 
 TfLiteStatus LoadFloatModelAndPerformInference() {
-  const tflite::Model* model =
-      ::tflite::GetModel(g_hello_world_float_model_data);
+  const tflite_micro::Model* model =
+      ::tflite_micro::GetModel(g_hello_world_float_model_data);
   TFLITE_CHECK_EQ(model->version(), TFLITE_SCHEMA_VERSION);
 
   HelloWorldOpResolver op_resolver;
@@ -79,7 +79,7 @@ TfLiteStatus LoadFloatModelAndPerformInference() {
   constexpr int kTensorArenaSize = 3000;
   uint8_t tensor_arena[kTensorArenaSize];
 
-  tflite::MicroInterpreter interpreter(model, op_resolver, tensor_arena,
+  tflite_micro::MicroInterpreter interpreter(model, op_resolver, tensor_arena,
                                        kTensorArenaSize);
   TF_LITE_ENSURE_STATUS(interpreter.AllocateTensors());
 
@@ -102,8 +102,8 @@ TfLiteStatus LoadFloatModelAndPerformInference() {
 TfLiteStatus LoadQuantModelAndPerformInference() {
   // Map the model into a usable data structure. This doesn't involve any
   // copying or parsing, it's a very lightweight operation.
-  const tflite::Model* model =
-      ::tflite::GetModel(g_hello_world_int8_model_data);
+  const tflite_micro::Model* model =
+      ::tflite_micro::GetModel(g_hello_world_int8_model_data);
   TFLITE_CHECK_EQ(model->version(), TFLITE_SCHEMA_VERSION);
 
   HelloWorldOpResolver op_resolver;
@@ -114,7 +114,7 @@ TfLiteStatus LoadQuantModelAndPerformInference() {
   constexpr int kTensorArenaSize = 3000;
   uint8_t tensor_arena[kTensorArenaSize];
 
-  tflite::MicroInterpreter interpreter(model, op_resolver, tensor_arena,
+  tflite_micro::MicroInterpreter interpreter(model, op_resolver, tensor_arena,
                                        kTensorArenaSize);
 
   TF_LITE_ENSURE_STATUS(interpreter.AllocateTensors());
@@ -150,7 +150,7 @@ TfLiteStatus LoadQuantModelAndPerformInference() {
 }
 
 int main(int argc, char* argv[]) {
-  tflite::InitializeTarget();
+  tflite_micro::InitializeTarget();
   TF_LITE_ENSURE_STATUS(ProfileMemoryAndLatency());
   TF_LITE_ENSURE_STATUS(LoadFloatModelAndPerformInference());
   TF_LITE_ENSURE_STATUS(LoadQuantModelAndPerformInference());
diff --git a/tensorflow/lite/micro/examples/memory_footprint/README.md b/tensorflow/lite/micro/examples/memory_footprint/README.md
index bf75ae23..f9f0c24a 100644
--- a/tensorflow/lite/micro/examples/memory_footprint/README.md
+++ b/tensorflow/lite/micro/examples/memory_footprint/README.md
@@ -110,15 +110,15 @@ snipet shows how to do so using the keyword detection as an example:
   KeywordOpResolver* op_resolver = new (op_resolver_buffer) KeywordOpResolver();
 
   // Only add the required kernel
-  op_resolver->AddFullyConnected(tflite::Register_FULLY_CONNECTED_INT8());
+  op_resolver->AddFullyConnected(tflite_micro::Register_FULLY_CONNECTED_INT8());
   op_resolver->AddQuantize();
-  op_resolver->AddSoftmax(tflite::Register_SOFTMAX_INT8_INT16());
-  op_resolver->AddSvdf(tflite::Register_SVDF_INT8());
+  op_resolver->AddSoftmax(tflite_micro::Register_SOFTMAX_INT8_INT16());
+  op_resolver->AddSvdf(tflite_micro::Register_SVDF_INT8());
 
   ...
 
   // Pass the OpResolver to the interpreter
-  tflite::MicroInterpreter * interpreter = tflite::MicroInterpeter::Create(
+  tflite_micro::MicroInterpreter * interpreter = tflite_micro::MicroInterpeter::Create(
       g_keyword_scrambled_model_data, op_resolver, tensor_arena, kTensorArenaSize, profiler);
 ```
 
diff --git a/tensorflow/lite/micro/examples/memory_footprint/baseline_memory_footprint.cc b/tensorflow/lite/micro/examples/memory_footprint/baseline_memory_footprint.cc
index 7a934b76..79fc3c26 100644
--- a/tensorflow/lite/micro/examples/memory_footprint/baseline_memory_footprint.cc
+++ b/tensorflow/lite/micro/examples/memory_footprint/baseline_memory_footprint.cc
@@ -18,4 +18,4 @@ limitations under the License.
 // (interpreter, memory planner etc). This is used to measure the bare minimum
 // application code size of a specific target platform without the TFLM
 // Framework. Please see README.md for more information.
-int main(int argc, char** argv) { tflite::InitializeTarget(); }
+int main(int argc, char** argv) { tflite_micro::InitializeTarget(); }
diff --git a/tensorflow/lite/micro/examples/memory_footprint/interpreter_memory_footprint.cc b/tensorflow/lite/micro/examples/memory_footprint/interpreter_memory_footprint.cc
index 036429b3..c83bc1bd 100644
--- a/tensorflow/lite/micro/examples/memory_footprint/interpreter_memory_footprint.cc
+++ b/tensorflow/lite/micro/examples/memory_footprint/interpreter_memory_footprint.cc
@@ -23,8 +23,8 @@ limitations under the License.
 
 // Use MicroBenchmarkRunner to avoid boiler plate code and more easily compare
 // the size with other benchmarks such as keyword_benchmark.
-using InterpreterMemoryFootprintRunner = tflite::MicroBenchmarkRunner<int16_t>;
-using InterpreterMemoryFootprintOpResolver = tflite::MicroMutableOpResolver<6>;
+using InterpreterMemoryFootprintRunner = tflite_micro::MicroBenchmarkRunner<int16_t>;
+using InterpreterMemoryFootprintOpResolver = tflite_micro::MicroMutableOpResolver<6>;
 
 // This binary includes the TFLM Framework (interpreter, memory planner etc),
 // but without any kernels.  This is used to measure the code size of the TFLM
@@ -36,8 +36,8 @@ int main(int argc, char** argv) {
   alignas(16) uint8_t tensor_arena[kTensorArenaSize];
   uint8_t runner_buffer[sizeof(InterpreterMemoryFootprintRunner)];
 
-  tflite::InitializeTarget();
-  tflite::MicroProfiler profiler;
+  tflite_micro::InitializeTarget();
+  tflite_micro::MicroProfiler profiler;
 
   InterpreterMemoryFootprintOpResolver op_resolver;
 
diff --git a/tensorflow/lite/micro/examples/micro_speech/micro_speech_test.cc b/tensorflow/lite/micro/examples/micro_speech/micro_speech_test.cc
index f31728c3..3b17ce71 100644
--- a/tensorflow/lite/micro/examples/micro_speech/micro_speech_test.cc
+++ b/tensorflow/lite/micro/examples/micro_speech/micro_speech_test.cc
@@ -55,8 +55,8 @@ constexpr int kAudioSampleDurationCount =
 constexpr int kAudioSampleStrideCount =
     kFeatureStrideMs * kAudioSampleFrequency / 1000;
 
-using MicroSpeechOpResolver = tflite::MicroMutableOpResolver<4>;
-using AudioPreprocessorOpResolver = tflite::MicroMutableOpResolver<18>;
+using MicroSpeechOpResolver = tflite_micro::MicroMutableOpResolver<4>;
+using AudioPreprocessorOpResolver = tflite_micro::MicroMutableOpResolver<18>;
 
 TfLiteStatus RegisterOps(MicroSpeechOpResolver& op_resolver) {
   TF_LITE_ENSURE_STATUS(op_resolver.AddReshape());
@@ -92,8 +92,8 @@ TfLiteStatus LoadMicroSpeechModelAndPerformInference(
     const Features& features, const char* expected_label) {
   // Map the model into a usable data structure. This doesn't involve any
   // copying or parsing, it's a very lightweight operation.
-  const tflite::Model* model =
-      tflite::GetModel(g_micro_speech_quantized_model_data);
+  const tflite_micro::Model* model =
+      tflite_micro::GetModel(g_micro_speech_quantized_model_data);
   TF_LITE_MICRO_EXPECT(model->version() == TFLITE_SCHEMA_VERSION);
   TF_LITE_MICRO_CHECK_FAIL();
 
@@ -101,7 +101,7 @@ TfLiteStatus LoadMicroSpeechModelAndPerformInference(
   TF_LITE_MICRO_EXPECT(RegisterOps(op_resolver) == kTfLiteOk);
   TF_LITE_MICRO_CHECK_FAIL();
 
-  tflite::MicroInterpreter interpreter(model, op_resolver, g_arena, kArenaSize);
+  tflite_micro::MicroInterpreter interpreter(model, op_resolver, g_arena, kArenaSize);
 
   TF_LITE_MICRO_EXPECT(interpreter.AllocateTensors() == kTfLiteOk);
   TF_LITE_MICRO_CHECK_FAIL();
@@ -129,7 +129,7 @@ TfLiteStatus LoadMicroSpeechModelAndPerformInference(
   int output_zero_point = output->params.zero_point;
 
   std::copy_n(&features[0][0], kFeatureElementCount,
-              tflite::GetTensorData<int8_t>(input));
+              tflite_micro::GetTensorData<int8_t>(input));
   TF_LITE_MICRO_EXPECT(interpreter.Invoke() == kTfLiteOk);
   TF_LITE_MICRO_CHECK_FAIL();
 
@@ -138,7 +138,7 @@ TfLiteStatus LoadMicroSpeechModelAndPerformInference(
   MicroPrintf("MicroSpeech category predictions for <%s>", expected_label);
   for (int i = 0; i < kCategoryCount; i++) {
     category_predictions[i] =
-        (tflite::GetTensorData<int8_t>(output)[i] - output_zero_point) *
+        (tflite_micro::GetTensorData<int8_t>(output)[i] - output_zero_point) *
         output_scale;
     MicroPrintf("  %.4f %s", static_cast<double>(category_predictions[i]),
                 kCategoryLabels[i]);
@@ -157,7 +157,7 @@ TfLiteStatus LoadMicroSpeechModelAndPerformInference(
 TfLiteStatus GenerateSingleFeature(const int16_t* audio_data,
                                    const int audio_data_size,
                                    int8_t* feature_output,
-                                   tflite::MicroInterpreter* interpreter) {
+                                   tflite_micro::MicroInterpreter* interpreter) {
   TfLiteTensor* input = interpreter->input(0);
   TF_LITE_MICRO_EXPECT(input != nullptr);
   TF_LITE_MICRO_CHECK_FAIL();
@@ -177,10 +177,10 @@ TfLiteStatus GenerateSingleFeature(const int16_t* audio_data,
   TF_LITE_MICRO_CHECK_FAIL();
 
   std::copy_n(audio_data, audio_data_size,
-              tflite::GetTensorData<int16_t>(input));
+              tflite_micro::GetTensorData<int16_t>(input));
   TF_LITE_MICRO_EXPECT(interpreter->Invoke() == kTfLiteOk);
   TF_LITE_MICRO_CHECK_FAIL();
-  std::copy_n(tflite::GetTensorData<int8_t>(output), kFeatureSize,
+  std::copy_n(tflite_micro::GetTensorData<int8_t>(output), kFeatureSize,
               feature_output);
 
   return kTfLiteOk;
@@ -191,8 +191,8 @@ TfLiteStatus GenerateFeatures(const int16_t* audio_data,
                               Features* features_output) {
   // Map the model into a usable data structure. This doesn't involve any
   // copying or parsing, it's a very lightweight operation.
-  const tflite::Model* model =
-      tflite::GetModel(g_audio_preprocessor_int8_model_data);
+  const tflite_micro::Model* model =
+      tflite_micro::GetModel(g_audio_preprocessor_int8_model_data);
   TF_LITE_MICRO_EXPECT(model->version() == TFLITE_SCHEMA_VERSION);
   TF_LITE_MICRO_CHECK_FAIL();
 
@@ -200,7 +200,7 @@ TfLiteStatus GenerateFeatures(const int16_t* audio_data,
   TF_LITE_MICRO_EXPECT(RegisterOps(op_resolver) == kTfLiteOk);
   TF_LITE_MICRO_CHECK_FAIL();
 
-  tflite::MicroInterpreter interpreter(model, op_resolver, g_arena, kArenaSize);
+  tflite_micro::MicroInterpreter interpreter(model, op_resolver, g_arena, kArenaSize);
 
   TF_LITE_MICRO_EXPECT(interpreter.AllocateTensors() == kTfLiteOk);
   TF_LITE_MICRO_CHECK_FAIL();
diff --git a/tensorflow/lite/micro/examples/network_tester/network_tester_test.cc b/tensorflow/lite/micro/examples/network_tester/network_tester_test.cc
index e62e0c42..e6b3d255 100644
--- a/tensorflow/lite/micro/examples/network_tester/network_tester_test.cc
+++ b/tensorflow/lite/micro/examples/network_tester/network_tester_test.cc
@@ -73,7 +73,7 @@ inline void print_output_data(TfLiteTensor* output) {
 template <typename T>
 void check_output_elem(TfLiteTensor* output, const T* expected_output,
                        const int index) {
-  TF_LITE_MICRO_EXPECT_EQ(tflite::GetTensorData<T>(output)[index],
+  TF_LITE_MICRO_EXPECT_EQ(tflite_micro::GetTensorData<T>(output)[index],
                           expected_output[index]);
 }
 
@@ -81,9 +81,9 @@ TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(TestInvoke) {
 #ifdef ETHOS_U
-  const tflite::Model* model = ::tflite::GetModel(g_person_detect_model_data);
+  const tflite_micro::Model* model = ::tflite_micro::GetModel(g_person_detect_model_data);
 #else
-  const tflite::Model* model = ::tflite::GetModel(network_model);
+  const tflite_micro::Model* model = ::tflite_micro::GetModel(network_model);
 #endif
   if (model->version() != TFLITE_SCHEMA_VERSION) {
     MicroPrintf(
@@ -93,15 +93,15 @@ TF_LITE_MICRO_TEST(TestInvoke) {
     return kTfLiteError;
   }
 
-  tflite::MicroMutableOpResolver<6> resolver;
-  resolver.AddAveragePool2D(tflite::Register_AVERAGE_POOL_2D_INT8());
-  resolver.AddConv2D(tflite::Register_CONV_2D_INT8());
-  resolver.AddDepthwiseConv2D(tflite::Register_DEPTHWISE_CONV_2D_INT8());
+  tflite_micro::MicroMutableOpResolver<6> resolver;
+  resolver.AddAveragePool2D(tflite_micro::Register_AVERAGE_POOL_2D_INT8());
+  resolver.AddConv2D(tflite_micro::Register_CONV_2D_INT8());
+  resolver.AddDepthwiseConv2D(tflite_micro::Register_DEPTHWISE_CONV_2D_INT8());
   resolver.AddEthosU();
   resolver.AddReshape();
-  resolver.AddSoftmax(tflite::Register_SOFTMAX_INT8());
+  resolver.AddSoftmax(tflite_micro::Register_SOFTMAX_INT8());
 
-  tflite::MicroInterpreter interpreter(model, resolver, tensor_arena,
+  tflite_micro::MicroInterpreter interpreter(model, resolver, tensor_arena,
                                        TENSOR_ARENA_SIZE);
 
   TfLiteStatus allocate_status = interpreter.AllocateTensors();
@@ -146,7 +146,7 @@ TF_LITE_MICRO_TEST(TestInvoke) {
 #ifndef NO_COMPARE_OUTPUT_DATA
     for (size_t i = 0; i < interpreter.outputs_size(); i++) {
       TfLiteTensor* output = interpreter.output(i);
-      for (int j = 0; j < tflite::ElementCount(*(output->dims)); ++j) {
+      for (int j = 0; j < tflite_micro::ElementCount(*(output->dims)); ++j) {
         check_output_elem(output, &expected_output_data[i], j);
       }
     }
diff --git a/tensorflow/lite/micro/examples/person_detection/main_functions.cc b/tensorflow/lite/micro/examples/person_detection/main_functions.cc
index f91a1a38..4d878e45 100644
--- a/tensorflow/lite/micro/examples/person_detection/main_functions.cc
+++ b/tensorflow/lite/micro/examples/person_detection/main_functions.cc
@@ -27,8 +27,8 @@ limitations under the License.
 
 // Globals, used for compatibility with Arduino-style sketches.
 namespace {
-const tflite::Model* model = nullptr;
-tflite::MicroInterpreter* interpreter = nullptr;
+const tflite_micro::Model* model = nullptr;
+tflite_micro::MicroInterpreter* interpreter = nullptr;
 TfLiteTensor* input = nullptr;
 
 // In order to use optimized tensorflow lite kernels, a signed int8_t quantized
@@ -45,11 +45,11 @@ alignas(16) static uint8_t tensor_arena[kTensorArenaSize];
 
 // The name of this function is important for Arduino compatibility.
 void setup() {
-  tflite::InitializeTarget();
+  tflite_micro::InitializeTarget();
 
   // Map the model into a usable data structure. This doesn't involve any
   // copying or parsing, it's a very lightweight operation.
-  model = tflite::GetModel(g_person_detect_model_data);
+  model = tflite_micro::GetModel(g_person_detect_model_data);
   if (model->version() != TFLITE_SCHEMA_VERSION) {
     MicroPrintf(
         "Model provided is schema version %d not equal "
@@ -62,17 +62,17 @@ void setup() {
   // This relies on a complete list of all the ops needed by this graph.
 
   // NOLINTNEXTLINE(runtime-global-variables)
-  static tflite::MicroMutableOpResolver<5> micro_op_resolver;
-  micro_op_resolver.AddAveragePool2D(tflite::Register_AVERAGE_POOL_2D_INT8());
-  micro_op_resolver.AddConv2D(tflite::Register_CONV_2D_INT8());
+  static tflite_micro::MicroMutableOpResolver<5> micro_op_resolver;
+  micro_op_resolver.AddAveragePool2D(tflite_micro::Register_AVERAGE_POOL_2D_INT8());
+  micro_op_resolver.AddConv2D(tflite_micro::Register_CONV_2D_INT8());
   micro_op_resolver.AddDepthwiseConv2D(
-      tflite::Register_DEPTHWISE_CONV_2D_INT8());
+      tflite_micro::Register_DEPTHWISE_CONV_2D_INT8());
   micro_op_resolver.AddReshape();
-  micro_op_resolver.AddSoftmax(tflite::Register_SOFTMAX_INT8());
+  micro_op_resolver.AddSoftmax(tflite_micro::Register_SOFTMAX_INT8());
 
   // Build an interpreter to run the model with.
   // NOLINTNEXTLINE(runtime-global-variables)
-  static tflite::MicroInterpreter static_interpreter(
+  static tflite_micro::MicroInterpreter static_interpreter(
       model, micro_op_resolver, tensor_arena, kTensorArenaSize);
   interpreter = &static_interpreter;
 
diff --git a/tensorflow/lite/micro/examples/person_detection/person_detection_test.cc b/tensorflow/lite/micro/examples/person_detection/person_detection_test.cc
index 679c26e5..47d62343 100644
--- a/tensorflow/lite/micro/examples/person_detection/person_detection_test.cc
+++ b/tensorflow/lite/micro/examples/person_detection/person_detection_test.cc
@@ -37,7 +37,7 @@ TF_LITE_MICRO_TESTS_BEGIN
 TF_LITE_MICRO_TEST(TestInvoke) {
   // Map the model into a usable data structure. This doesn't involve any
   // copying or parsing, it's a very lightweight operation.
-  const tflite::Model* model = ::tflite::GetModel(g_person_detect_model_data);
+  const tflite_micro::Model* model = ::tflite_micro::GetModel(g_person_detect_model_data);
   if (model->version() != TFLITE_SCHEMA_VERSION) {
     MicroPrintf(
         "Model provided is schema version %d not equal "
@@ -50,16 +50,16 @@ TF_LITE_MICRO_TEST(TestInvoke) {
   // An easier approach is to just use the AllOpsResolver, but this will
   // incur some penalty in code space for op implementations that are not
   // needed by this graph.
-  tflite::MicroMutableOpResolver<5> micro_op_resolver;
-  micro_op_resolver.AddAveragePool2D(tflite::Register_AVERAGE_POOL_2D_INT8());
-  micro_op_resolver.AddConv2D(tflite::Register_CONV_2D_INT8());
+  tflite_micro::MicroMutableOpResolver<5> micro_op_resolver;
+  micro_op_resolver.AddAveragePool2D(tflite_micro::Register_AVERAGE_POOL_2D_INT8());
+  micro_op_resolver.AddConv2D(tflite_micro::Register_CONV_2D_INT8());
   micro_op_resolver.AddDepthwiseConv2D(
-      tflite::Register_DEPTHWISE_CONV_2D_INT8());
+      tflite_micro::Register_DEPTHWISE_CONV_2D_INT8());
   micro_op_resolver.AddReshape();
-  micro_op_resolver.AddSoftmax(tflite::Register_SOFTMAX_INT8());
+  micro_op_resolver.AddSoftmax(tflite_micro::Register_SOFTMAX_INT8());
 
   // Build an interpreter to run the model with.
-  tflite::MicroInterpreter interpreter(model, micro_op_resolver, tensor_arena,
+  tflite_micro::MicroInterpreter interpreter(model, micro_op_resolver, tensor_arena,
                                        tensor_arena_size);
   interpreter.AllocateTensors();
 
diff --git a/tensorflow/lite/micro/fake_micro_context.cc b/tensorflow/lite/micro/fake_micro_context.cc
index 5787ffd0..4c967b2b 100644
--- a/tensorflow/lite/micro/fake_micro_context.cc
+++ b/tensorflow/lite/micro/fake_micro_context.cc
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_arena_constants.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 FakeMicroContext::FakeMicroContext(TfLiteTensor* tensors,
                                    SingleArenaBufferAllocator* allocator,
@@ -112,4 +112,4 @@ void* FakeMicroContext::external_context() { return nullptr; }
 
 MicroGraph& FakeMicroContext::graph() { return graph_; }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/fake_micro_context.h b/tensorflow/lite/micro/fake_micro_context.h
index 46d8a9b1..cd2f9e6c 100644
--- a/tensorflow/lite/micro/fake_micro_context.h
+++ b/tensorflow/lite/micro/fake_micro_context.h
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_context.h"
 #include "tensorflow/lite/micro/micro_graph.h"
 
-namespace tflite {
+namespace tflite_micro {
 // A fake of MicroContext for kernel util tests.
 // TODO(b/272759060): FakeMicroContext currently inherits from MicroContext.
 // Which allow tests to use functions from MicroContext that weren't added to
@@ -65,6 +65,6 @@ class FakeMicroContext : public MicroContext {
   TF_LITE_REMOVE_VIRTUAL_DELETE
 };
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_FAKE_MICRO_CONTEXT_H_
diff --git a/tensorflow/lite/micro/fake_micro_context_test.cc b/tensorflow/lite/micro/fake_micro_context_test.cc
index e792238f..3bfbe9ad 100644
--- a/tensorflow/lite/micro/fake_micro_context_test.cc
+++ b/tensorflow/lite/micro/fake_micro_context_test.cc
@@ -22,12 +22,12 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
-using ::tflite::testing::CreateTensor;
-using ::tflite::testing::IntArrayFromInts;
+using ::tflite_micro::testing::CreateTensor;
+using ::tflite_micro::testing::IntArrayFromInts;
 
-tflite::FakeMicroContext CreateFakeMicroContext(
+tflite_micro::FakeMicroContext CreateFakeMicroContext(
     SingleArenaBufferAllocator* simple_memory_allocator,
     MicroGraph* micro_graph) {
   // Some targets do not support dynamic memory (i.e., no malloc or new), thus,
@@ -44,24 +44,24 @@ tflite::FakeMicroContext CreateFakeMicroContext(
   tensors[0] = CreateTensor(input_data, IntArrayFromInts(input_shape));
   tensors[1] = CreateTensor(output_data, IntArrayFromInts(output_shape));
 
-  tflite::FakeMicroContext fake_micro_context(tensors, simple_memory_allocator,
+  tflite_micro::FakeMicroContext fake_micro_context(tensors, simple_memory_allocator,
                                               micro_graph);
   return fake_micro_context;
 }
 
 }  // namespace
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(TestGetBeforeRequestScratchBufferWouldReturnNull) {
   constexpr size_t kArenaSize = 1024;
   uint8_t arena_buffer[kArenaSize];
-  tflite::SingleArenaBufferAllocator simple_memory_allocator(arena_buffer,
+  tflite_micro::SingleArenaBufferAllocator simple_memory_allocator(arena_buffer,
                                                              kArenaSize);
-  tflite::MockMicroGraph dummy_micro_graph{&simple_memory_allocator};
+  tflite_micro::MockMicroGraph dummy_micro_graph{&simple_memory_allocator};
 
-  tflite::FakeMicroContext micro_context = tflite::CreateFakeMicroContext(
+  tflite_micro::FakeMicroContext micro_context = tflite_micro::CreateFakeMicroContext(
       &simple_memory_allocator, &dummy_micro_graph);
 
   TF_LITE_MICRO_EXPECT(micro_context.GetScratchBuffer(0) == nullptr);
@@ -70,11 +70,11 @@ TF_LITE_MICRO_TEST(TestGetBeforeRequestScratchBufferWouldReturnNull) {
 TF_LITE_MICRO_TEST(TestRequestScratchBufferAndThenGetShouldSucceed) {
   constexpr size_t kArenaSize = 1024;
   uint8_t arena_buffer[kArenaSize];
-  tflite::SingleArenaBufferAllocator simple_memory_allocator(arena_buffer,
+  tflite_micro::SingleArenaBufferAllocator simple_memory_allocator(arena_buffer,
                                                              kArenaSize);
-  tflite::MockMicroGraph dummy_micro_graph{&simple_memory_allocator};
+  tflite_micro::MockMicroGraph dummy_micro_graph{&simple_memory_allocator};
 
-  tflite::FakeMicroContext micro_context = tflite::CreateFakeMicroContext(
+  tflite_micro::FakeMicroContext micro_context = tflite_micro::CreateFakeMicroContext(
       &simple_memory_allocator, &dummy_micro_graph);
 
   constexpr size_t kScratchBufferSize = 16;
diff --git a/tensorflow/lite/micro/flatbuffer_utils.cc b/tensorflow/lite/micro/flatbuffer_utils.cc
index 9996172b..db9d458e 100644
--- a/tensorflow/lite/micro/flatbuffer_utils.cc
+++ b/tensorflow/lite/micro/flatbuffer_utils.cc
@@ -15,7 +15,7 @@ limitations under the License.
 
 #include "tensorflow/lite/micro/flatbuffer_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 FlexbufferWrapper::FlexbufferWrapper(const uint8_t* buffer, size_t size)
     : flexbuffers::Vector(flexbuffers::GetRoot(buffer, size).AsVector()) {}
@@ -81,4 +81,4 @@ TfLiteFloatArray* FlatBufferVectorToTfLiteTypeArray(
       reinterpret_cast<const TfLiteFloatArray*>(flatbuffer_array));
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/flatbuffer_utils.h b/tensorflow/lite/micro/flatbuffer_utils.h
index b4e0cdc2..374be3fa 100644
--- a/tensorflow/lite/micro/flatbuffer_utils.h
+++ b/tensorflow/lite/micro/flatbuffer_utils.h
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/c/common.h"
 #include "tensorflow/lite/schema/schema_generated.h"
 
-namespace tflite {
+namespace tflite_micro {
 // Kernels use flexbuffers::Map to pack their init parameters in a tflite file,
 // with the parameter names as map keys and the parameter values as the
 // corresponding map values.
@@ -60,6 +60,6 @@ TfLiteIntArray* FlatBufferVectorToTfLiteTypeArray(
 TfLiteFloatArray* FlatBufferVectorToTfLiteTypeArray(
     const flatbuffers::Vector<float>* flatbuffer_array);
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // THIRD_PARTY_TFLITE_MICRO_TENSORFLOW_LITE_MICRO_FLATBUFFER_UTILS_H_
diff --git a/tensorflow/lite/micro/flatbuffer_utils_test.cc b/tensorflow/lite/micro/flatbuffer_utils_test.cc
index faebce78..c6dd90b7 100644
--- a/tensorflow/lite/micro/flatbuffer_utils_test.cc
+++ b/tensorflow/lite/micro/flatbuffer_utils_test.cc
@@ -62,7 +62,7 @@ TF_LITE_MICRO_TEST(TestFlexbufferWrapper) {
   });
   fbb.Finish();
   const std::vector<uint8_t> buffer = fbb.GetBuffer();
-  tflite::FlexbufferWrapper wrapper(buffer.data(), buffer.size());
+  tflite_micro::FlexbufferWrapper wrapper(buffer.data(), buffer.size());
   for (int i = 0; i < param_num; i++) {
     std::string& param_value = params[params_sorted[i]].value;
     if (params[params_sorted[i]].type == "Int") {
diff --git a/tensorflow/lite/micro/hexagon/system_setup.cc b/tensorflow/lite/micro/hexagon/system_setup.cc
index 0ce5d180..6783d260 100644
--- a/tensorflow/lite/micro/hexagon/system_setup.cc
+++ b/tensorflow/lite/micro/hexagon/system_setup.cc
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/debug_log.h"
 #include "tensorflow/lite/micro/micro_time.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // Calling this method enables a timer that runs for eternity.
 void InitializeTarget() {
@@ -42,4 +42,4 @@ uint32_t GetCurrentTimeTicks() {
 #endif  // TF_LITE_STRIP_ERROR_STRINGS
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/integration_tests/seanet/add/integration_tests.cc b/tensorflow/lite/micro/integration_tests/seanet/add/integration_tests.cc
index 0141977c..cd8df0f1 100644
--- a/tensorflow/lite/micro/integration_tests/seanet/add/integration_tests.cc
+++ b/tensorflow/lite/micro/integration_tests/seanet/add/integration_tests.cc
@@ -96,7 +96,7 @@ constexpr size_t kTensorArenaSize = 1024 * 100;
 uint8_t tensor_arena[kTensorArenaSize];
 bool print_log = false;
 
-namespace tflite {
+namespace tflite_micro {
 namespace micro {
 namespace {
 
@@ -128,7 +128,7 @@ void RunModel(const uint8_t* model, const int16_t* input0,
 
   TfLiteTensor* output_tensor = interpreter.output(0);
   TF_LITE_MICRO_EXPECT_EQ(output_tensor->bytes, golden_size * sizeof(int16_t));
-  int16_t* output = ::tflite::GetTensorData<int16_t>(output_tensor);
+  int16_t* output = ::tflite_micro::GetTensorData<int16_t>(output_tensor);
   for (uint32_t i = 0; i < golden_size; i++) {
     // TODO(b/205046520): Better understand why TfLite and TFLM can sometimes be
     // off by 1.
@@ -138,12 +138,12 @@ void RunModel(const uint8_t* model, const int16_t* input0,
 
 }  // namespace
 }  // namespace micro
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(add0_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_add0_model_data, g_add0_input0_int16_test_data,
       g_add0_input0_int16_test_data_size, g_add0_input1_int16_test_data,
       g_add0_input1_int16_test_data_size, g_add0_golden_int16_test_data,
@@ -151,7 +151,7 @@ TF_LITE_MICRO_TEST(add0_test) {
 }
 
 TF_LITE_MICRO_TEST(add1_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_add1_model_data, g_add1_input0_int16_test_data,
       g_add1_input0_int16_test_data_size, g_add1_input1_int16_test_data,
       g_add1_input1_int16_test_data_size, g_add1_golden_int16_test_data,
@@ -159,7 +159,7 @@ TF_LITE_MICRO_TEST(add1_test) {
 }
 
 TF_LITE_MICRO_TEST(add2_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_add2_model_data, g_add2_input0_int16_test_data,
       g_add2_input0_int16_test_data_size, g_add2_input1_int16_test_data,
       g_add2_input1_int16_test_data_size, g_add2_golden_int16_test_data,
@@ -167,7 +167,7 @@ TF_LITE_MICRO_TEST(add2_test) {
 }
 
 TF_LITE_MICRO_TEST(add3_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_add3_model_data, g_add3_input0_int16_test_data,
       g_add3_input0_int16_test_data_size, g_add3_input1_int16_test_data,
       g_add3_input1_int16_test_data_size, g_add3_golden_int16_test_data,
@@ -175,7 +175,7 @@ TF_LITE_MICRO_TEST(add3_test) {
 }
 
 TF_LITE_MICRO_TEST(add4_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_add4_model_data, g_add4_input0_int16_test_data,
       g_add4_input0_int16_test_data_size, g_add4_input1_int16_test_data,
       g_add4_input1_int16_test_data_size, g_add4_golden_int16_test_data,
@@ -183,7 +183,7 @@ TF_LITE_MICRO_TEST(add4_test) {
 }
 
 TF_LITE_MICRO_TEST(add5_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_add5_model_data, g_add5_input0_int16_test_data,
       g_add5_input0_int16_test_data_size, g_add5_input1_int16_test_data,
       g_add5_input1_int16_test_data_size, g_add5_golden_int16_test_data,
@@ -191,7 +191,7 @@ TF_LITE_MICRO_TEST(add5_test) {
 }
 
 TF_LITE_MICRO_TEST(add6_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_add6_model_data, g_add6_input0_int16_test_data,
       g_add6_input0_int16_test_data_size, g_add6_input1_int16_test_data,
       g_add6_input1_int16_test_data_size, g_add6_golden_int16_test_data,
@@ -199,7 +199,7 @@ TF_LITE_MICRO_TEST(add6_test) {
 }
 
 TF_LITE_MICRO_TEST(add7_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_add7_model_data, g_add7_input0_int16_test_data,
       g_add7_input0_int16_test_data_size, g_add7_input1_int16_test_data,
       g_add7_input1_int16_test_data_size, g_add7_golden_int16_test_data,
@@ -207,7 +207,7 @@ TF_LITE_MICRO_TEST(add7_test) {
 }
 
 TF_LITE_MICRO_TEST(add8_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_add8_model_data, g_add8_input0_int16_test_data,
       g_add8_input0_int16_test_data_size, g_add8_input1_int16_test_data,
       g_add8_input1_int16_test_data_size, g_add8_golden_int16_test_data,
@@ -215,7 +215,7 @@ TF_LITE_MICRO_TEST(add8_test) {
 }
 
 TF_LITE_MICRO_TEST(add9_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_add9_model_data, g_add9_input0_int16_test_data,
       g_add9_input0_int16_test_data_size, g_add9_input1_int16_test_data,
       g_add9_input1_int16_test_data_size, g_add9_golden_int16_test_data,
@@ -223,7 +223,7 @@ TF_LITE_MICRO_TEST(add9_test) {
 }
 
 TF_LITE_MICRO_TEST(add10_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_add10_model_data, g_add10_input0_int16_test_data,
       g_add10_input0_int16_test_data_size, g_add10_input1_int16_test_data,
       g_add10_input1_int16_test_data_size, g_add10_golden_int16_test_data,
@@ -231,7 +231,7 @@ TF_LITE_MICRO_TEST(add10_test) {
 }
 
 TF_LITE_MICRO_TEST(add11_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_add11_model_data, g_add11_input0_int16_test_data,
       g_add11_input0_int16_test_data_size, g_add11_input1_int16_test_data,
       g_add11_input1_int16_test_data_size, g_add11_golden_int16_test_data,
@@ -239,7 +239,7 @@ TF_LITE_MICRO_TEST(add11_test) {
 }
 
 TF_LITE_MICRO_TEST(add12_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_add12_model_data, g_add12_input0_int16_test_data,
       g_add12_input0_int16_test_data_size, g_add12_input1_int16_test_data,
       g_add12_input1_int16_test_data_size, g_add12_golden_int16_test_data,
@@ -247,7 +247,7 @@ TF_LITE_MICRO_TEST(add12_test) {
 }
 
 TF_LITE_MICRO_TEST(add13_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_add13_model_data, g_add13_input0_int16_test_data,
       g_add13_input0_int16_test_data_size, g_add13_input1_int16_test_data,
       g_add13_input1_int16_test_data_size, g_add13_golden_int16_test_data,
@@ -255,7 +255,7 @@ TF_LITE_MICRO_TEST(add13_test) {
 }
 
 TF_LITE_MICRO_TEST(add14_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_add14_model_data, g_add14_input0_int16_test_data,
       g_add14_input0_int16_test_data_size, g_add14_input1_int16_test_data,
       g_add14_input1_int16_test_data_size, g_add14_golden_int16_test_data,
@@ -263,7 +263,7 @@ TF_LITE_MICRO_TEST(add14_test) {
 }
 
 TF_LITE_MICRO_TEST(add15_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_add15_model_data, g_add15_input0_int16_test_data,
       g_add15_input0_int16_test_data_size, g_add15_input1_int16_test_data,
       g_add15_input1_int16_test_data_size, g_add15_golden_int16_test_data,
@@ -271,7 +271,7 @@ TF_LITE_MICRO_TEST(add15_test) {
 }
 
 TF_LITE_MICRO_TEST(add16_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_add16_model_data, g_add16_input0_int16_test_data,
       g_add16_input0_int16_test_data_size, g_add16_input1_int16_test_data,
       g_add16_input1_int16_test_data_size, g_add16_golden_int16_test_data,
diff --git a/tensorflow/lite/micro/integration_tests/seanet/conv/integration_tests.cc b/tensorflow/lite/micro/integration_tests/seanet/conv/integration_tests.cc
index 0a8b41e4..e183f735 100644
--- a/tensorflow/lite/micro/integration_tests/seanet/conv/integration_tests.cc
+++ b/tensorflow/lite/micro/integration_tests/seanet/conv/integration_tests.cc
@@ -94,7 +94,7 @@ constexpr size_t kTensorArenaSize = 1024 * 100;
 uint8_t tensor_arena[kTensorArenaSize];
 bool print_log = false;
 
-namespace tflite {
+namespace tflite_micro {
 namespace micro {
 namespace {
 
@@ -122,7 +122,7 @@ void RunModel(const uint8_t* model, const int16_t* input0,
 
   TfLiteTensor* output_tensor = interpreter.output(0);
   TF_LITE_MICRO_EXPECT_EQ(output_tensor->bytes, golden_size * sizeof(int16_t));
-  int16_t* output = ::tflite::GetTensorData<int16_t>(output_tensor);
+  int16_t* output = ::tflite_micro::GetTensorData<int16_t>(output_tensor);
   for (uint32_t i = 0; i < golden_size; i++) {
     // TODO(b/205046520): Better understand why TfLite and TFLM can sometimes be
     // off by 1.
@@ -132,159 +132,159 @@ void RunModel(const uint8_t* model, const int16_t* input0,
 
 }  // namespace
 }  // namespace micro
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(conv0_test) {
-  tflite::micro::RunModel(g_conv0_model_data, g_conv0_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_conv0_model_data, g_conv0_input0_int16_test_data,
                           g_conv0_input0_int16_test_data_size,
                           g_conv0_golden_int16_test_data,
                           g_conv0_golden_int16_test_data_size, "conv0 test");
 }
 
 TF_LITE_MICRO_TEST(conv1_test) {
-  tflite::micro::RunModel(g_conv1_model_data, g_conv1_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_conv1_model_data, g_conv1_input0_int16_test_data,
                           g_conv1_input0_int16_test_data_size,
                           g_conv1_golden_int16_test_data,
                           g_conv1_golden_int16_test_data_size, "conv1 test");
 }
 
 TF_LITE_MICRO_TEST(conv2_test) {
-  tflite::micro::RunModel(g_conv2_model_data, g_conv2_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_conv2_model_data, g_conv2_input0_int16_test_data,
                           g_conv2_input0_int16_test_data_size,
                           g_conv2_golden_int16_test_data,
                           g_conv2_golden_int16_test_data_size, "conv2 test");
 }
 
 TF_LITE_MICRO_TEST(conv3_test) {
-  tflite::micro::RunModel(g_conv3_model_data, g_conv3_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_conv3_model_data, g_conv3_input0_int16_test_data,
                           g_conv3_input0_int16_test_data_size,
                           g_conv3_golden_int16_test_data,
                           g_conv3_golden_int16_test_data_size, "conv3 test");
 }
 
 TF_LITE_MICRO_TEST(conv4_test) {
-  tflite::micro::RunModel(g_conv4_model_data, g_conv4_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_conv4_model_data, g_conv4_input0_int16_test_data,
                           g_conv4_input0_int16_test_data_size,
                           g_conv4_golden_int16_test_data,
                           g_conv4_golden_int16_test_data_size, "conv4 test");
 }
 
 TF_LITE_MICRO_TEST(conv5_test) {
-  tflite::micro::RunModel(g_conv5_model_data, g_conv5_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_conv5_model_data, g_conv5_input0_int16_test_data,
                           g_conv5_input0_int16_test_data_size,
                           g_conv5_golden_int16_test_data,
                           g_conv5_golden_int16_test_data_size, "conv5 test");
 }
 
 TF_LITE_MICRO_TEST(conv6_test) {
-  tflite::micro::RunModel(g_conv6_model_data, g_conv6_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_conv6_model_data, g_conv6_input0_int16_test_data,
                           g_conv6_input0_int16_test_data_size,
                           g_conv6_golden_int16_test_data,
                           g_conv6_golden_int16_test_data_size, "conv6 test");
 }
 
 TF_LITE_MICRO_TEST(conv7_test) {
-  tflite::micro::RunModel(g_conv7_model_data, g_conv7_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_conv7_model_data, g_conv7_input0_int16_test_data,
                           g_conv7_input0_int16_test_data_size,
                           g_conv7_golden_int16_test_data,
                           g_conv7_golden_int16_test_data_size, "conv7 test");
 }
 
 TF_LITE_MICRO_TEST(conv8_test) {
-  tflite::micro::RunModel(g_conv8_model_data, g_conv8_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_conv8_model_data, g_conv8_input0_int16_test_data,
                           g_conv8_input0_int16_test_data_size,
                           g_conv8_golden_int16_test_data,
                           g_conv8_golden_int16_test_data_size, "conv8 test");
 }
 
 TF_LITE_MICRO_TEST(conv9_test) {
-  tflite::micro::RunModel(g_conv9_model_data, g_conv9_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_conv9_model_data, g_conv9_input0_int16_test_data,
                           g_conv9_input0_int16_test_data_size,
                           g_conv9_golden_int16_test_data,
                           g_conv9_golden_int16_test_data_size, "conv9 test");
 }
 
 TF_LITE_MICRO_TEST(conv10_test) {
-  tflite::micro::RunModel(g_conv10_model_data, g_conv10_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_conv10_model_data, g_conv10_input0_int16_test_data,
                           g_conv10_input0_int16_test_data_size,
                           g_conv10_golden_int16_test_data,
                           g_conv10_golden_int16_test_data_size, "conv10 test");
 }
 
 TF_LITE_MICRO_TEST(conv11_test) {
-  tflite::micro::RunModel(g_conv11_model_data, g_conv11_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_conv11_model_data, g_conv11_input0_int16_test_data,
                           g_conv11_input0_int16_test_data_size,
                           g_conv11_golden_int16_test_data,
                           g_conv11_golden_int16_test_data_size, "conv11 test");
 }
 
 TF_LITE_MICRO_TEST(conv12_test) {
-  tflite::micro::RunModel(g_conv12_model_data, g_conv12_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_conv12_model_data, g_conv12_input0_int16_test_data,
                           g_conv12_input0_int16_test_data_size,
                           g_conv12_golden_int16_test_data,
                           g_conv12_golden_int16_test_data_size, "conv12 test");
 }
 
 TF_LITE_MICRO_TEST(conv13_test) {
-  tflite::micro::RunModel(g_conv13_model_data, g_conv13_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_conv13_model_data, g_conv13_input0_int16_test_data,
                           g_conv13_input0_int16_test_data_size,
                           g_conv13_golden_int16_test_data,
                           g_conv13_golden_int16_test_data_size, "conv13 test");
 }
 
 TF_LITE_MICRO_TEST(conv14_test) {
-  tflite::micro::RunModel(g_conv14_model_data, g_conv14_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_conv14_model_data, g_conv14_input0_int16_test_data,
                           g_conv14_input0_int16_test_data_size,
                           g_conv14_golden_int16_test_data,
                           g_conv14_golden_int16_test_data_size, "conv14 test");
 }
 
 TF_LITE_MICRO_TEST(conv15_test) {
-  tflite::micro::RunModel(g_conv15_model_data, g_conv15_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_conv15_model_data, g_conv15_input0_int16_test_data,
                           g_conv15_input0_int16_test_data_size,
                           g_conv15_golden_int16_test_data,
                           g_conv15_golden_int16_test_data_size, "conv15 test");
 }
 
 TF_LITE_MICRO_TEST(conv16_test) {
-  tflite::micro::RunModel(g_conv16_model_data, g_conv16_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_conv16_model_data, g_conv16_input0_int16_test_data,
                           g_conv16_input0_int16_test_data_size,
                           g_conv16_golden_int16_test_data,
                           g_conv16_golden_int16_test_data_size, "conv16 test");
 }
 
 TF_LITE_MICRO_TEST(conv17_test) {
-  tflite::micro::RunModel(g_conv17_model_data, g_conv17_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_conv17_model_data, g_conv17_input0_int16_test_data,
                           g_conv17_input0_int16_test_data_size,
                           g_conv17_golden_int16_test_data,
                           g_conv17_golden_int16_test_data_size, "conv17 test");
 }
 
 TF_LITE_MICRO_TEST(conv18_test) {
-  tflite::micro::RunModel(g_conv18_model_data, g_conv18_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_conv18_model_data, g_conv18_input0_int16_test_data,
                           g_conv18_input0_int16_test_data_size,
                           g_conv18_golden_int16_test_data,
                           g_conv18_golden_int16_test_data_size, "conv18 test");
 }
 
 TF_LITE_MICRO_TEST(conv19_test) {
-  tflite::micro::RunModel(g_conv19_model_data, g_conv19_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_conv19_model_data, g_conv19_input0_int16_test_data,
                           g_conv19_input0_int16_test_data_size,
                           g_conv19_golden_int16_test_data,
                           g_conv19_golden_int16_test_data_size, "conv19 test");
 }
 
 TF_LITE_MICRO_TEST(conv20_test) {
-  tflite::micro::RunModel(g_conv20_model_data, g_conv20_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_conv20_model_data, g_conv20_input0_int16_test_data,
                           g_conv20_input0_int16_test_data_size,
                           g_conv20_golden_int16_test_data,
                           g_conv20_golden_int16_test_data_size, "conv20 test");
 }
 
 TF_LITE_MICRO_TEST(conv21_test) {
-  tflite::micro::RunModel(g_conv21_model_data, g_conv21_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_conv21_model_data, g_conv21_input0_int16_test_data,
                           g_conv21_input0_int16_test_data_size,
                           g_conv21_golden_int16_test_data,
                           g_conv21_golden_int16_test_data_size, "conv21 test");
diff --git a/tensorflow/lite/micro/integration_tests/seanet/leaky_relu/integration_tests.cc b/tensorflow/lite/micro/integration_tests/seanet/leaky_relu/integration_tests.cc
index 98e273b1..a6911ae6 100644
--- a/tensorflow/lite/micro/integration_tests/seanet/leaky_relu/integration_tests.cc
+++ b/tensorflow/lite/micro/integration_tests/seanet/leaky_relu/integration_tests.cc
@@ -97,7 +97,7 @@ constexpr size_t kTensorArenaSize = 1024 * 100;
 uint8_t tensor_arena[kTensorArenaSize];
 bool print_log = false;
 
-namespace tflite {
+namespace tflite_micro {
 namespace micro {
 namespace {
 
@@ -125,7 +125,7 @@ void RunModel(const uint8_t* model, const int16_t* input0,
 
   TfLiteTensor* output_tensor = interpreter.output(0);
   TF_LITE_MICRO_EXPECT_EQ(output_tensor->bytes, golden_size * sizeof(int16_t));
-  int16_t* output = ::tflite::GetTensorData<int16_t>(output_tensor);
+  int16_t* output = ::tflite_micro::GetTensorData<int16_t>(output_tensor);
   for (uint32_t i = 0; i < golden_size; i++) {
     // TODO(b/205046520): Better understand why TfLite and TFLM can sometimes be
     // off by 1.
@@ -135,12 +135,12 @@ void RunModel(const uint8_t* model, const int16_t* input0,
 
 }  // namespace
 }  // namespace micro
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(leaky_relu0_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_leaky_relu0_model_data, g_leaky_relu0_input0_int16_test_data,
       g_leaky_relu0_input0_int16_test_data_size,
       g_leaky_relu0_golden_int16_test_data,
@@ -148,7 +148,7 @@ TF_LITE_MICRO_TEST(leaky_relu0_test) {
 }
 
 TF_LITE_MICRO_TEST(leaky_relu1_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_leaky_relu1_model_data, g_leaky_relu1_input0_int16_test_data,
       g_leaky_relu1_input0_int16_test_data_size,
       g_leaky_relu1_golden_int16_test_data,
@@ -156,7 +156,7 @@ TF_LITE_MICRO_TEST(leaky_relu1_test) {
 }
 
 TF_LITE_MICRO_TEST(leaky_relu2_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_leaky_relu2_model_data, g_leaky_relu2_input0_int16_test_data,
       g_leaky_relu2_input0_int16_test_data_size,
       g_leaky_relu2_golden_int16_test_data,
@@ -164,7 +164,7 @@ TF_LITE_MICRO_TEST(leaky_relu2_test) {
 }
 
 TF_LITE_MICRO_TEST(leaky_relu3_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_leaky_relu3_model_data, g_leaky_relu3_input0_int16_test_data,
       g_leaky_relu3_input0_int16_test_data_size,
       g_leaky_relu3_golden_int16_test_data,
@@ -172,7 +172,7 @@ TF_LITE_MICRO_TEST(leaky_relu3_test) {
 }
 
 TF_LITE_MICRO_TEST(leaky_relu4_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_leaky_relu4_model_data, g_leaky_relu4_input0_int16_test_data,
       g_leaky_relu4_input0_int16_test_data_size,
       g_leaky_relu4_golden_int16_test_data,
@@ -180,7 +180,7 @@ TF_LITE_MICRO_TEST(leaky_relu4_test) {
 }
 
 TF_LITE_MICRO_TEST(leaky_relu5_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_leaky_relu5_model_data, g_leaky_relu5_input0_int16_test_data,
       g_leaky_relu5_input0_int16_test_data_size,
       g_leaky_relu5_golden_int16_test_data,
@@ -188,7 +188,7 @@ TF_LITE_MICRO_TEST(leaky_relu5_test) {
 }
 
 TF_LITE_MICRO_TEST(leaky_relu6_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_leaky_relu6_model_data, g_leaky_relu6_input0_int16_test_data,
       g_leaky_relu6_input0_int16_test_data_size,
       g_leaky_relu6_golden_int16_test_data,
@@ -196,7 +196,7 @@ TF_LITE_MICRO_TEST(leaky_relu6_test) {
 }
 
 TF_LITE_MICRO_TEST(leaky_relu7_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_leaky_relu7_model_data, g_leaky_relu7_input0_int16_test_data,
       g_leaky_relu7_input0_int16_test_data_size,
       g_leaky_relu7_golden_int16_test_data,
@@ -204,7 +204,7 @@ TF_LITE_MICRO_TEST(leaky_relu7_test) {
 }
 
 TF_LITE_MICRO_TEST(leaky_relu8_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_leaky_relu8_model_data, g_leaky_relu8_input0_int16_test_data,
       g_leaky_relu8_input0_int16_test_data_size,
       g_leaky_relu8_golden_int16_test_data,
@@ -212,7 +212,7 @@ TF_LITE_MICRO_TEST(leaky_relu8_test) {
 }
 
 TF_LITE_MICRO_TEST(leaky_relu9_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_leaky_relu9_model_data, g_leaky_relu9_input0_int16_test_data,
       g_leaky_relu9_input0_int16_test_data_size,
       g_leaky_relu9_golden_int16_test_data,
@@ -220,7 +220,7 @@ TF_LITE_MICRO_TEST(leaky_relu9_test) {
 }
 
 TF_LITE_MICRO_TEST(leaky_relu10_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_leaky_relu10_model_data, g_leaky_relu10_input0_int16_test_data,
       g_leaky_relu10_input0_int16_test_data_size,
       g_leaky_relu10_golden_int16_test_data,
@@ -228,7 +228,7 @@ TF_LITE_MICRO_TEST(leaky_relu10_test) {
 }
 
 TF_LITE_MICRO_TEST(leaky_relu11_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_leaky_relu11_model_data, g_leaky_relu11_input0_int16_test_data,
       g_leaky_relu11_input0_int16_test_data_size,
       g_leaky_relu11_golden_int16_test_data,
@@ -236,7 +236,7 @@ TF_LITE_MICRO_TEST(leaky_relu11_test) {
 }
 
 TF_LITE_MICRO_TEST(leaky_relu12_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_leaky_relu12_model_data, g_leaky_relu12_input0_int16_test_data,
       g_leaky_relu12_input0_int16_test_data_size,
       g_leaky_relu12_golden_int16_test_data,
@@ -244,7 +244,7 @@ TF_LITE_MICRO_TEST(leaky_relu12_test) {
 }
 
 TF_LITE_MICRO_TEST(leaky_relu13_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_leaky_relu13_model_data, g_leaky_relu13_input0_int16_test_data,
       g_leaky_relu13_input0_int16_test_data_size,
       g_leaky_relu13_golden_int16_test_data,
@@ -252,7 +252,7 @@ TF_LITE_MICRO_TEST(leaky_relu13_test) {
 }
 
 TF_LITE_MICRO_TEST(leaky_relu14_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_leaky_relu14_model_data, g_leaky_relu14_input0_int16_test_data,
       g_leaky_relu14_input0_int16_test_data_size,
       g_leaky_relu14_golden_int16_test_data,
@@ -260,7 +260,7 @@ TF_LITE_MICRO_TEST(leaky_relu14_test) {
 }
 
 TF_LITE_MICRO_TEST(leaky_relu15_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_leaky_relu15_model_data, g_leaky_relu15_input0_int16_test_data,
       g_leaky_relu15_input0_int16_test_data_size,
       g_leaky_relu15_golden_int16_test_data,
@@ -268,7 +268,7 @@ TF_LITE_MICRO_TEST(leaky_relu15_test) {
 }
 
 TF_LITE_MICRO_TEST(leaky_relu16_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_leaky_relu16_model_data, g_leaky_relu16_input0_int16_test_data,
       g_leaky_relu16_input0_int16_test_data_size,
       g_leaky_relu16_golden_int16_test_data,
@@ -276,7 +276,7 @@ TF_LITE_MICRO_TEST(leaky_relu16_test) {
 }
 
 TF_LITE_MICRO_TEST(leaky_relu17_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_leaky_relu17_model_data, g_leaky_relu17_input0_int16_test_data,
       g_leaky_relu17_input0_int16_test_data_size,
       g_leaky_relu17_golden_int16_test_data,
@@ -284,7 +284,7 @@ TF_LITE_MICRO_TEST(leaky_relu17_test) {
 }
 
 TF_LITE_MICRO_TEST(leaky_relu18_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_leaky_relu18_model_data, g_leaky_relu18_input0_int16_test_data,
       g_leaky_relu18_input0_int16_test_data_size,
       g_leaky_relu18_golden_int16_test_data,
@@ -292,7 +292,7 @@ TF_LITE_MICRO_TEST(leaky_relu18_test) {
 }
 
 TF_LITE_MICRO_TEST(leaky_relu19_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_leaky_relu19_model_data, g_leaky_relu19_input0_int16_test_data,
       g_leaky_relu19_input0_int16_test_data_size,
       g_leaky_relu19_golden_int16_test_data,
@@ -300,7 +300,7 @@ TF_LITE_MICRO_TEST(leaky_relu19_test) {
 }
 
 TF_LITE_MICRO_TEST(leaky_relu20_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_leaky_relu20_model_data, g_leaky_relu20_input0_int16_test_data,
       g_leaky_relu20_input0_int16_test_data_size,
       g_leaky_relu20_golden_int16_test_data,
@@ -308,7 +308,7 @@ TF_LITE_MICRO_TEST(leaky_relu20_test) {
 }
 
 TF_LITE_MICRO_TEST(leaky_relu21_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_leaky_relu21_model_data, g_leaky_relu21_input0_int16_test_data,
       g_leaky_relu21_input0_int16_test_data_size,
       g_leaky_relu21_golden_int16_test_data,
@@ -316,7 +316,7 @@ TF_LITE_MICRO_TEST(leaky_relu21_test) {
 }
 
 TF_LITE_MICRO_TEST(leaky_relu22_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_leaky_relu22_model_data, g_leaky_relu22_input0_int16_test_data,
       g_leaky_relu22_input0_int16_test_data_size,
       g_leaky_relu22_golden_int16_test_data,
diff --git a/tensorflow/lite/micro/integration_tests/seanet/pad/integration_tests.cc b/tensorflow/lite/micro/integration_tests/seanet/pad/integration_tests.cc
index 853d5074..b9cc9797 100644
--- a/tensorflow/lite/micro/integration_tests/seanet/pad/integration_tests.cc
+++ b/tensorflow/lite/micro/integration_tests/seanet/pad/integration_tests.cc
@@ -85,7 +85,7 @@ constexpr size_t kTensorArenaSize = 1024 * 100;
 uint8_t tensor_arena[kTensorArenaSize];
 bool print_log = false;
 
-namespace tflite {
+namespace tflite_micro {
 namespace micro {
 namespace {
 
@@ -113,7 +113,7 @@ void RunModel(const uint8_t* model, const int16_t* input0,
 
   TfLiteTensor* output_tensor = interpreter.output(0);
   TF_LITE_MICRO_EXPECT_EQ(output_tensor->bytes, golden_size * sizeof(int16_t));
-  int16_t* output = ::tflite::GetTensorData<int16_t>(output_tensor);
+  int16_t* output = ::tflite_micro::GetTensorData<int16_t>(output_tensor);
   for (uint32_t i = 0; i < golden_size; i++) {
     // TODO(b/205046520): Better understand why TfLite and TFLM can sometimes be
     // off by 1.
@@ -123,138 +123,138 @@ void RunModel(const uint8_t* model, const int16_t* input0,
 
 }  // namespace
 }  // namespace micro
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(pad0_test) {
-  tflite::micro::RunModel(g_pad0_model_data, g_pad0_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_pad0_model_data, g_pad0_input0_int16_test_data,
                           g_pad0_input0_int16_test_data_size,
                           g_pad0_golden_int16_test_data,
                           g_pad0_golden_int16_test_data_size, "pad0 test");
 }
 
 TF_LITE_MICRO_TEST(pad1_test) {
-  tflite::micro::RunModel(g_pad1_model_data, g_pad1_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_pad1_model_data, g_pad1_input0_int16_test_data,
                           g_pad1_input0_int16_test_data_size,
                           g_pad1_golden_int16_test_data,
                           g_pad1_golden_int16_test_data_size, "pad1 test");
 }
 
 TF_LITE_MICRO_TEST(pad2_test) {
-  tflite::micro::RunModel(g_pad2_model_data, g_pad2_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_pad2_model_data, g_pad2_input0_int16_test_data,
                           g_pad2_input0_int16_test_data_size,
                           g_pad2_golden_int16_test_data,
                           g_pad2_golden_int16_test_data_size, "pad2 test");
 }
 
 TF_LITE_MICRO_TEST(pad3_test) {
-  tflite::micro::RunModel(g_pad3_model_data, g_pad3_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_pad3_model_data, g_pad3_input0_int16_test_data,
                           g_pad3_input0_int16_test_data_size,
                           g_pad3_golden_int16_test_data,
                           g_pad3_golden_int16_test_data_size, "pad3 test");
 }
 
 TF_LITE_MICRO_TEST(pad4_test) {
-  tflite::micro::RunModel(g_pad4_model_data, g_pad4_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_pad4_model_data, g_pad4_input0_int16_test_data,
                           g_pad4_input0_int16_test_data_size,
                           g_pad4_golden_int16_test_data,
                           g_pad4_golden_int16_test_data_size, "pad4 test");
 }
 
 TF_LITE_MICRO_TEST(pad5_test) {
-  tflite::micro::RunModel(g_pad5_model_data, g_pad5_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_pad5_model_data, g_pad5_input0_int16_test_data,
                           g_pad5_input0_int16_test_data_size,
                           g_pad5_golden_int16_test_data,
                           g_pad5_golden_int16_test_data_size, "pad5 test");
 }
 
 TF_LITE_MICRO_TEST(pad6_test) {
-  tflite::micro::RunModel(g_pad6_model_data, g_pad6_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_pad6_model_data, g_pad6_input0_int16_test_data,
                           g_pad6_input0_int16_test_data_size,
                           g_pad6_golden_int16_test_data,
                           g_pad6_golden_int16_test_data_size, "pad6 test");
 }
 
 TF_LITE_MICRO_TEST(pad7_test) {
-  tflite::micro::RunModel(g_pad7_model_data, g_pad7_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_pad7_model_data, g_pad7_input0_int16_test_data,
                           g_pad7_input0_int16_test_data_size,
                           g_pad7_golden_int16_test_data,
                           g_pad7_golden_int16_test_data_size, "pad7 test");
 }
 
 TF_LITE_MICRO_TEST(pad8_test) {
-  tflite::micro::RunModel(g_pad8_model_data, g_pad8_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_pad8_model_data, g_pad8_input0_int16_test_data,
                           g_pad8_input0_int16_test_data_size,
                           g_pad8_golden_int16_test_data,
                           g_pad8_golden_int16_test_data_size, "pad8 test");
 }
 
 TF_LITE_MICRO_TEST(pad9_test) {
-  tflite::micro::RunModel(g_pad9_model_data, g_pad9_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_pad9_model_data, g_pad9_input0_int16_test_data,
                           g_pad9_input0_int16_test_data_size,
                           g_pad9_golden_int16_test_data,
                           g_pad9_golden_int16_test_data_size, "pad9 test");
 }
 
 TF_LITE_MICRO_TEST(pad10_test) {
-  tflite::micro::RunModel(g_pad10_model_data, g_pad10_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_pad10_model_data, g_pad10_input0_int16_test_data,
                           g_pad10_input0_int16_test_data_size,
                           g_pad10_golden_int16_test_data,
                           g_pad10_golden_int16_test_data_size, "pad10 test");
 }
 
 TF_LITE_MICRO_TEST(pad11_test) {
-  tflite::micro::RunModel(g_pad11_model_data, g_pad11_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_pad11_model_data, g_pad11_input0_int16_test_data,
                           g_pad11_input0_int16_test_data_size,
                           g_pad11_golden_int16_test_data,
                           g_pad11_golden_int16_test_data_size, "pad11 test");
 }
 
 TF_LITE_MICRO_TEST(pad12_test) {
-  tflite::micro::RunModel(g_pad12_model_data, g_pad12_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_pad12_model_data, g_pad12_input0_int16_test_data,
                           g_pad12_input0_int16_test_data_size,
                           g_pad12_golden_int16_test_data,
                           g_pad12_golden_int16_test_data_size, "pad12 test");
 }
 
 TF_LITE_MICRO_TEST(pad13_test) {
-  tflite::micro::RunModel(g_pad13_model_data, g_pad13_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_pad13_model_data, g_pad13_input0_int16_test_data,
                           g_pad13_input0_int16_test_data_size,
                           g_pad13_golden_int16_test_data,
                           g_pad13_golden_int16_test_data_size, "pad13 test");
 }
 
 TF_LITE_MICRO_TEST(pad14_test) {
-  tflite::micro::RunModel(g_pad14_model_data, g_pad14_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_pad14_model_data, g_pad14_input0_int16_test_data,
                           g_pad14_input0_int16_test_data_size,
                           g_pad14_golden_int16_test_data,
                           g_pad14_golden_int16_test_data_size, "pad14 test");
 }
 
 TF_LITE_MICRO_TEST(pad15_test) {
-  tflite::micro::RunModel(g_pad15_model_data, g_pad15_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_pad15_model_data, g_pad15_input0_int16_test_data,
                           g_pad15_input0_int16_test_data_size,
                           g_pad15_golden_int16_test_data,
                           g_pad15_golden_int16_test_data_size, "pad15 test");
 }
 
 TF_LITE_MICRO_TEST(pad16_test) {
-  tflite::micro::RunModel(g_pad16_model_data, g_pad16_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_pad16_model_data, g_pad16_input0_int16_test_data,
                           g_pad16_input0_int16_test_data_size,
                           g_pad16_golden_int16_test_data,
                           g_pad16_golden_int16_test_data_size, "pad16 test");
 }
 
 TF_LITE_MICRO_TEST(pad17_test) {
-  tflite::micro::RunModel(g_pad17_model_data, g_pad17_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_pad17_model_data, g_pad17_input0_int16_test_data,
                           g_pad17_input0_int16_test_data_size,
                           g_pad17_golden_int16_test_data,
                           g_pad17_golden_int16_test_data_size, "pad17 test");
 }
 
 TF_LITE_MICRO_TEST(pad18_test) {
-  tflite::micro::RunModel(g_pad18_model_data, g_pad18_input0_int16_test_data,
+  tflite_micro::micro::RunModel(g_pad18_model_data, g_pad18_input0_int16_test_data,
                           g_pad18_input0_int16_test_data_size,
                           g_pad18_golden_int16_test_data,
                           g_pad18_golden_int16_test_data_size, "pad18 test");
diff --git a/tensorflow/lite/micro/integration_tests/seanet/quantize/integration_tests.cc b/tensorflow/lite/micro/integration_tests/seanet/quantize/integration_tests.cc
index 3020c59f..eb6f5def 100644
--- a/tensorflow/lite/micro/integration_tests/seanet/quantize/integration_tests.cc
+++ b/tensorflow/lite/micro/integration_tests/seanet/quantize/integration_tests.cc
@@ -34,7 +34,7 @@ constexpr size_t kTensorArenaSize = 1024 * 100;
 uint8_t tensor_arena[kTensorArenaSize];
 bool print_log = false;
 
-namespace tflite {
+namespace tflite_micro {
 namespace micro {
 namespace {
 
@@ -63,7 +63,7 @@ void RunModel(const uint8_t* model, const inputT* input0,
 
   TfLiteTensor* output_tensor = interpreter.output(0);
   TF_LITE_MICRO_EXPECT_EQ(output_tensor->bytes, golden_size * sizeof(outputT));
-  outputT* output = ::tflite::GetTensorData<outputT>(output_tensor);
+  outputT* output = ::tflite_micro::GetTensorData<outputT>(output_tensor);
   for (uint32_t i = 0; i < golden_size; i++) {
     // TODO(b/205046520): Better understand why TfLite and TFLM can sometimes be
     // off by 1.
@@ -73,7 +73,7 @@ void RunModel(const uint8_t* model, const inputT* input0,
 
 }  // namespace
 }  // namespace micro
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -87,7 +87,7 @@ if (argc > 2) {
 }
 
 TF_LITE_MICRO_TEST(quantize0_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_quantize0_model_data, g_quantize0_input0_int32_test_data,
       g_quantize0_input0_int32_test_data_size,
       g_quantize0_golden_int16_test_data,
@@ -95,7 +95,7 @@ TF_LITE_MICRO_TEST(quantize0_test) {
 }
 
 TF_LITE_MICRO_TEST(quantize1_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_quantize1_model_data, g_quantize1_input0_int16_test_data,
       g_quantize1_input0_int16_test_data_size,
       g_quantize1_golden_int32_test_data,
diff --git a/tensorflow/lite/micro/integration_tests/seanet/strided_slice/integration_tests.cc b/tensorflow/lite/micro/integration_tests/seanet/strided_slice/integration_tests.cc
index b03738ab..6e81262e 100644
--- a/tensorflow/lite/micro/integration_tests/seanet/strided_slice/integration_tests.cc
+++ b/tensorflow/lite/micro/integration_tests/seanet/strided_slice/integration_tests.cc
@@ -130,7 +130,7 @@ constexpr size_t kTensorArenaSize = 1024 * 100;
 uint8_t tensor_arena[kTensorArenaSize];
 bool print_log = false;
 
-namespace tflite {
+namespace tflite_micro {
 namespace micro {
 namespace {
 
@@ -158,7 +158,7 @@ void RunModel(const uint8_t* model, const int16_t* input0,
 
   TfLiteTensor* output_tensor = interpreter.output(0);
   TF_LITE_MICRO_EXPECT_EQ(output_tensor->bytes, golden_size * sizeof(int16_t));
-  int16_t* output = ::tflite::GetTensorData<int16_t>(output_tensor);
+  int16_t* output = ::tflite_micro::GetTensorData<int16_t>(output_tensor);
   for (uint32_t i = 0; i < golden_size; i++) {
     // TODO(b/205046520): Better understand why TfLite and TFLM can sometimes be
     // off by 1.
@@ -168,12 +168,12 @@ void RunModel(const uint8_t* model, const int16_t* input0,
 
 }  // namespace
 }  // namespace micro
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(strided_slice0_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice0_model_data, g_strided_slice0_input0_int16_test_data,
       g_strided_slice0_input0_int16_test_data_size,
       g_strided_slice0_golden_int16_test_data,
@@ -181,7 +181,7 @@ TF_LITE_MICRO_TEST(strided_slice0_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice1_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice1_model_data, g_strided_slice1_input0_int16_test_data,
       g_strided_slice1_input0_int16_test_data_size,
       g_strided_slice1_golden_int16_test_data,
@@ -189,7 +189,7 @@ TF_LITE_MICRO_TEST(strided_slice1_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice2_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice2_model_data, g_strided_slice2_input0_int16_test_data,
       g_strided_slice2_input0_int16_test_data_size,
       g_strided_slice2_golden_int16_test_data,
@@ -197,7 +197,7 @@ TF_LITE_MICRO_TEST(strided_slice2_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice3_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice3_model_data, g_strided_slice3_input0_int16_test_data,
       g_strided_slice3_input0_int16_test_data_size,
       g_strided_slice3_golden_int16_test_data,
@@ -205,7 +205,7 @@ TF_LITE_MICRO_TEST(strided_slice3_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice4_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice4_model_data, g_strided_slice4_input0_int16_test_data,
       g_strided_slice4_input0_int16_test_data_size,
       g_strided_slice4_golden_int16_test_data,
@@ -213,7 +213,7 @@ TF_LITE_MICRO_TEST(strided_slice4_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice5_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice5_model_data, g_strided_slice5_input0_int16_test_data,
       g_strided_slice5_input0_int16_test_data_size,
       g_strided_slice5_golden_int16_test_data,
@@ -221,7 +221,7 @@ TF_LITE_MICRO_TEST(strided_slice5_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice6_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice6_model_data, g_strided_slice6_input0_int16_test_data,
       g_strided_slice6_input0_int16_test_data_size,
       g_strided_slice6_golden_int16_test_data,
@@ -229,7 +229,7 @@ TF_LITE_MICRO_TEST(strided_slice6_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice7_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice7_model_data, g_strided_slice7_input0_int16_test_data,
       g_strided_slice7_input0_int16_test_data_size,
       g_strided_slice7_golden_int16_test_data,
@@ -237,7 +237,7 @@ TF_LITE_MICRO_TEST(strided_slice7_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice8_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice8_model_data, g_strided_slice8_input0_int16_test_data,
       g_strided_slice8_input0_int16_test_data_size,
       g_strided_slice8_golden_int16_test_data,
@@ -245,7 +245,7 @@ TF_LITE_MICRO_TEST(strided_slice8_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice9_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice9_model_data, g_strided_slice9_input0_int16_test_data,
       g_strided_slice9_input0_int16_test_data_size,
       g_strided_slice9_golden_int16_test_data,
@@ -253,7 +253,7 @@ TF_LITE_MICRO_TEST(strided_slice9_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice10_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice10_model_data, g_strided_slice10_input0_int16_test_data,
       g_strided_slice10_input0_int16_test_data_size,
       g_strided_slice10_golden_int16_test_data,
@@ -261,7 +261,7 @@ TF_LITE_MICRO_TEST(strided_slice10_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice11_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice11_model_data, g_strided_slice11_input0_int16_test_data,
       g_strided_slice11_input0_int16_test_data_size,
       g_strided_slice11_golden_int16_test_data,
@@ -269,7 +269,7 @@ TF_LITE_MICRO_TEST(strided_slice11_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice12_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice12_model_data, g_strided_slice12_input0_int16_test_data,
       g_strided_slice12_input0_int16_test_data_size,
       g_strided_slice12_golden_int16_test_data,
@@ -277,7 +277,7 @@ TF_LITE_MICRO_TEST(strided_slice12_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice13_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice13_model_data, g_strided_slice13_input0_int16_test_data,
       g_strided_slice13_input0_int16_test_data_size,
       g_strided_slice13_golden_int16_test_data,
@@ -285,7 +285,7 @@ TF_LITE_MICRO_TEST(strided_slice13_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice14_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice14_model_data, g_strided_slice14_input0_int16_test_data,
       g_strided_slice14_input0_int16_test_data_size,
       g_strided_slice14_golden_int16_test_data,
@@ -293,7 +293,7 @@ TF_LITE_MICRO_TEST(strided_slice14_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice15_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice15_model_data, g_strided_slice15_input0_int16_test_data,
       g_strided_slice15_input0_int16_test_data_size,
       g_strided_slice15_golden_int16_test_data,
@@ -301,7 +301,7 @@ TF_LITE_MICRO_TEST(strided_slice15_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice16_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice16_model_data, g_strided_slice16_input0_int16_test_data,
       g_strided_slice16_input0_int16_test_data_size,
       g_strided_slice16_golden_int16_test_data,
@@ -309,7 +309,7 @@ TF_LITE_MICRO_TEST(strided_slice16_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice17_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice17_model_data, g_strided_slice17_input0_int16_test_data,
       g_strided_slice17_input0_int16_test_data_size,
       g_strided_slice17_golden_int16_test_data,
@@ -317,7 +317,7 @@ TF_LITE_MICRO_TEST(strided_slice17_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice18_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice18_model_data, g_strided_slice18_input0_int16_test_data,
       g_strided_slice18_input0_int16_test_data_size,
       g_strided_slice18_golden_int16_test_data,
@@ -325,7 +325,7 @@ TF_LITE_MICRO_TEST(strided_slice18_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice19_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice19_model_data, g_strided_slice19_input0_int16_test_data,
       g_strided_slice19_input0_int16_test_data_size,
       g_strided_slice19_golden_int16_test_data,
@@ -333,7 +333,7 @@ TF_LITE_MICRO_TEST(strided_slice19_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice20_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice20_model_data, g_strided_slice20_input0_int16_test_data,
       g_strided_slice20_input0_int16_test_data_size,
       g_strided_slice20_golden_int16_test_data,
@@ -341,7 +341,7 @@ TF_LITE_MICRO_TEST(strided_slice20_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice21_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice21_model_data, g_strided_slice21_input0_int16_test_data,
       g_strided_slice21_input0_int16_test_data_size,
       g_strided_slice21_golden_int16_test_data,
@@ -349,7 +349,7 @@ TF_LITE_MICRO_TEST(strided_slice21_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice22_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice22_model_data, g_strided_slice22_input0_int16_test_data,
       g_strided_slice22_input0_int16_test_data_size,
       g_strided_slice22_golden_int16_test_data,
@@ -357,7 +357,7 @@ TF_LITE_MICRO_TEST(strided_slice22_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice23_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice23_model_data, g_strided_slice23_input0_int16_test_data,
       g_strided_slice23_input0_int16_test_data_size,
       g_strided_slice23_golden_int16_test_data,
@@ -365,7 +365,7 @@ TF_LITE_MICRO_TEST(strided_slice23_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice24_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice24_model_data, g_strided_slice24_input0_int16_test_data,
       g_strided_slice24_input0_int16_test_data_size,
       g_strided_slice24_golden_int16_test_data,
@@ -373,7 +373,7 @@ TF_LITE_MICRO_TEST(strided_slice24_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice25_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice25_model_data, g_strided_slice25_input0_int16_test_data,
       g_strided_slice25_input0_int16_test_data_size,
       g_strided_slice25_golden_int16_test_data,
@@ -381,7 +381,7 @@ TF_LITE_MICRO_TEST(strided_slice25_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice26_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice26_model_data, g_strided_slice26_input0_int16_test_data,
       g_strided_slice26_input0_int16_test_data_size,
       g_strided_slice26_golden_int16_test_data,
@@ -389,7 +389,7 @@ TF_LITE_MICRO_TEST(strided_slice26_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice27_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice27_model_data, g_strided_slice27_input0_int16_test_data,
       g_strided_slice27_input0_int16_test_data_size,
       g_strided_slice27_golden_int16_test_data,
@@ -397,7 +397,7 @@ TF_LITE_MICRO_TEST(strided_slice27_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice28_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice28_model_data, g_strided_slice28_input0_int16_test_data,
       g_strided_slice28_input0_int16_test_data_size,
       g_strided_slice28_golden_int16_test_data,
@@ -405,7 +405,7 @@ TF_LITE_MICRO_TEST(strided_slice28_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice29_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice29_model_data, g_strided_slice29_input0_int16_test_data,
       g_strided_slice29_input0_int16_test_data_size,
       g_strided_slice29_golden_int16_test_data,
@@ -413,7 +413,7 @@ TF_LITE_MICRO_TEST(strided_slice29_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice30_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice30_model_data, g_strided_slice30_input0_int16_test_data,
       g_strided_slice30_input0_int16_test_data_size,
       g_strided_slice30_golden_int16_test_data,
@@ -421,7 +421,7 @@ TF_LITE_MICRO_TEST(strided_slice30_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice31_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice31_model_data, g_strided_slice31_input0_int16_test_data,
       g_strided_slice31_input0_int16_test_data_size,
       g_strided_slice31_golden_int16_test_data,
@@ -429,7 +429,7 @@ TF_LITE_MICRO_TEST(strided_slice31_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice32_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice32_model_data, g_strided_slice32_input0_int16_test_data,
       g_strided_slice32_input0_int16_test_data_size,
       g_strided_slice32_golden_int16_test_data,
@@ -437,7 +437,7 @@ TF_LITE_MICRO_TEST(strided_slice32_test) {
 }
 
 TF_LITE_MICRO_TEST(strided_slice33_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_strided_slice33_model_data, g_strided_slice33_input0_int16_test_data,
       g_strided_slice33_input0_int16_test_data_size,
       g_strided_slice33_golden_int16_test_data,
diff --git a/tensorflow/lite/micro/integration_tests/seanet/sub/integration_tests.cc b/tensorflow/lite/micro/integration_tests/seanet/sub/integration_tests.cc
index b9e92d06..cb864e53 100644
--- a/tensorflow/lite/micro/integration_tests/seanet/sub/integration_tests.cc
+++ b/tensorflow/lite/micro/integration_tests/seanet/sub/integration_tests.cc
@@ -48,7 +48,7 @@ constexpr size_t kTensorArenaSize = 1024 * 100;
 uint8_t tensor_arena[kTensorArenaSize];
 bool print_log = false;
 
-namespace tflite {
+namespace tflite_micro {
 namespace micro {
 namespace {
 
@@ -80,7 +80,7 @@ void RunModel(const uint8_t* model, const int16_t* input0,
 
   TfLiteTensor* output_tensor = interpreter.output(0);
   TF_LITE_MICRO_EXPECT_EQ(output_tensor->bytes, golden_size * sizeof(int16_t));
-  int16_t* output = ::tflite::GetTensorData<int16_t>(output_tensor);
+  int16_t* output = ::tflite_micro::GetTensorData<int16_t>(output_tensor);
   for (uint32_t i = 0; i < golden_size; i++) {
     // TODO(b/205046520): Better understand why TfLite and TFLM can sometimes be
     // off by 1.
@@ -90,12 +90,12 @@ void RunModel(const uint8_t* model, const int16_t* input0,
 
 }  // namespace
 }  // namespace micro
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(sub0_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_sub0_model_data, g_sub0_input0_int16_test_data,
       g_sub0_input0_int16_test_data_size, g_sub0_input1_int16_test_data,
       g_sub0_input1_int16_test_data_size, g_sub0_golden_int16_test_data,
@@ -103,7 +103,7 @@ TF_LITE_MICRO_TEST(sub0_test) {
 }
 
 TF_LITE_MICRO_TEST(sub1_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_sub1_model_data, g_sub1_input0_int16_test_data,
       g_sub1_input0_int16_test_data_size, g_sub1_input1_int16_test_data,
       g_sub1_input1_int16_test_data_size, g_sub1_golden_int16_test_data,
@@ -111,7 +111,7 @@ TF_LITE_MICRO_TEST(sub1_test) {
 }
 
 TF_LITE_MICRO_TEST(sub2_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_sub2_model_data, g_sub2_input0_int16_test_data,
       g_sub2_input0_int16_test_data_size, g_sub2_input1_int16_test_data,
       g_sub2_input1_int16_test_data_size, g_sub2_golden_int16_test_data,
@@ -119,7 +119,7 @@ TF_LITE_MICRO_TEST(sub2_test) {
 }
 
 TF_LITE_MICRO_TEST(sub3_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_sub3_model_data, g_sub3_input0_int16_test_data,
       g_sub3_input0_int16_test_data_size, g_sub3_input1_int16_test_data,
       g_sub3_input1_int16_test_data_size, g_sub3_golden_int16_test_data,
@@ -127,7 +127,7 @@ TF_LITE_MICRO_TEST(sub3_test) {
 }
 
 TF_LITE_MICRO_TEST(sub4_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_sub4_model_data, g_sub4_input0_int16_test_data,
       g_sub4_input0_int16_test_data_size, g_sub4_input1_int16_test_data,
       g_sub4_input1_int16_test_data_size, g_sub4_golden_int16_test_data,
diff --git a/tensorflow/lite/micro/integration_tests/seanet/transpose_conv/integration_tests.cc b/tensorflow/lite/micro/integration_tests/seanet/transpose_conv/integration_tests.cc
index be7c63e8..4789ffa1 100644
--- a/tensorflow/lite/micro/integration_tests/seanet/transpose_conv/integration_tests.cc
+++ b/tensorflow/lite/micro/integration_tests/seanet/transpose_conv/integration_tests.cc
@@ -48,7 +48,7 @@ constexpr size_t kTensorArenaSize = 1024 * 100;
 uint8_t tensor_arena[kTensorArenaSize];
 bool print_log = false;
 
-namespace tflite {
+namespace tflite_micro {
 namespace micro {
 namespace {
 
@@ -80,7 +80,7 @@ void RunModel(const uint8_t* model, const int32_t* input0,
 
   TfLiteTensor* output_tensor = interpreter.output(0);
   TF_LITE_MICRO_EXPECT_EQ(output_tensor->bytes, golden_size * sizeof(int16_t));
-  int16_t* output = ::tflite::GetTensorData<int16_t>(output_tensor);
+  int16_t* output = ::tflite_micro::GetTensorData<int16_t>(output_tensor);
   for (uint32_t i = 0; i < golden_size; i++) {
     // TODO(b/205046520): Better understand why TfLite and TFLM can sometimes be
     // off by 1.
@@ -90,12 +90,12 @@ void RunModel(const uint8_t* model, const int32_t* input0,
 
 }  // namespace
 }  // namespace micro
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(transpose_conv0_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_transpose_conv0_model_data, g_transpose_conv0_input0_int32_test_data,
       g_transpose_conv0_input0_int32_test_data_size,
       g_transpose_conv0_input1_int16_test_data,
@@ -105,7 +105,7 @@ TF_LITE_MICRO_TEST(transpose_conv0_test) {
 }
 
 TF_LITE_MICRO_TEST(transpose_conv1_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_transpose_conv1_model_data, g_transpose_conv1_input0_int32_test_data,
       g_transpose_conv1_input0_int32_test_data_size,
       g_transpose_conv1_input1_int16_test_data,
@@ -115,7 +115,7 @@ TF_LITE_MICRO_TEST(transpose_conv1_test) {
 }
 
 TF_LITE_MICRO_TEST(transpose_conv2_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_transpose_conv2_model_data, g_transpose_conv2_input0_int32_test_data,
       g_transpose_conv2_input0_int32_test_data_size,
       g_transpose_conv2_input1_int16_test_data,
@@ -125,7 +125,7 @@ TF_LITE_MICRO_TEST(transpose_conv2_test) {
 }
 
 TF_LITE_MICRO_TEST(transpose_conv3_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_transpose_conv3_model_data, g_transpose_conv3_input0_int32_test_data,
       g_transpose_conv3_input0_int32_test_data_size,
       g_transpose_conv3_input1_int16_test_data,
@@ -135,7 +135,7 @@ TF_LITE_MICRO_TEST(transpose_conv3_test) {
 }
 
 TF_LITE_MICRO_TEST(transpose_conv4_test) {
-  tflite::micro::RunModel(
+  tflite_micro::micro::RunModel(
       g_transpose_conv4_model_data, g_transpose_conv4_input0_int32_test_data,
       g_transpose_conv4_input0_int32_test_data_size,
       g_transpose_conv4_input1_int16_test_data,
diff --git a/tensorflow/lite/micro/integration_tests/templates/integration_tests_cc.mako b/tensorflow/lite/micro/integration_tests/templates/integration_tests_cc.mako
index 70b82f4c..13773b2c 100644
--- a/tensorflow/lite/micro/integration_tests/templates/integration_tests_cc.mako
+++ b/tensorflow/lite/micro/integration_tests/templates/integration_tests_cc.mako
@@ -37,7 +37,7 @@ constexpr size_t kTensorArenaSize = 1024 * 100;
 uint8_t tensor_arena[kTensorArenaSize];
 bool print_log = false;
 
-namespace tflite {
+namespace tflite_micro {
 namespace micro {
 namespace {
 
@@ -78,7 +78,7 @@ void RunModel(const uint8_t* model,
   TfLiteTensor* output_tensor = interpreter.output(0);
   TF_LITE_MICRO_EXPECT_EQ(output_tensor->bytes,
                           golden_size * sizeof(${output_dtype}_t));
-  ${output_dtype}_t* output = ::tflite::GetTensorData<${output_dtype}_t>(output_tensor);
+  ${output_dtype}_t* output = ::tflite_micro::GetTensorData<${output_dtype}_t>(output_tensor);
   for (uint32_t i = 0; i < golden_size; i++) {
     // TODO(b/205046520): Better understand why TfLite and TFLM can sometimes be
     // off by 1.
@@ -88,13 +88,13 @@ void RunModel(const uint8_t* model,
 
 }  // namespace
 }  // namespace micro
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
 % for target in targets:
 
-TF_LITE_MICRO_TEST(${target}_test) {tflite::micro::RunModel(
+TF_LITE_MICRO_TEST(${target}_test) {tflite_micro::micro::RunModel(
 g_${target}_model_data,
 % for input_idx, input in enumerate(inputs):
 g_${target}_input${input_idx}_${input_dtypes[input_idx]}_test_data,
diff --git a/tensorflow/lite/micro/integration_tests/templates/integration_tests_cc.mako.orig b/tensorflow/lite/micro/integration_tests/templates/integration_tests_cc.mako.orig
index ba533296..f2db012d 100644
--- a/tensorflow/lite/micro/integration_tests/templates/integration_tests_cc.mako.orig
+++ b/tensorflow/lite/micro/integration_tests/templates/integration_tests_cc.mako.orig
@@ -36,7 +36,7 @@ limitations under the License.
 constexpr size_t kTensorArenaSize = 1024 * 100;
 uint8_t tensor_arena[kTensorArenaSize];
 
-namespace tflite {
+namespace tflite_micro {
 namespace micro {
 namespace {
 
@@ -75,7 +75,7 @@ void RunModel(const uint8_t* model,
   TfLiteTensor* output_tensor = interpreter.output(0);
   TF_LITE_MICRO_EXPECT_EQ(output_tensor->bytes,
                           golden_size * sizeof(${output_dtype}_t));
-  ${output_dtype}_t* output = ::tflite::GetTensorData<${output_dtype}_t>(output_tensor);
+  ${output_dtype}_t* output = ::tflite_micro::GetTensorData<${output_dtype}_t>(output_tensor);
   for (uint32_t i = 0; i < golden_size; i++) {
     // TODO(b/205046520): Better understand why TfLite and TFLM can sometimes be
     // off by 1.
@@ -85,13 +85,13 @@ void RunModel(const uint8_t* model,
 
 }  // namespace
 }  // namespace micro
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
 % for target in targets:
 
-TF_LITE_MICRO_TEST(${target}_test) {tflite::micro::RunModel(
+TF_LITE_MICRO_TEST(${target}_test) {tflite_micro::micro::RunModel(
 g_${target}_model_data,
 % for input_idx, input in enumerate(inputs):
 g_${target}_input${input_idx}_${input_dtypes[input_idx]}_test_data,
diff --git a/tensorflow/lite/micro/kernels/activation_utils.h b/tensorflow/lite/micro/kernels/activation_utils.h
index 95ecc26d..5b8c016d 100644
--- a/tensorflow/lite/micro/kernels/activation_utils.h
+++ b/tensorflow/lite/micro/kernels/activation_utils.h
@@ -24,7 +24,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/max.h"
 #include "tensorflow/lite/kernels/internal/min.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace ops {
 namespace micro {
 
@@ -52,6 +52,6 @@ inline float ActivationValFloat(TfLiteFusedActivation act, float a) {
 
 }  // namespace micro
 }  // namespace ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_ACTIVATION_UTILS_H_
diff --git a/tensorflow/lite/micro/kernels/activations.cc b/tensorflow/lite/micro/kernels/activations.cc
index 1086325c..4c9f5b21 100644
--- a/tensorflow/lite/micro/kernels/activations.cc
+++ b/tensorflow/lite/micro/kernels/activations.cc
@@ -27,7 +27,26 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_log.h"
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
+
+template <typename T>
+void ReluQuantized(const ReluOpData& data, const RuntimeShape& input_shape,
+                   const RuntimeShape& output_shape, const T* input_data,
+                   T* output_data) {
+  const int flat_size = MatchingFlatSize(input_shape, output_shape);
+  for (int i = 0; i < flat_size; ++i) {
+    const int32_t val = static_cast<int32_t>(input_data[i]);
+    int32_t clamped =
+        data.params.output_offset +
+        MultiplyByQuantizedMultiplier(val - data.params.input_offset,
+                                      data.params.output_multiplier,
+                                      data.params.output_shift);
+    clamped = std::max(data.params.quantized_activation_min, clamped);
+    clamped = std::min(data.params.quantized_activation_max, clamped);
+    output_data[i] = static_cast<T>(clamped);
+  }
+}
+
 namespace {
 
 void* ReluInit(TfLiteContext* context, const char* buffer, size_t length) {
@@ -40,24 +59,31 @@ TfLiteStatus ReluEval(TfLiteContext* context, TfLiteNode* node) {
   const ReluOpData& data = *(static_cast<const ReluOpData*>(node->user_data));
 
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kActivationsInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kActivationsInputTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kActivationsOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kActivationsOutputTensor);
 
   switch (input->type) {
     case kTfLiteFloat32: {
-      ReluFloat(tflite::micro::GetTensorShape(input),
-                tflite::micro::GetTensorData<float>(input),
-                tflite::micro::GetTensorShape(output),
-                tflite::micro::GetTensorData<float>(output));
+      ReluFloat(tflite_micro::micro::GetTensorShape(input),
+                tflite_micro::micro::GetTensorData<float>(input),
+                tflite_micro::micro::GetTensorShape(output),
+                tflite_micro::micro::GetTensorData<float>(output));
 
       return kTfLiteOk;
     }
+    case kTfLiteInt16: {
+      tflite_micro::ReluQuantized<int16_t>(data, tflite_micro::micro::GetTensorShape(input),
+                            tflite_micro::micro::GetTensorShape(output),
+                            tflite_micro::micro::GetTensorData<int16_t>(input),
+                            tflite_micro::micro::GetTensorData<int16_t>(output));
+      return kTfLiteOk;
+    }
     case kTfLiteInt8: {
-      tflite::ReluQuantized(data, tflite::micro::GetTensorShape(input),
-                            tflite::micro::GetTensorShape(output),
-                            tflite::micro::GetTensorData<int8_t>(input),
-                            tflite::micro::GetTensorData<int8_t>(output));
+      tflite_micro::ReluQuantized(data, tflite_micro::micro::GetTensorShape(input),
+                            tflite_micro::micro::GetTensorShape(output),
+                            tflite_micro::micro::GetTensorData<int8_t>(input),
+                            tflite_micro::micro::GetTensorData<int8_t>(output));
       return kTfLiteOk;
     }
     default: {
@@ -78,25 +104,25 @@ TfLiteStatus Relu6Eval(TfLiteContext* context, TfLiteNode* node) {
   const Relu6OpData& data = *(static_cast<const Relu6OpData*>(node->user_data));
 
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kActivationsInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kActivationsInputTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kActivationsOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kActivationsOutputTensor);
 
   switch (input->type) {
     case kTfLiteFloat32: {
-      Relu6Float(tflite::micro::GetTensorShape(input),
-                 tflite::micro::GetTensorData<float>(input),
-                 tflite::micro::GetTensorShape(output),
-                 tflite::micro::GetTensorData<float>(output));
+      Relu6Float(tflite_micro::micro::GetTensorShape(input),
+                 tflite_micro::micro::GetTensorData<float>(input),
+                 tflite_micro::micro::GetTensorShape(output),
+                 tflite_micro::micro::GetTensorData<float>(output));
 
       return kTfLiteOk;
     }
     case kTfLiteInt8: {
       Relu6Quantized(data.zero_int8, data.six_int8,
-                     tflite::micro::GetTensorShape(input),
-                     tflite::micro::GetTensorData<int8_t>(input),
-                     tflite::micro::GetTensorShape(output),
-                     tflite::micro::GetTensorData<int8_t>(output));
+                     tflite_micro::micro::GetTensorShape(input),
+                     tflite_micro::micro::GetTensorData<int8_t>(input),
+                     tflite_micro::micro::GetTensorShape(output),
+                     tflite_micro::micro::GetTensorData<int8_t>(output));
       return kTfLiteOk;
     }
     default: {
@@ -110,11 +136,11 @@ TfLiteStatus Relu6Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_RELU() {
-  return tflite::micro::RegisterOp(ReluInit, ReluPrepare, ReluEval);
+  return tflite_micro::micro::RegisterOp(ReluInit, ReluPrepare, ReluEval);
 }
 
 TFLMRegistration Register_RELU6() {
-  return tflite::micro::RegisterOp(Relu6Init, Relu6Prepare, Relu6Eval);
+  return tflite_micro::micro::RegisterOp(Relu6Init, Relu6Prepare, Relu6Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/activations.h b/tensorflow/lite/micro/kernels/activations.h
index e953f0e0..15727298 100644
--- a/tensorflow/lite/micro/kernels/activations.h
+++ b/tensorflow/lite/micro/kernels/activations.h
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/c/common.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 extern const int kActivationsInputTensor;
 extern const int kActivationsOutputTensor;
@@ -36,9 +36,10 @@ struct Relu6OpData {
   int8_t zero_int8;
 };
 
+template <typename T>
 void ReluQuantized(const ReluOpData& data, const RuntimeShape& input_shape,
-                   const RuntimeShape& output_shape, const int8_t* input_data,
-                   int8_t* output_data);
+                   const RuntimeShape& output_shape, const T* input_data,
+                   T* output_data);
 
 template <typename T>
 void CalculateReluOpData(const TfLiteTensor* input, TfLiteTensor* output,
@@ -58,6 +59,6 @@ TfLiteStatus ReluPrepare(TfLiteContext* context, TfLiteNode* node);
 
 TfLiteStatus Relu6Prepare(TfLiteContext* context, TfLiteNode* node);
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_ACTIVATIONS_H_
diff --git a/tensorflow/lite/micro/kernels/activations_common.cc b/tensorflow/lite/micro/kernels/activations_common.cc
index 2ec3a1bf..9f24c7db 100644
--- a/tensorflow/lite/micro/kernels/activations_common.cc
+++ b/tensorflow/lite/micro/kernels/activations_common.cc
@@ -28,28 +28,11 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 const int kActivationsInputTensor = 0;
 const int kActivationsOutputTensor = 0;
 
-void ReluQuantized(const ReluOpData& data, const RuntimeShape& input_shape,
-                   const RuntimeShape& output_shape, const int8_t* input_data,
-                   int8_t* output_data) {
-  const int flat_size = MatchingFlatSize(input_shape, output_shape);
-  for (int i = 0; i < flat_size; ++i) {
-    const int32_t val = static_cast<int32_t>(input_data[i]);
-    int32_t clamped =
-        data.params.output_offset +
-        MultiplyByQuantizedMultiplier(val - data.params.input_offset,
-                                      data.params.output_multiplier,
-                                      data.params.output_shift);
-    clamped = std::max(data.params.quantized_activation_min, clamped);
-    clamped = std::min(data.params.quantized_activation_max, clamped);
-    output_data[i] = static_cast<int8_t>(clamped);
-  }
-}
-
 template <typename T>
 void CalculateReluOpData(const TfLiteTensor* input, TfLiteTensor* output,
                          ReluOpData* data) {
@@ -127,6 +110,8 @@ TfLiteStatus ReluPrepare(TfLiteContext* context, TfLiteNode* node) {
 
   if (input->type == kTfLiteInt8) {
     CalculateReluOpData<int8_t>(input, output, data);
+  } else if (input->type == kTfLiteInt16) {
+    CalculateReluOpData<int16_t>(input, output, data);
   }
 
   micro_context->DeallocateTempTfLiteTensor(input);
@@ -155,4 +140,4 @@ TfLiteStatus Relu6Prepare(TfLiteContext* context, TfLiteNode* node) {
   return kTfLiteOk;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/activations_test.cc b/tensorflow/lite/micro/kernels/activations_test.cc
index 25402a80..1bf02d4e 100644
--- a/tensorflow/lite/micro/kernels/activations_test.cc
+++ b/tensorflow/lite/micro/kernels/activations_test.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -171,7 +171,7 @@ void TestRelu6Int8(int* input_dims_data, const float* input_data,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -184,7 +184,7 @@ TF_LITE_MICRO_TEST(SimpleReluTestFloat) {
   const float golden[] = {1.0, 2.0, 3.0, 4.0, 5.0, 0, 0, 0, 0, 0};
   int output_shape[] = {2, 1, 5};
   float output_data[output_elements_count];
-  tflite::testing::TestReluFloat(input_shape, input_data, output_shape, golden,
+  tflite_micro::testing::TestReluFloat(input_shape, input_data, output_shape, golden,
                                  output_data);
 }
 
@@ -199,7 +199,7 @@ TF_LITE_MICRO_TEST(SimpleRelu6TestFloat) {
       4.0, 5.0, 6.0, 6.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0,
   };
 
-  tflite::testing::TestRelu6Float(input_shape, input_data, output_shape, golden,
+  tflite_micro::testing::TestRelu6Float(input_shape, input_data, output_shape, golden,
                                   output_data);
 }
 
@@ -219,7 +219,7 @@ TF_LITE_MICRO_TEST(SimpleReluTestInt8) {
   const float output_scale = 0.5f;
   const int output_zero_point = 0;
 
-  tflite::testing::TestReluInt8(input_shape, input_data, input_quantized,
+  tflite_micro::testing::TestReluInt8(input_shape, input_data, input_quantized,
                                 input_scale, input_zero_point, golden,
                                 golden_quantized, output_shape, output_scale,
                                 output_zero_point, output_data);
@@ -241,7 +241,7 @@ TF_LITE_MICRO_TEST(SimpleRelu6TestInt8) {
   const float output_scale = 0.5f;
   const int output_zero_point = 127;
 
-  tflite::testing::TestRelu6Int8(input_shape, input_data, input_quantized,
+  tflite_micro::testing::TestRelu6Int8(input_shape, input_data, input_quantized,
                                  input_scale, input_zero_point, golden,
                                  golden_quantized, output_shape, output_scale,
                                  output_zero_point, output_data);
diff --git a/tensorflow/lite/micro/kernels/add.cc b/tensorflow/lite/micro/kernels/add.cc
index b27206c6..e0e2512f 100644
--- a/tensorflow/lite/micro/kernels/add.cc
+++ b/tensorflow/lite/micro/kernels/add.cc
@@ -30,7 +30,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/memory_helpers.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 TfLiteStatus EvalAdd(TfLiteContext* context, TfLiteNode* node,
                      TfLiteAddParams* params, const OpDataAdd* data,
@@ -38,45 +38,45 @@ TfLiteStatus EvalAdd(TfLiteContext* context, TfLiteNode* node,
                      const TfLiteEvalTensor* input2, TfLiteEvalTensor* output) {
   switch (output->type) {
     case kTfLiteFloat32: {
-      tflite::ArithmeticParams op_params;
+      tflite_micro::ArithmeticParams op_params;
       SetActivationParams(data->output_activation_min_f32,
                           data->output_activation_max_f32, &op_params);
       if (data->requires_broadcast) {
         reference_ops::BroadcastAdd4DSlow(
-            op_params, tflite::micro::GetTensorShape(input1),
-            tflite::micro::GetTensorData<float>(input1),
-            tflite::micro::GetTensorShape(input2),
-            tflite::micro::GetTensorData<float>(input2),
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<float>(output));
+            op_params, tflite_micro::micro::GetTensorShape(input1),
+            tflite_micro::micro::GetTensorData<float>(input1),
+            tflite_micro::micro::GetTensorShape(input2),
+            tflite_micro::micro::GetTensorData<float>(input2),
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<float>(output));
       } else {
-        reference_ops::Add(op_params, tflite::micro::GetTensorShape(input1),
-                           tflite::micro::GetTensorData<float>(input1),
-                           tflite::micro::GetTensorShape(input2),
-                           tflite::micro::GetTensorData<float>(input2),
-                           tflite::micro::GetTensorShape(output),
-                           tflite::micro::GetTensorData<float>(output));
+        reference_ops::Add(op_params, tflite_micro::micro::GetTensorShape(input1),
+                           tflite_micro::micro::GetTensorData<float>(input1),
+                           tflite_micro::micro::GetTensorShape(input2),
+                           tflite_micro::micro::GetTensorData<float>(input2),
+                           tflite_micro::micro::GetTensorShape(output),
+                           tflite_micro::micro::GetTensorData<float>(output));
       }
     } break;
     case kTfLiteInt32: {
-      tflite::ArithmeticParams op_params;
+      tflite_micro::ArithmeticParams op_params;
       SetActivationParams(std::numeric_limits<int32_t>::lowest(),
                           std::numeric_limits<int32_t>::max(), &op_params);
       if (data->requires_broadcast) {
         reference_ops::BroadcastAdd4DSlow(
-            op_params, tflite::micro::GetTensorShape(input1),
-            tflite::micro::GetTensorData<int32_t>(input1),
-            tflite::micro::GetTensorShape(input2),
-            tflite::micro::GetTensorData<int32_t>(input2),
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<int32_t>(output));
+            op_params, tflite_micro::micro::GetTensorShape(input1),
+            tflite_micro::micro::GetTensorData<int32_t>(input1),
+            tflite_micro::micro::GetTensorShape(input2),
+            tflite_micro::micro::GetTensorData<int32_t>(input2),
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int32_t>(output));
       } else {
-        reference_ops::Add(op_params, tflite::micro::GetTensorShape(input1),
-                           tflite::micro::GetTensorData<int32_t>(input1),
-                           tflite::micro::GetTensorShape(input2),
-                           tflite::micro::GetTensorData<int32_t>(input2),
-                           tflite::micro::GetTensorShape(output),
-                           tflite::micro::GetTensorData<int32_t>(output));
+        reference_ops::Add(op_params, tflite_micro::micro::GetTensorShape(input1),
+                           tflite_micro::micro::GetTensorData<int32_t>(input1),
+                           tflite_micro::micro::GetTensorShape(input2),
+                           tflite_micro::micro::GetTensorData<int32_t>(input2),
+                           tflite_micro::micro::GetTensorShape(output),
+                           tflite_micro::micro::GetTensorData<int32_t>(output));
       }
     } break;
     default:
@@ -93,7 +93,7 @@ TfLiteStatus EvalAddQuantized(TfLiteContext* context, TfLiteNode* node,
                               const TfLiteEvalTensor* input1,
                               const TfLiteEvalTensor* input2,
                               TfLiteEvalTensor* output) {
-  tflite::ArithmeticParams op_params;
+  tflite_micro::ArithmeticParams op_params;
   op_params.left_shift = data->left_shift;
   op_params.input1_offset = data->input1_offset;
   op_params.input1_multiplier = data->input1_multiplier;
@@ -107,46 +107,46 @@ TfLiteStatus EvalAddQuantized(TfLiteContext* context, TfLiteNode* node,
   SetActivationParams(data->output_activation_min, data->output_activation_max,
                       &op_params);
   bool need_broadcast = reference_ops::ProcessBroadcastShapes(
-      tflite::micro::GetTensorShape(input1),
-      tflite::micro::GetTensorShape(input2), &op_params);
+      tflite_micro::micro::GetTensorShape(input1),
+      tflite_micro::micro::GetTensorShape(input2), &op_params);
 
   switch (output->type) {
     case kTfLiteInt8: {
       if (need_broadcast) {
         reference_integer_ops::BroadcastAdd4DSlow(
-            op_params, tflite::micro::GetTensorShape(input1),
-            tflite::micro::GetTensorData<int8_t>(input1),
-            tflite::micro::GetTensorShape(input2),
-            tflite::micro::GetTensorData<int8_t>(input2),
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<int8_t>(output));
+            op_params, tflite_micro::micro::GetTensorShape(input1),
+            tflite_micro::micro::GetTensorData<int8_t>(input1),
+            tflite_micro::micro::GetTensorShape(input2),
+            tflite_micro::micro::GetTensorData<int8_t>(input2),
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int8_t>(output));
       } else {
         reference_integer_ops::Add(
-            op_params, tflite::micro::GetTensorShape(input1),
-            tflite::micro::GetTensorData<int8_t>(input1),
-            tflite::micro::GetTensorShape(input2),
-            tflite::micro::GetTensorData<int8_t>(input2),
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<int8_t>(output));
+            op_params, tflite_micro::micro::GetTensorShape(input1),
+            tflite_micro::micro::GetTensorData<int8_t>(input1),
+            tflite_micro::micro::GetTensorShape(input2),
+            tflite_micro::micro::GetTensorData<int8_t>(input2),
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int8_t>(output));
       }
       break;
     }
     case kTfLiteInt16: {
       if (need_broadcast) {
         reference_ops::BroadcastAdd4DSlow(
-            op_params, tflite::micro::GetTensorShape(input1),
-            tflite::micro::GetTensorData<int16_t>(input1),
-            tflite::micro::GetTensorShape(input2),
-            tflite::micro::GetTensorData<int16_t>(input2),
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<int16_t>(output));
+            op_params, tflite_micro::micro::GetTensorShape(input1),
+            tflite_micro::micro::GetTensorData<int16_t>(input1),
+            tflite_micro::micro::GetTensorShape(input2),
+            tflite_micro::micro::GetTensorData<int16_t>(input2),
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int16_t>(output));
       } else {
-        reference_ops::Add(op_params, tflite::micro::GetTensorShape(input1),
-                           tflite::micro::GetTensorData<int16_t>(input1),
-                           tflite::micro::GetTensorShape(input2),
-                           tflite::micro::GetTensorData<int16_t>(input2),
-                           tflite::micro::GetTensorShape(output),
-                           tflite::micro::GetTensorData<int16_t>(output),
+        reference_ops::Add(op_params, tflite_micro::micro::GetTensorShape(input1),
+                           tflite_micro::micro::GetTensorData<int16_t>(input1),
+                           tflite_micro::micro::GetTensorShape(input2),
+                           tflite_micro::micro::GetTensorData<int16_t>(input2),
+                           tflite_micro::micro::GetTensorShape(output),
+                           tflite_micro::micro::GetTensorData<int16_t>(output),
                            false);
       }
       break;
@@ -172,11 +172,11 @@ TfLiteStatus AddEval(TfLiteContext* context, TfLiteNode* node) {
   const OpDataAdd* data = static_cast<const OpDataAdd*>(node->user_data);
 
   const TfLiteEvalTensor* input1 =
-      tflite::micro::GetEvalInput(context, node, kAddInputTensor1);
+      tflite_micro::micro::GetEvalInput(context, node, kAddInputTensor1);
   const TfLiteEvalTensor* input2 =
-      tflite::micro::GetEvalInput(context, node, kAddInputTensor2);
+      tflite_micro::micro::GetEvalInput(context, node, kAddInputTensor2);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kAddOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kAddOutputTensor);
 
   if (output->type == kTfLiteFloat32 || output->type == kTfLiteInt32) {
     TF_LITE_ENSURE_OK(
@@ -194,7 +194,7 @@ TfLiteStatus AddEval(TfLiteContext* context, TfLiteNode* node) {
 }
 
 TFLMRegistration Register_ADD() {
-  return tflite::micro::RegisterOp(AddInit, AddPrepare, AddEval);
+  return tflite_micro::micro::RegisterOp(AddInit, AddPrepare, AddEval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/add.h b/tensorflow/lite/micro/kernels/add.h
index 7ee0fc3e..201ec9f8 100644
--- a/tensorflow/lite/micro/kernels/add.h
+++ b/tensorflow/lite/micro/kernels/add.h
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/c/common.h"
 #include "tensorflow/lite/micro/micro_common.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 extern const int kAddInputTensor1;
 extern const int kAddInputTensor2;
@@ -73,6 +73,6 @@ inline TFLMRegistration Register_ADD_INT8() { return Register_ADD(); }
 
 inline TFLMRegistration Register_ADD_INT16() { return Register_ADD(); }
 #endif
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_ADD_H_
diff --git a/tensorflow/lite/micro/kernels/add_common.cc b/tensorflow/lite/micro/kernels/add_common.cc
index cc945091..034fa5ef 100644
--- a/tensorflow/lite/micro/kernels/add_common.cc
+++ b/tensorflow/lite/micro/kernels/add_common.cc
@@ -26,7 +26,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/memory_helpers.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 const int kAddInputTensor1 = 0;
 const int kAddInputTensor2 = 1;
@@ -113,4 +113,4 @@ TfLiteStatus AddPrepare(TfLiteContext* context, TfLiteNode* node) {
   return kTfLiteOk;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/add_n.cc b/tensorflow/lite/micro/kernels/add_n.cc
index 765d5d63..46fce901 100644
--- a/tensorflow/lite/micro/kernels/add_n.cc
+++ b/tensorflow/lite/micro/kernels/add_n.cc
@@ -24,7 +24,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 constexpr int kInputTensor0 = 0;
@@ -146,8 +146,8 @@ inline const T** CopyInputsToScratchBuffer(TfLiteContext* context,
   const T** all_inputs = static_cast<decltype(all_inputs)>(scratch_buffer);
   for (int i = 0; i < num_inputs; i++) {
     const TfLiteEvalTensor* next_input =
-        tflite::micro::GetEvalInput(context, node, kInputTensor0 + i);
-    all_inputs[i] = tflite::micro::GetTensorData<T>(next_input);
+        tflite_micro::micro::GetEvalInput(context, node, kInputTensor0 + i);
+    all_inputs[i] = tflite_micro::micro::GetTensorData<T>(next_input);
   }
 
   return all_inputs;
@@ -163,8 +163,8 @@ void EvalAddN(TfLiteContext* context, TfLiteNode* node,
   const T** all_inputs =
       CopyInputsToScratchBuffer<T>(context, node, scratch_index);
 
-  reference_ops::AddN<T>(tflite::micro::GetTensorShape(output), num_inputs,
-                         all_inputs, tflite::micro::GetTensorData<T>(output));
+  reference_ops::AddN<T>(tflite_micro::micro::GetTensorShape(output), num_inputs,
+                         all_inputs, tflite_micro::micro::GetTensorData<T>(output));
 }
 
 template <typename T>
@@ -187,13 +187,13 @@ void EvalAddNQuantized(TfLiteContext* context, TfLiteNode* node,
   SetActivationParams(data->output_activation_min, data->output_activation_max,
                       &params);
 
-  reference_ops::AddN(params, tflite::micro::GetTensorShape(output), num_inputs,
-                      all_inputs, tflite::micro::GetTensorData<T>(output));
+  reference_ops::AddN(params, tflite_micro::micro::GetTensorShape(output), num_inputs,
+                      all_inputs, tflite_micro::micro::GetTensorData<T>(output));
 }
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
   if (output->type == kTfLiteFloat32) {
     EvalAddN<float>(context, node, output);
   } else if (output->type == kTfLiteInt8) {
@@ -209,7 +209,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_ADD_N() {
-  return tflite::micro::RegisterOp(nullptr, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/add_n_test.cc b/tensorflow/lite/micro/kernels/add_n_test.cc
index 8cc5cdc6..99f0ec94 100644
--- a/tensorflow/lite/micro/kernels/add_n_test.cc
+++ b/tensorflow/lite/micro/kernels/add_n_test.cc
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -38,7 +38,7 @@ void ExecuteAddN(TfLiteTensor* tensors, int tensors_count) {
   int kOutputArrayData[] = {1, tensors_count - 1};
   TfLiteIntArray* outputs_array = IntArrayFromInts(kOutputArrayData);
 
-  const TFLMRegistration registration = tflite::Register_ADD_N();
+  const TFLMRegistration registration = tflite_micro::Register_ADD_N();
   micro::KernelRunner runner(registration, tensors, tensors_count, inputs_array,
                              outputs_array, nullptr);
 
@@ -120,7 +120,7 @@ void TestAddNQuantized(TestQuantParams<T, kNumInputs, kOutputSize>* params,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -130,7 +130,7 @@ TF_LITE_MICRO_TEST(FloatAddNOpAddMultipleTensors) {
   constexpr float kInput2[] = {0.1, 0.2, 0.3, 0.5};
   constexpr float kInput3[] = {0.5, 0.1, 0.1, 0.2};
   constexpr float kExpect[] = {-1.4, 0.5, 1.1, 1.5};
-  const float* kInputs[tflite::testing::kMaxInputTensors] = {
+  const float* kInputs[tflite_micro::testing::kMaxInputTensors] = {
       kInput1,
       kInput2,
       kInput3,
@@ -139,7 +139,7 @@ TF_LITE_MICRO_TEST(FloatAddNOpAddMultipleTensors) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
 
-  tflite::testing::TestAddN(kDims, kInputs, kInputCount, kDims, kExpect,
+  tflite_micro::testing::TestAddN(kDims, kInputs, kInputCount, kDims, kExpect,
                             output_data);
 }
 
@@ -149,7 +149,7 @@ TF_LITE_MICRO_TEST(Int8AddNOpAddMultipleTensors) {
   constexpr float kInput2[] = {0.1, 0.2, 0.3, 0.5};
   constexpr float kInput3[] = {0.5, 0.1, 0.1, 0.2};
   constexpr float kExpect[] = {-1.4, 0.5, 1.1, 1.5};
-  const float* kInputs[tflite::testing::kMaxInputTensors] = {
+  const float* kInputs[tflite_micro::testing::kMaxInputTensors] = {
       kInput1,
       kInput2,
       kInput3,
@@ -158,12 +158,12 @@ TF_LITE_MICRO_TEST(Int8AddNOpAddMultipleTensors) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
 
-  tflite::testing::TestQuantParams<int8_t, kInputCount, kOutputCount> params =
+  tflite_micro::testing::TestQuantParams<int8_t, kInputCount, kOutputCount> params =
       {};
   params.data_min = -3.0;
   params.data_max = 3.0;
 
-  tflite::testing::TestAddNQuantized<int8_t, kInputCount, kOutputCount>(
+  tflite_micro::testing::TestAddNQuantized<int8_t, kInputCount, kOutputCount>(
       &params, kDims, kInputs, kDims, kExpect, output_data);
 }
 
diff --git a/tensorflow/lite/micro/kernels/add_test.cc b/tensorflow/lite/micro/kernels/add_test.cc
index 6e8b40c0..41fda2dc 100644
--- a/tensorflow/lite/micro/kernels/add_test.cc
+++ b/tensorflow/lite/micro/kernels/add_test.cc
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -145,16 +145,16 @@ void TestAddQuantized(int* input1_dims_data, const float* input1_data,
   constexpr int outputs_size = 1;
   constexpr int tensors_size = inputs_size + outputs_size;
   TfLiteTensor tensors[tensors_size] = {
-      tflite::testing::CreateQuantizedTensor(input1_data, input1_quantized,
+      tflite_micro::testing::CreateQuantizedTensor(input1_data, input1_quantized,
                                              input1_dims, input1_scale,
                                              input1_zero_point),
-      tflite::testing::CreateQuantizedTensor(input2_data, input2_quantized,
+      tflite_micro::testing::CreateQuantizedTensor(input2_data, input2_quantized,
                                              input2_dims, input2_scale,
                                              input2_zero_point),
-      tflite::testing::CreateQuantizedTensor(output_data, output_dims,
+      tflite_micro::testing::CreateQuantizedTensor(output_data, output_dims,
                                              output_scale, output_zero_point),
   };
-  tflite::Quantize(golden, golden_quantized, ElementCount(*output_dims),
+  tflite_micro::Quantize(golden, golden_quantized, ElementCount(*output_dims),
                    output_scale, output_zero_point);
 
   ValidateAddGoldens(tensors, tensors_size, golden_quantized, output_data,
@@ -163,7 +163,7 @@ void TestAddQuantized(int* input1_dims_data, const float* input1_data,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -174,7 +174,7 @@ TF_LITE_MICRO_TEST(FloatAddNoActivation) {
   const float golden_values[] = {-1.9, 0.4, 1.0, 1.3};
   constexpr int kOutputDimsCount = 4;
   float output_data[kOutputDimsCount];
-  tflite::testing::TestAddFloat(inout_shape, input1_values, inout_shape,
+  tflite_micro::testing::TestAddFloat(inout_shape, input1_values, inout_shape,
                                 input2_values, inout_shape, golden_values,
                                 kTfLiteActNone, output_data);
 }
@@ -187,7 +187,7 @@ TF_LITE_MICRO_TEST(FloatAddActivationRelu1) {
 
   constexpr int kOutputDimsCount = 4;
   float output_data[kOutputDimsCount];
-  tflite::testing::TestAddFloat(inout_shape, input1_values, inout_shape,
+  tflite_micro::testing::TestAddFloat(inout_shape, input1_values, inout_shape,
                                 input2_values, inout_shape, golden_values,
                                 kTfLiteActReluN1To1, output_data);
 }
@@ -210,7 +210,7 @@ TF_LITE_MICRO_TEST(FloatAddVariousInputShapes) {
   };
 
   for (int i = 0; i < num_shapes; ++i) {
-    tflite::testing::TestAddFloat(test_shapes[i], input1_values, test_shapes[i],
+    tflite_micro::testing::TestAddFloat(test_shapes[i], input1_values, test_shapes[i],
                                   input2_values, test_shapes[i],
                                   expected_output, kTfLiteActNone, output_data);
   }
@@ -235,7 +235,7 @@ TF_LITE_MICRO_TEST(FloatAddWithScalarBroadcast) {
   };
 
   for (int i = 0; i < num_shapes; ++i) {
-    tflite::testing::TestAddFloat(test_shapes[i], input1_values, input2_shape,
+    tflite_micro::testing::TestAddFloat(test_shapes[i], input1_values, input2_shape,
                                   input2_values, test_shapes[i],
                                   expected_output, kTfLiteActNone, output_data);
   }
@@ -248,7 +248,7 @@ TF_LITE_MICRO_TEST(Int32AddNoActivation) {
   const int32_t golden_values[] = {1, 2147483647, -2147483648, 419644487};
   constexpr int kOutputDimsCount = 4;
   int32_t output_data[kOutputDimsCount];
-  tflite::testing::TestAddInt32(inout_shape, input1_values, inout_shape,
+  tflite_micro::testing::TestAddInt32(inout_shape, input1_values, inout_shape,
                                 input2_values, inout_shape, golden_values,
                                 kTfLiteActNone, output_data);
 }
@@ -267,7 +267,7 @@ TF_LITE_MICRO_TEST(QuantizedAddNoActivationInt8) {
   int8_t golden_quantized[kOutputDimsCount];
   int8_t output[kOutputDimsCount];
 
-  tflite::testing::TestAddQuantized(
+  tflite_micro::testing::TestAddQuantized(
       inout_shape, input1_values, input1_quantized, scales[0], zero_points[0],
       inout_shape, input2_values, input2_quantized, scales[1], zero_points[1],
       inout_shape, golden_values, golden_quantized, scales[2], zero_points[2],
@@ -288,7 +288,7 @@ TF_LITE_MICRO_TEST(QuantizedAddActivationRelu1Int8) {
   int8_t golden_quantized[kOutputDimsCount];
   int8_t output[kOutputDimsCount];
 
-  tflite::testing::TestAddQuantized(
+  tflite_micro::testing::TestAddQuantized(
       inout_shape, input1_values, input1_quantized, scales[0], zero_points[0],
       inout_shape, input2_values, input2_quantized, scales[1], zero_points[1],
       inout_shape, golden_values, golden_quantized, scales[2], zero_points[2],
@@ -319,7 +319,7 @@ TF_LITE_MICRO_TEST(QuantizedAddVariousInputShapesInt8) {
   int8_t output[kOutputDimsCount];
 
   for (int i = 0; i < num_shapes; i++) {
-    tflite::testing::TestAddQuantized(
+    tflite_micro::testing::TestAddQuantized(
         test_shapes[i], input1_values, input1_quantized, scales[0],
         zero_points[0], test_shapes[i], input2_values, input2_quantized,
         scales[1], zero_points[1], test_shapes[i], golden_values,
@@ -328,15 +328,15 @@ TF_LITE_MICRO_TEST(QuantizedAddVariousInputShapesInt8) {
 }
 
 TF_LITE_MICRO_TEST(QuantizedAddWithScalarBroadcastFloat) {
-  float output_float[tflite::testing::kBroadcastOutputDimsCount];
-
-  for (int i = 0; i < tflite::testing::kBroadcastNumShapes; ++i) {
-    tflite::testing::TestAddFloat(tflite::testing::broadcast_input1_shape,
-                                  tflite::testing::broadcast_input1_values,
-                                  tflite::testing::broadcast_input2_shapes[i],
-                                  tflite::testing::broadcast_input2_values,
-                                  tflite::testing::broadcast_output_shapes[i],
-                                  tflite::testing::broadcast_goldens[i],
+  float output_float[tflite_micro::testing::kBroadcastOutputDimsCount];
+
+  for (int i = 0; i < tflite_micro::testing::kBroadcastNumShapes; ++i) {
+    tflite_micro::testing::TestAddFloat(tflite_micro::testing::broadcast_input1_shape,
+                                  tflite_micro::testing::broadcast_input1_values,
+                                  tflite_micro::testing::broadcast_input2_shapes[i],
+                                  tflite_micro::testing::broadcast_input2_values,
+                                  tflite_micro::testing::broadcast_output_shapes[i],
+                                  tflite_micro::testing::broadcast_goldens[i],
                                   kTfLiteActNone, output_float);
   }
 }
@@ -366,7 +366,7 @@ TF_LITE_MICRO_TEST(QuantizedAddWithScalarBroadcastInt8) {
   int8_t output[kOutputDimsCount];
 
   for (int i = 0; i < num_shapes; ++i) {
-    tflite::testing::TestAddQuantized(
+    tflite_micro::testing::TestAddQuantized(
         test_shapes[i], input1_values, input1_quantized, scales[0],
         zero_points[0], input2_shape, input2_values, input2_quantized,
         scales[1], zero_points[1], test_shapes[i], golden, golden_quantized,
@@ -377,19 +377,19 @@ TF_LITE_MICRO_TEST(QuantizedAddWithScalarBroadcastInt8) {
 TF_LITE_MICRO_TEST(QuantizedAddWithMixedBroadcastInt8) {
   const float scales[] = {0.1, 0.05, 0.1};
   const int zero_points[] = {-10, -5, 7};
-  int8_t input1_quantized[tflite::testing::kBroadcastOutputDimsCount];
-  int8_t input2_quantized[tflite::testing::kBroadcastOutputDimsCount];
-  int8_t golden_quantized[tflite::testing::kBroadcastOutputDimsCount];
-  int8_t output[tflite::testing::kBroadcastOutputDimsCount];
-
-  for (int i = 0; i < tflite::testing::kBroadcastNumShapes; ++i) {
-    tflite::testing::TestAddQuantized(
-        tflite::testing::broadcast_input1_shape,
-        tflite::testing::broadcast_input1_values, input1_quantized, scales[0],
-        zero_points[0], tflite::testing::broadcast_input2_shapes[i],
-        tflite::testing::broadcast_input2_values, input2_quantized, scales[1],
-        zero_points[1], tflite::testing::broadcast_output_shapes[i],
-        tflite::testing::broadcast_goldens[i], golden_quantized, scales[2],
+  int8_t input1_quantized[tflite_micro::testing::kBroadcastOutputDimsCount];
+  int8_t input2_quantized[tflite_micro::testing::kBroadcastOutputDimsCount];
+  int8_t golden_quantized[tflite_micro::testing::kBroadcastOutputDimsCount];
+  int8_t output[tflite_micro::testing::kBroadcastOutputDimsCount];
+
+  for (int i = 0; i < tflite_micro::testing::kBroadcastNumShapes; ++i) {
+    tflite_micro::testing::TestAddQuantized(
+        tflite_micro::testing::broadcast_input1_shape,
+        tflite_micro::testing::broadcast_input1_values, input1_quantized, scales[0],
+        zero_points[0], tflite_micro::testing::broadcast_input2_shapes[i],
+        tflite_micro::testing::broadcast_input2_values, input2_quantized, scales[1],
+        zero_points[1], tflite_micro::testing::broadcast_output_shapes[i],
+        tflite_micro::testing::broadcast_goldens[i], golden_quantized, scales[2],
         zero_points[2], kTfLiteActNone, output);
   }
 }
@@ -408,7 +408,7 @@ TF_LITE_MICRO_TEST(QuantizedAddNoActivationInt16) {
   int16_t golden_quantized[kOutputDimsCount];
   int16_t output[kOutputDimsCount];
 
-  tflite::testing::TestAddQuantized(
+  tflite_micro::testing::TestAddQuantized(
       inout_shape, input1_values, input1_quantized, scales[0], zero_points[0],
       inout_shape, input2_values, input2_quantized, scales[1], zero_points[1],
       inout_shape, golden_values, golden_quantized, scales[2], zero_points[2],
@@ -429,7 +429,7 @@ TF_LITE_MICRO_TEST(QuantizedAddActivationRelu1Int16) {
   int16_t golden_quantized[kOutputDimsCount];
   int16_t output[kOutputDimsCount];
 
-  tflite::testing::TestAddQuantized(
+  tflite_micro::testing::TestAddQuantized(
       inout_shape, input1_values, input1_quantized, scales[0], zero_points[0],
       inout_shape, input2_values, input2_quantized, scales[1], zero_points[1],
       inout_shape, golden_values, golden_quantized, scales[2], zero_points[2],
@@ -460,7 +460,7 @@ TF_LITE_MICRO_TEST(QuantizedAddVariousInputShapesInt16) {
   int16_t output[kOutputDimsCount];
 
   for (int i = 0; i < num_shapes; i++) {
-    tflite::testing::TestAddQuantized(
+    tflite_micro::testing::TestAddQuantized(
         test_shapes[i], input1_values, input1_quantized, scales[0],
         zero_points[0], test_shapes[i], input2_values, input2_quantized,
         scales[1], zero_points[1], test_shapes[i], golden_values,
@@ -493,7 +493,7 @@ TF_LITE_MICRO_TEST(QuantizedAddWithScalarBroadcastInt16) {
   int16_t output[kOutputDimsCount];
 
   for (int i = 0; i < num_shapes; ++i) {
-    tflite::testing::TestAddQuantized(
+    tflite_micro::testing::TestAddQuantized(
         test_shapes[i], input1_values, input1_quantized, scales[0],
         zero_points[0], input2_shape, input2_values, input2_quantized,
         scales[1], zero_points[1], test_shapes[i], golden, golden_quantized,
@@ -504,19 +504,19 @@ TF_LITE_MICRO_TEST(QuantizedAddWithScalarBroadcastInt16) {
 TF_LITE_MICRO_TEST(QuantizedAddWithMixedBroadcastInt16) {
   const float scales[] = {0.1, 0.05, 0.1};
   const int zero_points[] = {0, 0, 0};
-  int16_t input1_quantized[tflite::testing::kBroadcastOutputDimsCount];
-  int16_t input2_quantized[tflite::testing::kBroadcastOutputDimsCount];
-  int16_t golden_quantized[tflite::testing::kBroadcastOutputDimsCount];
-  int16_t output[tflite::testing::kBroadcastOutputDimsCount];
-
-  for (int i = 0; i < tflite::testing::kBroadcastNumShapes; ++i) {
-    tflite::testing::TestAddQuantized(
-        tflite::testing::broadcast_input1_shape,
-        tflite::testing::broadcast_input1_values, input1_quantized, scales[0],
-        zero_points[0], tflite::testing::broadcast_input2_shapes[i],
-        tflite::testing::broadcast_input2_values, input2_quantized, scales[1],
-        zero_points[1], tflite::testing::broadcast_output_shapes[i],
-        tflite::testing::broadcast_goldens[i], golden_quantized, scales[2],
+  int16_t input1_quantized[tflite_micro::testing::kBroadcastOutputDimsCount];
+  int16_t input2_quantized[tflite_micro::testing::kBroadcastOutputDimsCount];
+  int16_t golden_quantized[tflite_micro::testing::kBroadcastOutputDimsCount];
+  int16_t output[tflite_micro::testing::kBroadcastOutputDimsCount];
+
+  for (int i = 0; i < tflite_micro::testing::kBroadcastNumShapes; ++i) {
+    tflite_micro::testing::TestAddQuantized(
+        tflite_micro::testing::broadcast_input1_shape,
+        tflite_micro::testing::broadcast_input1_values, input1_quantized, scales[0],
+        zero_points[0], tflite_micro::testing::broadcast_input2_shapes[i],
+        tflite_micro::testing::broadcast_input2_values, input2_quantized, scales[1],
+        zero_points[1], tflite_micro::testing::broadcast_output_shapes[i],
+        tflite_micro::testing::broadcast_goldens[i], golden_quantized, scales[2],
         zero_points[2], kTfLiteActNone, output);
   }
 }
diff --git a/tensorflow/lite/micro/kernels/arc_mli/add.cc b/tensorflow/lite/micro/kernels/arc_mli/add.cc
index d6cbddd6..ab3160fa 100644
--- a/tensorflow/lite/micro/kernels/arc_mli/add.cc
+++ b/tensorflow/lite/micro/kernels/arc_mli/add.cc
@@ -35,7 +35,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/memory_helpers.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 constexpr int kInputTensor1 = 0;
 constexpr int kInputTensor2 = 1;
@@ -173,24 +173,24 @@ TfLiteStatus EvalAdd(TfLiteContext* context, TfLiteNode* node,
                      const TfLiteEvalTensor* input1,
                      const TfLiteEvalTensor* input2, TfLiteEvalTensor* output) {
 #if !defined(TF_LITE_STRIP_REFERENCE_IMPL)
-  tflite::ArithmeticParams op_params;
+  tflite_micro::ArithmeticParams op_params;
   SetActivationParams(data->output_activation_min_f32,
                       data->output_activation_max_f32, &op_params);
   if (data->requires_broadcast) {
     reference_ops::BroadcastAdd4DSlow(
-        op_params, tflite::micro::GetTensorShape(input1),
-        tflite::micro::GetTensorData<float>(input1),
-        tflite::micro::GetTensorShape(input2),
-        tflite::micro::GetTensorData<float>(input2),
-        tflite::micro::GetTensorShape(output),
-        tflite::micro::GetTensorData<float>(output));
+        op_params, tflite_micro::micro::GetTensorShape(input1),
+        tflite_micro::micro::GetTensorData<float>(input1),
+        tflite_micro::micro::GetTensorShape(input2),
+        tflite_micro::micro::GetTensorData<float>(input2),
+        tflite_micro::micro::GetTensorShape(output),
+        tflite_micro::micro::GetTensorData<float>(output));
   } else {
-    reference_ops::Add(op_params, tflite::micro::GetTensorShape(input1),
-                       tflite::micro::GetTensorData<float>(input1),
-                       tflite::micro::GetTensorShape(input2),
-                       tflite::micro::GetTensorData<float>(input2),
-                       tflite::micro::GetTensorShape(output),
-                       tflite::micro::GetTensorData<float>(output));
+    reference_ops::Add(op_params, tflite_micro::micro::GetTensorShape(input1),
+                       tflite_micro::micro::GetTensorData<float>(input1),
+                       tflite_micro::micro::GetTensorShape(input2),
+                       tflite_micro::micro::GetTensorData<float>(input2),
+                       tflite_micro::micro::GetTensorShape(output),
+                       tflite_micro::micro::GetTensorData<float>(output));
   }
   return kTfLiteOk;
 #else
@@ -205,7 +205,7 @@ TfLiteStatus EvalAddQuantized(TfLiteContext* context, TfLiteNode* node,
                               const TfLiteEvalTensor* input2,
                               TfLiteEvalTensor* output) {
 #if !defined(TF_LITE_STRIP_REFERENCE_IMPL)
-  tflite::ArithmeticParams op_params;
+  tflite_micro::ArithmeticParams op_params;
   op_params.left_shift = data->left_shift;
   op_params.input1_offset = data->input1_offset;
   op_params.input1_multiplier = data->input1_multiplier;
@@ -219,46 +219,46 @@ TfLiteStatus EvalAddQuantized(TfLiteContext* context, TfLiteNode* node,
   SetActivationParams(data->output_activation_min, data->output_activation_max,
                       &op_params);
   bool need_broadcast = reference_ops::ProcessBroadcastShapes(
-      tflite::micro::GetTensorShape(input1),
-      tflite::micro::GetTensorShape(input2), &op_params);
+      tflite_micro::micro::GetTensorShape(input1),
+      tflite_micro::micro::GetTensorShape(input2), &op_params);
 
   switch (output->type) {
     case kTfLiteInt8: {
       if (need_broadcast) {
         reference_integer_ops::BroadcastAdd4DSlow(
-            op_params, tflite::micro::GetTensorShape(input1),
-            tflite::micro::GetTensorData<int8_t>(input1),
-            tflite::micro::GetTensorShape(input2),
-            tflite::micro::GetTensorData<int8_t>(input2),
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<int8_t>(output));
+            op_params, tflite_micro::micro::GetTensorShape(input1),
+            tflite_micro::micro::GetTensorData<int8_t>(input1),
+            tflite_micro::micro::GetTensorShape(input2),
+            tflite_micro::micro::GetTensorData<int8_t>(input2),
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int8_t>(output));
       } else {
         reference_integer_ops::Add(
-            op_params, tflite::micro::GetTensorShape(input1),
-            tflite::micro::GetTensorData<int8_t>(input1),
-            tflite::micro::GetTensorShape(input2),
-            tflite::micro::GetTensorData<int8_t>(input2),
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<int8_t>(output));
+            op_params, tflite_micro::micro::GetTensorShape(input1),
+            tflite_micro::micro::GetTensorData<int8_t>(input1),
+            tflite_micro::micro::GetTensorShape(input2),
+            tflite_micro::micro::GetTensorData<int8_t>(input2),
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int8_t>(output));
       }
       break;
     }
     case kTfLiteInt16: {
       if (need_broadcast) {
         reference_ops::BroadcastAdd4DSlow(
-            op_params, tflite::micro::GetTensorShape(input1),
-            tflite::micro::GetTensorData<int16_t>(input1),
-            tflite::micro::GetTensorShape(input2),
-            tflite::micro::GetTensorData<int16_t>(input2),
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<int16_t>(output));
+            op_params, tflite_micro::micro::GetTensorShape(input1),
+            tflite_micro::micro::GetTensorData<int16_t>(input1),
+            tflite_micro::micro::GetTensorShape(input2),
+            tflite_micro::micro::GetTensorData<int16_t>(input2),
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int16_t>(output));
       } else {
-        reference_ops::Add(op_params, tflite::micro::GetTensorShape(input1),
-                           tflite::micro::GetTensorData<int16_t>(input1),
-                           tflite::micro::GetTensorShape(input2),
-                           tflite::micro::GetTensorData<int16_t>(input2),
-                           tflite::micro::GetTensorShape(output),
-                           tflite::micro::GetTensorData<int16_t>(output),
+        reference_ops::Add(op_params, tflite_micro::micro::GetTensorShape(input1),
+                           tflite_micro::micro::GetTensorData<int16_t>(input1),
+                           tflite_micro::micro::GetTensorShape(input2),
+                           tflite_micro::micro::GetTensorData<int16_t>(input2),
+                           tflite_micro::micro::GetTensorShape(output),
+                           tflite_micro::micro::GetTensorData<int16_t>(output),
                            false);
       }
       break;
@@ -395,11 +395,11 @@ TfLiteStatus AddEval(TfLiteContext* context, TfLiteNode* node) {
   const OpData* data = static_cast<const OpData*>(node->user_data);
 
   const TfLiteEvalTensor* input1 =
-      tflite::micro::GetEvalInput(context, node, kInputTensor1);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor1);
   const TfLiteEvalTensor* input2 =
-      tflite::micro::GetEvalInput(context, node, kInputTensor2);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor2);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
   if (data->is_mli_applicable) {
     ret_val =
         EvalMLIAddInt8(context, node, params, data, input1, input2, output);
@@ -418,7 +418,7 @@ TfLiteStatus AddEval(TfLiteContext* context, TfLiteNode* node) {
 }
 
 TFLMRegistration Register_ADD() {
-  return tflite::micro::RegisterOp(AddInit, AddPrepare, AddEval);
+  return tflite_micro::micro::RegisterOp(AddInit, AddPrepare, AddEval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/arc_mli/conv.cc b/tensorflow/lite/micro/kernels/arc_mli/conv.cc
index 41d2c535..847c3ace 100644
--- a/tensorflow/lite/micro/kernels/arc_mli/conv.cc
+++ b/tensorflow/lite/micro/kernels/arc_mli/conv.cc
@@ -32,7 +32,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 constexpr int kInputTensor = 0;
@@ -151,7 +151,7 @@ TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node,
   if (data_type != kTfLiteFloat32 && !data->is_mli_applicable) {
     int output_channels = filter->dims->data[kConvQuantizedDimension];
 
-    TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(
+    TF_LITE_ENSURE_STATUS(tflite_micro::PopulateConvolutionQuantizationParams(
         context, input, filter, bias, output, params->activation,
         &data->output_multiplier, &data->output_shift,
         &data->output_activation_min, &data->output_activation_max,
@@ -571,14 +571,14 @@ void EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,
 
   reference_integer_ops::ConvPerChannel(
       op_params, data.per_channel_output_multiplier,
-      data.per_channel_output_shift, tflite::micro::GetTensorShape(input),
-      tflite::micro::GetTensorData<int8_t>(input),
-      tflite::micro::GetTensorShape(filter),
-      tflite::micro::GetTensorData<int8_t>(filter),
-      tflite::micro::GetTensorShape(bias),
-      tflite::micro::GetTensorData<int32_t>(bias),
-      tflite::micro::GetTensorShape(output),
-      tflite::micro::GetTensorData<int8_t>(output));
+      data.per_channel_output_shift, tflite_micro::micro::GetTensorShape(input),
+      tflite_micro::micro::GetTensorData<int8_t>(input),
+      tflite_micro::micro::GetTensorShape(filter),
+      tflite_micro::micro::GetTensorData<int8_t>(filter),
+      tflite_micro::micro::GetTensorShape(bias),
+      tflite_micro::micro::GetTensorData<int32_t>(bias),
+      tflite_micro::micro::GetTensorShape(output),
+      tflite_micro::micro::GetTensorData<int8_t>(output));
 #else
   MicroPrintf("Node configuration is not supported by ARC MLI Library.");
 #endif
@@ -605,14 +605,14 @@ void EvalQuantizedPerChannelInt16(TfLiteContext* context, TfLiteNode* node,
 
   reference_integer_ops::ConvPerChannel(
       op_params, data.per_channel_output_multiplier,
-      data.per_channel_output_shift, tflite::micro::GetTensorShape(input),
-      tflite::micro::GetTensorData<int16_t>(input),
-      tflite::micro::GetTensorShape(filter),
-      tflite::micro::GetTensorData<int8_t>(filter),
-      tflite::micro::GetTensorShape(bias),
-      tflite::micro::GetTensorData<std::int64_t>(bias),
-      tflite::micro::GetTensorShape(output),
-      tflite::micro::GetTensorData<int16_t>(output));
+      data.per_channel_output_shift, tflite_micro::micro::GetTensorShape(input),
+      tflite_micro::micro::GetTensorData<int16_t>(input),
+      tflite_micro::micro::GetTensorShape(filter),
+      tflite_micro::micro::GetTensorData<int8_t>(filter),
+      tflite_micro::micro::GetTensorShape(bias),
+      tflite_micro::micro::GetTensorData<std::int64_t>(bias),
+      tflite_micro::micro::GetTensorShape(output),
+      tflite_micro::micro::GetTensorData<int16_t>(output));
 #else
   MicroPrintf("Node configuration is not supported by ARC MLI Library.");
 #endif
@@ -638,16 +638,16 @@ void EvalFloat(TfLiteContext* context, TfLiteNode* node,
   op_params.float_activation_min = output_activation_min;
   op_params.float_activation_max = output_activation_max;
 
-  reference_ops::Conv(op_params, tflite::micro::GetTensorShape(input),
-                      tflite::micro::GetTensorData<float>(input),
-                      tflite::micro::GetTensorShape(filter),
-                      tflite::micro::GetTensorData<float>(filter),
-                      tflite::micro::GetTensorShape(bias),
-                      tflite::micro::GetTensorData<float>(bias),
-                      tflite::micro::GetTensorShape(output),
-                      tflite::micro::GetTensorData<float>(output),
-                      tflite::micro::GetTensorShape(im2col),
-                      tflite::micro::GetTensorData<float>(im2col));
+  reference_ops::Conv(op_params, tflite_micro::micro::GetTensorShape(input),
+                      tflite_micro::micro::GetTensorData<float>(input),
+                      tflite_micro::micro::GetTensorShape(filter),
+                      tflite_micro::micro::GetTensorData<float>(filter),
+                      tflite_micro::micro::GetTensorShape(bias),
+                      tflite_micro::micro::GetTensorData<float>(bias),
+                      tflite_micro::micro::GetTensorShape(output),
+                      tflite_micro::micro::GetTensorData<float>(output),
+                      tflite_micro::micro::GetTensorShape(im2col),
+                      tflite_micro::micro::GetTensorData<float>(im2col));
 #else
   MicroPrintf("Type %s (%d) is not supported by ARC MLI Library.",
               TfLiteTypeGetName(input->type), input->type);
@@ -658,13 +658,13 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   auto* params = reinterpret_cast<TfLiteConvParams*>(node->builtin_data);
 
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   const TfLiteEvalTensor* filter =
-      tflite::micro::GetEvalInput(context, node, kFilterTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kFilterTensor);
   const TfLiteEvalTensor* bias =
-      tflite::micro::GetEvalInput(context, node, kBiasTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kBiasTensor);
 
   TFLITE_DCHECK(node->user_data != nullptr);
   const OpData& data = *(static_cast<const OpData*>(node->user_data));
@@ -705,7 +705,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_CONV_2D() {
-  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/arc_mli/depthwise_conv.cc b/tensorflow/lite/micro/kernels/arc_mli/depthwise_conv.cc
index c2c9cd5c..a01853ba 100644
--- a/tensorflow/lite/micro/kernels/arc_mli/depthwise_conv.cc
+++ b/tensorflow/lite/micro/kernels/arc_mli/depthwise_conv.cc
@@ -33,7 +33,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 constexpr int kInputTensor = 0;
@@ -147,7 +147,7 @@ TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node,
   if (data_type != kTfLiteFloat32 && !data->is_mli_applicable) {
     int num_channels = filter->dims->data[kDepthwiseConvQuantizedDimension];
 
-    return tflite::PopulateConvolutionQuantizationParams(
+    return tflite_micro::PopulateConvolutionQuantizationParams(
         context, input, filter, bias, output, params->activation,
         &data->output_multiplier, &data->output_shift,
         &data->output_activation_min, &data->output_activation_max,
@@ -363,7 +363,7 @@ void EvalFloat(TfLiteContext* context, TfLiteNode* node,
   CalculateActivationRange(params->activation, &output_activation_min,
                            &output_activation_max);
 
-  tflite::DepthwiseParams op_params;
+  tflite_micro::DepthwiseParams op_params;
   // Padding type is ignored, but still set.
   op_params.padding_type = PaddingType::kSame;
   op_params.padding_values.width = data.padding.width;
@@ -376,15 +376,15 @@ void EvalFloat(TfLiteContext* context, TfLiteNode* node,
   op_params.float_activation_min = output_activation_min;
   op_params.float_activation_max = output_activation_max;
 
-  tflite::reference_ops::DepthwiseConv(
-      op_params, tflite::micro::GetTensorShape(input),
-      tflite::micro::GetTensorData<float>(input),
-      tflite::micro::GetTensorShape(filter),
-      tflite::micro::GetTensorData<float>(filter),
-      tflite::micro::GetTensorShape(bias),
-      tflite::micro::GetTensorData<float>(bias),
-      tflite::micro::GetTensorShape(output),
-      tflite::micro::GetTensorData<float>(output));
+  tflite_micro::reference_ops::DepthwiseConv(
+      op_params, tflite_micro::micro::GetTensorShape(input),
+      tflite_micro::micro::GetTensorData<float>(input),
+      tflite_micro::micro::GetTensorShape(filter),
+      tflite_micro::micro::GetTensorData<float>(filter),
+      tflite_micro::micro::GetTensorShape(bias),
+      tflite_micro::micro::GetTensorData<float>(bias),
+      tflite_micro::micro::GetTensorShape(output),
+      tflite_micro::micro::GetTensorData<float>(output));
 #else
   MicroPrintf("Type %s (%d) is not supported by ARC MLI Library.",
               TfLiteTypeGetName(input->type), input->type);
@@ -615,14 +615,14 @@ void EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,
 
   reference_integer_ops::DepthwiseConvPerChannel(
       op_params, data.per_channel_output_multiplier,
-      data.per_channel_output_shift, tflite::micro::GetTensorShape(input),
-      tflite::micro::GetTensorData<int8_t>(input),
-      tflite::micro::GetTensorShape(filter),
-      tflite::micro::GetTensorData<int8_t>(filter),
-      tflite::micro::GetTensorShape(bias),
-      tflite::micro::GetTensorData<int32_t>(bias),
-      tflite::micro::GetTensorShape(output),
-      tflite::micro::GetTensorData<int8_t>(output));
+      data.per_channel_output_shift, tflite_micro::micro::GetTensorShape(input),
+      tflite_micro::micro::GetTensorData<int8_t>(input),
+      tflite_micro::micro::GetTensorShape(filter),
+      tflite_micro::micro::GetTensorData<int8_t>(filter),
+      tflite_micro::micro::GetTensorShape(bias),
+      tflite_micro::micro::GetTensorData<int32_t>(bias),
+      tflite_micro::micro::GetTensorShape(output),
+      tflite_micro::micro::GetTensorData<int8_t>(output));
 #else
   MicroPrintf("Node configuration is not supported by ARC MLI Library.");
 #endif
@@ -637,14 +637,14 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const OpData& data = *(static_cast<const OpData*>(node->user_data));
 
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   const TfLiteEvalTensor* filter =
-      tflite::micro::GetEvalInput(context, node, kFilterTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kFilterTensor);
   const TfLiteEvalTensor* bias =
       (NumInputs(node) == 3)
-          ? tflite::micro::GetEvalInput(context, node, kBiasTensor)
+          ? tflite_micro::micro::GetEvalInput(context, node, kBiasTensor)
           : nullptr;
 
   switch (input->type) {  // Already know in/out types are same.
@@ -666,14 +666,14 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
           reference_integer_ops::DepthwiseConvPerChannel(
               DepthwiseConvParamsQuantized(params, data),
               data.per_channel_output_multiplier, data.per_channel_output_shift,
-              tflite::micro::GetTensorShape(input),
-              tflite::micro::GetTensorData<int16_t>(input),
-              tflite::micro::GetTensorShape(filter),
-              tflite::micro::GetTensorData<int8_t>(filter),
-              tflite::micro::GetTensorShape(bias),
-              tflite::micro::GetOptionalTensorData<int64_t>(bias),
-              tflite::micro::GetTensorShape(output),
-              tflite::micro::GetTensorData<int16_t>(output));
+              tflite_micro::micro::GetTensorShape(input),
+              tflite_micro::micro::GetTensorData<int16_t>(input),
+              tflite_micro::micro::GetTensorShape(filter),
+              tflite_micro::micro::GetTensorData<int8_t>(filter),
+              tflite_micro::micro::GetTensorShape(bias),
+              tflite_micro::micro::GetOptionalTensorData<int64_t>(bias),
+              tflite_micro::micro::GetTensorShape(output),
+              tflite_micro::micro::GetTensorData<int16_t>(output));
           break;
         }
         default:
@@ -695,7 +695,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_DEPTHWISE_CONV_2D() {
-  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/arc_mli/fully_connected.cc b/tensorflow/lite/micro/kernels/arc_mli/fully_connected.cc
index 4af06601..8f82cd80 100644
--- a/tensorflow/lite/micro/kernels/arc_mli/fully_connected.cc
+++ b/tensorflow/lite/micro/kernels/arc_mli/fully_connected.cc
@@ -30,7 +30,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 struct OpData {
@@ -377,7 +377,7 @@ TfLiteStatus EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                            const TfLiteEvalTensor* bias,
                            TfLiteEvalTensor* output) {
 #if !defined(TF_LITE_STRIP_REFERENCE_IMPL)
-  tflite::FullyConnectedParams op_params;
+  tflite_micro::FullyConnectedParams op_params;
   op_params.input_offset = -data.input_zero_point;
   op_params.weights_offset = -data.filter_zero_point;
   op_params.output_offset = data.output_zero_point;
@@ -387,14 +387,14 @@ TfLiteStatus EvalQuantized(TfLiteContext* context, TfLiteNode* node,
   op_params.quantized_activation_max = data.output_activation_max;
 
   reference_integer_ops::FullyConnected(
-      op_params, tflite::micro::GetTensorShape(input),
-      tflite::micro::GetTensorData<int8_t>(input),
-      tflite::micro::GetTensorShape(filter),
-      tflite::micro::GetTensorData<int8_t>(filter),
-      tflite::micro::GetTensorShape(bias),
-      tflite::micro::GetTensorData<int32_t>(bias),
-      tflite::micro::GetTensorShape(output),
-      tflite::micro::GetTensorData<int8_t>(output));
+      op_params, tflite_micro::micro::GetTensorShape(input),
+      tflite_micro::micro::GetTensorData<int8_t>(input),
+      tflite_micro::micro::GetTensorShape(filter),
+      tflite_micro::micro::GetTensorData<int8_t>(filter),
+      tflite_micro::micro::GetTensorShape(bias),
+      tflite_micro::micro::GetTensorData<int32_t>(bias),
+      tflite_micro::micro::GetTensorShape(output),
+      tflite_micro::micro::GetTensorData<int8_t>(output));
   return kTfLiteOk;
 #else
   MicroPrintf("Node configuration is not supported by ARC MLI Library.");
@@ -411,18 +411,18 @@ TfLiteStatus EvalFloat(TfLiteContext* context, TfLiteNode* node,
   float output_activation_min, output_activation_max;
   CalculateActivationRange(activation, &output_activation_min,
                            &output_activation_max);
-  tflite::FullyConnectedParams op_params;
+  tflite_micro::FullyConnectedParams op_params;
   op_params.float_activation_min = output_activation_min;
   op_params.float_activation_max = output_activation_max;
-  tflite::reference_ops::FullyConnected(
-      op_params, tflite::micro::GetTensorShape(input),
-      tflite::micro::GetTensorData<float>(input),
-      tflite::micro::GetTensorShape(filter),
-      tflite::micro::GetTensorData<float>(filter),
-      tflite::micro::GetTensorShape(bias),
-      tflite::micro::GetTensorData<float>(bias),
-      tflite::micro::GetTensorShape(output),
-      tflite::micro::GetTensorData<float>(output));
+  tflite_micro::reference_ops::FullyConnected(
+      op_params, tflite_micro::micro::GetTensorShape(input),
+      tflite_micro::micro::GetTensorData<float>(input),
+      tflite_micro::micro::GetTensorShape(filter),
+      tflite_micro::micro::GetTensorData<float>(filter),
+      tflite_micro::micro::GetTensorShape(bias),
+      tflite_micro::micro::GetTensorData<float>(bias),
+      tflite_micro::micro::GetTensorShape(output),
+      tflite_micro::micro::GetTensorData<float>(output));
   return kTfLiteOk;
 #else
   MicroPrintf("Type %s (%d) is not supported by ARC MLI Library.",
@@ -437,13 +437,13 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
       static_cast<const TfLiteFullyConnectedParams*>(node->builtin_data);
 
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   const TfLiteEvalTensor* filter =
-      tflite::micro::GetEvalInput(context, node, kWeightsTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kWeightsTensor);
   const TfLiteEvalTensor* bias =
-      tflite::micro::GetEvalInput(context, node, kBiasTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kBiasTensor);
 
   TFLITE_DCHECK(node->user_data != nullptr);
   const OpData& data = *(static_cast<const OpData*>(node->user_data));
@@ -470,7 +470,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }
 
 TFLMRegistration Register_FULLY_CONNECTED() {
-  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/arc_mli/mli_function_specializations.h b/tensorflow/lite/micro/kernels/arc_mli/mli_function_specializations.h
index 6276fe73..2191439c 100644
--- a/tensorflow/lite/micro/kernels/arc_mli/mli_function_specializations.h
+++ b/tensorflow/lite/micro/kernels/arc_mli/mli_function_specializations.h
@@ -15,7 +15,7 @@ limitations under the License.
 
 #include "mli_api.h"  // NOLINT
 
-namespace tflite {
+namespace tflite_micro {
 
 // Convolution specialized function.
 typedef mli_status (*conv_func_ptr)(const mli_tensor* /*in*/,
@@ -138,4 +138,4 @@ mli_krn_maxpool(const mli_pool_cfg* cfg) {
 }
 #endif
 
-}  // namespace tflite
\ No newline at end of file
+}  // namespace tflite_micro
\ No newline at end of file
diff --git a/tensorflow/lite/micro/kernels/arc_mli/mli_interface.cc b/tensorflow/lite/micro/kernels/arc_mli/mli_interface.cc
index 3a9890b2..842328cb 100644
--- a/tensorflow/lite/micro/kernels/arc_mli/mli_interface.cc
+++ b/tensorflow/lite/micro/kernels/arc_mli/mli_interface.cc
@@ -19,7 +19,7 @@ limitations under the License.
 
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace ops {
 namespace micro {
 
@@ -152,4 +152,4 @@ void MliTensorInterface::SetElType(TfLiteType type) {
 
 }  // namespace micro
 }  // namespace ops
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/arc_mli/mli_interface.h b/tensorflow/lite/micro/kernels/arc_mli/mli_interface.h
index b4087f3b..2f542918 100644
--- a/tensorflow/lite/micro/kernels/arc_mli/mli_interface.h
+++ b/tensorflow/lite/micro/kernels/arc_mli/mli_interface.h
@@ -18,7 +18,7 @@ limitations under the License.
 
 #include "mli_api.h"  // NOLINT
 #include "tensorflow/lite/kernels/internal/tensor_ctypes.h"
-namespace tflite {
+namespace tflite_micro {
 namespace ops {
 namespace micro {
 
@@ -70,6 +70,6 @@ class MliTensorInterface {
 
 }  // namespace micro
 }  // namespace ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_ARC_MLI_SLICERS_H_
diff --git a/tensorflow/lite/micro/kernels/arc_mli/mli_interface_mli_20.cc b/tensorflow/lite/micro/kernels/arc_mli/mli_interface_mli_20.cc
index cef2a6e9..4e98bbf7 100644
--- a/tensorflow/lite/micro/kernels/arc_mli/mli_interface_mli_20.cc
+++ b/tensorflow/lite/micro/kernels/arc_mli/mli_interface_mli_20.cc
@@ -18,7 +18,7 @@ limitations under the License.
 #include "mli_interface.h"  // NOLINT
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace ops {
 namespace micro {
 
@@ -161,4 +161,4 @@ void MliTensorInterface::SetElType(TfLiteType type) {
 
 }  // namespace micro
 }  // namespace ops
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/arc_mli/mli_slicers.cc b/tensorflow/lite/micro/kernels/arc_mli/mli_slicers.cc
index 905c6fed..03fd674d 100644
--- a/tensorflow/lite/micro/kernels/arc_mli/mli_slicers.cc
+++ b/tensorflow/lite/micro/kernels/arc_mli/mli_slicers.cc
@@ -17,7 +17,7 @@ limitations under the License.
 
 #include <algorithm>
 
-namespace tflite {
+namespace tflite_micro {
 namespace ops {
 namespace micro {
 
@@ -123,4 +123,4 @@ mli_tensor* TensorSlicer::Sub(void) { return &sub_tensor_; }
 
 }  // namespace micro
 }  // namespace ops
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/arc_mli/mli_slicers.h b/tensorflow/lite/micro/kernels/arc_mli/mli_slicers.h
index b21a5b68..f92eb98a 100644
--- a/tensorflow/lite/micro/kernels/arc_mli/mli_slicers.h
+++ b/tensorflow/lite/micro/kernels/arc_mli/mli_slicers.h
@@ -17,7 +17,7 @@ limitations under the License.
 #define TENSORFLOW_LITE_MICRO_KERNELS_ARC_MLI_SLICERS_H_
 
 #include "mli_api.h"  // NOLINT
-namespace tflite {
+namespace tflite_micro {
 namespace ops {
 namespace micro {
 
@@ -52,5 +52,5 @@ class TensorSlicer {
 
 }  // namespace micro
 }  // namespace ops
-}  // namespace tflite
+}  // namespace tflite_micro
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_ARC_MLI_SLICERS_H_
diff --git a/tensorflow/lite/micro/kernels/arc_mli/mli_tf_utils.h b/tensorflow/lite/micro/kernels/arc_mli/mli_tf_utils.h
index 6e4e16e8..67c84e93 100644
--- a/tensorflow/lite/micro/kernels/arc_mli/mli_tf_utils.h
+++ b/tensorflow/lite/micro/kernels/arc_mli/mli_tf_utils.h
@@ -25,7 +25,7 @@ limitations under the License.
 
 #define KRNL_C_DIM_NHWC 0  // output channels
 
-namespace tflite {
+namespace tflite_micro {
 namespace ops {
 namespace micro {
 
@@ -118,7 +118,7 @@ inline void MliTensorAttachBuffer<int8_t>(const TfLiteEvalTensor* tfT,
   // non-const mli_tensor. This is required by current implementation of MLI
   // backend and planned for redesign due to this and some other aspects.
   mliT->SetData<int8_t>(
-      const_cast<int8_t*>(tflite::micro::GetTensorData<int8_t>(tfT)),
+      const_cast<int8_t*>(tflite_micro::micro::GetTensorData<int8_t>(tfT)),
       *mliT->DataCapacity());
 }
 
@@ -129,7 +129,7 @@ inline void MliTensorAttachBuffer<int32_t>(const TfLiteEvalTensor* tfT,
   // non-const mli_tensor. This is required by current implementation of MLI
   // backend and planned for redesign due to this and some other aspects.
   mliT->SetData<int32_t>(
-      const_cast<int32_t*>(tflite::micro::GetTensorData<int32_t>(tfT)),
+      const_cast<int32_t*>(tflite_micro::micro::GetTensorData<int32_t>(tfT)),
       *mliT->DataCapacity());
 }
 
@@ -305,6 +305,6 @@ inline void permute_weights(const mli_tensor* weights_src,
 
 }  // namespace micro
 }  // namespace ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_ARC_MLI_TF_UTILS_H_
diff --git a/tensorflow/lite/micro/kernels/arc_mli/pooling.cc b/tensorflow/lite/micro/kernels/arc_mli/pooling.cc
index 104ec311..c4ecb1c2 100644
--- a/tensorflow/lite/micro/kernels/arc_mli/pooling.cc
+++ b/tensorflow/lite/micro/kernels/arc_mli/pooling.cc
@@ -28,7 +28,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -172,10 +172,10 @@ void AverageEvalFloat(TfLiteContext* context, const TfLiteNode* node,
   op_params.padding_values.width = data.padding.width;
   op_params.float_activation_min = activation_min;
   op_params.float_activation_max = activation_max;
-  reference_ops::AveragePool(op_params, tflite::micro::GetTensorShape(input),
-                             tflite::micro::GetTensorData<float>(input),
-                             tflite::micro::GetTensorShape(output),
-                             tflite::micro::GetTensorData<float>(output));
+  reference_ops::AveragePool(op_params, tflite_micro::micro::GetTensorShape(input),
+                             tflite_micro::micro::GetTensorData<float>(input),
+                             tflite_micro::micro::GetTensorShape(output),
+                             tflite_micro::micro::GetTensorData<float>(output));
 #else
   MicroPrintf("Type %s (%d) is not supported by ARC MLI Library.",
               TfLiteTypeGetName(input->type), input->type);
@@ -281,10 +281,10 @@ void AverageEvalQuantized(TfLiteContext* context, const TfLiteNode* node,
   op_params.quantized_activation_max = data.activation_max;
 
   reference_integer_ops::AveragePool(
-      op_params, tflite::micro::GetTensorShape(input),
-      tflite::micro::GetTensorData<int8_t>(input),
-      tflite::micro::GetTensorShape(output),
-      tflite::micro::GetTensorData<int8_t>(output));
+      op_params, tflite_micro::micro::GetTensorShape(input),
+      tflite_micro::micro::GetTensorData<int8_t>(input),
+      tflite_micro::micro::GetTensorShape(output),
+      tflite_micro::micro::GetTensorData<int8_t>(output));
 #else
   MicroPrintf("Type %s (%d) is not supported by ARC MLI Library.",
               TfLiteTypeGetName(input->type), input->type);
@@ -295,7 +295,7 @@ void MaxEvalFloat(TfLiteContext* context, TfLiteNode* node,
                   TfLitePoolParams* params, const OpData& data,
                   const TfLiteEvalTensor* input, TfLiteEvalTensor* output) {
 #if !defined(TF_LITE_STRIP_REFERENCE_IMPL)
-  tflite::PoolParams op_params;
+  tflite_micro::PoolParams op_params;
   op_params.stride_height = params->stride_height;
   op_params.stride_width = params->stride_width;
   op_params.filter_height = params->filter_height;
@@ -304,10 +304,10 @@ void MaxEvalFloat(TfLiteContext* context, TfLiteNode* node,
   op_params.padding_values.width = data.padding.width;
   op_params.float_activation_min = data.activation_min_f32;
   op_params.float_activation_max = data.activation_max_f32;
-  reference_ops::MaxPool(op_params, tflite::micro::GetTensorShape(input),
-                         tflite::micro::GetTensorData<float>(input),
-                         tflite::micro::GetTensorShape(output),
-                         tflite::micro::GetTensorData<float>(output));
+  reference_ops::MaxPool(op_params, tflite_micro::micro::GetTensorShape(input),
+                         tflite_micro::micro::GetTensorData<float>(input),
+                         tflite_micro::micro::GetTensorShape(output),
+                         tflite_micro::micro::GetTensorData<float>(output));
 #else
   MicroPrintf(
 
@@ -321,7 +321,7 @@ void MaxEvalQuantized(TfLiteContext* context, TfLiteNode* node,
                       const TfLiteEvalTensor* input, TfLiteEvalTensor* output) {
 #if !defined(TF_LITE_STRIP_REFERENCE_IMPL)
   TFLITE_DCHECK(input->type == kTfLiteInt8);
-  tflite::PoolParams op_params;
+  tflite_micro::PoolParams op_params;
   op_params.stride_height = params->stride_height;
   op_params.stride_width = params->stride_width;
   op_params.filter_height = params->filter_height;
@@ -332,10 +332,10 @@ void MaxEvalQuantized(TfLiteContext* context, TfLiteNode* node,
   op_params.quantized_activation_max = data.activation_max;
 
   reference_integer_ops::MaxPool(op_params,
-                                 tflite::micro::GetTensorShape(input),
-                                 tflite::micro::GetTensorData<int8_t>(input),
-                                 tflite::micro::GetTensorShape(output),
-                                 tflite::micro::GetTensorData<int8_t>(output));
+                                 tflite_micro::micro::GetTensorShape(input),
+                                 tflite_micro::micro::GetTensorData<int8_t>(input),
+                                 tflite_micro::micro::GetTensorShape(output),
+                                 tflite_micro::micro::GetTensorData<int8_t>(output));
 #else
   MicroPrintf(
 
@@ -349,9 +349,9 @@ TfLiteStatus AverageEval(TfLiteContext* context, TfLiteNode* node) {
   auto* params = reinterpret_cast<TfLitePoolParams*>(node->builtin_data);
 
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
   TFLITE_DCHECK(node->user_data != nullptr);
   const OpData& data = *(static_cast<const OpData*>(node->user_data));
@@ -380,9 +380,9 @@ TfLiteStatus MaxEval(TfLiteContext* context, TfLiteNode* node) {
   auto* params = reinterpret_cast<TfLitePoolParams*>(node->builtin_data);
 
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
   TFLITE_DCHECK(node->user_data != nullptr);
   const OpData& data = *(static_cast<const OpData*>(node->user_data));
@@ -409,11 +409,11 @@ TfLiteStatus MaxEval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_AVERAGE_POOL_2D() {
-  return tflite::micro::RegisterOp(Init, Prepare, AverageEval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, AverageEval);
 }
 
 TFLMRegistration Register_MAX_POOL_2D() {
-  return tflite::micro::RegisterOp(Init, Prepare, MaxEval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, MaxEval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/arc_mli/scratch_buf_mgr.cc b/tensorflow/lite/micro/kernels/arc_mli/scratch_buf_mgr.cc
index ef489fa6..a0951910 100644
--- a/tensorflow/lite/micro/kernels/arc_mli/scratch_buf_mgr.cc
+++ b/tensorflow/lite/micro/kernels/arc_mli/scratch_buf_mgr.cc
@@ -21,7 +21,7 @@ limitations under the License.
 
 #include "tensorflow/lite/micro/kernels/arc_mli/scratch_buffers.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace ops {
 namespace micro {
 
@@ -389,4 +389,4 @@ TfLiteStatus get_arc_scratch_buffer_for_pooling_tensors(
 
 }  // namespace micro
 }  // namespace ops
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/arc_mli/scratch_buf_mgr.h b/tensorflow/lite/micro/kernels/arc_mli/scratch_buf_mgr.h
index be6dd8fb..fd14cf39 100644
--- a/tensorflow/lite/micro/kernels/arc_mli/scratch_buf_mgr.h
+++ b/tensorflow/lite/micro/kernels/arc_mli/scratch_buf_mgr.h
@@ -20,7 +20,7 @@ limitations under the License.
 #include "mli_interface.h"
 #include "tensorflow/lite/c/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace ops {
 namespace micro {
 
@@ -140,6 +140,6 @@ TfLiteStatus arc_scratch_buffer_calc_slice_size_weights(
 
 }  // namespace micro
 }  // namespace ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_ARC_SCRATCH_BUF_MGR_H_
diff --git a/tensorflow/lite/micro/kernels/arc_mli/scratch_buffers.cc b/tensorflow/lite/micro/kernels/arc_mli/scratch_buffers.cc
index bf87122e..3a93a72f 100644
--- a/tensorflow/lite/micro/kernels/arc_mli/scratch_buffers.cc
+++ b/tensorflow/lite/micro/kernels/arc_mli/scratch_buffers.cc
@@ -17,7 +17,7 @@ limitations under the License.
 
 #include <limits.h>
 
-namespace tflite {
+namespace tflite_micro {
 namespace ops {
 namespace micro {
 
@@ -189,4 +189,4 @@ void init_arc_scratch_buffers(void) {
 
 }  // namespace micro
 }  // namespace ops
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/arc_mli/scratch_buffers.h b/tensorflow/lite/micro/kernels/arc_mli/scratch_buffers.h
index 645781bc..0e5dd02b 100644
--- a/tensorflow/lite/micro/kernels/arc_mli/scratch_buffers.h
+++ b/tensorflow/lite/micro/kernels/arc_mli/scratch_buffers.h
@@ -19,7 +19,7 @@ limitations under the License.
 #include "mli_api.h"  // NOLINT
 #include "tensorflow/lite/c/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace ops {
 namespace micro {
 
@@ -73,6 +73,6 @@ static inline bool inside_arc_ccm(void* p) {
 
 }  // namespace micro
 }  // namespace ops
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_ARC_SCRATCH_BUFFERS_H_
diff --git a/tensorflow/lite/micro/kernels/arg_min_max.cc b/tensorflow/lite/micro/kernels/arg_min_max.cc
index 2ba058cc..91b86dab 100644
--- a/tensorflow/lite/micro/kernels/arg_min_max.cc
+++ b/tensorflow/lite/micro/kernels/arg_min_max.cc
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -52,18 +52,18 @@ inline void ArgMinMaxHelper(const RuntimeShape& input1_shape,
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node, bool is_arg_max) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   const TfLiteEvalTensor* axis =
-      tflite::micro::GetEvalInput(context, node, kAxis);
+      tflite_micro::micro::GetEvalInput(context, node, kAxis);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
 #define TF_LITE_ARG_MIN_MAX(data_type, axis_type, output_type)       \
-  ArgMinMaxHelper(tflite::micro::GetTensorShape(input),              \
-                  tflite::micro::GetTensorData<data_type>(input),    \
-                  tflite::micro::GetTensorData<axis_type>(axis),     \
-                  tflite::micro::GetTensorShape(output),             \
-                  tflite::micro::GetTensorData<output_type>(output), \
+  ArgMinMaxHelper(tflite_micro::micro::GetTensorShape(input),              \
+                  tflite_micro::micro::GetTensorData<data_type>(input),    \
+                  tflite_micro::micro::GetTensorData<axis_type>(axis),     \
+                  tflite_micro::micro::GetTensorShape(output),             \
+                  tflite_micro::micro::GetTensorData<output_type>(output), \
                   is_arg_max)
   if (axis->type == kTfLiteInt32) {
     if (output->type == kTfLiteInt32) {
@@ -108,11 +108,11 @@ TfLiteStatus ArgMaxEval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_ARG_MAX() {
-  return tflite::micro::RegisterOp(nullptr, nullptr, ArgMaxEval);
+  return tflite_micro::micro::RegisterOp(nullptr, nullptr, ArgMaxEval);
 }
 
 TFLMRegistration Register_ARG_MIN() {
-  return tflite::micro::RegisterOp(nullptr, nullptr, ArgMinEval);
+  return tflite_micro::micro::RegisterOp(nullptr, nullptr, ArgMinEval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/arg_min_max_test.cc b/tensorflow/lite/micro/kernels/arg_min_max_test.cc
index ab06fecf..6c56bf1e 100644
--- a/tensorflow/lite/micro/kernels/arg_min_max_test.cc
+++ b/tensorflow/lite/micro/kernels/arg_min_max_test.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -95,7 +95,7 @@ void TestArgMinMaxQuantized(int* input_dims_data, const float* input_values,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -108,7 +108,7 @@ TF_LITE_MICRO_TEST(GetMaxArgFloat) {
   int output_dims[] = {3, 1, 1, 1};
   const int32_t goldens[] = {1};
 
-  tflite::testing::TestArgMinMaxFloat(input_dims, input_values, axis_dims,
+  tflite_micro::testing::TestArgMinMaxFloat(input_dims, input_values, axis_dims,
                                       axis_values, output_dims, output_data,
                                       goldens, false);
 }
@@ -122,7 +122,7 @@ TF_LITE_MICRO_TEST(GetMinArgFloat) {
   int output_dims[] = {3, 1, 1, 1};
   const int32_t goldens[] = {0};
 
-  tflite::testing::TestArgMinMaxFloat(input_dims, input_values, axis_dims,
+  tflite_micro::testing::TestArgMinMaxFloat(input_dims, input_values, axis_dims,
                                       axis_values, output_dims, output_data,
                                       goldens, true);
 }
@@ -141,7 +141,7 @@ TF_LITE_MICRO_TEST(GetMaxArgInt8) {
   int input_zero_point = -9;
   int8_t input_quantized[input_size];
 
-  tflite::testing::TestArgMinMaxQuantized(
+  tflite_micro::testing::TestArgMinMaxQuantized(
       input_dims, input_values, input_quantized, input_scale, input_zero_point,
       axis_dims, axis_values, output_dims, output_data, goldens, false);
 }
@@ -160,7 +160,7 @@ TF_LITE_MICRO_TEST(GetMinArgInt8) {
   int input_zero_point = -9;
   int8_t input_quantized[input_size];
 
-  tflite::testing::TestArgMinMaxQuantized(
+  tflite_micro::testing::TestArgMinMaxQuantized(
       input_dims, input_values, input_quantized, input_scale, input_zero_point,
       axis_dims, axis_values, output_dims, output_data, goldens, true);
 }
@@ -179,7 +179,7 @@ TF_LITE_MICRO_TEST(GetMaxArgMulDimensions) {
   int input_zero_point = -9;
   int8_t input_quantized[input_size];
 
-  tflite::testing::TestArgMinMaxQuantized(
+  tflite_micro::testing::TestArgMinMaxQuantized(
       input_dims, input_values, input_quantized, input_scale, input_zero_point,
       axis_dims, axis_values, output_dims, output_data, goldens, false);
 }
@@ -198,7 +198,7 @@ TF_LITE_MICRO_TEST(GetMinArgMulDimensions) {
   int input_zero_point = -9;
   int8_t input_quantized[input_size];
 
-  tflite::testing::TestArgMinMaxQuantized(
+  tflite_micro::testing::TestArgMinMaxQuantized(
       input_dims, input_values, input_quantized, input_scale, input_zero_point,
       axis_dims, axis_values, output_dims, output_data, goldens, true);
 }
@@ -218,7 +218,7 @@ TF_LITE_MICRO_TEST(GetMaxArgNegativeAxis) {
   int32_t output_data[output_size];
   int8_t input_quantized[input_size];
 
-  tflite::testing::TestArgMinMaxQuantized(
+  tflite_micro::testing::TestArgMinMaxQuantized(
       input_dims, input_values, input_quantized, input_scale, input_zero_point,
       axis_dims, axis_values, output_dims, output_data, goldens, false);
 }
diff --git a/tensorflow/lite/micro/kernels/assign_variable.cc b/tensorflow/lite/micro/kernels/assign_variable.cc
index bd99bd1a..b4f5dfa7 100644
--- a/tensorflow/lite/micro/kernels/assign_variable.cc
+++ b/tensorflow/lite/micro/kernels/assign_variable.cc
@@ -28,7 +28,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_resource_variable.h"
 #include "tensorflow/lite/schema/schema_generated.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -46,13 +46,13 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   // init/prepare, and VAR_HANDLE Prepare() references its own op_data in the
   // TfLiteEvalTensor, so reading the ID here is valid.
   const TfLiteEvalTensor* input_resource_id_tensor =
-      tflite::micro::GetEvalInput(context, node, kInputVariableId);
+      tflite_micro::micro::GetEvalInput(context, node, kInputVariableId);
   TFLITE_DCHECK(input_resource_id_tensor != nullptr);
   TF_LITE_ENSURE(context, (input_resource_id_tensor->type == kTfLiteResource ||
                            input_resource_id_tensor->type == kTfLiteInt32));
   TF_LITE_ENSURE_EQ(context, NumElements(input_resource_id_tensor->dims), 1);
 
-  tflite::MicroContext* micro_context = tflite::GetMicroContext(context);
+  tflite_micro::MicroContext* micro_context = tflite_micro::GetMicroContext(context);
   TfLiteTensor* input_value =
       micro_context->AllocateTempInputTensor(node, kInputValue);
   TFLITE_DCHECK(input_value != nullptr);
@@ -76,14 +76,14 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input_id =
-      tflite::micro::GetEvalInput(context, node, kInputVariableId);
+      tflite_micro::micro::GetEvalInput(context, node, kInputVariableId);
   TFLITE_DCHECK(input_id != nullptr);
 
   const TfLiteEvalTensor* input_value =
-      tflite::micro::GetEvalInput(context, node, kInputValue);
+      tflite_micro::micro::GetEvalInput(context, node, kInputValue);
   TFLITE_DCHECK(input_value != nullptr);
 
-  tflite::MicroContext* micro_context = tflite::GetMicroContext(context);
+  tflite_micro::MicroContext* micro_context = tflite_micro::GetMicroContext(context);
   MicroGraph& graph_info = micro_context->graph();
 
   MicroResourceVariables* resources = graph_info.GetResourceVariables();
@@ -101,7 +101,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace.
 
 TFLMRegistration Register_ASSIGN_VARIABLE() {
-  return tflite::micro::RegisterOp(nullptr, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/batch_matmul.cc b/tensorflow/lite/micro/kernels/batch_matmul.cc
index 9ecc4fc3..15d4e915 100644
--- a/tensorflow/lite/micro/kernels/batch_matmul.cc
+++ b/tensorflow/lite/micro/kernels/batch_matmul.cc
@@ -28,7 +28,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 constexpr int kInputLhsTensor = 0;
@@ -103,9 +103,9 @@ struct PrepareOpContext : OpContext {
 struct EvalOpContext : OpContext {
   EvalOpContext(TfLiteContext* context, TfLiteNode* node)
       : OpContext(context, node),
-        lhs(tflite::micro::GetEvalInput(context, node, kInputLhsTensor)),
-        rhs(tflite::micro::GetEvalInput(context, node, kInputRhsTensor)),
-        output(tflite::micro::GetEvalOutput(context, node, kOutputTensor)) {}
+        lhs(tflite_micro::micro::GetEvalInput(context, node, kInputLhsTensor)),
+        rhs(tflite_micro::micro::GetEvalInput(context, node, kInputRhsTensor)),
+        output(tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor)) {}
 
   const TfLiteEvalTensor* lhs;
   const TfLiteEvalTensor* rhs;
@@ -124,8 +124,8 @@ TfLiteStatus ReshapeOutputTensor(TfLiteContext* context, TfLiteNode* node,
 
   // make sure output tensor dims are not in the FlatBuffer
   TfLiteEvalTensor* output_eval =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
-  TF_LITE_ENSURE_OK(context, tflite::micro::CreateWritableTensorDimsWithCopy(
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
+  TF_LITE_ENSURE_OK(context, tflite_micro::micro::CreateWritableTensorDimsWithCopy(
                                  context, output, output_eval));
 
   // Fill in any broadcast dimensions.
@@ -234,9 +234,9 @@ TfLiteStatus InitializeTemporaries(TfLiteContext* context, TfLiteNode* node,
 template <typename Scalar>
 void TransposeRowsColumnsImpl(const TfLiteEvalTensor& tensor_in,
                               TfLiteEvalTensor* tensor_out) {
-  const Scalar* input = tflite::micro::GetTensorData<Scalar>(&tensor_in);
-  Scalar* output = tflite::micro::GetTensorData<Scalar>(tensor_out);
-  RuntimeShape transposed_shape(tflite::micro::GetTensorShape(&tensor_in));
+  const Scalar* input = tflite_micro::micro::GetTensorData<Scalar>(&tensor_in);
+  Scalar* output = tflite_micro::micro::GetTensorData<Scalar>(tensor_out);
+  RuntimeShape transposed_shape(tflite_micro::micro::GetTensorShape(&tensor_in));
   RuntimeShape shape(transposed_shape);
   TransposeParams params;
   const int rank = shape.DimensionsCount();
@@ -415,9 +415,9 @@ TfLiteStatus EvalInt8(TfLiteContext* context, const OpData& data,
 
   // Note we pass RHS args first, LHS args second. See note for Eval.
   reference_ops::BatchMatMul<int8_t, int32_t>(
-      op_params, rhs_shape, tflite::micro::GetTensorData<int8_t>(&rhs),
-      lhs_shape, tflite::micro::GetTensorData<int8_t>(&lhs), output_shape,
-      tflite::micro::GetTensorData<int8_t>(output));
+      op_params, rhs_shape, tflite_micro::micro::GetTensorData<int8_t>(&rhs),
+      lhs_shape, tflite_micro::micro::GetTensorData<int8_t>(&lhs), output_shape,
+      tflite_micro::micro::GetTensorData<int8_t>(output));
 
   return kTfLiteOk;
 }
@@ -445,9 +445,9 @@ TfLiteStatus EvalInt16(TfLiteContext* context, const OpData& data,
 
   // Note we pass RHS args first, LHS args second. See note for Eval.
   reference_ops::BatchMatMul<int16_t, int64_t>(
-      op_params, rhs_shape, tflite::micro::GetTensorData<int16_t>(&rhs),
-      lhs_shape, tflite::micro::GetTensorData<int16_t>(&lhs), output_shape,
-      tflite::micro::GetTensorData<int16_t>(output));
+      op_params, rhs_shape, tflite_micro::micro::GetTensorData<int16_t>(&rhs),
+      lhs_shape, tflite_micro::micro::GetTensorData<int16_t>(&lhs), output_shape,
+      tflite_micro::micro::GetTensorData<int16_t>(output));
 
   return kTfLiteOk;
 }
@@ -469,8 +469,8 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* lhs = op_context.lhs;
   const TfLiteEvalTensor* rhs = op_context.rhs;
   TfLiteEvalTensor* output = op_context.output;
-  RuntimeShape orig_lhs_shape = tflite::micro::GetTensorShape(lhs);
-  RuntimeShape orig_rhs_shape = tflite::micro::GetTensorShape(rhs);
+  RuntimeShape orig_lhs_shape = tflite_micro::micro::GetTensorShape(lhs);
+  RuntimeShape orig_rhs_shape = tflite_micro::micro::GetTensorShape(rhs);
 
   bool adj_y = op_context.params->adj_y;
   bool adj_x = op_context.params->adj_x;
@@ -526,18 +526,18 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
     case kTfLiteFloat32:
       // Note we pass RHS args first, LHS args second. See note above.
       reference_ops::BatchMatMul(
-          rhs_shape, tflite::micro::GetTensorData<float>(rhs_tensor), lhs_shape,
-          tflite::micro::GetTensorData<float>(lhs_tensor),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<float>(output));
+          rhs_shape, tflite_micro::micro::GetTensorData<float>(rhs_tensor), lhs_shape,
+          tflite_micro::micro::GetTensorData<float>(lhs_tensor),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<float>(output));
       break;
     case kTfLiteInt8:
       return EvalInt8(context, *op_data, lhs_shape, *lhs_tensor, rhs_shape,
-                      *rhs_tensor, tflite::micro::GetTensorShape(output),
+                      *rhs_tensor, tflite_micro::micro::GetTensorShape(output),
                       output);
     case kTfLiteInt16:
       return EvalInt16(context, *op_data, lhs_shape, *lhs_tensor, rhs_shape,
-                       *rhs_tensor, tflite::micro::GetTensorShape(output),
+                       *rhs_tensor, tflite_micro::micro::GetTensorShape(output),
                        output);
     default:
       MicroPrintf("BATCH_MATMUL doesn't support input type %s",
@@ -550,7 +550,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_BATCH_MATMUL() {
-  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/batch_matmul_test.cc b/tensorflow/lite/micro/kernels/batch_matmul_test.cc
index abba7577..27d27c37 100644
--- a/tensorflow/lite/micro/kernels/batch_matmul_test.cc
+++ b/tensorflow/lite/micro/kernels/batch_matmul_test.cc
@@ -24,7 +24,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -58,7 +58,7 @@ micro::KernelRunner* GetKernelRunnerInstance(
   static int kOutputArrayData[] = {kNumOutputs, kOutputTensorIndex};
   TfLiteIntArray* outputs_array = IntArrayFromInts(kOutputArrayData);
 
-  static const TFLMRegistration registration = tflite::Register_BATCH_MATMUL();
+  static const TFLMRegistration registration = tflite_micro::Register_BATCH_MATMUL();
 
   alignas(micro::KernelRunner) static char
       kernel_runner_buffer[sizeof(micro::KernelRunner)] = {};
@@ -190,14 +190,14 @@ void TestBatchMatMulQuantized(
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_Ones) {
   constexpr int kLhsInputDims[] = {4, 3, 2, 1, 4};
   constexpr int kRhsInputDims[] = {4, 3, 1, 4, 1};
-  const int* kInputDims[tflite::testing::kNumInputs] = {kLhsInputDims,
+  const int* kInputDims[tflite_micro::testing::kNumInputs] = {kLhsInputDims,
                                                         kRhsInputDims};
 
   constexpr size_t kLhsInputSize = 24;
@@ -219,7 +219,7 @@ TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_Ones) {
       false   // asymmetric_quantize_inputs
   };
 
-  tflite::testing::TestBatchMatMulFloat(params, kInputDims, lhs_input,
+  tflite_micro::testing::TestBatchMatMulFloat(params, kInputDims, lhs_input,
                                         rhs_input, kOutputDims, kExpect,
                                         output_data);
 }
@@ -227,7 +227,7 @@ TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_Ones) {
 TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_Flatten) {
   constexpr int kLhsInputDims[] = {4, 3, 2, 2, 4};
   constexpr int kRhsInputDims[] = {4, 3, 1, 4, 1};
-  const int* kInputDims[tflite::testing::kNumInputs] = {kLhsInputDims,
+  const int* kInputDims[tflite_micro::testing::kNumInputs] = {kLhsInputDims,
                                                         kRhsInputDims};
 
   constexpr size_t kLhsInputSize = 48;
@@ -250,7 +250,7 @@ TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_Flatten) {
       false   // asymmetric_quantize_inputs
   };
 
-  tflite::testing::TestBatchMatMulFloat(params, kInputDims, lhs_input,
+  tflite_micro::testing::TestBatchMatMulFloat(params, kInputDims, lhs_input,
                                         rhs_input, kOutputDims, kExpect,
                                         output_data);
 }
@@ -258,7 +258,7 @@ TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_Flatten) {
 TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_Simple) {
   constexpr int kLhsInputDims[] = {3, 1, 2, 3};
   constexpr int kRhsInputDims[] = {3, 1, 3, 4};
-  const int* kInputDims[tflite::testing::kNumInputs] = {kLhsInputDims,
+  const int* kInputDims[tflite_micro::testing::kNumInputs] = {kLhsInputDims,
                                                         kRhsInputDims};
 
   constexpr size_t kLhsInputSize = 6;
@@ -280,7 +280,7 @@ TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_Simple) {
       false   // asymmetric_quantize_inputs
   };
 
-  tflite::testing::TestBatchMatMulFloat(params, kInputDims, lhs_input,
+  tflite_micro::testing::TestBatchMatMulFloat(params, kInputDims, lhs_input,
                                         rhs_input, kOutputDims, kExpect,
                                         output_data);
 }
@@ -288,7 +288,7 @@ TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_Simple) {
 TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_SimpleRHSAdjoint) {
   constexpr int kLhsInputDims[] = {3, 1, 2, 3};
   constexpr int kRhsInputDims[] = {3, 1, 4, 3};
-  const int* kInputDims[tflite::testing::kNumInputs] = {kLhsInputDims,
+  const int* kInputDims[tflite_micro::testing::kNumInputs] = {kLhsInputDims,
                                                         kRhsInputDims};
 
   constexpr size_t kLhsInputSize = 6;
@@ -308,7 +308,7 @@ TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_SimpleRHSAdjoint) {
       false   // asymmetric_quantize_inputs
   };
 
-  tflite::testing::TestBatchMatMulFloat(params, kInputDims, lhs_input,
+  tflite_micro::testing::TestBatchMatMulFloat(params, kInputDims, lhs_input,
                                         kRhsInput, kOutputDims, kExpect,
                                         output_data);
 }
@@ -316,7 +316,7 @@ TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_SimpleRHSAdjoint) {
 TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_SimpleLHSAdjoint) {
   constexpr int kLhsInputDims[] = {3, 1, 3, 2};
   constexpr int kRhsInputDims[] = {3, 1, 3, 4};
-  const int* kInputDims[tflite::testing::kNumInputs] = {kLhsInputDims,
+  const int* kInputDims[tflite_micro::testing::kNumInputs] = {kLhsInputDims,
                                                         kRhsInputDims};
   constexpr float kLhsInput[] = {1, 4, 2, 5, 3, 6};
 
@@ -335,7 +335,7 @@ TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_SimpleLHSAdjoint) {
       false   // asymmetric_quantize_inputs
   };
 
-  tflite::testing::TestBatchMatMulFloat(params, kInputDims, kLhsInput,
+  tflite_micro::testing::TestBatchMatMulFloat(params, kInputDims, kLhsInput,
                                         rhs_input, kOutputDims, kExpect,
                                         output_data);
 }
@@ -343,7 +343,7 @@ TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_SimpleLHSAdjoint) {
 TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_BatchSizeTwo) {
   constexpr int kLhsInputDims[] = {3, 2, 2, 3};
   constexpr int kRhsInputDims[] = {3, 2, 3, 4};
-  const int* kInputDims[tflite::testing::kNumInputs] = {kLhsInputDims,
+  const int* kInputDims[tflite_micro::testing::kNumInputs] = {kLhsInputDims,
                                                         kRhsInputDims};
   constexpr size_t kLhsInputSize = 12;
   float lhs_input[kLhsInputSize];
@@ -365,7 +365,7 @@ TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_BatchSizeTwo) {
       false   // asymmetric_quantize_inputs
   };
 
-  tflite::testing::TestBatchMatMulFloat(params, kInputDims, lhs_input,
+  tflite_micro::testing::TestBatchMatMulFloat(params, kInputDims, lhs_input,
                                         rhs_input, kOutputDims, kExpect,
                                         output_data);
 }
@@ -373,7 +373,7 @@ TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_BatchSizeTwo) {
 TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_Broadcast) {
   constexpr int kLhsInputDims[] = {3, 2, 2, 3};
   constexpr int kRhsInputDims[] = {2, 3, 4};
-  const int* kInputDims[tflite::testing::kNumInputs] = {kLhsInputDims,
+  const int* kInputDims[tflite_micro::testing::kNumInputs] = {kLhsInputDims,
                                                         kRhsInputDims};
   constexpr size_t kLhsInputSize = 12;
   float lhs_input[kLhsInputSize];
@@ -395,7 +395,7 @@ TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_Broadcast) {
       false   // asymmetric_quantize_inputs
   };
 
-  tflite::testing::TestBatchMatMulFloat(params, kInputDims, lhs_input,
+  tflite_micro::testing::TestBatchMatMulFloat(params, kInputDims, lhs_input,
                                         rhs_input, kOutputDims, kExpect,
                                         output_data);
 }
@@ -403,7 +403,7 @@ TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_Broadcast) {
 TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_BroadcastLHSAdjoint) {
   constexpr int kLhsInputDims[] = {3, 2, 3, 2};
   constexpr int kRhsInputDims[] = {2, 3, 4};
-  const int* kInputDims[tflite::testing::kNumInputs] = {kLhsInputDims,
+  const int* kInputDims[tflite_micro::testing::kNumInputs] = {kLhsInputDims,
                                                         kRhsInputDims};
 
   constexpr float kLhsInput[] = {1, 4, 2, 5, 3, 6, 7, 10, 8, 11, 9, 12};
@@ -424,7 +424,7 @@ TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_BroadcastLHSAdjoint) {
       false   // asymmetric_quantize_inputs
   };
 
-  tflite::testing::TestBatchMatMulFloat(params, kInputDims, kLhsInput,
+  tflite_micro::testing::TestBatchMatMulFloat(params, kInputDims, kLhsInput,
                                         rhs_input, kOutputDims, kExpect,
                                         output_data);
 }
@@ -432,7 +432,7 @@ TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_BroadcastLHSAdjoint) {
 TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_Broadcast2) {
   constexpr int kLhsInputDims[] = {4, 2, 1, 3, 2};
   constexpr int kRhsInputDims[] = {3, 3, 2, 4};
-  const int* kInputDims[tflite::testing::kNumInputs] = {kLhsInputDims,
+  const int* kInputDims[tflite_micro::testing::kNumInputs] = {kLhsInputDims,
                                                         kRhsInputDims};
 
   constexpr size_t kLhsInputSize = 12;
@@ -460,7 +460,7 @@ TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_Broadcast2) {
       false   // asymmetric_quantize_inputs
   };
 
-  tflite::testing::TestBatchMatMulFloat(params, kInputDims, lhs_input,
+  tflite_micro::testing::TestBatchMatMulFloat(params, kInputDims, lhs_input,
                                         rhs_input, kOutputDims, kExpect,
                                         output_data);
 }
@@ -468,7 +468,7 @@ TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_Broadcast2) {
 TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_Broadcast2LHSAdjoint) {
   constexpr int kLhsInputDims[] = {4, 2, 1, 2, 3};
   constexpr int kRhsInputDims[] = {3, 3, 2, 4};
-  const int* kInputDims[tflite::testing::kNumInputs] = {kLhsInputDims,
+  const int* kInputDims[tflite_micro::testing::kNumInputs] = {kLhsInputDims,
                                                         kRhsInputDims};
 
   constexpr float kLhsInput[] = {1, 3, 5, 2, 4, 6, 7, 9, 11, 8, 10, 12};
@@ -494,7 +494,7 @@ TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_Broadcast2LHSAdjoint) {
       false   // asymmetric_quantize_inputs
   };
 
-  tflite::testing::TestBatchMatMulFloat(params, kInputDims, kLhsInput,
+  tflite_micro::testing::TestBatchMatMulFloat(params, kInputDims, kLhsInput,
                                         rhs_input, kOutputDims, kExpect,
                                         output_data);
 }
@@ -502,7 +502,7 @@ TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_Broadcast2LHSAdjoint) {
 TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_Broadcast2RHSAdjoint) {
   constexpr int kLhsInputDims[] = {4, 2, 1, 3, 2};
   constexpr int kRhsInputDims[] = {3, 3, 4, 2};
-  const int* kInputDims[tflite::testing::kNumInputs] = {kLhsInputDims,
+  const int* kInputDims[tflite_micro::testing::kNumInputs] = {kLhsInputDims,
                                                         kRhsInputDims};
 
   constexpr size_t kLhsInputSize = 12;
@@ -530,7 +530,7 @@ TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_Broadcast2RHSAdjoint) {
       false   // asymmetric_quantize_inputs
   };
 
-  tflite::testing::TestBatchMatMulFloat(params, kInputDims, lhs_input,
+  tflite_micro::testing::TestBatchMatMulFloat(params, kInputDims, lhs_input,
                                         kRhsInput, kOutputDims, kExpect,
                                         output_data);
 }
@@ -538,7 +538,7 @@ TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_Broadcast2RHSAdjoint) {
 TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_Broadcast2BothAdjoint) {
   constexpr int kLhsInputDims[] = {4, 2, 1, 2, 3};
   constexpr int kRhsInputDims[] = {3, 3, 4, 2};
-  const int* kInputDims[tflite::testing::kNumInputs] = {kLhsInputDims,
+  const int* kInputDims[tflite_micro::testing::kNumInputs] = {kLhsInputDims,
                                                         kRhsInputDims};
 
   constexpr float kLhsInput[] = {1, 3, 5, 2, 4, 6, 7, 9, 11, 8, 10, 12};
@@ -564,7 +564,7 @@ TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_Broadcast2BothAdjoint) {
       false  // asymmetric_quantize_inputs
   };
 
-  tflite::testing::TestBatchMatMulFloat(params, kInputDims, kLhsInput,
+  tflite_micro::testing::TestBatchMatMulFloat(params, kInputDims, kLhsInput,
                                         kRhsInput, kOutputDims, kExpect,
                                         output_data);
 }
@@ -572,7 +572,7 @@ TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_Broadcast2BothAdjoint) {
 TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_BroadcastFromRHS) {
   constexpr int kLhsInputDims[] = {2, 4, 5};
   constexpr int kRhsInputDims[] = {4, 3, 1, 5, 2};
-  const int* kInputDims[tflite::testing::kNumInputs] = {kLhsInputDims,
+  const int* kInputDims[tflite_micro::testing::kNumInputs] = {kLhsInputDims,
                                                         kRhsInputDims};
 
   constexpr size_t kLhsInputSize = 20;
@@ -597,7 +597,7 @@ TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_BroadcastFromRHS) {
       false   // asymmetric_quantize_inputs
   };
 
-  tflite::testing::TestBatchMatMulFloat(params, kInputDims, lhs_input,
+  tflite_micro::testing::TestBatchMatMulFloat(params, kInputDims, lhs_input,
                                         rhs_input, kOutputDims, kExpect,
                                         output_data);
 }
@@ -605,7 +605,7 @@ TF_LITE_MICRO_TEST(BatchMatMulOpTestFloat32Test_BroadcastFromRHS) {
 TF_LITE_MICRO_TEST(ConstRHSBatchMatMulOpModelRHSNotAdjoint) {
   constexpr int kLhsInputDims[] = {3, 1, 6, 2};
   constexpr int kRhsInputDims[] = {2, 2, 3};
-  const int* kInputDims[tflite::testing::kNumInputs] = {kLhsInputDims,
+  const int* kInputDims[tflite_micro::testing::kNumInputs] = {kLhsInputDims,
                                                         kRhsInputDims};
 
   constexpr float kLhsInput[] = {6, 3, 7, 4, 6, 9, 2, 6, 7, 4, 3, 7};
@@ -624,11 +624,11 @@ TF_LITE_MICRO_TEST(ConstRHSBatchMatMulOpModelRHSNotAdjoint) {
       false   // asymmetric_quantize_inputs
   };
 
-  tflite::testing::TestBatchMatMulFloat(params, kInputDims, kLhsInput,
+  tflite_micro::testing::TestBatchMatMulFloat(params, kInputDims, kLhsInput,
                                         kRhsInput, kOutputDims, kExpect,
                                         output_data, true);
   // Eval twice to make sure constant transposed RHS is persistent.
-  tflite::testing::TestBatchMatMulFloat(params, kInputDims, kLhsInput,
+  tflite_micro::testing::TestBatchMatMulFloat(params, kInputDims, kLhsInput,
                                         kRhsInput, kOutputDims, kExpect,
                                         output_data, true, false);
 }
@@ -636,7 +636,7 @@ TF_LITE_MICRO_TEST(ConstRHSBatchMatMulOpModelRHSNotAdjoint) {
 TF_LITE_MICRO_TEST(QuantizedBatchMatMulOpTestSimpleTestQuantizedInt8) {
   constexpr int kLhsInputDims[] = {2, 2, 10};
   constexpr int kRhsInputDims[] = {2, 10, 3};
-  const int* kInputDims[tflite::testing::kNumInputs] = {kLhsInputDims,
+  const int* kInputDims[tflite_micro::testing::kNumInputs] = {kLhsInputDims,
                                                         kRhsInputDims};
 
   constexpr float kLhsInput[] = {
@@ -662,26 +662,26 @@ TF_LITE_MICRO_TEST(QuantizedBatchMatMulOpTestSimpleTestQuantizedInt8) {
       false   // asymmetric_quantize_inputs
   };
 
-  tflite::testing::TestQuantizationParams<int8_t, kLhsInputCount>
+  tflite_micro::testing::TestQuantizationParams<int8_t, kLhsInputCount>
       quantization_params_lhs = {0.0f,    // scale
                                  0,       // zero_point
                                  -63.5f,  // data_min
                                  64.0f,   // data_max
                                  {}};
-  tflite::testing::TestQuantizationParams<int8_t, kRhsInputCount>
+  tflite_micro::testing::TestQuantizationParams<int8_t, kRhsInputCount>
       quantization_params_rhs = {0.0f,    // scale
                                  0,       // zero_point
                                  -63.5f,  // data_min
                                  64.0f,   // data_max
                                  {}};
-  tflite::testing::TestQuantizationParams<int8_t, kOutputCount>
+  tflite_micro::testing::TestQuantizationParams<int8_t, kOutputCount>
       quantization_params_output = {0.0f,     // scale
                                     0,        // zero_point
                                     -127.0f,  // data_min
                                     128.0f,   // data_max
                                     {}};
 
-  tflite::testing::TestBatchMatMulQuantized<int8_t>(
+  tflite_micro::testing::TestBatchMatMulQuantized<int8_t>(
       params, &quantization_params_lhs, &quantization_params_rhs,
       &quantization_params_output, kInputDims, kLhsInput, kRhsInput,
       kOutputDims, kExpect, output_data);
@@ -690,7 +690,7 @@ TF_LITE_MICRO_TEST(QuantizedBatchMatMulOpTestSimpleTestQuantizedInt8) {
 TF_LITE_MICRO_TEST(QuantizedBatchMatMulOpTestSimpleTestQuantizedInt16) {
   constexpr int kLhsInputDims[] = {2, 2, 10};
   constexpr int kRhsInputDims[] = {2, 10, 3};
-  const int* kInputDims[tflite::testing::kNumInputs] = {kLhsInputDims,
+  const int* kInputDims[tflite_micro::testing::kNumInputs] = {kLhsInputDims,
                                                         kRhsInputDims};
 
   constexpr float kLhsInput[] = {
@@ -716,18 +716,18 @@ TF_LITE_MICRO_TEST(QuantizedBatchMatMulOpTestSimpleTestQuantizedInt16) {
       false   // asymmetric_quantize_inputs
   };
 
-  tflite::testing::TestQuantizationParams<int16_t, kLhsInputCount>
+  tflite_micro::testing::TestQuantizationParams<int16_t, kLhsInputCount>
       quantization_params_lhs = {};
   quantization_params_lhs.scale = 10.0f / std::numeric_limits<int16_t>::max();
-  tflite::testing::TestQuantizationParams<int16_t, kRhsInputCount>
+  tflite_micro::testing::TestQuantizationParams<int16_t, kRhsInputCount>
       quantization_params_rhs = {};
   quantization_params_rhs.scale = 10.0f / std::numeric_limits<int16_t>::max();
 
-  tflite::testing::TestQuantizationParams<int16_t, kOutputCount>
+  tflite_micro::testing::TestQuantizationParams<int16_t, kOutputCount>
       quantization_params_output = {};
   quantization_params_output.scale = 1.0f;
 
-  tflite::testing::TestBatchMatMulQuantized<int16_t>(
+  tflite_micro::testing::TestBatchMatMulQuantized<int16_t>(
       params, &quantization_params_lhs, &quantization_params_rhs,
       &quantization_params_output, kInputDims, kLhsInput, kRhsInput,
       kOutputDims, kExpect, output_data);
diff --git a/tensorflow/lite/micro/kernels/batch_to_space_nd.cc b/tensorflow/lite/micro/kernels/batch_to_space_nd.cc
index 090a040a..873a6876 100644
--- a/tensorflow/lite/micro/kernels/batch_to_space_nd.cc
+++ b/tensorflow/lite/micro/kernels/batch_to_space_nd.cc
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_log.h"
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -64,36 +64,36 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   const TfLiteEvalTensor* block_shape =
-      tflite::micro::GetEvalInput(context, node, kBlockShapeTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kBlockShapeTensor);
   const TfLiteEvalTensor* crops =
-      tflite::micro::GetEvalInput(context, node, kCropsTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kCropsTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
   switch (input->type) {  // Already know in/out types are same.
     case kTfLiteFloat32:
       reference_ops::BatchToSpaceND(
-          tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<float>(input),
-          tflite::micro::GetTensorShape(block_shape),
-          tflite::micro::GetTensorData<int32_t>(block_shape),
-          tflite::micro::GetTensorShape(crops),
-          tflite::micro::GetTensorData<int32_t>(crops),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<float>(output));
+          tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<float>(input),
+          tflite_micro::micro::GetTensorShape(block_shape),
+          tflite_micro::micro::GetTensorData<int32_t>(block_shape),
+          tflite_micro::micro::GetTensorShape(crops),
+          tflite_micro::micro::GetTensorData<int32_t>(crops),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<float>(output));
       break;
     case kTfLiteInt8:
       reference_ops::BatchToSpaceND(
-          tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<int8_t>(input),
-          tflite::micro::GetTensorShape(block_shape),
-          tflite::micro::GetTensorData<int32_t>(block_shape),
-          tflite::micro::GetTensorShape(crops),
-          tflite::micro::GetTensorData<int32_t>(crops),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<int8_t>(output));
+          tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<int8_t>(input),
+          tflite_micro::micro::GetTensorShape(block_shape),
+          tflite_micro::micro::GetTensorData<int32_t>(block_shape),
+          tflite_micro::micro::GetTensorShape(crops),
+          tflite_micro::micro::GetTensorData<int32_t>(crops),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<int8_t>(output));
       break;
     default:
       MicroPrintf("Type %s (%d) not supported.", TfLiteTypeGetName(input->type),
@@ -106,7 +106,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace.
 
 TFLMRegistration Register_BATCH_TO_SPACE_ND() {
-  return tflite::micro::RegisterOp(nullptr, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/batch_to_space_nd_test.cc b/tensorflow/lite/micro/kernels/batch_to_space_nd_test.cc
index 455c325a..263cbfd8 100644
--- a/tensorflow/lite/micro/kernels/batch_to_space_nd_test.cc
+++ b/tensorflow/lite/micro/kernels/batch_to_space_nd_test.cc
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -104,15 +104,15 @@ TfLiteStatus TestBatchToSpaceNdQuantized(
   constexpr int outputs_size = 1;
   constexpr int tensors_size = inputs_size + outputs_size;
   TfLiteTensor tensors[tensors_size] = {
-      tflite::testing::CreateQuantizedTensor(input_data, input_quantized,
+      tflite_micro::testing::CreateQuantizedTensor(input_data, input_quantized,
                                              input_dims, input_scale,
                                              input_zero_point),
-      tflite::testing::CreateTensor(block_shape_data, block_shape_dims),
-      tflite::testing::CreateTensor(crops_data, crops_dims),
-      tflite::testing::CreateQuantizedTensor(output_data, output_dims,
+      tflite_micro::testing::CreateTensor(block_shape_data, block_shape_dims),
+      tflite_micro::testing::CreateTensor(crops_data, crops_dims),
+      tflite_micro::testing::CreateQuantizedTensor(output_data, output_dims,
                                              output_scale, output_zero_point),
   };
-  tflite::Quantize(golden, golden_quantized, ElementCount(*output_dims),
+  tflite_micro::Quantize(golden, golden_quantized, ElementCount(*output_dims),
                    output_scale, output_zero_point);
 
   return ValidateBatchToSpaceNdGoldens(tensors, tensors_size, golden_quantized,
@@ -121,34 +121,34 @@ TfLiteStatus TestBatchToSpaceNdQuantized(
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(BatchToSpaceBasicFloat) {
-  float output[tflite::testing::kBasicInputOutputSize];
+  float output[tflite_micro::testing::kBasicInputOutputSize];
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteOk,
-      tflite::testing::TestBatchToSpaceNdFloat(
-          tflite::testing::basic_input_dims, tflite::testing::basic_input,
-          tflite::testing::basic_block_shape_dims,
-          tflite::testing::basic_block_shape, tflite::testing::basic_crops_dims,
-          tflite::testing::basic_crops, tflite::testing::basic_output_dims,
-          tflite::testing::basic_golden, output));
+      tflite_micro::testing::TestBatchToSpaceNdFloat(
+          tflite_micro::testing::basic_input_dims, tflite_micro::testing::basic_input,
+          tflite_micro::testing::basic_block_shape_dims,
+          tflite_micro::testing::basic_block_shape, tflite_micro::testing::basic_crops_dims,
+          tflite_micro::testing::basic_crops, tflite_micro::testing::basic_output_dims,
+          tflite_micro::testing::basic_golden, output));
 }
 
 TF_LITE_MICRO_TEST(BatchToSpaceBasicInt8) {
-  int8_t output[tflite::testing::kBasicInputOutputSize];
-  int8_t input_quantized[tflite::testing::kBasicInputOutputSize];
-  int8_t golden_quantized[tflite::testing::kBasicInputOutputSize];
+  int8_t output[tflite_micro::testing::kBasicInputOutputSize];
+  int8_t input_quantized[tflite_micro::testing::kBasicInputOutputSize];
+  int8_t golden_quantized[tflite_micro::testing::kBasicInputOutputSize];
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteOk,
-      tflite::testing::TestBatchToSpaceNdQuantized(
-          tflite::testing::basic_input_dims, tflite::testing::basic_input,
-          input_quantized, 1.0f, 0, tflite::testing::basic_block_shape_dims,
-          tflite::testing::basic_block_shape, tflite::testing::basic_crops_dims,
-          tflite::testing::basic_crops, tflite::testing::basic_output_dims,
-          tflite::testing::basic_golden, golden_quantized, 1.0f, 0, output));
+      tflite_micro::testing::TestBatchToSpaceNdQuantized(
+          tflite_micro::testing::basic_input_dims, tflite_micro::testing::basic_input,
+          input_quantized, 1.0f, 0, tflite_micro::testing::basic_block_shape_dims,
+          tflite_micro::testing::basic_block_shape, tflite_micro::testing::basic_crops_dims,
+          tflite_micro::testing::basic_crops, tflite_micro::testing::basic_output_dims,
+          tflite_micro::testing::basic_golden, golden_quantized, 1.0f, 0, output));
 }
 
 TF_LITE_MICRO_TESTS_END
diff --git a/tensorflow/lite/micro/kernels/broadcast_args.cc b/tensorflow/lite/micro/kernels/broadcast_args.cc
index 283410ab..09574f24 100644
--- a/tensorflow/lite/micro/kernels/broadcast_args.cc
+++ b/tensorflow/lite/micro/kernels/broadcast_args.cc
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_context.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 constexpr int kShape1Tensor = 0;
 constexpr int kShape2Tensor = 1;
@@ -84,8 +84,8 @@ TfLiteStatus BroadcastArgsEval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_BROADCAST_ARGS() {
-  return tflite::micro::RegisterOp(nullptr, BroadcastArgsPrepare,
+  return tflite_micro::micro::RegisterOp(nullptr, BroadcastArgsPrepare,
                                    BroadcastArgsEval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/broadcast_args_test.cc b/tensorflow/lite/micro/kernels/broadcast_args_test.cc
index ff5f0bb9..19899c83 100644
--- a/tensorflow/lite/micro/kernels/broadcast_args_test.cc
+++ b/tensorflow/lite/micro/kernels/broadcast_args_test.cc
@@ -20,8 +20,8 @@ limitations under the License.
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
 namespace {
-using ::tflite::testing::CreateTensor;
-using ::tflite::testing::IntArrayFromInts;
+using ::tflite_micro::testing::CreateTensor;
+using ::tflite_micro::testing::IntArrayFromInts;
 
 // The layout of tensors is fixed.
 constexpr int kShape1Index = 0;
@@ -32,7 +32,7 @@ constexpr int kOutputsTensor[] = {1, kOutputIndex};
 
 // This function is NOT thread safe.
 template <typename DimsType>
-tflite::micro::KernelRunner CreateBroadcastArgsTestRunner(
+tflite_micro::micro::KernelRunner CreateBroadcastArgsTestRunner(
     int* input1_shape, DimsType* input1_data, int* input2_shape,
     DimsType* input2_data, int* output_shape, DimsType* output_data) {
   // Some targets do not support dynamic memory (i.e., no malloc or new), thus,
@@ -46,8 +46,8 @@ tflite::micro::KernelRunner CreateBroadcastArgsTestRunner(
   tensors[1] = CreateTensor(input2_data, IntArrayFromInts(input2_shape));
   tensors[2] = CreateTensor(output_data, IntArrayFromInts(output_shape));
 
-  registration = tflite::Register_BROADCAST_ARGS();
-  tflite::micro::KernelRunner runner = tflite::micro::KernelRunner(
+  registration = tflite_micro::Register_BROADCAST_ARGS();
+  tflite_micro::micro::KernelRunner runner = tflite_micro::micro::KernelRunner(
       registration, tensors, sizeof(tensors) / sizeof(TfLiteTensor),
       IntArrayFromInts(const_cast<int*>(kInputsTensor)),
       IntArrayFromInts(const_cast<int*>(kOutputsTensor)),
@@ -60,7 +60,7 @@ void TestBroadcastArgs(int* input1_shape, DimsType* input1_data,
                        int* input2_shape, DimsType* input2_data,
                        int* output_shape, DimsType* output_data,
                        DimsType* expected_output_data) {
-  tflite::micro::KernelRunner runner =
+  tflite_micro::micro::KernelRunner runner =
       CreateBroadcastArgsTestRunner(input1_shape, input1_data, input2_shape,
                                     input2_data, output_shape, output_data);
 
@@ -68,7 +68,7 @@ void TestBroadcastArgs(int* input1_shape, DimsType* input1_data,
   TF_LITE_MICRO_EXPECT_EQ(runner.Invoke(), kTfLiteOk);
 
   // The output elements contain the fill value.
-  const auto elements = tflite::ElementCount(*IntArrayFromInts(output_shape));
+  const auto elements = tflite_micro::ElementCount(*IntArrayFromInts(output_shape));
   for (int i = 0; i < elements; ++i) {
     TF_LITE_MICRO_EXPECT_EQ(output_data[i], expected_output_data[i]);
   }
diff --git a/tensorflow/lite/micro/kernels/broadcast_to.cc b/tensorflow/lite/micro/kernels/broadcast_to.cc
index 61deaa31..28be7c7e 100644
--- a/tensorflow/lite/micro/kernels/broadcast_to.cc
+++ b/tensorflow/lite/micro/kernels/broadcast_to.cc
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_context.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 constexpr int kInputTensor = 0;
@@ -66,9 +66,8 @@ TfLiteStatus ValidateOutputTensor(TfLiteContext* context, TfLiteTensor* input,
   }
 
   // Validating the shape of the output tensor.
-  tflite::RuntimeShape output_shape = tflite::GetTensorShape(output);
   for (int idx = 0; idx < output_num_dims; ++idx) {
-    TF_LITE_ENSURE(context, output_shape.Dims(idx) == get_shape_data(idx));
+    TF_LITE_ENSURE(context, SizeOfDimension(output, idx) == get_shape_data(idx));
   }
   return kTfLiteOk;
 }
@@ -116,8 +115,8 @@ TfLiteStatus BroadcastToEval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_BROADCAST_TO() {
-  return tflite::micro::RegisterOp(nullptr, BroadcastToPrepare,
+  return tflite_micro::micro::RegisterOp(nullptr, BroadcastToPrepare,
                                    BroadcastToEval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/broadcast_to_test.cc b/tensorflow/lite/micro/kernels/broadcast_to_test.cc
index 09a97567..a0f3dadc 100644
--- a/tensorflow/lite/micro/kernels/broadcast_to_test.cc
+++ b/tensorflow/lite/micro/kernels/broadcast_to_test.cc
@@ -20,8 +20,8 @@ limitations under the License.
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
 namespace {
-using ::tflite::testing::CreateTensor;
-using ::tflite::testing::IntArrayFromInts;
+using ::tflite_micro::testing::CreateTensor;
+using ::tflite_micro::testing::IntArrayFromInts;
 
 // The layout of tensors is fixed.
 constexpr int kInputIndex = 0;
@@ -32,7 +32,7 @@ constexpr int kOutputsTensor[] = {1, kOutputIndex};
 
 // This function is NOT thread safe.
 template <typename DimsType, typename ValueType>
-tflite::micro::KernelRunner CreateBroadcastToTestRunner(
+tflite_micro::micro::KernelRunner CreateBroadcastToTestRunner(
     int* dims_shape, DimsType* dims_data, int* input_shape,
     ValueType* input_data, int* output_shape, ValueType* output_data) {
   // Some targets do not support dynamic memory (i.e., no malloc or new), thus,
@@ -50,8 +50,8 @@ tflite::micro::KernelRunner CreateBroadcastToTestRunner(
   TF_LITE_MICRO_EXPECT_EQ(tensors[kOutputIndex].type,
                           tensors[kInputIndex].type);
 
-  registration = tflite::Register_BROADCAST_TO();
-  tflite::micro::KernelRunner runner = tflite::micro::KernelRunner(
+  registration = tflite_micro::Register_BROADCAST_TO();
+  tflite_micro::micro::KernelRunner runner = tflite_micro::micro::KernelRunner(
       registration, tensors, sizeof(tensors) / sizeof(TfLiteTensor),
       IntArrayFromInts(const_cast<int*>(kInputsTensor)),
       IntArrayFromInts(const_cast<int*>(kOutputsTensor)),
@@ -63,7 +63,7 @@ template <typename DimsType, typename ValueType>
 void TestBroadcastTo(int* dims_shape, DimsType* dims_data, int* input_shape,
                      ValueType* input_data, int* output_shape,
                      ValueType* output_data, ValueType* expected_output_data) {
-  tflite::micro::KernelRunner runner =
+  tflite_micro::micro::KernelRunner runner =
       CreateBroadcastToTestRunner(dims_shape, dims_data, input_shape,
                                   input_data, output_shape, output_data);
 
@@ -71,7 +71,7 @@ void TestBroadcastTo(int* dims_shape, DimsType* dims_data, int* input_shape,
   TF_LITE_MICRO_EXPECT_EQ(runner.Invoke(), kTfLiteOk);
 
   // The output elements contain the fill value.
-  const auto elements = tflite::ElementCount(*IntArrayFromInts(output_shape));
+  const auto elements = tflite_micro::ElementCount(*IntArrayFromInts(output_shape));
   for (int i = 0; i < elements; ++i) {
     TF_LITE_MICRO_EXPECT_EQ(output_data[i], expected_output_data[i]);
   }
@@ -90,7 +90,7 @@ TF_LITE_MICRO_TEST(ShapeMustBe1D) {
   int output_shape[] = {2, 2, 2};
   int output_data[] = {2, 3, 4, 4};
 
-  tflite::micro::KernelRunner runner =
+  tflite_micro::micro::KernelRunner runner =
       CreateBroadcastToTestRunner(dims_shape, dims_data, input_shape,
                                   input_data, output_shape, output_data);
 
@@ -107,7 +107,7 @@ TF_LITE_MICRO_TEST(TooManyDimensionShouldFail) {
   int output_shape[] = {6, 2, 2, 2, 2, 2, 2};
   int output_data[12];
 
-  tflite::micro::KernelRunner runner =
+  tflite_micro::micro::KernelRunner runner =
       CreateBroadcastToTestRunner(dims_shape, dims_data, input_shape,
                                   input_data, output_shape, output_data);
 
@@ -124,7 +124,7 @@ TF_LITE_MICRO_TEST(MismatchDimensionShouldFail) {
   int output_shape[] = {4, 2, 4, 1, 2};
   int output_data[24];
 
-  tflite::micro::KernelRunner runner =
+  tflite_micro::micro::KernelRunner runner =
       CreateBroadcastToTestRunner(dims_shape, dims_data, input_shape,
                                   input_data, output_shape, output_data);
 
diff --git a/tensorflow/lite/micro/kernels/call_once.cc b/tensorflow/lite/micro/kernels/call_once.cc
index 8ad1c201..6802df53 100644
--- a/tensorflow/lite/micro/kernels/call_once.cc
+++ b/tensorflow/lite/micro/kernels/call_once.cc
@@ -27,7 +27,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_graph.h"
 #include "tensorflow/lite/schema/schema_generated.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -51,7 +51,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   TF_LITE_ENSURE(context, NumInputs(node) == 0);
   TF_LITE_ENSURE(context, NumOutputs(node) == 0);
 
-  tflite::MicroContext* micro_context = tflite::GetMicroContext(context);
+  tflite_micro::MicroContext* micro_context = tflite_micro::GetMicroContext(context);
   MicroGraph& graph_info = micro_context->graph();
 
   TF_LITE_ENSURE(context,
@@ -68,7 +68,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
     return kTfLiteOk;
   }
 
-  tflite::MicroContext* micro_context = tflite::GetMicroContext(context);
+  tflite_micro::MicroContext* micro_context = tflite_micro::GetMicroContext(context);
   MicroGraph& graph_info = micro_context->graph();
 
   TF_LITE_ENSURE_OK(context,
@@ -82,7 +82,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace.
 
 TFLMRegistration Register_CALL_ONCE() {
-  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/call_once_test.cc b/tensorflow/lite/micro/kernels/call_once_test.cc
index aec5261a..9f3e6394 100644
--- a/tensorflow/lite/micro/kernels/call_once_test.cc
+++ b/tensorflow/lite/micro/kernels/call_once_test.cc
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -35,7 +35,7 @@ void TestCallOnce(const int subgraph0_invoke_count_golden,
   TfLiteCallOnceParams params;
   params.init_subgraph_index = 1;
 
-  const TFLMRegistration registration = tflite::Register_CALL_ONCE();
+  const TFLMRegistration registration = tflite_micro::Register_CALL_ONCE();
   micro::KernelRunner runner(registration, nullptr, 0, inputs_array,
                              outputs_array, &params);
 
@@ -50,13 +50,13 @@ void TestCallOnce(const int subgraph0_invoke_count_golden,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(CallOnceShouldOnlyInvokeSubgraphOnce) {
-  tflite::testing::TestCallOnce(1, 1);
-  tflite::testing::TestCallOnce(10, 1);
+  tflite_micro::testing::TestCallOnce(1, 1);
+  tflite_micro::testing::TestCallOnce(10, 1);
 }
 
 TF_LITE_MICRO_TESTS_END
diff --git a/tensorflow/lite/micro/kernels/cast.cc b/tensorflow/lite/micro/kernels/cast.cc
index 10d25eb4..44d97845 100644
--- a/tensorflow/lite/micro/kernels/cast.cc
+++ b/tensorflow/lite/micro/kernels/cast.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 constexpr int kInputTensor = 0;
@@ -67,7 +67,7 @@ TfLiteStatus copyToTensor(TfLiteContext* context, const FromT* in,
       copyCast(in, out->data.u32, num_elements);
       break;
     case kTfLiteFloat32:
-      copyCast(in, tflite::micro::GetTensorData<float>(out), num_elements);
+      copyCast(in, tflite_micro::micro::GetTensorData<float>(out), num_elements);
       break;
     default:
       // Unsupported type.
@@ -79,27 +79,27 @@ TfLiteStatus copyToTensor(TfLiteContext* context, const FromT* in,
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
-  int num_elements = MatchingFlatSize(tflite::micro::GetTensorShape(input),
-                                      tflite::micro::GetTensorShape(output));
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
+  int num_elements = MatchingFlatSize(tflite_micro::micro::GetTensorShape(input),
+                                      tflite_micro::micro::GetTensorShape(output));
 
   switch (input->type) {
     case kTfLiteInt8:
       return copyToTensor(context, input->data.int8, output, num_elements);
     case kTfLiteInt16:
-      return copyToTensor(context, tflite::micro::GetTensorData<int16_t>(input),
+      return copyToTensor(context, tflite_micro::micro::GetTensorData<int16_t>(input),
                           output, num_elements);
     case kTfLiteInt32:
-      return copyToTensor(context, tflite::micro::GetTensorData<int32_t>(input),
+      return copyToTensor(context, tflite_micro::micro::GetTensorData<int32_t>(input),
                           output, num_elements);
     case kTfLiteUInt32:
       return copyToTensor(context,
-                          tflite::micro::GetTensorData<uint32_t>(input), output,
+                          tflite_micro::micro::GetTensorData<uint32_t>(input), output,
                           num_elements);
     case kTfLiteFloat32:
-      return copyToTensor(context, tflite::micro::GetTensorData<float>(input),
+      return copyToTensor(context, tflite_micro::micro::GetTensorData<float>(input),
                           output, num_elements);
     default:
       // Unsupported type.
@@ -111,7 +111,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_CAST() {
-  return tflite::micro::RegisterOp(nullptr, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/cast_test.cc b/tensorflow/lite/micro/kernels/cast_test.cc
index fc098065..2d6e3787 100644
--- a/tensorflow/lite/micro/kernels/cast_test.cc
+++ b/tensorflow/lite/micro/kernels/cast_test.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -57,7 +57,7 @@ void TestCast(int* input_dims_data, const inputT* input_data,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -68,7 +68,7 @@ TF_LITE_MICRO_TEST(CastFloatToInt8) {
   // TODO(b/178391195): Test negative and out-of-range numbers.
   const float input_values[] = {100.f, 1.0f, 0.f, 0.4f, 1.999f, 1.1f};
   const int8_t golden[] = {100, 1, 0, 0, 1, 1};
-  tflite::testing::TestCast(input_dims, input_values, golden, output_data);
+  tflite_micro::testing::TestCast(input_dims, input_values, golden, output_data);
 }
 
 TF_LITE_MICRO_TEST(CastFloatToInt16) {
@@ -78,7 +78,7 @@ TF_LITE_MICRO_TEST(CastFloatToInt16) {
   // TODO(b/178391195): Test negative and out-of-range numbers.
   const float input_values[] = {100.f, 1.0f, 0.f, 0.4f, 1.999f, 1.1f};
   const int16_t golden[] = {100, 1, 0, 0, 1, 1};
-  tflite::testing::TestCast(input_dims, input_values, golden, output_data);
+  tflite_micro::testing::TestCast(input_dims, input_values, golden, output_data);
 }
 
 TF_LITE_MICRO_TEST(CastInt8ToFloat) {
@@ -86,7 +86,7 @@ TF_LITE_MICRO_TEST(CastInt8ToFloat) {
   int input_dims[] = {2, 3, 2};
   const int8_t input_values[] = {123, 0, 1, 2, 3, 4};
   const float golden[] = {123.f, 0.f, 1.f, 2.f, 3.f, 4.f};
-  tflite::testing::TestCast(input_dims, input_values, golden, output_data);
+  tflite_micro::testing::TestCast(input_dims, input_values, golden, output_data);
 }
 
 TF_LITE_MICRO_TEST(CastInt16ToFloat) {
@@ -94,7 +94,7 @@ TF_LITE_MICRO_TEST(CastInt16ToFloat) {
   int input_dims[] = {2, 3, 2};
   const int16_t input_values[] = {123, 0, 1, 2, 3, 4};
   const float golden[] = {123.f, 0.f, 1.f, 2.f, 3.f, 4.f};
-  tflite::testing::TestCast(input_dims, input_values, golden, output_data);
+  tflite_micro::testing::TestCast(input_dims, input_values, golden, output_data);
 }
 
 TF_LITE_MICRO_TEST(CastInt16ToInt32) {
@@ -102,7 +102,7 @@ TF_LITE_MICRO_TEST(CastInt16ToInt32) {
   int input_dims[] = {2, 3, 2};
   const int16_t input_values[] = {123, 0, 1, 2, 3, 4};
   const int32_t golden[] = {123, 0, 1, 2, 3, 4};
-  tflite::testing::TestCast(input_dims, input_values, golden, output_data);
+  tflite_micro::testing::TestCast(input_dims, input_values, golden, output_data);
 }
 
 TF_LITE_MICRO_TEST(CastInt32ToInt16) {
@@ -110,7 +110,7 @@ TF_LITE_MICRO_TEST(CastInt32ToInt16) {
   int input_dims[] = {2, 3, 2};
   const int32_t input_values[] = {123, 0, 1, 2, 3, 4};
   const int16_t golden[] = {123, 0, 1, 2, 3, 4};
-  tflite::testing::TestCast(input_dims, input_values, golden, output_data);
+  tflite_micro::testing::TestCast(input_dims, input_values, golden, output_data);
 }
 
 TF_LITE_MICRO_TEST(CastUInt32ToInt32) {
@@ -118,7 +118,7 @@ TF_LITE_MICRO_TEST(CastUInt32ToInt32) {
   int input_dims[] = {2, 2, 3};
   const uint32_t input_values[] = {100, 200, 300, 400, 500, 600};
   const int32_t golden[] = {100, 200, 300, 400, 500, 600};
-  tflite::testing::TestCast(input_dims, input_values, golden, output_data);
+  tflite_micro::testing::TestCast(input_dims, input_values, golden, output_data);
 }
 
 TF_LITE_MICRO_TEST(CastUInt32ToInt32) {
@@ -126,7 +126,7 @@ TF_LITE_MICRO_TEST(CastUInt32ToInt32) {
   int input_dims[] = {2, 2, 3};
   const int32_t input_values[] = {100, 200, 300, 400, 500, 600};
   const uint32_t golden[] = {100, 200, 300, 400, 500, 600};
-  tflite::testing::TestCast(input_dims, input_values, golden, output_data);
+  tflite_micro::testing::TestCast(input_dims, input_values, golden, output_data);
 }
 
 TF_LITE_MICRO_TESTS_END
diff --git a/tensorflow/lite/micro/kernels/ceil.cc b/tensorflow/lite/micro/kernels/ceil.cc
index 46b55e7c..d3f809f9 100644
--- a/tensorflow/lite/micro/kernels/ceil.cc
+++ b/tensorflow/lite/micro/kernels/ceil.cc
@@ -20,7 +20,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -52,14 +52,14 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
-  reference_ops::Ceil(tflite::micro::GetTensorShape(input),
-                      tflite::micro::GetTensorData<float>(input),
-                      tflite::micro::GetTensorShape(output),
-                      tflite::micro::GetTensorData<float>(output));
+  reference_ops::Ceil(tflite_micro::micro::GetTensorShape(input),
+                      tflite_micro::micro::GetTensorData<float>(input),
+                      tflite_micro::micro::GetTensorShape(output),
+                      tflite_micro::micro::GetTensorData<float>(output));
 
   return kTfLiteOk;
 }
@@ -67,7 +67,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_CEIL() {
-  return tflite::micro::RegisterOp(nullptr, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/ceil_test.cc b/tensorflow/lite/micro/kernels/ceil_test.cc
index caba85e9..4343c4d9 100644
--- a/tensorflow/lite/micro/kernels/ceil_test.cc
+++ b/tensorflow/lite/micro/kernels/ceil_test.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -56,7 +56,7 @@ void TestCeil(int* input_dims_data, const float* input_data,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -65,7 +65,7 @@ TF_LITE_MICRO_TEST(SingleDim) {
   int input_dims[] = {1, 2};
   const float input_values[] = {8.5, 0.0};
   const float golden[] = {9, 0};
-  tflite::testing::TestCeil(input_dims, input_values, golden, output_data);
+  tflite_micro::testing::TestCeil(input_dims, input_values, golden, output_data);
 }
 
 TF_LITE_MICRO_TEST(MultiDims) {
@@ -76,7 +76,7 @@ TF_LITE_MICRO_TEST(MultiDims) {
       -0.0001, -8.0001, -0.9999, -9.9999, -0.5,
   };
   const float golden[] = {1, 9, 1, 10, 1, 0, -8, 0, -9, 0};
-  tflite::testing::TestCeil(input_dims, input_values, golden, output_data);
+  tflite_micro::testing::TestCeil(input_dims, input_values, golden, output_data);
 }
 
 TF_LITE_MICRO_TESTS_END
diff --git a/tensorflow/lite/micro/kernels/ceva/conv.cc b/tensorflow/lite/micro/kernels/ceva/conv.cc
index c68913d5..6632210c 100644
--- a/tensorflow/lite/micro/kernels/ceva/conv.cc
+++ b/tensorflow/lite/micro/kernels/ceva/conv.cc
@@ -33,7 +33,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/ceva/mcps_macros.h"
 #endif
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 void* Init(TfLiteContext* context, const char* buffer, size_t length) {
@@ -58,11 +58,11 @@ void EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,
   const int32_t* bias_data;
   int8_t* output_data;
 
-  const RuntimeShape& input_shape = tflite::micro::GetTensorShape(input);
-  const RuntimeShape& filter_shape = tflite::micro::GetTensorShape(filter);
-  const RuntimeShape& bias_shape = tflite::micro::GetTensorShape(bias);
-  const RuntimeShape& output_shape = tflite::micro::GetTensorShape(output);
-  const RuntimeShape& im2col_shape = tflite::micro::GetTensorShape(im2col);
+  const RuntimeShape& input_shape = tflite_micro::micro::GetTensorShape(input);
+  const RuntimeShape& filter_shape = tflite_micro::micro::GetTensorShape(filter);
+  const RuntimeShape& bias_shape = tflite_micro::micro::GetTensorShape(bias);
+  const RuntimeShape& output_shape = tflite_micro::micro::GetTensorShape(output);
+  const RuntimeShape& im2col_shape = tflite_micro::micro::GetTensorShape(im2col);
 
   const int stride_width = op_params.stride_width;
   const int stride_height = op_params.stride_height;
@@ -87,10 +87,10 @@ void EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,
   const int output_width = output_shape.Dims(2);
   const int output_depth_Dims3 = output_shape.Dims(3);
 
-  input_data = tflite::micro::GetTensorData<int8_t>(input);
-  filter_data = tflite::micro::GetTensorData<int8_t>(filter);
-  bias_data = tflite::micro::GetTensorData<int32_t>(bias);
-  output_data = tflite::micro::GetTensorData<int8_t>(output);
+  input_data = tflite_micro::micro::GetTensorData<int8_t>(input);
+  filter_data = tflite_micro::micro::GetTensorData<int8_t>(filter);
+  bias_data = tflite_micro::micro::GetTensorData<int32_t>(bias);
+  output_data = tflite_micro::micro::GetTensorData<int8_t>(output);
 
   int sizeof_scratch = filter_depth;
   if (sizeof_scratch < output_depth_Dims3) sizeof_scratch = output_depth_Dims3;
@@ -144,11 +144,11 @@ void EvalFloat(TfLiteContext* context, TfLiteNode* node,
   const float *input_data, *filter_data, *bias_data, *im2col_data;
   float* output_data;
 
-  const RuntimeShape& input_shape = tflite::micro::GetTensorShape(input);
-  const RuntimeShape& filter_shape = tflite::micro::GetTensorShape(filter);
-  const RuntimeShape& bias_shape = tflite::micro::GetTensorShape(bias);
-  const RuntimeShape& output_shape = tflite::micro::GetTensorShape(output);
-  const RuntimeShape& im2col_shape = tflite::micro::GetTensorShape(im2col);
+  const RuntimeShape& input_shape = tflite_micro::micro::GetTensorShape(input);
+  const RuntimeShape& filter_shape = tflite_micro::micro::GetTensorShape(filter);
+  const RuntimeShape& bias_shape = tflite_micro::micro::GetTensorShape(bias);
+  const RuntimeShape& output_shape = tflite_micro::micro::GetTensorShape(output);
+  const RuntimeShape& im2col_shape = tflite_micro::micro::GetTensorShape(im2col);
 
   const int stride_width = op_params.stride_width;
   const int stride_height = op_params.stride_height;
@@ -174,11 +174,11 @@ void EvalFloat(TfLiteContext* context, TfLiteNode* node,
   const int output_width = output_shape.Dims(2);
   const int output_depth_Dims3 = output_shape.Dims(3);
 
-  input_data = tflite::micro::GetTensorData<float>(input);
-  filter_data = tflite::micro::GetTensorData<float>(filter);
-  bias_data = tflite::micro::GetTensorData<float>(bias);
-  output_data = tflite::micro::GetTensorData<float>(output);
-  im2col_data = tflite::micro::GetTensorData<float>(im2col);
+  input_data = tflite_micro::micro::GetTensorData<float>(input);
+  filter_data = tflite_micro::micro::GetTensorData<float>(filter);
+  bias_data = tflite_micro::micro::GetTensorData<float>(bias);
+  output_data = tflite_micro::micro::GetTensorData<float>(output);
+  im2col_data = tflite_micro::micro::GetTensorData<float>(im2col);
 
 #ifdef MCPS_MEASUREMENT
   MCPS_START_ONE;
@@ -209,15 +209,15 @@ TfLiteStatus EvalCEVA(TfLiteContext* context, TfLiteNode* node) {
   auto* params = reinterpret_cast<TfLiteConvParams*>(node->builtin_data);
 
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kConvInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kConvInputTensor);
   const TfLiteEvalTensor* filter =
-      tflite::micro::GetEvalInput(context, node, kConvWeightsTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kConvWeightsTensor);
   const TfLiteEvalTensor* bias =
       (NumInputs(node) == 3)
-          ? tflite::micro::GetEvalInput(context, node, kConvBiasTensor)
+          ? tflite_micro::micro::GetEvalInput(context, node, kConvBiasTensor)
           : nullptr;
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kConvOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kConvOutputTensor);
 
   TFLITE_DCHECK(node->user_data != nullptr);
   const OpDataConv& data = *(static_cast<const OpDataConv*>(node->user_data));
@@ -252,7 +252,7 @@ TfLiteStatus ConvEval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_CONV_2D() {
-  return tflite::micro::RegisterOp(Init, ConvPrepare, ConvEval);
+  return tflite_micro::micro::RegisterOp(Init, ConvPrepare, ConvEval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/ceva/depthwise_conv.cc b/tensorflow/lite/micro/kernels/ceva/depthwise_conv.cc
index eed5d9ee..9e098706 100644
--- a/tensorflow/lite/micro/kernels/ceva/depthwise_conv.cc
+++ b/tensorflow/lite/micro/kernels/ceva/depthwise_conv.cc
@@ -34,7 +34,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/ceva/mcps_macros.h"
 #endif
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 void* Init(TfLiteContext* context, const char* buffer, size_t length) {
@@ -50,19 +50,19 @@ void EvalFloat(TfLiteContext* context, TfLiteNode* node,
   CalculateActivationRange(params->activation, &output_activation_min,
                            &output_activation_max);
 
-  tflite::DepthwiseParams op_params = DepthwiseConvParamsFloat(*params, data);
+  tflite_micro::DepthwiseParams op_params = DepthwiseConvParamsFloat(*params, data);
 
   const float *input_data, *filter_data, *bias_data;
   float* output_data;
-  input_data = tflite::micro::GetTensorData<float>(input);
-  filter_data = tflite::micro::GetTensorData<float>(filter);
-  bias_data = tflite::micro::GetTensorData<float>(bias);
-  output_data = tflite::micro::GetTensorData<float>(output);
+  input_data = tflite_micro::micro::GetTensorData<float>(input);
+  filter_data = tflite_micro::micro::GetTensorData<float>(filter);
+  bias_data = tflite_micro::micro::GetTensorData<float>(bias);
+  output_data = tflite_micro::micro::GetTensorData<float>(output);
 
-  const RuntimeShape& input_shape = tflite::micro::GetTensorShape(input);
-  const RuntimeShape& filter_shape = tflite::micro::GetTensorShape(filter);
-  const RuntimeShape& bias_shape = tflite::micro::GetTensorShape(bias);
-  const RuntimeShape& output_shape = tflite::micro::GetTensorShape(output);
+  const RuntimeShape& input_shape = tflite_micro::micro::GetTensorShape(input);
+  const RuntimeShape& filter_shape = tflite_micro::micro::GetTensorShape(filter);
+  const RuntimeShape& bias_shape = tflite_micro::micro::GetTensorShape(bias);
+  const RuntimeShape& output_shape = tflite_micro::micro::GetTensorShape(output);
 
   const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   const int input_height = input_shape.Dims(1);
@@ -130,15 +130,15 @@ void EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,
   const int32_t input_offset = op_params.input_offset;
   const int32_t output_offset = op_params.output_offset;
 
-  input_data = tflite::micro::GetTensorData<int8_t>(input);
-  filter_data = tflite::micro::GetTensorData<int8_t>(filter);
-  bias_data = tflite::micro::GetTensorData<int32_t>(bias);
-  output_data = tflite::micro::GetTensorData<int8_t>(output);
+  input_data = tflite_micro::micro::GetTensorData<int8_t>(input);
+  filter_data = tflite_micro::micro::GetTensorData<int8_t>(filter);
+  bias_data = tflite_micro::micro::GetTensorData<int32_t>(bias);
+  output_data = tflite_micro::micro::GetTensorData<int8_t>(output);
 
-  const RuntimeShape& input_shape = tflite::micro::GetTensorShape(input);
-  const RuntimeShape& filter_shape = tflite::micro::GetTensorShape(filter);
-  const RuntimeShape& bias_shape = tflite::micro::GetTensorShape(bias);
-  const RuntimeShape& output_shape = tflite::micro::GetTensorShape(output);
+  const RuntimeShape& input_shape = tflite_micro::micro::GetTensorShape(input);
+  const RuntimeShape& filter_shape = tflite_micro::micro::GetTensorShape(filter);
+  const RuntimeShape& bias_shape = tflite_micro::micro::GetTensorShape(bias);
+  const RuntimeShape& output_shape = tflite_micro::micro::GetTensorShape(output);
 
   const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   const int input_height = input_shape.Dims(1);
@@ -204,14 +204,14 @@ TfLiteStatus EvalCEVA(TfLiteContext* context, TfLiteNode* node) {
   const OpDataConv& data = *(static_cast<const OpDataConv*>(node->user_data));
 
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kDepthwiseConvOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kDepthwiseConvOutputTensor);
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kDepthwiseConvInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kDepthwiseConvInputTensor);
   const TfLiteEvalTensor* filter =
-      tflite::micro::GetEvalInput(context, node, kDepthwiseConvWeightsTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kDepthwiseConvWeightsTensor);
   const TfLiteEvalTensor* bias =
       (NumInputs(node) == 3)
-          ? tflite::micro::GetEvalInput(context, node, kDepthwiseConvBiasTensor)
+          ? tflite_micro::micro::GetEvalInput(context, node, kDepthwiseConvBiasTensor)
           : nullptr;
 
   // TODO(aselle): Consider whether float conv and quantized conv should be
@@ -230,14 +230,14 @@ TfLiteStatus EvalCEVA(TfLiteContext* context, TfLiteNode* node) {
           reference_integer_ops::DepthwiseConvPerChannel(
               DepthwiseConvParamsQuantized(*params, data),
               data.per_channel_output_multiplier, data.per_channel_output_shift,
-              tflite::micro::GetTensorShape(input),
-              tflite::micro::GetTensorData<int16_t>(input),
-              tflite::micro::GetTensorShape(filter),
-              tflite::micro::GetTensorData<int8_t>(filter),
-              tflite::micro::GetTensorShape(bias),
-              tflite::micro::GetOptionalTensorData<int64_t>(bias),
-              tflite::micro::GetTensorShape(output),
-              tflite::micro::GetTensorData<int16_t>(output));
+              tflite_micro::micro::GetTensorShape(input),
+              tflite_micro::micro::GetTensorData<int16_t>(input),
+              tflite_micro::micro::GetTensorShape(filter),
+              tflite_micro::micro::GetTensorData<int8_t>(filter),
+              tflite_micro::micro::GetTensorShape(bias),
+              tflite_micro::micro::GetOptionalTensorData<int64_t>(bias),
+              tflite_micro::micro::GetTensorShape(output),
+              tflite_micro::micro::GetTensorData<int16_t>(output));
           break;
         }
         default:
@@ -267,8 +267,8 @@ TfLiteStatus DepthWiseConvEval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_DEPTHWISE_CONV_2D() {
-  return tflite::micro::RegisterOp(Init, DepthwiseConvPrepare,
+  return tflite_micro::micro::RegisterOp(Init, DepthwiseConvPrepare,
                                    DepthWiseConvEval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/ceva/fully_connected.cc b/tensorflow/lite/micro/kernels/ceva/fully_connected.cc
index 0e0b14d4..739d395e 100644
--- a/tensorflow/lite/micro/kernels/ceva/fully_connected.cc
+++ b/tensorflow/lite/micro/kernels/ceva/fully_connected.cc
@@ -37,7 +37,7 @@ extern int32_t* CEVA_TFLM_KERNELS_SCRATCH;
 extern int32_t CEVA_TFLM_KERNELS_SCRATCH_SIZE_VAL;
 #endif  // CEVA platform
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 void* Init(TfLiteContext* context, const char* buffer, size_t length) {
@@ -77,30 +77,30 @@ __attribute__((optnone)) TfLiteStatus EvalQuantizedInt8CEVA(
     TfLiteContext* context, TfLiteNode* node, const OpDataFullyConnected& data,
     const TfLiteEvalTensor* input, const TfLiteEvalTensor* filter,
     const TfLiteEvalTensor* bias, TfLiteEvalTensor* output) {
-  tflite::FullyConnectedParams op_params = FullyConnectedParamsQuantized(data);
+  tflite_micro::FullyConnectedParams op_params = FullyConnectedParamsQuantized(data);
 
   int input_shape_dimensions_count =
-      tflite::micro::GetTensorShape(input).DimensionsCount();
+      tflite_micro::micro::GetTensorShape(input).DimensionsCount();
   int weights_shape_dimensions_count =
-      tflite::micro::GetTensorShape(filter).DimensionsCount();
+      tflite_micro::micro::GetTensorShape(filter).DimensionsCount();
   int* weights_shape_dims_data =
-      const_cast<int*>(tflite::micro::GetTensorShape(filter).DimsData());
+      const_cast<int*>(tflite_micro::micro::GetTensorShape(filter).DimsData());
   int bias_shape_dimensions_count =
-      tflite::micro::GetTensorShape(bias).DimensionsCount();
+      tflite_micro::micro::GetTensorShape(bias).DimensionsCount();
   int output_shape_dimensions_count =
-      tflite::micro::GetTensorShape(output).DimensionsCount();
+      tflite_micro::micro::GetTensorShape(output).DimensionsCount();
   int* output_shape_dims_data =
-      const_cast<int*>(tflite::micro::GetTensorShape(output).DimsData());
+      const_cast<int*>(tflite_micro::micro::GetTensorShape(output).DimsData());
 
   void* params = (void*)&op_params;
   int8_t* inputp =
-      const_cast<int8_t*>(tflite::micro::GetTensorData<int8_t>(input));
+      const_cast<int8_t*>(tflite_micro::micro::GetTensorData<int8_t>(input));
   int8_t* filterp =
-      const_cast<int8_t*>(tflite::micro::GetTensorData<int8_t>(filter));
+      const_cast<int8_t*>(tflite_micro::micro::GetTensorData<int8_t>(filter));
   int32_t* biasp =
-      const_cast<int32_t*>(tflite::micro::GetTensorData<int32_t>(bias));
+      const_cast<int32_t*>(tflite_micro::micro::GetTensorData<int32_t>(bias));
   int8_t* outputp =
-      const_cast<int8_t*>(tflite::micro::GetTensorData<int8_t>(output));
+      const_cast<int8_t*>(tflite_micro::micro::GetTensorData<int8_t>(output));
 
 #ifdef MCPS_MEASUREMENT
   int batches = output_shape_dims_data[0];
@@ -139,7 +139,7 @@ TfLiteStatus EvalFloatCEVA(TfLiteContext* context, TfLiteNode* node,
                            const TfLiteEvalTensor* bias,
                            TfLiteEvalTensor* output) {
   // float output_activation_min, output_activation_max;
-  tflite::FullyConnectedParams op_params;
+  tflite_micro::FullyConnectedParams op_params;
   CalculateActivationRange(activation, &op_params.float_activation_min,
                            &op_params.float_activation_max);
 
@@ -147,26 +147,26 @@ TfLiteStatus EvalFloatCEVA(TfLiteContext* context, TfLiteNode* node,
   // op_params.float_activation_max = output_activation_max;
 
   int input_shape_dimensions_count =
-      tflite::micro::GetTensorShape(input).DimensionsCount();
+      tflite_micro::micro::GetTensorShape(input).DimensionsCount();
   int weights_shape_dimensions_count =
-      tflite::micro::GetTensorShape(filter).DimensionsCount();
+      tflite_micro::micro::GetTensorShape(filter).DimensionsCount();
   int* weights_shape_dims_data =
-      const_cast<int*>(tflite::micro::GetTensorShape(filter).DimsData());
+      const_cast<int*>(tflite_micro::micro::GetTensorShape(filter).DimsData());
   int bias_shape_dimensions_count =
-      tflite::micro::GetTensorShape(bias).DimensionsCount();
+      tflite_micro::micro::GetTensorShape(bias).DimensionsCount();
   int output_shape_dimensions_count =
-      tflite::micro::GetTensorShape(output).DimensionsCount();
+      tflite_micro::micro::GetTensorShape(output).DimensionsCount();
   int* output_shape_dims_data =
-      const_cast<int*>(tflite::micro::GetTensorShape(output).DimsData());
+      const_cast<int*>(tflite_micro::micro::GetTensorShape(output).DimsData());
 
   void* params = (void*)&op_params;
   float* inputp =
-      const_cast<float*>(tflite::micro::GetTensorData<float>(input));
+      const_cast<float*>(tflite_micro::micro::GetTensorData<float>(input));
   float* filterp =
-      const_cast<float*>(tflite::micro::GetTensorData<float>(filter));
-  float* biasp = const_cast<float*>(tflite::micro::GetTensorData<float>(bias));
+      const_cast<float*>(tflite_micro::micro::GetTensorData<float>(filter));
+  float* biasp = const_cast<float*>(tflite_micro::micro::GetTensorData<float>(bias));
   float* outputp =
-      const_cast<float*>(tflite::micro::GetTensorData<float>(output));
+      const_cast<float*>(tflite_micro::micro::GetTensorData<float>(output));
 
 #ifdef MCPS_MEASUREMENT
   int batches = 1;
@@ -205,13 +205,13 @@ TfLiteStatus EvalCEVA(TfLiteContext* context, TfLiteNode* node) {
       static_cast<const TfLiteFullyConnectedParams*>(node->builtin_data);
 
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kFullyConnectedInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kFullyConnectedInputTensor);
   const TfLiteEvalTensor* filter =
-      tflite::micro::GetEvalInput(context, node, kFullyConnectedWeightsTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kFullyConnectedWeightsTensor);
   const TfLiteEvalTensor* bias =
-      tflite::micro::GetEvalInput(context, node, kFullyConnectedBiasTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kFullyConnectedBiasTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kFullyConnectedOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kFullyConnectedOutputTensor);
 
   TFLITE_DCHECK(node->user_data != nullptr);
   const OpDataFullyConnected& data =
@@ -244,7 +244,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_FULLY_CONNECTED() {
-  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/ceva/logistic.cc b/tensorflow/lite/micro/kernels/ceva/logistic.cc
index 00e438fb..11e6e2b8 100644
--- a/tensorflow/lite/micro/kernels/ceva/logistic.cc
+++ b/tensorflow/lite/micro/kernels/ceva/logistic.cc
@@ -33,7 +33,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/ceva/mcps_macros.h"
 #endif
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 void* LogisticInit(TfLiteContext* context, const char* buffer, size_t length) {
@@ -43,9 +43,9 @@ void* LogisticInit(TfLiteContext* context, const char* buffer, size_t length) {
 
 TfLiteStatus LogisticEval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kLogisticInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kLogisticInputTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kLogisticOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kLogisticOutputTensor);
 
   TFLITE_DCHECK(node->user_data != nullptr);
   OpDataLogistic* data = static_cast<OpDataLogistic*>(node->user_data);
@@ -55,11 +55,11 @@ TfLiteStatus LogisticEval(TfLiteContext* context, TfLiteNode* node) {
       case kTfLiteFloat32: {
 #if defined(CEVA_BX1) || defined(CEVA_SP500)
 
-        const float* input_data = tflite::micro::GetTensorData<float>(input);
-        float* output_data = tflite::micro::GetTensorData<float>(output);
+        const float* input_data = tflite_micro::micro::GetTensorData<float>(input);
+        float* output_data = tflite_micro::micro::GetTensorData<float>(output);
         const int flat_size =
-            MatchingFlatSize(tflite::micro::GetTensorShape(input),
-                             tflite::micro::GetTensorShape(output));
+            MatchingFlatSize(tflite_micro::micro::GetTensorShape(input),
+                             tflite_micro::micro::GetTensorShape(output));
 #ifdef MCPS_MEASUREMENT
         MCPS_START_ONE;
 #endif
@@ -70,10 +70,10 @@ TfLiteStatus LogisticEval(TfLiteContext* context, TfLiteNode* node) {
 #endif
 
 #else
-        reference_ops::Logistic(tflite::micro::GetTensorShape(input),
-                                tflite::micro::GetTensorData<float>(input),
-                                tflite::micro::GetTensorShape(output),
-                                tflite::micro::GetTensorData<float>(output));
+        reference_ops::Logistic(tflite_micro::micro::GetTensorShape(input),
+                                tflite_micro::micro::GetTensorData<float>(input),
+                                tflite_micro::micro::GetTensorShape(output),
+                                tflite_micro::micro::GetTensorData<float>(output));
 #endif  // ceva platform
         return kTfLiteOk;
       }
@@ -92,8 +92,8 @@ TfLiteStatus LogisticEval(TfLiteContext* context, TfLiteNode* node) {
         int32_t input_multiplier = data->input_multiplier;
         int32_t input_left_shift = data->input_left_shift;
         int32_t input_size = NumElements(input->dims);
-        const int8_t* input_data = tflite::micro::GetTensorData<int8_t>(input);
-        int8_t* output_data = tflite::micro::GetTensorData<int8_t>(output);
+        const int8_t* input_data = tflite_micro::micro::GetTensorData<int8_t>(input);
+        int8_t* output_data = tflite_micro::micro::GetTensorData<int8_t>(output);
 
 #ifdef MCPS_MEASUREMENT
         MCPS_START_ONE;
@@ -110,8 +110,8 @@ TfLiteStatus LogisticEval(TfLiteContext* context, TfLiteNode* node) {
             data->input_zero_point, data->input_range_radius,
             data->input_multiplier, data->input_left_shift,
             NumElements(input->dims),
-            tflite::micro::GetTensorData<int8_t>(input),
-            tflite::micro::GetTensorData<int8_t>(output));
+            tflite_micro::micro::GetTensorData<int8_t>(input),
+            tflite_micro::micro::GetTensorData<int8_t>(output));
 #endif  // ceva platform
         return kTfLiteOk;
       }
@@ -133,6 +133,6 @@ TfLiteStatus LogisticEval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_LOGISTIC() {
-  return tflite::micro::RegisterOp(LogisticInit, LogisticPrepare, LogisticEval);
+  return tflite_micro::micro::RegisterOp(LogisticInit, LogisticPrepare, LogisticEval);
 }
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/ceva/quantize.cc b/tensorflow/lite/micro/kernels/ceva/quantize.cc
index 585f968b..43860664 100644
--- a/tensorflow/lite/micro/kernels/ceva/quantize.cc
+++ b/tensorflow/lite/micro/kernels/ceva/quantize.cc
@@ -27,7 +27,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/ceva/mcps_macros.h "
 #endif
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 void* Init(TfLiteContext* context, const char* buffer, size_t length) {
@@ -41,15 +41,15 @@ TfLiteStatus EvalCEVA(TfLiteContext* context, TfLiteNode* node) {
 
   auto* data = static_cast<OpDataQuantizeReference*>(node->user_data);
 
-  const TfLiteEvalTensor* input = tflite::micro::GetEvalInput(context, node, 0);
-  TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, 0);
+  const TfLiteEvalTensor* input = tflite_micro::micro::GetEvalInput(context, node, 0);
+  TfLiteEvalTensor* output = tflite_micro::micro::GetEvalOutput(context, node, 0);
 
   if (input->type == kTfLiteFloat32 && output->type == kTfLiteInt8) {
-    const float* input_data = tflite::micro::GetTensorData<float>(input);
-    int8_t* output_data = tflite::micro::GetTensorData<int8_t>(output);
+    const float* input_data = tflite_micro::micro::GetTensorData<float>(input);
+    int8_t* output_data = tflite_micro::micro::GetTensorData<int8_t>(output);
     const int flat_size =
-        MatchingFlatSize(tflite::micro::GetTensorShape(input),
-                         tflite::micro::GetTensorShape(output));
+        MatchingFlatSize(tflite_micro::micro::GetTensorShape(input),
+                         tflite_micro::micro::GetTensorShape(output));
 
 #ifdef MCPS_MEASUREMENT
     MCPS_START_ONE;
@@ -80,7 +80,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 // AffineQuantize takes scale and zero point and quantizes the float value to
 // quantized output, in int8_t or uint8_t format.
 TFLMRegistration Register_QUANTIZE() {
-  return tflite::micro::RegisterOp(Init, PrepareQuantizeReference, Eval);
+  return tflite_micro::micro::RegisterOp(Init, PrepareQuantizeReference, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/ceva/softmax.cc b/tensorflow/lite/micro/kernels/ceva/softmax.cc
index beb43cd4..96c57d30 100644
--- a/tensorflow/lite/micro/kernels/ceva/softmax.cc
+++ b/tensorflow/lite/micro/kernels/ceva/softmax.cc
@@ -32,16 +32,16 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/ceva/mcps_macros.h"
 #endif
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 // Takes a tensor and performs softmax along the last dimension.
 void SoftmaxFloatCEVA(const TfLiteEvalTensor* input, TfLiteEvalTensor* output,
                       const SoftmaxParams& op_data) {
-  const RuntimeShape& input_shape = tflite::micro::GetTensorShape(input);
-  const float* input_data = tflite::micro::GetTensorData<float>(input);
-  const RuntimeShape& output_shape = tflite::micro::GetTensorShape(output);
-  float* output_data = tflite::micro::GetTensorData<float>(output);
+  const RuntimeShape& input_shape = tflite_micro::micro::GetTensorShape(input);
+  const float* input_data = tflite_micro::micro::GetTensorData<float>(input);
+  const RuntimeShape& output_shape = tflite_micro::micro::GetTensorShape(output);
+  float* output_data = tflite_micro::micro::GetTensorData<float>(output);
 
   const float beta = static_cast<float>(op_data.beta);
   const int trailing_dim = input_shape.DimensionsCount() - 1;
@@ -72,17 +72,17 @@ TfLiteStatus SoftmaxQuantizedCEVA(TfLiteContext* context,
                                   const SoftmaxParams& op_data) {
   if (input->type == kTfLiteInt8) {
     if (output->type == kTfLiteInt16) {
-      tflite::reference_ops::Softmax(
-          op_data, tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<int8_t>(input),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<int16_t>(output));
+      tflite_micro::reference_ops::Softmax(
+          op_data, tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<int8_t>(input),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<int16_t>(output));
     } else {
-      const RuntimeShape& input_shape = tflite::micro::GetTensorShape(input);
-      const int8_t* input_data = tflite::micro::GetTensorData<int8_t>(input);
+      const RuntimeShape& input_shape = tflite_micro::micro::GetTensorShape(input);
+      const int8_t* input_data = tflite_micro::micro::GetTensorData<int8_t>(input);
 
-      const RuntimeShape& output_shape = tflite::micro::GetTensorShape(output);
-      int8_t* output_data = tflite::micro::GetTensorData<int8_t>(output);
+      const RuntimeShape& output_shape = tflite_micro::micro::GetTensorShape(output);
+      int8_t* output_data = tflite_micro::micro::GetTensorData<int8_t>(output);
 
       const int32_t input_beta_multiplier =
           static_cast<int32_t>(op_data.input_multiplier);
@@ -118,19 +118,19 @@ TfLiteStatus SoftmaxQuantizedCEVA(TfLiteContext* context,
 #endif
     }
   } else {
-    tflite::reference_ops::SoftmaxInt16(
-        op_data, tflite::micro::GetTensorShape(input),
-        tflite::micro::GetTensorData<int16_t>(input),
-        tflite::micro::GetTensorShape(output),
-        tflite::micro::GetTensorData<int16_t>(output));
+    tflite_micro::reference_ops::SoftmaxInt16(
+        op_data, tflite_micro::micro::GetTensorShape(input),
+        tflite_micro::micro::GetTensorData<int16_t>(input),
+        tflite_micro::micro::GetTensorShape(output),
+        tflite_micro::micro::GetTensorData<int16_t>(output));
   }
 
   return kTfLiteOk;
 }
 
 TfLiteStatus SoftmaxEvalCEVA(TfLiteContext* context, TfLiteNode* node) {
-  const TfLiteEvalTensor* input = tflite::micro::GetEvalInput(context, node, 0);
-  TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, 0);
+  const TfLiteEvalTensor* input = tflite_micro::micro::GetEvalInput(context, node, 0);
+  TfLiteEvalTensor* output = tflite_micro::micro::GetEvalOutput(context, node, 0);
 
   TFLITE_DCHECK(node->user_data != nullptr);
   SoftmaxParams op_data = *static_cast<SoftmaxParams*>(node->user_data);
@@ -161,7 +161,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_SOFTMAX() {
-  return tflite::micro::RegisterOp(SoftmaxInit, SoftmaxPrepare, Eval);
+  return tflite_micro::micro::RegisterOp(SoftmaxInit, SoftmaxPrepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/ceva/types.h b/tensorflow/lite/micro/kernels/ceva/types.h
index 048a1160..9b766c0d 100644
--- a/tensorflow/lite/micro/kernels/ceva/types.h
+++ b/tensorflow/lite/micro/kernels/ceva/types.h
@@ -209,7 +209,7 @@ inline bool NextIndex(const int num_dims, const int* dims, int* current) {
 
 #include "tensorflow/lite/kernels/internal/compatibility.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 enum class FusedActivationFunctionType : uint8_t { kNone, kRelu6, kRelu1, kRelu };
 enum class PaddingType : uint8_t { kNone, kSame, kValid };
@@ -485,9 +485,9 @@ class RuntimeShape {
   };
 };
 
-// Converts inference-style shape to legacy tflite::Dims<4>.
-inline tflite::Dims<4> ToRuntimeDims(const tflite::RuntimeShape& array_shape) {
-  tflite::Dims<4> result;
+// Converts inference-style shape to legacy tflite_micro::Dims<4>.
+inline tflite_micro::Dims<4> ToRuntimeDims(const tflite_micro::RuntimeShape& array_shape) {
+  tflite_micro::Dims<4> result;
   const int dimensions_count = array_shape.DimensionsCount();
   TFLITE_CHECK_LE(dimensions_count, 4);
   int cum_prod = 1;
@@ -502,7 +502,7 @@ inline tflite::Dims<4> ToRuntimeDims(const tflite::RuntimeShape& array_shape) {
 }
 
 // TODO(b/80418076): Move to legacy ops file, update invocations.
-inline RuntimeShape DimsToShape(const tflite::Dims<4>& dims) {
+inline RuntimeShape DimsToShape(const tflite_micro::Dims<4>& dims) {
   return RuntimeShape(
       {dims.sizes[3], dims.sizes[2], dims.sizes[1], dims.sizes[0]});
 }
@@ -1281,6 +1281,6 @@ inline void GetActivationParams(const P& params, float* min, float* max) {
   *max = params.float_activation_max;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
 #endif
 #endif  // CEVA_TYPES_H_
diff --git a/tensorflow/lite/micro/kernels/circular_buffer.cc b/tensorflow/lite/micro/kernels/circular_buffer.cc
index 3e901047..4f385a85 100644
--- a/tensorflow/lite/micro/kernels/circular_buffer.cc
+++ b/tensorflow/lite/micro/kernels/circular_buffer.cc
@@ -47,7 +47,7 @@ limitations under the License.
  * - Input and output types must match.
  * - Input and output quantization params must be identical.
  */
-namespace tflite {
+namespace tflite_micro {
 
 void* CircularBufferInit(TfLiteContext* context, const char* buffer,
                          size_t length) {
@@ -57,7 +57,7 @@ void* CircularBufferInit(TfLiteContext* context, const char* buffer,
 
   if (buffer != nullptr && length > 0) {
     const uint8_t* buffer_t = reinterpret_cast<const uint8_t*>(buffer);
-    tflite::FlexbufferWrapper wrapper(buffer_t, length);
+    tflite_micro::FlexbufferWrapper wrapper(buffer_t, length);
     op_data->cycles_max = wrapper.ElementAsInt32(kCircularBufferCyclesMaxIndex);
   } else {
     op_data->cycles_max = 0;
@@ -76,9 +76,9 @@ void EvalInt8(const int8_t* input, int num_slots, int depth, int8_t* output) {
 
 TfLiteStatus CircularBufferEval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kCircularBufferInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kCircularBufferInputTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kCircularBufferOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kCircularBufferOutputTensor);
 
   TFLITE_DCHECK(node->user_data != nullptr);
   OpDataCircularBuffer* data =
@@ -88,8 +88,8 @@ TfLiteStatus CircularBufferEval(TfLiteContext* context, TfLiteNode* node) {
   int depth = output->dims->data[2] * output->dims->data[3];
 
   if (input->type == kTfLiteInt8) {
-    EvalInt8(tflite::micro::GetTensorData<int8_t>(input), num_slots, depth,
-             tflite::micro::GetTensorData<int8_t>(output));
+    EvalInt8(tflite_micro::micro::GetTensorData<int8_t>(input), num_slots, depth,
+             tflite_micro::micro::GetTensorData<int8_t>(output));
   } else {
     MicroPrintf("Type %s (%d) not supported.",
                        TfLiteTypeGetName(input->type), input->type);
@@ -108,9 +108,9 @@ TfLiteStatus CircularBufferEval(TfLiteContext* context, TfLiteNode* node) {
 }
 
 TFLMRegistration* Register_CIRCULAR_BUFFER() {
-  static TFLMRegistration r = tflite::micro::RegisterOp(
+  static TFLMRegistration r = tflite_micro::micro::RegisterOp(
       CircularBufferInit, CircularBufferPrepare, CircularBufferEval);
   return &r;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/circular_buffer.h b/tensorflow/lite/micro/kernels/circular_buffer.h
index c0a55b84..dd3ffcb2 100644
--- a/tensorflow/lite/micro/kernels/circular_buffer.h
+++ b/tensorflow/lite/micro/kernels/circular_buffer.h
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/c/builtin_op_data.h"
 #include "tensorflow/lite/c/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // The CircularBuffer op has one input and one output tensor.
 extern const int kCircularBufferInputTensor;
@@ -40,6 +40,6 @@ struct OpDataCircularBuffer {
 
 TfLiteStatus CircularBufferPrepare(TfLiteContext* context, TfLiteNode* node);
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_CIRCULAR_BUFFER_H_
diff --git a/tensorflow/lite/micro/kernels/circular_buffer_common.cc b/tensorflow/lite/micro/kernels/circular_buffer_common.cc
index bf45c06f..94b05e37 100644
--- a/tensorflow/lite/micro/kernels/circular_buffer_common.cc
+++ b/tensorflow/lite/micro/kernels/circular_buffer_common.cc
@@ -24,7 +24,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/circular_buffer.h"
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // The CircularBuffer op has one input and one output tensor.
 const int kCircularBufferInputTensor = 0;
@@ -91,4 +91,4 @@ TfLiteStatus CircularBufferPrepare(TfLiteContext* context, TfLiteNode* node) {
   return kTfLiteOk;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/circular_buffer_test.cc b/tensorflow/lite/micro/kernels/circular_buffer_test.cc
index 8fde7c51..7f1b64eb 100644
--- a/tensorflow/lite/micro/kernels/circular_buffer_test.cc
+++ b/tensorflow/lite/micro/kernels/circular_buffer_test.cc
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -29,7 +29,7 @@ constexpr int kRunPeriod = 2;
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -47,33 +47,33 @@ TF_LITE_MICRO_TEST(OutputTensorLength4) {
   int output_dims[] = {4, 1, num_slots, 1, depth};
 
   TfLiteIntArray* input_tensor_dims =
-      tflite::testing::IntArrayFromInts(input_dims);
+      tflite_micro::testing::IntArrayFromInts(input_dims);
   TfLiteIntArray* output_tensor_dims =
-      tflite::testing::IntArrayFromInts(output_dims);
+      tflite_micro::testing::IntArrayFromInts(output_dims);
 
-  const int output_dims_count = tflite::ElementCount(*output_tensor_dims);
+  const int output_dims_count = tflite_micro::ElementCount(*output_tensor_dims);
 
   constexpr int inputs_size = 2;
   constexpr int outputs_size = 1;
   constexpr int tensors_size = inputs_size + outputs_size;
   TfLiteTensor tensors[tensors_size] = {
-      tflite::testing::CreateQuantizedTensor(input_data, input_tensor_dims, 1,
+      tflite_micro::testing::CreateQuantizedTensor(input_data, input_tensor_dims, 1,
                                              0),
-      tflite::testing::CreateQuantizedTensor(output_data, output_tensor_dims, 1,
+      tflite_micro::testing::CreateQuantizedTensor(output_data, output_tensor_dims, 1,
                                              0),
   };
 
   // There is one input - tensor 0.
   int inputs_array_data[] = {1, 0};
   TfLiteIntArray* inputs_array =
-      tflite::testing::IntArrayFromInts(inputs_array_data);
+      tflite_micro::testing::IntArrayFromInts(inputs_array_data);
   // There is one output - tensor 1.
   int outputs_array_data[] = {1, 1};
   TfLiteIntArray* outputs_array =
-      tflite::testing::IntArrayFromInts(outputs_array_data);
+      tflite_micro::testing::IntArrayFromInts(outputs_array_data);
 
-  const TFLMRegistration* registration = tflite::Register_CIRCULAR_BUFFER();
-  tflite::micro::KernelRunner runner = tflite::micro::KernelRunner(
+  const TFLMRegistration* registration = tflite_micro::Register_CIRCULAR_BUFFER();
+  tflite_micro::micro::KernelRunner runner = tflite_micro::micro::KernelRunner(
       *registration, tensors, tensors_size, inputs_array, outputs_array,
       /*builtin_data=*/nullptr);
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, runner.InitAndPrepare());
@@ -96,10 +96,10 @@ TF_LITE_MICRO_TEST(OutputTensorLength4) {
     }
 
     // Every kRunPeriod iterations, the circular buffer should return kTfLiteOk.
-    if (i % tflite::testing::kRunPeriod == tflite::testing::kRunPeriod - 1) {
+    if (i % tflite_micro::testing::kRunPeriod == tflite_micro::testing::kRunPeriod - 1) {
       TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, status);
     } else {
-      TF_LITE_MICRO_EXPECT_EQ(tflite::kTfLiteAbort, status);
+      TF_LITE_MICRO_EXPECT_EQ(tflite_micro::kTfLiteAbort, status);
     }
   }
 }
@@ -118,33 +118,33 @@ TF_LITE_MICRO_TEST(OutputTensorOnEveryIterationLength4) {
   int output_dims[] = {4, 1, num_slots, 1, depth};
 
   TfLiteIntArray* input_tensor_dims =
-      tflite::testing::IntArrayFromInts(input_dims);
+      tflite_micro::testing::IntArrayFromInts(input_dims);
   TfLiteIntArray* output_tensor_dims =
-      tflite::testing::IntArrayFromInts(output_dims);
+      tflite_micro::testing::IntArrayFromInts(output_dims);
 
-  const int output_dims_count = tflite::ElementCount(*output_tensor_dims);
+  const int output_dims_count = tflite_micro::ElementCount(*output_tensor_dims);
 
   constexpr int inputs_size = 2;
   constexpr int outputs_size = 1;
   constexpr int tensors_size = inputs_size + outputs_size;
   TfLiteTensor tensors[tensors_size] = {
-      tflite::testing::CreateQuantizedTensor(input_data, input_tensor_dims, 1,
+      tflite_micro::testing::CreateQuantizedTensor(input_data, input_tensor_dims, 1,
                                              0),
-      tflite::testing::CreateQuantizedTensor(output_data, output_tensor_dims, 1,
+      tflite_micro::testing::CreateQuantizedTensor(output_data, output_tensor_dims, 1,
                                              0),
   };
 
   // There is one input - tensor 0.
   int inputs_array_data[] = {1, 0};
   TfLiteIntArray* inputs_array =
-      tflite::testing::IntArrayFromInts(inputs_array_data);
+      tflite_micro::testing::IntArrayFromInts(inputs_array_data);
   // There is one output - tensor 1.
   int outputs_array_data[] = {1, 1};
   TfLiteIntArray* outputs_array =
-      tflite::testing::IntArrayFromInts(outputs_array_data);
+      tflite_micro::testing::IntArrayFromInts(outputs_array_data);
 
-  const TFLMRegistration* registration = tflite::Register_CIRCULAR_BUFFER();
-  tflite::micro::KernelRunner runner = tflite::micro::KernelRunner(
+  const TFLMRegistration* registration = tflite_micro::Register_CIRCULAR_BUFFER();
+  tflite_micro::micro::KernelRunner runner = tflite_micro::micro::KernelRunner(
       *registration, tensors, tensors_size, inputs_array, outputs_array,
       /*builtin_data=*/nullptr);
 
@@ -183,33 +183,33 @@ TF_LITE_MICRO_TEST(OutputTensorLength5) {
   int input_dims[] = {4, 1, 1, 1, depth};
   int output_dims[] = {4, 1, num_slots, 1, depth};
   TfLiteIntArray* input_tensor_dims =
-      tflite::testing::IntArrayFromInts(input_dims);
+      tflite_micro::testing::IntArrayFromInts(input_dims);
   TfLiteIntArray* output_tensor_dims =
-      tflite::testing::IntArrayFromInts(output_dims);
+      tflite_micro::testing::IntArrayFromInts(output_dims);
 
-  const int output_dims_count = tflite::ElementCount(*output_tensor_dims);
+  const int output_dims_count = tflite_micro::ElementCount(*output_tensor_dims);
 
   constexpr int inputs_size = 2;
   constexpr int outputs_size = 1;
   constexpr int tensors_size = inputs_size + outputs_size;
   TfLiteTensor tensors[tensors_size] = {
-      tflite::testing::CreateQuantizedTensor(input_data, input_tensor_dims, 1,
+      tflite_micro::testing::CreateQuantizedTensor(input_data, input_tensor_dims, 1,
                                              0),
-      tflite::testing::CreateQuantizedTensor(output_data, output_tensor_dims, 1,
+      tflite_micro::testing::CreateQuantizedTensor(output_data, output_tensor_dims, 1,
                                              0),
   };
 
   // There is one input - tensor 0.
   int inputs_array_data[] = {1, 0};
   TfLiteIntArray* inputs_array =
-      tflite::testing::IntArrayFromInts(inputs_array_data);
+      tflite_micro::testing::IntArrayFromInts(inputs_array_data);
   // There is one output - tensor 1.
   int outputs_array_data[] = {1, 1};
   TfLiteIntArray* outputs_array =
-      tflite::testing::IntArrayFromInts(outputs_array_data);
+      tflite_micro::testing::IntArrayFromInts(outputs_array_data);
 
-  const TFLMRegistration* registration = tflite::Register_CIRCULAR_BUFFER();
-  tflite::micro::KernelRunner runner = tflite::micro::KernelRunner(
+  const TFLMRegistration* registration = tflite_micro::Register_CIRCULAR_BUFFER();
+  tflite_micro::micro::KernelRunner runner = tflite_micro::micro::KernelRunner(
       *registration, tensors, tensors_size, inputs_array, outputs_array,
       /*builtin_data=*/nullptr);
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, runner.InitAndPrepare());
diff --git a/tensorflow/lite/micro/kernels/cmsis_nn/add.cc b/tensorflow/lite/micro/kernels/cmsis_nn/add.cc
index 898410a3..3a499c06 100644
--- a/tensorflow/lite/micro/kernels/cmsis_nn/add.cc
+++ b/tensorflow/lite/micro/kernels/cmsis_nn/add.cc
@@ -27,7 +27,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/memory_helpers.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 constexpr int kInputTensor1 = 0;
@@ -105,7 +105,7 @@ TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteAddParams* params,
   return kTfLiteOk;
 }
 
-void UpdateOpParams(tflite::ArithmeticParams* const op_params,
+void UpdateOpParams(tflite_micro::ArithmeticParams* const op_params,
                     const OpData* data) {
   op_params->left_shift = data->left_shift;
   op_params->input1_offset = data->input1_offset;
@@ -126,35 +126,35 @@ TfLiteStatus EvalAddQuantizedInt8(TfLiteContext* context, TfLiteNode* node,
                                   const TfLiteEvalTensor* input1,
                                   const TfLiteEvalTensor* input2,
                                   TfLiteEvalTensor* output) {
-  tflite::ArithmeticParams op_params;
+  tflite_micro::ArithmeticParams op_params;
   UpdateOpParams(&op_params, data);
 
   bool need_broadcast = reference_ops::ProcessBroadcastShapes(
-      tflite::micro::GetTensorShape(input1),
-      tflite::micro::GetTensorShape(input2), &op_params);
+      tflite_micro::micro::GetTensorShape(input1),
+      tflite_micro::micro::GetTensorShape(input2), &op_params);
 
   if (need_broadcast) {
     reference_integer_ops::BroadcastAdd4DSlow(
-        op_params, tflite::micro::GetTensorShape(input1),
-        tflite::micro::GetTensorData<int8_t>(input1),
-        tflite::micro::GetTensorShape(input2),
-        tflite::micro::GetTensorData<int8_t>(input2),
-        tflite::micro::GetTensorShape(output),
-        tflite::micro::GetTensorData<int8_t>(output));
+        op_params, tflite_micro::micro::GetTensorShape(input1),
+        tflite_micro::micro::GetTensorData<int8_t>(input1),
+        tflite_micro::micro::GetTensorShape(input2),
+        tflite_micro::micro::GetTensorData<int8_t>(input2),
+        tflite_micro::micro::GetTensorShape(output),
+        tflite_micro::micro::GetTensorData<int8_t>(output));
   } else {
     arm_elementwise_add_s8(
-        tflite::micro::GetTensorData<int8_t>(input1),
+        tflite_micro::micro::GetTensorData<int8_t>(input1),
 
-        tflite::micro::GetTensorData<int8_t>(input2), op_params.input1_offset,
+        tflite_micro::micro::GetTensorData<int8_t>(input2), op_params.input1_offset,
         op_params.input1_multiplier, op_params.input1_shift,
         op_params.input2_offset, op_params.input2_multiplier,
         op_params.input2_shift, op_params.left_shift,
-        tflite::micro::GetTensorData<int8_t>(output), op_params.output_offset,
+        tflite_micro::micro::GetTensorData<int8_t>(output), op_params.output_offset,
         op_params.output_multiplier, op_params.output_shift,
         op_params.quantized_activation_min, op_params.quantized_activation_max,
-        MatchingElementsSize(tflite::micro::GetTensorShape(input1),
-                             tflite::micro::GetTensorShape(input2),
-                             tflite::micro::GetTensorShape(output)));
+        MatchingElementsSize(tflite_micro::micro::GetTensorShape(input1),
+                             tflite_micro::micro::GetTensorShape(input2),
+                             tflite_micro::micro::GetTensorShape(output)));
   }
 
   return kTfLiteOk;
@@ -165,34 +165,34 @@ TfLiteStatus EvalAddQuantizedInt16(TfLiteContext* context, TfLiteNode* node,
                                    const TfLiteEvalTensor* input1,
                                    const TfLiteEvalTensor* input2,
                                    TfLiteEvalTensor* output) {
-  tflite::ArithmeticParams op_params;
+  tflite_micro::ArithmeticParams op_params;
   UpdateOpParams(&op_params, data);
 
   bool need_broadcast = reference_ops::ProcessBroadcastShapes(
-      tflite::micro::GetTensorShape(input1),
-      tflite::micro::GetTensorShape(input2), &op_params);
+      tflite_micro::micro::GetTensorShape(input1),
+      tflite_micro::micro::GetTensorShape(input2), &op_params);
 
   if (need_broadcast) {
     reference_ops::BroadcastAdd4DSlow(
-        op_params, tflite::micro::GetTensorShape(input1),
-        tflite::micro::GetTensorData<int16_t>(input1),
-        tflite::micro::GetTensorShape(input2),
-        tflite::micro::GetTensorData<int16_t>(input2),
-        tflite::micro::GetTensorShape(output),
-        tflite::micro::GetTensorData<int16_t>(output));
+        op_params, tflite_micro::micro::GetTensorShape(input1),
+        tflite_micro::micro::GetTensorData<int16_t>(input1),
+        tflite_micro::micro::GetTensorShape(input2),
+        tflite_micro::micro::GetTensorData<int16_t>(input2),
+        tflite_micro::micro::GetTensorShape(output),
+        tflite_micro::micro::GetTensorData<int16_t>(output));
   } else {
     arm_elementwise_add_s16(
-        tflite::micro::GetTensorData<int16_t>(input1),
-        tflite::micro::GetTensorData<int16_t>(input2), op_params.input1_offset,
+        tflite_micro::micro::GetTensorData<int16_t>(input1),
+        tflite_micro::micro::GetTensorData<int16_t>(input2), op_params.input1_offset,
         op_params.input1_multiplier, op_params.input1_shift,
         op_params.input2_offset, op_params.input2_multiplier,
         op_params.input2_shift, op_params.left_shift,
-        tflite::micro::GetTensorData<int16_t>(output), op_params.output_offset,
+        tflite_micro::micro::GetTensorData<int16_t>(output), op_params.output_offset,
         op_params.output_multiplier, op_params.output_shift,
         op_params.quantized_activation_min, op_params.quantized_activation_max,
-        MatchingElementsSize(tflite::micro::GetTensorShape(input1),
-                             tflite::micro::GetTensorShape(input2),
-                             tflite::micro::GetTensorShape(output)));
+        MatchingElementsSize(tflite_micro::micro::GetTensorShape(input1),
+                             tflite_micro::micro::GetTensorShape(input2),
+                             tflite_micro::micro::GetTensorShape(output)));
   }
 
   return kTfLiteOk;
@@ -204,45 +204,45 @@ TfLiteStatus EvalAdd(TfLiteContext* context, TfLiteNode* node,
                      const TfLiteEvalTensor* input2, TfLiteEvalTensor* output) {
   switch (output->type) {
     case kTfLiteFloat32: {
-      tflite::ArithmeticParams op_params;
+      tflite_micro::ArithmeticParams op_params;
       SetActivationParams(data->output_activation_min_f32,
                           data->output_activation_max_f32, &op_params);
       if (data->requires_broadcast) {
         reference_ops::BroadcastAdd4DSlow(
-            op_params, tflite::micro::GetTensorShape(input1),
-            tflite::micro::GetTensorData<float>(input1),
-            tflite::micro::GetTensorShape(input2),
-            tflite::micro::GetTensorData<float>(input2),
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<float>(output));
+            op_params, tflite_micro::micro::GetTensorShape(input1),
+            tflite_micro::micro::GetTensorData<float>(input1),
+            tflite_micro::micro::GetTensorShape(input2),
+            tflite_micro::micro::GetTensorData<float>(input2),
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<float>(output));
       } else {
-        reference_ops::Add(op_params, tflite::micro::GetTensorShape(input1),
-                           tflite::micro::GetTensorData<float>(input1),
-                           tflite::micro::GetTensorShape(input2),
-                           tflite::micro::GetTensorData<float>(input2),
-                           tflite::micro::GetTensorShape(output),
-                           tflite::micro::GetTensorData<float>(output));
+        reference_ops::Add(op_params, tflite_micro::micro::GetTensorShape(input1),
+                           tflite_micro::micro::GetTensorData<float>(input1),
+                           tflite_micro::micro::GetTensorShape(input2),
+                           tflite_micro::micro::GetTensorData<float>(input2),
+                           tflite_micro::micro::GetTensorShape(output),
+                           tflite_micro::micro::GetTensorData<float>(output));
       }
     } break;
     case kTfLiteInt32: {
-      tflite::ArithmeticParams op_params;
+      tflite_micro::ArithmeticParams op_params;
       SetActivationParams(std::numeric_limits<int32_t>::lowest(),
                           std::numeric_limits<int32_t>::max(), &op_params);
       if (data->requires_broadcast) {
         reference_ops::BroadcastAdd4DSlow(
-            op_params, tflite::micro::GetTensorShape(input1),
-            tflite::micro::GetTensorData<int32_t>(input1),
-            tflite::micro::GetTensorShape(input2),
-            tflite::micro::GetTensorData<int32_t>(input2),
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<int32_t>(output));
+            op_params, tflite_micro::micro::GetTensorShape(input1),
+            tflite_micro::micro::GetTensorData<int32_t>(input1),
+            tflite_micro::micro::GetTensorShape(input2),
+            tflite_micro::micro::GetTensorData<int32_t>(input2),
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int32_t>(output));
       } else {
-        reference_ops::Add(op_params, tflite::micro::GetTensorShape(input1),
-                           tflite::micro::GetTensorData<int32_t>(input1),
-                           tflite::micro::GetTensorShape(input2),
-                           tflite::micro::GetTensorData<int32_t>(input2),
-                           tflite::micro::GetTensorShape(output),
-                           tflite::micro::GetTensorData<int32_t>(output));
+        reference_ops::Add(op_params, tflite_micro::micro::GetTensorShape(input1),
+                           tflite_micro::micro::GetTensorData<int32_t>(input1),
+                           tflite_micro::micro::GetTensorShape(input2),
+                           tflite_micro::micro::GetTensorData<int32_t>(input2),
+                           tflite_micro::micro::GetTensorShape(output),
+                           tflite_micro::micro::GetTensorData<int32_t>(output));
       }
     } break;
     default:
@@ -332,11 +332,11 @@ TfLiteStatus EvalAdd(TfLiteContext* context, TfLiteNode* node) {
   auto* params = reinterpret_cast<TfLiteAddParams*>(node->builtin_data);
 
   const TfLiteEvalTensor* input1 =
-      tflite::micro::GetEvalInput(context, node, kInputTensor1);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor1);
   const TfLiteEvalTensor* input2 =
-      tflite::micro::GetEvalInput(context, node, kInputTensor2);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor2);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
   TFLITE_DCHECK(node->user_data != nullptr);
   const OpData* data = static_cast<const OpData*>(node->user_data);
@@ -360,11 +360,11 @@ TfLiteStatus EvalAddInt8(TfLiteContext* context, TfLiteNode* node) {
   auto* params = reinterpret_cast<TfLiteAddParams*>(node->builtin_data);
 
   const TfLiteEvalTensor* input1 =
-      tflite::micro::GetEvalInput(context, node, kInputTensor1);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor1);
   const TfLiteEvalTensor* input2 =
-      tflite::micro::GetEvalInput(context, node, kInputTensor2);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor2);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
   TFLITE_DCHECK(node->user_data != nullptr);
   TFLITE_DCHECK(output->type == kTfLiteInt8);
@@ -380,11 +380,11 @@ TfLiteStatus EvalAddInt16(TfLiteContext* context, TfLiteNode* node) {
   auto* params = reinterpret_cast<TfLiteAddParams*>(node->builtin_data);
 
   const TfLiteEvalTensor* input1 =
-      tflite::micro::GetEvalInput(context, node, kInputTensor1);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor1);
   const TfLiteEvalTensor* input2 =
-      tflite::micro::GetEvalInput(context, node, kInputTensor2);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor2);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
   TFLITE_DCHECK(node->user_data != nullptr);
   TFLITE_DCHECK(output->type == kTfLiteInt16);
@@ -397,15 +397,15 @@ TfLiteStatus EvalAddInt16(TfLiteContext* context, TfLiteNode* node) {
 }
 
 TFLMRegistration Register_ADD() {
-  return tflite::micro::RegisterOp(InitAdd, PrepareAdd, EvalAdd);
+  return tflite_micro::micro::RegisterOp(InitAdd, PrepareAdd, EvalAdd);
 }
 
 TFLMRegistration Register_ADD_INT8() {
-  return tflite::micro::RegisterOp(InitAdd, PrepareAdd, EvalAddInt8);
+  return tflite_micro::micro::RegisterOp(InitAdd, PrepareAdd, EvalAddInt8);
 }
 
 TFLMRegistration Register_ADD_INT16() {
-  return tflite::micro::RegisterOp(InitAdd, PrepareAdd, EvalAddInt16);
+  return tflite_micro::micro::RegisterOp(InitAdd, PrepareAdd, EvalAddInt16);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/cmsis_nn/conv.cc b/tensorflow/lite/micro/kernels/cmsis_nn/conv.cc
index b387c6d7..bc4d914e 100644
--- a/tensorflow/lite/micro/kernels/cmsis_nn/conv.cc
+++ b/tensorflow/lite/micro/kernels/cmsis_nn/conv.cc
@@ -28,7 +28,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 struct OpData {
@@ -196,10 +196,10 @@ TfLiteStatus EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,
   quant_params.shift =
       const_cast<int32_t*>(data.reference_op_data.per_channel_output_shift);
 
-  RuntimeShape filter_shape = tflite::micro::GetTensorShape(filter);
-  RuntimeShape input_shape = tflite::micro::GetTensorShape(input);
-  RuntimeShape output_shape = tflite::micro::GetTensorShape(output);
-  RuntimeShape bias_shape = tflite::micro::GetTensorShape(bias);
+  RuntimeShape filter_shape = tflite_micro::micro::GetTensorShape(filter);
+  RuntimeShape input_shape = tflite_micro::micro::GetTensorShape(input);
+  RuntimeShape output_shape = tflite_micro::micro::GetTensorShape(output);
+  RuntimeShape bias_shape = tflite_micro::micro::GetTensorShape(bias);
 
   // Consistency check.
   TFLITE_DCHECK_LE(conv_params.activation.min, conv_params.activation.max);
@@ -209,7 +209,7 @@ TfLiteStatus EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,
   const int batch_size = MatchingDim(input_shape, 0, output_shape, 0);
   const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
   const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
-  if (tflite::micro::GetOptionalTensorData<int32_t>(bias)) {
+  if (tflite_micro::micro::GetOptionalTensorData<int32_t>(bias)) {
     TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
   }
 
@@ -259,10 +259,10 @@ TfLiteStatus EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,
   TFLITE_DCHECK_EQ(
       arm_convolve_wrapper_s8(
           &ctx, &conv_params, &quant_params, &input_dims,
-          tflite::micro::GetTensorData<int8_t>(input), &filter_dims,
-          tflite::micro::GetTensorData<int8_t>(filter), &bias_dims,
-          tflite::micro::GetOptionalTensorData<int32_t>(bias), &output_dims,
-          tflite::micro::GetTensorData<int8_t>(output)),
+          tflite_micro::micro::GetTensorData<int8_t>(input), &filter_dims,
+          tflite_micro::micro::GetTensorData<int8_t>(filter), &bias_dims,
+          tflite_micro::micro::GetOptionalTensorData<int32_t>(bias), &output_dims,
+          tflite_micro::micro::GetTensorData<int8_t>(output)),
       ARM_CMSIS_NN_SUCCESS);
 
   return kTfLiteOk;
@@ -294,10 +294,10 @@ TfLiteStatus EvalQuantizedPerChannel16x8(
   quant_params.shift =
       const_cast<int32_t*>(data.reference_op_data.per_channel_output_shift);
 
-  RuntimeShape filter_shape = tflite::micro::GetTensorShape(filter);
-  RuntimeShape input_shape = tflite::micro::GetTensorShape(input);
-  RuntimeShape output_shape = tflite::micro::GetTensorShape(output);
-  RuntimeShape bias_shape = tflite::micro::GetTensorShape(bias);
+  RuntimeShape filter_shape = tflite_micro::micro::GetTensorShape(filter);
+  RuntimeShape input_shape = tflite_micro::micro::GetTensorShape(input);
+  RuntimeShape output_shape = tflite_micro::micro::GetTensorShape(output);
+  RuntimeShape bias_shape = tflite_micro::micro::GetTensorShape(bias);
 
   // Consistency check.
   TFLITE_DCHECK_LE(conv_params.activation.min, conv_params.activation.max);
@@ -307,7 +307,7 @@ TfLiteStatus EvalQuantizedPerChannel16x8(
   const int batch_size = MatchingDim(input_shape, 0, output_shape, 0);
   const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
   const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
-  if (tflite::micro::GetOptionalTensorData<int64_t>(bias)) {
+  if (tflite_micro::micro::GetOptionalTensorData<int64_t>(bias)) {
     TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
   }
 
@@ -355,10 +355,10 @@ TfLiteStatus EvalQuantizedPerChannel16x8(
   TFLITE_DCHECK_EQ(
       arm_convolve_wrapper_s16(
           &ctx, &conv_params, &quant_params, &input_dims,
-          tflite::micro::GetTensorData<int16_t>(input), &filter_dims,
-          tflite::micro::GetTensorData<int8_t>(filter), &bias_dims,
-          tflite::micro::GetOptionalTensorData<int64_t>(bias), &output_dims,
-          tflite::micro::GetTensorData<int16_t>(output)),
+          tflite_micro::micro::GetTensorData<int16_t>(input), &filter_dims,
+          tflite_micro::micro::GetTensorData<int8_t>(filter), &bias_dims,
+          tflite_micro::micro::GetOptionalTensorData<int64_t>(bias), &output_dims,
+          tflite_micro::micro::GetTensorData<int16_t>(output)),
       ARM_CMSIS_NN_SUCCESS);
 
   return kTfLiteOk;
@@ -366,22 +366,22 @@ TfLiteStatus EvalQuantizedPerChannel16x8(
 
 TfLiteStatus EvalInt8(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kConvInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kConvInputTensor);
   const TfLiteEvalTensor* filter =
-      tflite::micro::GetEvalInput(context, node, kConvWeightsTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kConvWeightsTensor);
   const TfLiteEvalTensor* bias =
       (NumInputs(node) == 3)
-          ? tflite::micro::GetEvalInput(context, node, kConvBiasTensor)
+          ? tflite_micro::micro::GetEvalInput(context, node, kConvBiasTensor)
           : nullptr;
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kConvOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kConvOutputTensor);
 
   TFLITE_DCHECK(node->builtin_data != nullptr);
   const auto& params =
       *(reinterpret_cast<TfLiteConvParams*>(node->builtin_data));
   TFLITE_DCHECK(node->user_data != nullptr);
   const OpData& data = *(static_cast<const OpData*>(node->user_data));
-  TfLiteEvalTensor filter_int8 = tflite::micro::MakeUnpackedInt4Tensor(
+  TfLiteEvalTensor filter_int8 = tflite_micro::micro::MakeUnpackedInt4Tensor(
       context, data.reference_op_data.filter_buffer_index, filter);
 
   return EvalQuantizedPerChannel(context, node, params, data, input,
@@ -390,15 +390,15 @@ TfLiteStatus EvalInt8(TfLiteContext* context, TfLiteNode* node) {
 
 TfLiteStatus EvalInt16x8(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kConvInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kConvInputTensor);
   const TfLiteEvalTensor* filter =
-      tflite::micro::GetEvalInput(context, node, kConvWeightsTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kConvWeightsTensor);
   const TfLiteEvalTensor* bias =
       (NumInputs(node) == 3)
-          ? tflite::micro::GetEvalInput(context, node, kConvBiasTensor)
+          ? tflite_micro::micro::GetEvalInput(context, node, kConvBiasTensor)
           : nullptr;
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kConvOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kConvOutputTensor);
 
   TFLITE_DCHECK(node->builtin_data != nullptr);
   const auto& params =
@@ -414,14 +414,14 @@ TfLiteStatus EvalInt16x8(TfLiteContext* context, TfLiteNode* node) {
         ConvParamsQuantized(params, data.reference_op_data),
         data.reference_op_data.per_channel_output_multiplier,
         data.reference_op_data.per_channel_output_shift,
-        tflite::micro::GetTensorShape(input),
-        tflite::micro::GetTensorData<int16_t>(input),
-        tflite::micro::GetTensorShape(filter),
-        tflite::micro::GetTensorData<int8_t>(filter),
-        tflite::micro::GetTensorShape(bias),
-        tflite::micro::GetOptionalTensorData<std::int32_t>(bias),
-        tflite::micro::GetTensorShape(output),
-        tflite::micro::GetTensorData<int16_t>(output));
+        tflite_micro::micro::GetTensorShape(input),
+        tflite_micro::micro::GetTensorData<int16_t>(input),
+        tflite_micro::micro::GetTensorShape(filter),
+        tflite_micro::micro::GetTensorData<int8_t>(filter),
+        tflite_micro::micro::GetTensorShape(bias),
+        tflite_micro::micro::GetOptionalTensorData<std::int32_t>(bias),
+        tflite_micro::micro::GetTensorShape(output),
+        tflite_micro::micro::GetTensorData<int16_t>(output));
   }
 
   return kTfLiteOk;
@@ -429,15 +429,15 @@ TfLiteStatus EvalInt16x8(TfLiteContext* context, TfLiteNode* node) {
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kConvInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kConvInputTensor);
   const TfLiteEvalTensor* filter =
-      tflite::micro::GetEvalInput(context, node, kConvWeightsTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kConvWeightsTensor);
   const TfLiteEvalTensor* bias =
       (NumInputs(node) == 3)
-          ? tflite::micro::GetEvalInput(context, node, kConvBiasTensor)
+          ? tflite_micro::micro::GetEvalInput(context, node, kConvBiasTensor)
           : nullptr;
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kConvOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kConvOutputTensor);
 
   TFLITE_DCHECK(node->builtin_data != nullptr);
   const auto& params =
@@ -445,22 +445,22 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   TFLITE_DCHECK(node->user_data != nullptr);
   const OpData& data = *(static_cast<const OpData*>(node->user_data));
 
-  TfLiteEvalTensor filter_int8 = tflite::micro::MakeUnpackedInt4Tensor(
+  TfLiteEvalTensor filter_int8 = tflite_micro::micro::MakeUnpackedInt4Tensor(
       context, data.reference_op_data.filter_buffer_index, filter);
 
   switch (input->type) {  // Already know in/out types are same.
     case kTfLiteFloat32: {
-      tflite::reference_ops::Conv(
+      tflite_micro::reference_ops::Conv(
           ConvParamsFloat(params, data.reference_op_data),
-          tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<float>(input),
-          tflite::micro::GetTensorShape(filter),
-          tflite::micro::GetTensorData<float>(filter),
-          tflite::micro::GetTensorShape(bias),
-          tflite::micro::GetOptionalTensorData<float>(bias),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<float>(output),
-          tflite::micro::GetTensorShape(nullptr), nullptr);
+          tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<float>(input),
+          tflite_micro::micro::GetTensorShape(filter),
+          tflite_micro::micro::GetTensorData<float>(filter),
+          tflite_micro::micro::GetTensorShape(bias),
+          tflite_micro::micro::GetOptionalTensorData<float>(bias),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<float>(output),
+          tflite_micro::micro::GetTensorShape(nullptr), nullptr);
       break;
     }
     case kTfLiteInt8:
@@ -486,14 +486,14 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
             ConvParamsQuantized(params, data.reference_op_data),
             data.reference_op_data.per_channel_output_multiplier,
             data.reference_op_data.per_channel_output_shift,
-            tflite::micro::GetTensorShape(input),
-            tflite::micro::GetTensorData<int16_t>(input),
-            tflite::micro::GetTensorShape(filter),
-            tflite::micro::GetTensorData<int8_t>(filter),
-            tflite::micro::GetTensorShape(bias),
-            tflite::micro::GetOptionalTensorData<std::int32_t>(bias),
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<int16_t>(output));
+            tflite_micro::micro::GetTensorShape(input),
+            tflite_micro::micro::GetTensorData<int16_t>(input),
+            tflite_micro::micro::GetTensorShape(filter),
+            tflite_micro::micro::GetTensorData<int8_t>(filter),
+            tflite_micro::micro::GetTensorShape(bias),
+            tflite_micro::micro::GetOptionalTensorData<std::int32_t>(bias),
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int16_t>(output));
       } else {
         MicroPrintf("Bias type %s (%d) not supported.",
                     TfLiteTypeGetName(bias->type), bias->type);
@@ -513,15 +513,15 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_CONV_2D() {
-  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, Eval);
 }
 
 TFLMRegistration Register_CONV_2D_INT8() {
-  return tflite::micro::RegisterOp(Init, Prepare, EvalInt8);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, EvalInt8);
 }
 
 TFLMRegistration Register_CONV_2D_INT16() {
-  return tflite::micro::RegisterOp(Init, Prepare, EvalInt16x8);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, EvalInt16x8);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/cmsis_nn/depthwise_conv.cc b/tensorflow/lite/micro/kernels/cmsis_nn/depthwise_conv.cc
index 7b733b76..e7d74b5c 100644
--- a/tensorflow/lite/micro/kernels/cmsis_nn/depthwise_conv.cc
+++ b/tensorflow/lite/micro/kernels/cmsis_nn/depthwise_conv.cc
@@ -29,7 +29,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 struct OpData {
@@ -213,10 +213,10 @@ inline void PopulateDwConvParams(
       data.reference_op_data.per_channel_output_multiplier;
   quant_params->shift = data.reference_op_data.per_channel_output_shift;
 
-  RuntimeShape filter_shape = tflite::micro::GetTensorShape(filter);
-  RuntimeShape input_shape = tflite::micro::GetTensorShape(input);
-  RuntimeShape output_shape = tflite::micro::GetTensorShape(output);
-  RuntimeShape bias_shape = tflite::micro::GetTensorShape(bias);
+  RuntimeShape filter_shape = tflite_micro::micro::GetTensorShape(filter);
+  RuntimeShape input_shape = tflite_micro::micro::GetTensorShape(input);
+  RuntimeShape output_shape = tflite_micro::micro::GetTensorShape(output);
+  RuntimeShape bias_shape = tflite_micro::micro::GetTensorShape(bias);
 
   TFLITE_DCHECK_LE(dw_conv_params->activation.min,
                    dw_conv_params->activation.max);
@@ -224,7 +224,7 @@ inline void PopulateDwConvParams(
   const int batch_size = MatchingDim(input_shape, 0, output_shape, 0);
   const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);
 
-  if (tflite::micro::GetOptionalTensorData<int8_t>(bias)) {
+  if (tflite_micro::micro::GetOptionalTensorData<int8_t>(bias)) {
     TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
   }
 
@@ -278,10 +278,10 @@ void EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,
   TFLITE_DCHECK_EQ(
       arm_depthwise_conv_wrapper_s8(
           &ctx, &dw_conv_params, &quant_params, &input_dims,
-          tflite::micro::GetTensorData<int8_t>(input), &filter_dims,
-          tflite::micro::GetTensorData<int8_t>(filter), &bias_dims,
-          tflite::micro::GetOptionalTensorData<int32_t>(bias), &output_dims,
-          tflite::micro::GetTensorData<int8_t>(output)),
+          tflite_micro::micro::GetTensorData<int8_t>(input), &filter_dims,
+          tflite_micro::micro::GetTensorData<int8_t>(filter), &bias_dims,
+          tflite_micro::micro::GetOptionalTensorData<int32_t>(bias), &output_dims,
+          tflite_micro::micro::GetTensorData<int8_t>(output)),
       ARM_CMSIS_NN_SUCCESS);
 }
 
@@ -311,10 +311,10 @@ void EvalQuantizedPerChannel16x8(TfLiteContext* context, TfLiteNode* node,
   TFLITE_DCHECK_EQ(
       arm_depthwise_conv_s16(
           &ctx, &dw_conv_params, &quant_params, &input_dims,
-          tflite::micro::GetTensorData<int16_t>(input), &filter_dims,
-          tflite::micro::GetTensorData<int8_t>(filter), &bias_dims,
-          tflite::micro::GetOptionalTensorData<int64_t>(bias), &output_dims,
-          tflite::micro::GetTensorData<int16_t>(output)),
+          tflite_micro::micro::GetTensorData<int16_t>(input), &filter_dims,
+          tflite_micro::micro::GetTensorData<int8_t>(filter), &bias_dims,
+          tflite_micro::micro::GetOptionalTensorData<int64_t>(bias), &output_dims,
+          tflite_micro::micro::GetTensorData<int16_t>(output)),
       ARM_CMSIS_NN_SUCCESS);
 }
 
@@ -327,31 +327,31 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const OpData& data = *(static_cast<OpData*>(node->user_data));
 
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kDepthwiseConvOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kDepthwiseConvOutputTensor);
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kDepthwiseConvInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kDepthwiseConvInputTensor);
   const TfLiteEvalTensor* filter =
-      tflite::micro::GetEvalInput(context, node, kDepthwiseConvWeightsTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kDepthwiseConvWeightsTensor);
   const TfLiteEvalTensor* bias =
       (NumInputs(node) == 3)
-          ? tflite::micro::GetEvalInput(context, node, kDepthwiseConvBiasTensor)
+          ? tflite_micro::micro::GetEvalInput(context, node, kDepthwiseConvBiasTensor)
           : nullptr;
 
-  TfLiteEvalTensor filter_int8 = tflite::micro::MakeUnpackedInt4Tensor(
+  TfLiteEvalTensor filter_int8 = tflite_micro::micro::MakeUnpackedInt4Tensor(
       context, data.reference_op_data.filter_buffer_index, filter);
 
   switch (input->type) {  // Already know in/out types are same.
     case kTfLiteFloat32: {
-      tflite::reference_ops::DepthwiseConv(
+      tflite_micro::reference_ops::DepthwiseConv(
           DepthwiseConvParamsFloat(params, data.reference_op_data),
-          tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<float>(input),
-          tflite::micro::GetTensorShape(filter),
-          tflite::micro::GetTensorData<float>(filter),
-          tflite::micro::GetTensorShape(bias),
-          tflite::micro::GetOptionalTensorData<float>(bias),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<float>(output));
+          tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<float>(input),
+          tflite_micro::micro::GetTensorShape(filter),
+          tflite_micro::micro::GetTensorData<float>(filter),
+          tflite_micro::micro::GetTensorShape(bias),
+          tflite_micro::micro::GetOptionalTensorData<float>(bias),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<float>(output));
       break;
     }
     case kTfLiteInt8:
@@ -389,17 +389,17 @@ TfLiteStatus EvalInt8(TfLiteContext* context, TfLiteNode* node) {
   const OpData& data = *(static_cast<OpData*>(node->user_data));
 
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kDepthwiseConvOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kDepthwiseConvOutputTensor);
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kDepthwiseConvInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kDepthwiseConvInputTensor);
   const TfLiteEvalTensor* filter =
-      tflite::micro::GetEvalInput(context, node, kDepthwiseConvWeightsTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kDepthwiseConvWeightsTensor);
   const TfLiteEvalTensor* bias =
       (NumInputs(node) == 3)
-          ? tflite::micro::GetEvalInput(context, node, kDepthwiseConvBiasTensor)
+          ? tflite_micro::micro::GetEvalInput(context, node, kDepthwiseConvBiasTensor)
           : nullptr;
 
-  TfLiteEvalTensor filter_int8 = tflite::micro::MakeUnpackedInt4Tensor(
+  TfLiteEvalTensor filter_int8 = tflite_micro::micro::MakeUnpackedInt4Tensor(
       context, data.reference_op_data.filter_buffer_index, filter);
 
   EvalQuantizedPerChannel(context, node, params, data, input, &filter_int8,
@@ -416,14 +416,14 @@ TfLiteStatus EvalInt16x8(TfLiteContext* context, TfLiteNode* node) {
   const OpData& data = *(static_cast<OpData*>(node->user_data));
 
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kDepthwiseConvOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kDepthwiseConvOutputTensor);
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kDepthwiseConvInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kDepthwiseConvInputTensor);
   const TfLiteEvalTensor* filter =
-      tflite::micro::GetEvalInput(context, node, kDepthwiseConvWeightsTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kDepthwiseConvWeightsTensor);
   const TfLiteEvalTensor* bias =
       (NumInputs(node) == 3)
-          ? tflite::micro::GetEvalInput(context, node, kDepthwiseConvBiasTensor)
+          ? tflite_micro::micro::GetEvalInput(context, node, kDepthwiseConvBiasTensor)
           : nullptr;
 
   EvalQuantizedPerChannel16x8(context, node, params, data, input, filter, bias,
@@ -434,15 +434,15 @@ TfLiteStatus EvalInt16x8(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_DEPTHWISE_CONV_2D() {
-  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, Eval);
 }
 
 TFLMRegistration Register_DEPTHWISE_CONV_2D_INT8() {
-  return tflite::micro::RegisterOp(Init, Prepare, EvalInt8);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, EvalInt8);
 }
 
 TFLMRegistration Register_DEPTHWISE_CONV_2D_INT16() {
-  return tflite::micro::RegisterOp(Init, Prepare, EvalInt16x8);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, EvalInt16x8);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/cmsis_nn/fully_connected.cc b/tensorflow/lite/micro/kernels/cmsis_nn/fully_connected.cc
index dbba5b27..08d646a4 100644
--- a/tensorflow/lite/micro/kernels/cmsis_nn/fully_connected.cc
+++ b/tensorflow/lite/micro/kernels/cmsis_nn/fully_connected.cc
@@ -29,7 +29,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_arena_constants.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 struct OpData {
@@ -139,9 +139,9 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
           size_t filter_size = GetTensorShape(filter).FlatSize();
           int8_t* unpacked_filter_buf =
               reinterpret_cast<int8_t*>(micro_context->AllocateTempBuffer(
-                  filter_size, tflite::MicroArenaBufferAlignment()));
+                  filter_size, tflite_micro::MicroArenaBufferAlignment()));
 
-          tflite::tensor_utils::UnpackDenseInt4IntoInt8(
+          tflite_micro::tensor_utils::UnpackDenseInt4IntoInt8(
               filter_data, filter_size, unpacked_filter_buf);
           filter_data = unpacked_filter_buf;
         }
@@ -227,7 +227,7 @@ TfLiteStatus EvalQuantizedInt8(TfLiteContext* context, TfLiteNode* node,
                                const TfLiteEvalTensor* filter,
                                const TfLiteEvalTensor* bias,
                                TfLiteEvalTensor* output) {
-  const RuntimeShape output_shape = tflite::micro::GetTensorShape(output);
+  const RuntimeShape output_shape = tflite_micro::micro::GetTensorShape(output);
   const int output_dim_count = output_shape.DimensionsCount();
   TFLITE_DCHECK_GE(output_dim_count, 2);
   TFLITE_DCHECK_LE(output_dim_count, 4);
@@ -243,7 +243,7 @@ TfLiteStatus EvalQuantizedInt8(TfLiteContext* context, TfLiteNode* node,
                        &bias_dims, &output_dims, &ctx, data);
 
   const int32_t* bias_data =
-      tflite::micro::GetOptionalTensorData<int32_t>(bias);
+      tflite_micro::micro::GetOptionalTensorData<int32_t>(bias);
 
   if (output_dim_count > 2 && data.accum_depth % 4 == 0) {
     cmsis_nn_conv_params conv_params;
@@ -273,9 +273,9 @@ TfLiteStatus EvalQuantizedInt8(TfLiteContext* context, TfLiteNode* node,
         context,
         arm_convolve_1x1_s8_fast(
             &ctx, &conv_params, &per_channel_quant_params, &input_dims,
-            tflite::micro::GetTensorData<int8_t>(input), &filter_dims,
-            tflite::micro::GetTensorData<int8_t>(filter), &bias_dims, bias_data,
-            &output_dims, tflite::micro::GetTensorData<int8_t>(output)),
+            tflite_micro::micro::GetTensorData<int8_t>(input), &filter_dims,
+            tflite_micro::micro::GetTensorData<int8_t>(filter), &bias_dims, bias_data,
+            &output_dims, tflite_micro::micro::GetTensorData<int8_t>(output)),
         ARM_CMSIS_NN_SUCCESS);
   } else {
     cmsis_nn_fc_params fc_params;
@@ -290,9 +290,9 @@ TfLiteStatus EvalQuantizedInt8(TfLiteContext* context, TfLiteNode* node,
         context,
         arm_fully_connected_s8(
             &ctx, &fc_params, &quant_params, &input_dims,
-            tflite::micro::GetTensorData<int8_t>(input), &filter_dims,
-            tflite::micro::GetTensorData<int8_t>(filter), &bias_dims, bias_data,
-            &output_dims, tflite::micro::GetTensorData<int8_t>(output)),
+            tflite_micro::micro::GetTensorData<int8_t>(input), &filter_dims,
+            tflite_micro::micro::GetTensorData<int8_t>(filter), &bias_dims, bias_data,
+            &output_dims, tflite_micro::micro::GetTensorData<int8_t>(output)),
         ARM_CMSIS_NN_SUCCESS);
   }
   return kTfLiteOk;
@@ -315,7 +315,7 @@ TfLiteStatus EvalQuantizedInt16(TfLiteContext* context, TfLiteNode* node,
                        &bias_dims, &output_dims, &ctx, data);
 
   const int64_t* bias_data =
-      tflite::micro::GetOptionalTensorData<int64_t>(bias);
+      tflite_micro::micro::GetOptionalTensorData<int64_t>(bias);
 
   cmsis_nn_fc_params fc_params;
   fc_params.input_offset = -data.reference_op_data.input_zero_point;
@@ -328,9 +328,9 @@ TfLiteStatus EvalQuantizedInt16(TfLiteContext* context, TfLiteNode* node,
       context,
       arm_fully_connected_s16(
           &ctx, &fc_params, &quant_params, &input_dims,
-          tflite::micro::GetTensorData<int16_t>(input), &filter_dims,
-          tflite::micro::GetTensorData<int8_t>(filter), &bias_dims, bias_data,
-          &output_dims, tflite::micro::GetTensorData<int16_t>(output)),
+          tflite_micro::micro::GetTensorData<int16_t>(input), &filter_dims,
+          tflite_micro::micro::GetTensorData<int8_t>(filter), &bias_dims, bias_data,
+          &output_dims, tflite_micro::micro::GetTensorData<int16_t>(output)),
       ARM_CMSIS_NN_SUCCESS);
 
   return kTfLiteOk;
@@ -342,34 +342,34 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
       static_cast<const TfLiteFullyConnectedParams*>(node->builtin_data);
 
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kFullyConnectedInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kFullyConnectedInputTensor);
   const TfLiteEvalTensor* filter =
-      tflite::micro::GetEvalInput(context, node, kFullyConnectedWeightsTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kFullyConnectedWeightsTensor);
   const TfLiteEvalTensor* bias =
-      tflite::micro::GetEvalInput(context, node, kFullyConnectedBiasTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kFullyConnectedBiasTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kFullyConnectedOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kFullyConnectedOutputTensor);
 
   TFLITE_DCHECK(node->user_data != nullptr);
   const OpData& data = *(static_cast<const OpData*>(node->user_data));
 
-  TfLiteEvalTensor filter_int8 = tflite::micro::MakeUnpackedInt4Tensor(
+  TfLiteEvalTensor filter_int8 = tflite_micro::micro::MakeUnpackedInt4Tensor(
       context, data.reference_op_data.filter_buffer_index, filter);
 
   // Checks in Prepare ensure input, output and filter types are all the same.
   switch (input->type) {
     case kTfLiteFloat32: {
       const float* bias_data =
-          tflite::micro::GetOptionalTensorData<float>(bias);
-      tflite::reference_ops::FullyConnected(
+          tflite_micro::micro::GetOptionalTensorData<float>(bias);
+      tflite_micro::reference_ops::FullyConnected(
           FullyConnectedParamsFloat(params->activation),
-          tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<float>(input),
-          tflite::micro::GetTensorShape(filter),
-          tflite::micro::GetTensorData<float>(filter),
-          tflite::micro::GetTensorShape(bias), bias_data,
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<float>(output));
+          tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<float>(input),
+          tflite_micro::micro::GetTensorShape(filter),
+          tflite_micro::micro::GetTensorData<float>(filter),
+          tflite_micro::micro::GetTensorShape(bias), bias_data,
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<float>(output));
       break;
     }
     case kTfLiteInt8: {
@@ -379,16 +379,16 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
             return EvalQuantizedInt8(context, node, data, input, &filter_int8,
                                      bias, output);
           } else {
-            tflite::reference_integer_ops::FullyConnected(
+            tflite_micro::reference_integer_ops::FullyConnected(
                 FullyConnectedParamsQuantized(data.reference_op_data),
-                tflite::micro::GetTensorShape(input),
-                tflite::micro::GetTensorData<int8_t>(input),
-                tflite::micro::GetTensorShape(filter),
-                tflite::micro::GetTensorData<int8_t>(filter),
-                tflite::micro::GetTensorShape(bias),
-                tflite::micro::GetOptionalTensorData<int32_t>(bias),
-                tflite::micro::GetTensorShape(output),
-                tflite::micro::GetTensorData<int8_t>(output));
+                tflite_micro::micro::GetTensorShape(input),
+                tflite_micro::micro::GetTensorData<int8_t>(input),
+                tflite_micro::micro::GetTensorShape(filter),
+                tflite_micro::micro::GetTensorData<int8_t>(filter),
+                tflite_micro::micro::GetTensorShape(bias),
+                tflite_micro::micro::GetOptionalTensorData<int32_t>(bias),
+                tflite_micro::micro::GetTensorShape(output),
+                tflite_micro::micro::GetTensorData<int8_t>(output));
             return kTfLiteOk;
           }
         default:
@@ -419,13 +419,13 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 // refactoring.
 TfLiteStatus EvalInt8(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kFullyConnectedInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kFullyConnectedInputTensor);
   const TfLiteEvalTensor* filter =
-      tflite::micro::GetEvalInput(context, node, kFullyConnectedWeightsTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kFullyConnectedWeightsTensor);
   const TfLiteEvalTensor* bias =
-      tflite::micro::GetEvalInput(context, node, kFullyConnectedBiasTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kFullyConnectedBiasTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kFullyConnectedOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kFullyConnectedOutputTensor);
 
   TFLITE_DCHECK(node->user_data != nullptr);
   const OpData& data = *(static_cast<const OpData*>(node->user_data));
@@ -437,7 +437,7 @@ TfLiteStatus EvalInt8(TfLiteContext* context, TfLiteNode* node) {
     return kTfLiteError;
   }
 
-  TfLiteEvalTensor filter_int8 = tflite::micro::MakeUnpackedInt4Tensor(
+  TfLiteEvalTensor filter_int8 = tflite_micro::micro::MakeUnpackedInt4Tensor(
       context, data.reference_op_data.filter_buffer_index, filter);
 
   return EvalQuantizedInt8(context, node, data, input, &filter_int8, bias,
@@ -446,13 +446,13 @@ TfLiteStatus EvalInt8(TfLiteContext* context, TfLiteNode* node) {
 
 TfLiteStatus EvalInt16(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kFullyConnectedInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kFullyConnectedInputTensor);
   const TfLiteEvalTensor* filter =
-      tflite::micro::GetEvalInput(context, node, kFullyConnectedWeightsTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kFullyConnectedWeightsTensor);
   const TfLiteEvalTensor* bias =
-      tflite::micro::GetEvalInput(context, node, kFullyConnectedBiasTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kFullyConnectedBiasTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kFullyConnectedOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kFullyConnectedOutputTensor);
 
   TFLITE_DCHECK(node->user_data != nullptr);
   const OpData& data = *(static_cast<const OpData*>(node->user_data));
@@ -470,19 +470,19 @@ TfLiteStatus EvalInt16(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_FULLY_CONNECTED() {
-  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, Eval);
 }
 
 TFLMRegistration Register_FULLY_CONNECTED_INT8() {
-  return tflite::micro::RegisterOp(Init, Prepare, EvalInt8);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, EvalInt8);
 }
 
 TFLMRegistration Register_FULLY_CONNECTED_INT16() {
-  return tflite::micro::RegisterOp(Init, Prepare, EvalInt16);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, EvalInt16);
 }
 
 TFLMInferenceRegistration RegisterInference_FULLY_CONNECTED() {
-  return tflite::micro::RegisterOp(Eval);
+  return tflite_micro::micro::RegisterOp(Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/cmsis_nn/mul.cc b/tensorflow/lite/micro/kernels/cmsis_nn/mul.cc
index 571d88ab..d7617825 100644
--- a/tensorflow/lite/micro/kernels/cmsis_nn/mul.cc
+++ b/tensorflow/lite/micro/kernels/cmsis_nn/mul.cc
@@ -26,13 +26,13 @@ limitations under the License.
 #include "tensorflow/lite/micro/memory_helpers.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                    const OpDataMul* data, const TfLiteEvalTensor* input1,
                    const TfLiteEvalTensor* input2, TfLiteEvalTensor* output) {
-  tflite::ArithmeticParams op_params = {};
+  tflite_micro::ArithmeticParams op_params = {};
 
   op_params.quantized_activation_min = data->output_activation_min;
   op_params.quantized_activation_max = data->output_activation_max;
@@ -44,52 +44,52 @@ void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
   op_params.output_shift = data->output_shift;
 
   bool need_broadcast = reference_ops::ProcessBroadcastShapes(
-      tflite::micro::GetTensorShape(input1),
-      tflite::micro::GetTensorShape(input2), &op_params);
+      tflite_micro::micro::GetTensorShape(input1),
+      tflite_micro::micro::GetTensorShape(input2), &op_params);
 
   if (need_broadcast) {
     if (input1->type == kTfLiteInt8) {
       reference_integer_ops::BroadcastMul4DSlow(
-          op_params, tflite::micro::GetTensorShape(input1),
-          tflite::micro::GetTensorData<int8_t>(input1),
-          tflite::micro::GetTensorShape(input2),
-          tflite::micro::GetTensorData<int8_t>(input2),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<int8_t>(output));
+          op_params, tflite_micro::micro::GetTensorShape(input1),
+          tflite_micro::micro::GetTensorData<int8_t>(input1),
+          tflite_micro::micro::GetTensorShape(input2),
+          tflite_micro::micro::GetTensorData<int8_t>(input2),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<int8_t>(output));
     } else if (input1->type == kTfLiteInt16) {
       reference_integer_ops::BroadcastMul4DSlow(
-          op_params, tflite::micro::GetTensorShape(input1),
-          tflite::micro::GetTensorData<int16_t>(input1),
-          tflite::micro::GetTensorShape(input2),
-          tflite::micro::GetTensorData<int16_t>(input2),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<int16_t>(output));
+          op_params, tflite_micro::micro::GetTensorShape(input1),
+          tflite_micro::micro::GetTensorData<int16_t>(input1),
+          tflite_micro::micro::GetTensorShape(input2),
+          tflite_micro::micro::GetTensorData<int16_t>(input2),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<int16_t>(output));
     }
 
   } else {
     if (input1->type == kTfLiteInt8) {
       arm_elementwise_mul_s8(
-          tflite::micro::GetTensorData<int8_t>(input1),
-          tflite::micro::GetTensorData<int8_t>(input2), op_params.input1_offset,
-          op_params.input2_offset, tflite::micro::GetTensorData<int8_t>(output),
+          tflite_micro::micro::GetTensorData<int8_t>(input1),
+          tflite_micro::micro::GetTensorData<int8_t>(input2), op_params.input1_offset,
+          op_params.input2_offset, tflite_micro::micro::GetTensorData<int8_t>(output),
           op_params.output_offset, op_params.output_multiplier,
           op_params.output_shift, op_params.quantized_activation_min,
           op_params.quantized_activation_max,
-          MatchingElementsSize(tflite::micro::GetTensorShape(input1),
-                               tflite::micro::GetTensorShape(input2),
-                               tflite::micro::GetTensorShape(output)));
+          MatchingElementsSize(tflite_micro::micro::GetTensorShape(input1),
+                               tflite_micro::micro::GetTensorShape(input2),
+                               tflite_micro::micro::GetTensorShape(output)));
     } else if (input1->type == kTfLiteInt16) {
       arm_elementwise_mul_s16(
-          tflite::micro::GetTensorData<int16_t>(input1),
-          tflite::micro::GetTensorData<int16_t>(input2),
+          tflite_micro::micro::GetTensorData<int16_t>(input1),
+          tflite_micro::micro::GetTensorData<int16_t>(input2),
           op_params.input1_offset, op_params.input2_offset,
-          tflite::micro::GetTensorData<int16_t>(output),
+          tflite_micro::micro::GetTensorData<int16_t>(output),
           op_params.output_offset, op_params.output_multiplier,
           op_params.output_shift, op_params.quantized_activation_min,
           op_params.quantized_activation_max,
-          MatchingElementsSize(tflite::micro::GetTensorShape(input1),
-                               tflite::micro::GetTensorShape(input2),
-                               tflite::micro::GetTensorShape(output)));
+          MatchingElementsSize(tflite_micro::micro::GetTensorShape(input1),
+                               tflite_micro::micro::GetTensorShape(input2),
+                               tflite_micro::micro::GetTensorShape(output)));
     }
   }
 }
@@ -104,11 +104,11 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const OpDataMul* data = static_cast<const OpDataMul*>(node->user_data);
 
   const TfLiteEvalTensor* input1 =
-      tflite::micro::GetEvalInput(context, node, kMulInput1Tensor);
+      tflite_micro::micro::GetEvalInput(context, node, kMulInput1Tensor);
   const TfLiteEvalTensor* input2 =
-      tflite::micro::GetEvalInput(context, node, kMulInput2Tensor);
+      tflite_micro::micro::GetEvalInput(context, node, kMulInput2Tensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kMulOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kMulOutputTensor);
 
   switch (input1->type) {
     case kTfLiteInt8:
@@ -139,11 +139,11 @@ TfLiteStatus EvalInt8(TfLiteContext* context, TfLiteNode* node) {
 
   const OpDataMul* data = static_cast<const OpDataMul*>(node->user_data);
   const TfLiteEvalTensor* input1 =
-      tflite::micro::GetEvalInput(context, node, kMulInput1Tensor);
+      tflite_micro::micro::GetEvalInput(context, node, kMulInput1Tensor);
   const TfLiteEvalTensor* input2 =
-      tflite::micro::GetEvalInput(context, node, kMulInput2Tensor);
+      tflite_micro::micro::GetEvalInput(context, node, kMulInput2Tensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kMulOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kMulOutputTensor);
   TFLITE_DCHECK(input1->type == kTfLiteInt8);
 
   EvalQuantized(context, node, data, input1, input2, output);
@@ -157,11 +157,11 @@ TfLiteStatus EvalInt16(TfLiteContext* context, TfLiteNode* node) {
 
   const OpDataMul* data = static_cast<const OpDataMul*>(node->user_data);
   const TfLiteEvalTensor* input1 =
-      tflite::micro::GetEvalInput(context, node, kMulInput1Tensor);
+      tflite_micro::micro::GetEvalInput(context, node, kMulInput1Tensor);
   const TfLiteEvalTensor* input2 =
-      tflite::micro::GetEvalInput(context, node, kMulInput2Tensor);
+      tflite_micro::micro::GetEvalInput(context, node, kMulInput2Tensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kMulOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kMulOutputTensor);
   TFLITE_DCHECK(input1->type == kTfLiteInt16);
 
   EvalQuantized(context, node, data, input1, input2, output);
@@ -170,15 +170,15 @@ TfLiteStatus EvalInt16(TfLiteContext* context, TfLiteNode* node) {
 }
 
 TFLMRegistration Register_MUL() {
-  return tflite::micro::RegisterOp(MulInit, MulPrepare, Eval);
+  return tflite_micro::micro::RegisterOp(MulInit, MulPrepare, Eval);
 }
 
 TFLMRegistration Register_MUL_INT8() {
-  return tflite::micro::RegisterOp(MulInit, MulPrepare, EvalInt8);
+  return tflite_micro::micro::RegisterOp(MulInit, MulPrepare, EvalInt8);
 }
 
 TFLMRegistration Register_MUL_INT16() {
-  return tflite::micro::RegisterOp(MulInit, MulPrepare, EvalInt16);
+  return tflite_micro::micro::RegisterOp(MulInit, MulPrepare, EvalInt16);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/cmsis_nn/pooling.cc b/tensorflow/lite/micro/kernels/cmsis_nn/pooling.cc
index d8311dbb..b33963ff 100644
--- a/tensorflow/lite/micro/kernels/cmsis_nn/pooling.cc
+++ b/tensorflow/lite/micro/kernels/cmsis_nn/pooling.cc
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/pooling.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -320,27 +320,27 @@ TfLiteStatus MaxEvalInt16(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_AVERAGE_POOL_2D_INT8() {
-  return tflite::micro::RegisterOp(Init, AveragePrepare, AverageEvalInt8);
+  return tflite_micro::micro::RegisterOp(Init, AveragePrepare, AverageEvalInt8);
 }
 
 TFLMRegistration Register_AVERAGE_POOL_2D_INT16() {
-  return tflite::micro::RegisterOp(Init, AveragePrepare, AverageEvalInt16);
+  return tflite_micro::micro::RegisterOp(Init, AveragePrepare, AverageEvalInt16);
 }
 
 TFLMRegistration Register_AVERAGE_POOL_2D() {
-  return tflite::micro::RegisterOp(Init, AveragePrepare, AverageEval);
+  return tflite_micro::micro::RegisterOp(Init, AveragePrepare, AverageEval);
 }
 
 TFLMRegistration Register_MAX_POOL_2D_INT8() {
-  return tflite::micro::RegisterOp(Init, MaxPrepare, MaxEvalInt8);
+  return tflite_micro::micro::RegisterOp(Init, MaxPrepare, MaxEvalInt8);
 }
 
 TFLMRegistration Register_MAX_POOL_2D_INT16() {
-  return tflite::micro::RegisterOp(Init, MaxPrepare, MaxEvalInt16);
+  return tflite_micro::micro::RegisterOp(Init, MaxPrepare, MaxEvalInt16);
 }
 
 TFLMRegistration Register_MAX_POOL_2D() {
-  return tflite::micro::RegisterOp(Init, MaxPrepare, MaxEval);
+  return tflite_micro::micro::RegisterOp(Init, MaxPrepare, MaxEval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/cmsis_nn/softmax.cc b/tensorflow/lite/micro/kernels/cmsis_nn/softmax.cc
index f83a0902..594bff97 100644
--- a/tensorflow/lite/micro/kernels/cmsis_nn/softmax.cc
+++ b/tensorflow/lite/micro/kernels/cmsis_nn/softmax.cc
@@ -26,7 +26,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 struct CMSISNNSoftmaxParams {
@@ -76,8 +76,8 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 }
 
 TfLiteStatus SoftmaxEval(TfLiteContext* context, TfLiteNode* node) {
-  const TfLiteEvalTensor* input = tflite::micro::GetEvalInput(context, node, 0);
-  TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, 0);
+  const TfLiteEvalTensor* input = tflite_micro::micro::GetEvalInput(context, node, 0);
+  TfLiteEvalTensor* output = tflite_micro::micro::GetEvalOutput(context, node, 0);
 
   TFLITE_DCHECK(node->user_data != nullptr);
   const CMSISNNSoftmaxParams op_data =
@@ -85,28 +85,28 @@ TfLiteStatus SoftmaxEval(TfLiteContext* context, TfLiteNode* node) {
 
   switch (input->type) {
     case kTfLiteFloat32: {
-      tflite::reference_ops::Softmax(
-          op_data.softmax_params, tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<float>(input),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<float>(output));
+      tflite_micro::reference_ops::Softmax(
+          op_data.softmax_params, tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<float>(input),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<float>(output));
       return kTfLiteOk;
     }
     case kTfLiteInt8: {
       if (output->type == kTfLiteInt8) {
-        arm_softmax_s8(tflite::micro::GetTensorData<int8_t>(input),
+        arm_softmax_s8(tflite_micro::micro::GetTensorData<int8_t>(input),
                        op_data.num_rows, op_data.row_size,
                        op_data.softmax_params.input_multiplier,
                        op_data.softmax_params.input_left_shift,
                        op_data.softmax_params.diff_min,
-                       tflite::micro::GetTensorData<int8_t>(output));
+                       tflite_micro::micro::GetTensorData<int8_t>(output));
       } else {
-        arm_softmax_s8_s16(tflite::micro::GetTensorData<int8_t>(input),
+        arm_softmax_s8_s16(tflite_micro::micro::GetTensorData<int8_t>(input),
                            op_data.num_rows, op_data.row_size,
                            op_data.softmax_params.input_multiplier,
                            op_data.softmax_params.input_left_shift,
                            op_data.softmax_params.diff_min,
-                           tflite::micro::GetTensorData<int16_t>(output));
+                           tflite_micro::micro::GetTensorData<int16_t>(output));
       }
       return kTfLiteOk;
     }
@@ -117,10 +117,10 @@ TfLiteStatus SoftmaxEval(TfLiteContext* context, TfLiteNode* node) {
 
       TFLITE_DCHECK_EQ(
           arm_softmax_s16(
-              tflite::micro::GetTensorData<int16_t>(input), op_data.num_rows,
+              tflite_micro::micro::GetTensorData<int16_t>(input), op_data.num_rows,
               op_data.row_size, op_data.softmax_params.input_multiplier,
               op_data.softmax_params.input_left_shift, &softmax_params,
-              tflite::micro::GetTensorData<int16_t>(output)),
+              tflite_micro::micro::GetTensorData<int16_t>(output)),
           ARM_CMSIS_NN_SUCCESS);
       return kTfLiteOk;
     }
@@ -132,42 +132,42 @@ TfLiteStatus SoftmaxEval(TfLiteContext* context, TfLiteNode* node) {
 }
 
 TfLiteStatus SoftmaxEvalInt8(TfLiteContext* context, TfLiteNode* node) {
-  const TfLiteEvalTensor* input = tflite::micro::GetEvalInput(context, node, 0);
-  TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, 0);
+  const TfLiteEvalTensor* input = tflite_micro::micro::GetEvalInput(context, node, 0);
+  TfLiteEvalTensor* output = tflite_micro::micro::GetEvalOutput(context, node, 0);
 
   TFLITE_DCHECK(node->user_data != nullptr);
   const CMSISNNSoftmaxParams op_data =
       *static_cast<const CMSISNNSoftmaxParams*>(node->user_data);
 
-  arm_softmax_s8(tflite::micro::GetTensorData<int8_t>(input), op_data.num_rows,
+  arm_softmax_s8(tflite_micro::micro::GetTensorData<int8_t>(input), op_data.num_rows,
                  op_data.row_size, op_data.softmax_params.input_multiplier,
                  op_data.softmax_params.input_left_shift,
                  op_data.softmax_params.diff_min,
-                 tflite::micro::GetTensorData<int8_t>(output));
+                 tflite_micro::micro::GetTensorData<int8_t>(output));
 
   return kTfLiteOk;
 }
 
 TfLiteStatus SoftmaxEvalInt8_Int16(TfLiteContext* context, TfLiteNode* node) {
-  const TfLiteEvalTensor* input = tflite::micro::GetEvalInput(context, node, 0);
-  TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, 0);
+  const TfLiteEvalTensor* input = tflite_micro::micro::GetEvalInput(context, node, 0);
+  TfLiteEvalTensor* output = tflite_micro::micro::GetEvalOutput(context, node, 0);
 
   TFLITE_DCHECK(node->user_data != nullptr);
   const CMSISNNSoftmaxParams op_data =
       *static_cast<const CMSISNNSoftmaxParams*>(node->user_data);
 
   arm_softmax_s8_s16(
-      tflite::micro::GetTensorData<int8_t>(input), op_data.num_rows,
+      tflite_micro::micro::GetTensorData<int8_t>(input), op_data.num_rows,
       op_data.row_size, op_data.softmax_params.input_multiplier,
       op_data.softmax_params.input_left_shift, op_data.softmax_params.diff_min,
-      tflite::micro::GetTensorData<int16_t>(output));
+      tflite_micro::micro::GetTensorData<int16_t>(output));
 
   return kTfLiteOk;
 }
 
 TfLiteStatus SoftmaxEvalInt16(TfLiteContext* context, TfLiteNode* node) {
-  const TfLiteEvalTensor* input = tflite::micro::GetEvalInput(context, node, 0);
-  TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, 0);
+  const TfLiteEvalTensor* input = tflite_micro::micro::GetEvalInput(context, node, 0);
+  TfLiteEvalTensor* output = tflite_micro::micro::GetEvalOutput(context, node, 0);
 
   TFLITE_DCHECK(node->user_data != nullptr);
   const CMSISNNSoftmaxParams op_data =
@@ -178,11 +178,11 @@ TfLiteStatus SoftmaxEvalInt16(TfLiteContext* context, TfLiteNode* node) {
       .one_by_one_lut = op_data.softmax_params.one_over_one_plus_x_lut};
 
   TFLITE_DCHECK_EQ(
-      arm_softmax_s16(tflite::micro::GetTensorData<int16_t>(input),
+      arm_softmax_s16(tflite_micro::micro::GetTensorData<int16_t>(input),
                       op_data.num_rows, op_data.row_size,
                       op_data.softmax_params.input_multiplier,
                       op_data.softmax_params.input_left_shift, &softmax_params,
-                      tflite::micro::GetTensorData<int16_t>(output)),
+                      tflite_micro::micro::GetTensorData<int16_t>(output)),
       ARM_CMSIS_NN_SUCCESS);
 
   return kTfLiteOk;
@@ -191,19 +191,19 @@ TfLiteStatus SoftmaxEvalInt16(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_SOFTMAX() {
-  return tflite::micro::RegisterOp(Init, Prepare, SoftmaxEval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, SoftmaxEval);
 }
 
 TFLMRegistration Register_SOFTMAX_INT8() {
-  return tflite::micro::RegisterOp(Init, Prepare, SoftmaxEvalInt8);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, SoftmaxEvalInt8);
 }
 
 TFLMRegistration Register_SOFTMAX_INT8_INT16() {
-  return tflite::micro::RegisterOp(Init, Prepare, SoftmaxEvalInt8_Int16);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, SoftmaxEvalInt8_Int16);
 }
 
 TFLMRegistration Register_SOFTMAX_INT16() {
-  return tflite::micro::RegisterOp(Init, Prepare, SoftmaxEvalInt16);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, SoftmaxEvalInt16);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/cmsis_nn/svdf.cc b/tensorflow/lite/micro/kernels/cmsis_nn/svdf.cc
index 97563887..4eb042ec 100644
--- a/tensorflow/lite/micro/kernels/cmsis_nn/svdf.cc
+++ b/tensorflow/lite/micro/kernels/cmsis_nn/svdf.cc
@@ -28,7 +28,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_log.h"
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 struct CmsisNnOpDataSvdf {
@@ -284,7 +284,7 @@ TfLiteStatus EvalIntegerSVDF(TfLiteContext* context, TfLiteNode* node,
   scratch_output_ctx.buf = static_cast<int32_t*>(
       context->GetScratchBuffer(context, data.scratch_output_tensor_index));
 
-  int8_t* output_data = tflite::micro::GetTensorData<int8_t>(output_tensor);
+  int8_t* output_data = tflite_micro::micro::GetTensorData<int8_t>(output_tensor);
 
   switch (weights_time_tensor->type) {
     case kTfLiteInt8: {
@@ -294,13 +294,13 @@ TfLiteStatus EvalIntegerSVDF(TfLiteContext* context, TfLiteNode* node,
       arm_svdf_s8(
           &ctx, &scratch_ctx, &scratch_output_ctx, &svdf_params,
           &in_quant_params, &out_quant_params, &input_dims,
-          tflite::micro::GetTensorData<int8_t>(input_tensor), &state_dims,
-          tflite::micro::GetTensorData<int8_t>(activation_state_tensor),
+          tflite_micro::micro::GetTensorData<int8_t>(input_tensor), &state_dims,
+          tflite_micro::micro::GetTensorData<int8_t>(activation_state_tensor),
           &weights_feature_dims,
-          tflite::micro::GetTensorData<int8_t>(weights_feature_tensor),
+          tflite_micro::micro::GetTensorData<int8_t>(weights_feature_tensor),
           &weights_time_dims,
-          tflite::micro::GetTensorData<int8_t>(weights_time_tensor), &bias_dims,
-          tflite::micro::GetTensorData<int32_t>(bias_tensor), &output_dims,
+          tflite_micro::micro::GetTensorData<int8_t>(weights_time_tensor), &bias_dims,
+          tflite_micro::micro::GetTensorData<int32_t>(bias_tensor), &output_dims,
           output_data);
       return kTfLiteOk;
     }
@@ -309,13 +309,13 @@ TfLiteStatus EvalIntegerSVDF(TfLiteContext* context, TfLiteNode* node,
       arm_svdf_state_s16_s8(
           &scratch_ctx, &scratch_output_ctx, &svdf_params, &in_quant_params,
           &out_quant_params, &input_dims,
-          tflite::micro::GetTensorData<int8_t>(input_tensor), &state_dims,
-          tflite::micro::GetTensorData<int16_t>(activation_state_tensor),
+          tflite_micro::micro::GetTensorData<int8_t>(input_tensor), &state_dims,
+          tflite_micro::micro::GetTensorData<int16_t>(activation_state_tensor),
           &weights_feature_dims,
-          tflite::micro::GetTensorData<int8_t>(weights_feature_tensor),
+          tflite_micro::micro::GetTensorData<int8_t>(weights_feature_tensor),
           &weights_time_dims,
-          tflite::micro::GetTensorData<int16_t>(weights_time_tensor),
-          &bias_dims, tflite::micro::GetTensorData<int32_t>(bias_tensor),
+          tflite_micro::micro::GetTensorData<int16_t>(weights_time_tensor),
+          &bias_dims, tflite_micro::micro::GetTensorData<int32_t>(bias_tensor),
           &output_dims, output_data);
       return kTfLiteOk;
     }
@@ -334,19 +334,19 @@ TfLiteStatus EvalSvdf(TfLiteContext* context, TfLiteNode* node) {
       *(static_cast<const CmsisNnOpDataSvdf*>(node->user_data));
 
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kSvdfInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kSvdfInputTensor);
   const TfLiteEvalTensor* weights_feature =
-      tflite::micro::GetEvalInput(context, node, kSvdfWeightsFeatureTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kSvdfWeightsFeatureTensor);
   const TfLiteEvalTensor* weights_time =
-      tflite::micro::GetEvalInput(context, node, kSvdfWeightsTimeTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kSvdfWeightsTimeTensor);
   const TfLiteEvalTensor* bias =
       (NumInputs(node) == 5)
-          ? tflite::micro::GetEvalInput(context, node, kSvdfBiasTensor)
+          ? tflite_micro::micro::GetEvalInput(context, node, kSvdfBiasTensor)
           : nullptr;
-  TfLiteEvalTensor* activation_state = tflite::micro::GetMutableEvalInput(
+  TfLiteEvalTensor* activation_state = tflite_micro::micro::GetMutableEvalInput(
       context, node, kSvdfInputActivationStateTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kSvdfOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kSvdfOutputTensor);
 
   switch (weights_time->type) {
     case kTfLiteFloat32: {
@@ -378,19 +378,19 @@ TfLiteStatus EvalSvdfInt8(TfLiteContext* context, TfLiteNode* node) {
       *(static_cast<const CmsisNnOpDataSvdf*>(node->user_data));
 
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kSvdfInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kSvdfInputTensor);
   const TfLiteEvalTensor* weights_feature =
-      tflite::micro::GetEvalInput(context, node, kSvdfWeightsFeatureTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kSvdfWeightsFeatureTensor);
   const TfLiteEvalTensor* weights_time =
-      tflite::micro::GetEvalInput(context, node, kSvdfWeightsTimeTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kSvdfWeightsTimeTensor);
   const TfLiteEvalTensor* bias =
       (NumInputs(node) == 5)
-          ? tflite::micro::GetEvalInput(context, node, kSvdfBiasTensor)
+          ? tflite_micro::micro::GetEvalInput(context, node, kSvdfBiasTensor)
           : nullptr;
-  TfLiteEvalTensor* activation_state = tflite::micro::GetMutableEvalInput(
+  TfLiteEvalTensor* activation_state = tflite_micro::micro::GetMutableEvalInput(
       context, node, kSvdfInputActivationStateTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kSvdfOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kSvdfOutputTensor);
 
   TFLITE_DCHECK((weights_time->type == kTfLiteInt8) ||
                 (weights_time->type == kTfLiteInt16));
@@ -404,11 +404,11 @@ TfLiteStatus EvalSvdfInt8(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_SVDF() {
-  return tflite::micro::RegisterOp(Init, CmsisNnPrepareSvdf, EvalSvdf);
+  return tflite_micro::micro::RegisterOp(Init, CmsisNnPrepareSvdf, EvalSvdf);
 }
 
 TFLMRegistration Register_SVDF_INT8() {
-  return tflite::micro::RegisterOp(Init, CmsisNnPrepareSvdf, EvalSvdfInt8);
+  return tflite_micro::micro::RegisterOp(Init, CmsisNnPrepareSvdf, EvalSvdfInt8);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/cmsis_nn/unidirectional_sequence_lstm.cc b/tensorflow/lite/micro/kernels/cmsis_nn/unidirectional_sequence_lstm.cc
index f66ce804..e7ecfa6a 100644
--- a/tensorflow/lite/micro/kernels/cmsis_nn/unidirectional_sequence_lstm.cc
+++ b/tensorflow/lite/micro/kernels/cmsis_nn/unidirectional_sequence_lstm.cc
@@ -29,7 +29,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/lstm_shared.h"
 #include "tensorflow/lite/micro/kernels/micro_tensor_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -64,7 +64,7 @@ TfLiteStatus PrecomputeZeroPointTimesWeightWithBias(
 
   if (zero_point != 0) {
     const int8_t* weight = GetTensorData<int8_t>(weight_tensor);
-    tflite::tensor_utils::MatrixScalarMultiplyAccumulate(weight, zero_point,
+    tflite_micro::tensor_utils::MatrixScalarMultiplyAccumulate(weight, zero_point,
                                                          row, col, *output);
   }
   return kTfLiteOk;
@@ -74,7 +74,7 @@ TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node,
                              const LstmTensors& lstm_tensors, OpData* op_data) {
   const TfLiteTensor* input = lstm_tensors.GetInternalTensor(kLstmInputTensor);
   const TfLiteTensor* output_state =
-      lstm_tensors.GetInternalTensor(tflite::kLstmOutputStateTensor);
+      lstm_tensors.GetInternalTensor(tflite_micro::kLstmOutputStateTensor);
 
   TF_LITE_ENSURE(context, input->type == kTfLiteInt8);
 
@@ -381,45 +381,45 @@ TfLiteStatus CMSIS_NN_EvalInteger8x8_16Lstm(
     const LSTMBuffers<CellType>& buffers) {
   const OpDataLSTM& op_data_lstm = op_data.params_ref;
   const TfLiteEvalTensor* input =
-      kernel_content.GetInternalTensor(tflite::kLstmInputTensor);
+      kernel_content.GetInternalTensor(tflite_micro::kLstmInputTensor);
   const TfLiteEvalTensor* input_gate_bias =
-      kernel_content.GetInternalTensor(tflite::kLstmInputGateBiasTensor);
+      kernel_content.GetInternalTensor(tflite_micro::kLstmInputGateBiasTensor);
   const TfLiteEvalTensor* forget_gate_bias =
-      kernel_content.GetInternalTensor(tflite::kLstmForgetGateBiasTensor);
+      kernel_content.GetInternalTensor(tflite_micro::kLstmForgetGateBiasTensor);
   const TfLiteEvalTensor* cell_gate_bias =
-      kernel_content.GetInternalTensor(tflite::kLstmCellGateBiasTensor);
+      kernel_content.GetInternalTensor(tflite_micro::kLstmCellGateBiasTensor);
   const TfLiteEvalTensor* output_gate_bias =
-      kernel_content.GetInternalTensor(tflite::kLstmOutputGateBiasTensor);
+      kernel_content.GetInternalTensor(tflite_micro::kLstmOutputGateBiasTensor);
   const TfLiteEvalTensor* input_to_output_weights =
-      kernel_content.GetInternalTensor(tflite::kLstmInputToOutputWeightsTensor);
+      kernel_content.GetInternalTensor(tflite_micro::kLstmInputToOutputWeightsTensor);
   const TfLiteEvalTensor* recurrent_to_output_weights =
       kernel_content.GetInternalTensor(
-          tflite::kLstmRecurrentToOutputWeightsTensor);
+          tflite_micro::kLstmRecurrentToOutputWeightsTensor);
   const TfLiteEvalTensor* input_to_input_weights =
-      kernel_content.GetInternalTensor(tflite::kLstmInputToInputWeightsTensor);
+      kernel_content.GetInternalTensor(tflite_micro::kLstmInputToInputWeightsTensor);
   const TfLiteEvalTensor* input_to_forget_weights =
-      kernel_content.GetInternalTensor(tflite::kLstmInputToForgetWeightsTensor);
+      kernel_content.GetInternalTensor(tflite_micro::kLstmInputToForgetWeightsTensor);
   const TfLiteEvalTensor* input_to_cell_weights =
-      kernel_content.GetInternalTensor(tflite::kLstmInputToCellWeightsTensor);
+      kernel_content.GetInternalTensor(tflite_micro::kLstmInputToCellWeightsTensor);
   const TfLiteEvalTensor* recurrent_to_input_weights =
       kernel_content.GetInternalTensor(
-          tflite::kLstmRecurrentToInputWeightsTensor);
+          tflite_micro::kLstmRecurrentToInputWeightsTensor);
   const TfLiteEvalTensor* recurrent_to_forget_weights =
       kernel_content.GetInternalTensor(
-          tflite::kLstmRecurrentToForgetWeightsTensor);
+          tflite_micro::kLstmRecurrentToForgetWeightsTensor);
   const TfLiteEvalTensor* recurrent_to_cell_weights =
       kernel_content.GetInternalTensor(
-          tflite::kLstmRecurrentToCellWeightsTensor);
+          tflite_micro::kLstmRecurrentToCellWeightsTensor);
   const TfLiteEvalTensor* cell_to_input_weights =
-      kernel_content.GetInternalTensor(tflite::kLstmCellToInputWeightsTensor);
+      kernel_content.GetInternalTensor(tflite_micro::kLstmCellToInputWeightsTensor);
   const TfLiteEvalTensor* cell_to_forget_weights =
-      kernel_content.GetInternalTensor(tflite::kLstmCellToForgetWeightsTensor);
+      kernel_content.GetInternalTensor(tflite_micro::kLstmCellToForgetWeightsTensor);
   const TfLiteEvalTensor* cell_to_output_weights =
-      kernel_content.GetInternalTensor(tflite::kLstmCellToOutputWeightsTensor);
+      kernel_content.GetInternalTensor(tflite_micro::kLstmCellToOutputWeightsTensor);
   const TfLiteEvalTensor* cell_state =
-      kernel_content.GetInternalTensor(tflite::kLstmCellStateTensor);
+      kernel_content.GetInternalTensor(tflite_micro::kLstmCellStateTensor);
   const TfLiteEvalTensor* output_state =
-      kernel_content.GetInternalTensor(tflite::kLstmOutputStateTensor);
+      kernel_content.GetInternalTensor(tflite_micro::kLstmOutputStateTensor);
   const TfLiteEvalTensor* output = kernel_content.output_tensor;
 
   TFLITE_DCHECK(input->dims->size >= 2 && input->dims->size <= 3);
@@ -436,13 +436,13 @@ TfLiteStatus CMSIS_NN_EvalInteger8x8_16Lstm(
       op_data_lstm.cell_state_info.quantized_cell_clip;
 
   cmsis_lstm_params.input_gate_bias = const_cast<int32_t*>(
-      tflite::micro::GetOptionalTensorData<int32_t>(input_gate_bias));
+      tflite_micro::micro::GetOptionalTensorData<int32_t>(input_gate_bias));
   cmsis_lstm_params.forget_gate_bias = const_cast<int32_t*>(
-      tflite::micro::GetOptionalTensorData<int32_t>(forget_gate_bias));
+      tflite_micro::micro::GetOptionalTensorData<int32_t>(forget_gate_bias));
   cmsis_lstm_params.cell_gate_bias = const_cast<int32_t*>(
-      tflite::micro::GetOptionalTensorData<int32_t>(cell_gate_bias));
+      tflite_micro::micro::GetOptionalTensorData<int32_t>(cell_gate_bias));
   cmsis_lstm_params.output_gate_bias = const_cast<int32_t*>(
-      tflite::micro::GetOptionalTensorData<int32_t>(output_gate_bias));
+      tflite_micro::micro::GetOptionalTensorData<int32_t>(output_gate_bias));
 
   const bool time_major = op_data_lstm.size_info.time_major;
   const int n_input = input->dims->data[input->dims->size - 1];
@@ -465,34 +465,34 @@ TfLiteStatus CMSIS_NN_EvalInteger8x8_16Lstm(
 
   arm_lstm_unidirectional_s16_s8(
       &scratch_buffers,
-      const_cast<int8_t*>(tflite::micro::GetTensorData<int8_t>(input)),
+      const_cast<int8_t*>(tflite_micro::micro::GetTensorData<int8_t>(input)),
       &lstm_dims,
       const_cast<int8_t*>(
-          tflite::micro::GetOptionalTensorData<int8_t>(input_to_input_weights)),
-      const_cast<int8_t*>(tflite::micro::GetOptionalTensorData<int8_t>(
+          tflite_micro::micro::GetOptionalTensorData<int8_t>(input_to_input_weights)),
+      const_cast<int8_t*>(tflite_micro::micro::GetOptionalTensorData<int8_t>(
           input_to_forget_weights)),
       const_cast<int8_t*>(
-          tflite::micro::GetOptionalTensorData<int8_t>(input_to_cell_weights)),
-      const_cast<int8_t*>(tflite::micro::GetOptionalTensorData<int8_t>(
+          tflite_micro::micro::GetOptionalTensorData<int8_t>(input_to_cell_weights)),
+      const_cast<int8_t*>(tflite_micro::micro::GetOptionalTensorData<int8_t>(
           input_to_output_weights)),
-      const_cast<int8_t*>(tflite::micro::GetOptionalTensorData<int8_t>(
+      const_cast<int8_t*>(tflite_micro::micro::GetOptionalTensorData<int8_t>(
           recurrent_to_input_weights)),
-      const_cast<int8_t*>(tflite::micro::GetOptionalTensorData<int8_t>(
+      const_cast<int8_t*>(tflite_micro::micro::GetOptionalTensorData<int8_t>(
           recurrent_to_forget_weights)),
-      const_cast<int8_t*>(tflite::micro::GetOptionalTensorData<int8_t>(
+      const_cast<int8_t*>(tflite_micro::micro::GetOptionalTensorData<int8_t>(
           recurrent_to_cell_weights)),
-      const_cast<int8_t*>(tflite::micro::GetOptionalTensorData<int8_t>(
+      const_cast<int8_t*>(tflite_micro::micro::GetOptionalTensorData<int8_t>(
           recurrent_to_output_weights)),
       const_cast<int16_t*>(
-          tflite::micro::GetOptionalTensorData<int16_t>(cell_to_input_weights)),
-      const_cast<int16_t*>(tflite::micro::GetOptionalTensorData<int16_t>(
+          tflite_micro::micro::GetOptionalTensorData<int16_t>(cell_to_input_weights)),
+      const_cast<int16_t*>(tflite_micro::micro::GetOptionalTensorData<int16_t>(
           cell_to_forget_weights)),
-      const_cast<int16_t*>(tflite::micro::GetOptionalTensorData<int16_t>(
+      const_cast<int16_t*>(tflite_micro::micro::GetOptionalTensorData<int16_t>(
           cell_to_output_weights)),
       nullptr, &cmsis_lstm_params,
-      const_cast<int8_t*>(tflite::micro::GetTensorData<int8_t>(output_state)),
-      const_cast<int16_t*>(tflite::micro::GetTensorData<int16_t>(cell_state)),
-      const_cast<int8_t*>(tflite::micro::GetTensorData<int8_t>(output)));
+      const_cast<int8_t*>(tflite_micro::micro::GetTensorData<int8_t>(output_state)),
+      const_cast<int16_t*>(tflite_micro::micro::GetTensorData<int16_t>(cell_state)),
+      const_cast<int8_t*>(tflite_micro::micro::GetTensorData<int8_t>(output)));
 
   return kTfLiteOk;
 }
@@ -669,15 +669,15 @@ TfLiteStatus UnidirectionalSequenceLstmEvalInt8(TfLiteContext* context,
 }  // namespace
 
 TFLMRegistration Register_UNIDIRECTIONAL_SEQUENCE_LSTM() {
-  return tflite::micro::RegisterOp(UnidirectionalSequenceLstmInit,
+  return tflite_micro::micro::RegisterOp(UnidirectionalSequenceLstmInit,
                                    UnidirectionalSequenceLstmPrepare,
                                    UnidirectionalSequenceLstmEval);
 }
 
 TFLMRegistration Register_UNIDIRECTIONAL_SEQUENCE_LSTM_INT8() {
-  return tflite::micro::RegisterOp(UnidirectionalSequenceLstmInit,
+  return tflite_micro::micro::RegisterOp(UnidirectionalSequenceLstmInit,
                                    UnidirectionalSequenceLstmPrepare,
                                    UnidirectionalSequenceLstmEvalInt8);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/comparisons.cc b/tensorflow/lite/micro/kernels/comparisons.cc
index 4056316f..3526af04 100644
--- a/tensorflow/lite/micro/kernels/comparisons.cc
+++ b/tensorflow/lite/micro/kernels/comparisons.cc
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -38,82 +38,82 @@ TfLiteStatus EqualEval(TfLiteContext* context, TfLiteNode* node) {
   const OpData* data = static_cast<const OpData*>(node->user_data);
 
   const TfLiteEvalTensor* input1 =
-      tflite::micro::GetEvalInput(context, node, kInputTensor1);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor1);
   const TfLiteEvalTensor* input2 =
-      tflite::micro::GetEvalInput(context, node, kInputTensor2);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor2);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
-  RuntimeShape input1_shape = tflite::micro::GetTensorShape(input1);
-  RuntimeShape input2_shape = tflite::micro::GetTensorShape(input2);
-  RuntimeShape output_shape = tflite::micro::GetTensorShape(output);
-  bool* output_data = tflite::micro::GetTensorData<bool>(output);
+  RuntimeShape input1_shape = tflite_micro::micro::GetTensorShape(input1);
+  RuntimeShape input2_shape = tflite_micro::micro::GetTensorShape(input2);
+  RuntimeShape output_shape = tflite_micro::micro::GetTensorShape(output);
+  bool* output_data = tflite_micro::micro::GetTensorData<bool>(output);
 
-  bool requires_broadcast = !tflite::micro::HaveSameShapes(input1, input2);
+  bool requires_broadcast = !tflite_micro::micro::HaveSameShapes(input1, input2);
   switch (input1->type) {
     case kTfLiteBool:
       requires_broadcast
           ? reference_ops::Broadcast4DSlowEqualNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<bool>(input1), input2_shape,
-                tflite::micro::GetTensorData<bool>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<bool>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<bool>(input2), output_shape,
                 output_data)
           : reference_ops::EqualNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<bool>(input1), input2_shape,
-                tflite::micro::GetTensorData<bool>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<bool>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<bool>(input2), output_shape,
                 output_data);
       break;
     case kTfLiteFloat32:
       requires_broadcast
           ? reference_ops::Broadcast4DSlowEqualNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<float>(input1), input2_shape,
-                tflite::micro::GetTensorData<float>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<float>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<float>(input2), output_shape,
                 output_data)
           : reference_ops::EqualNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<float>(input1), input2_shape,
-                tflite::micro::GetTensorData<float>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<float>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<float>(input2), output_shape,
                 output_data);
       break;
     case kTfLiteInt32:
       requires_broadcast
           ? reference_ops::Broadcast4DSlowEqualNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int32_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int32_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int32_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int32_t>(input2), output_shape,
                 output_data)
           : reference_ops::EqualNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int32_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int32_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int32_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int32_t>(input2), output_shape,
                 output_data);
       break;
     case kTfLiteInt64:
       requires_broadcast
           ? reference_ops::Broadcast4DSlowEqualNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int64_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int64_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int64_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int64_t>(input2), output_shape,
                 output_data)
           : reference_ops::EqualNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int64_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int64_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int64_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int64_t>(input2), output_shape,
                 output_data);
       break;
     case kTfLiteInt8:
       requires_broadcast
           ? reference_ops::Broadcast4DSlowEqualWithScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int8_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int8_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int8_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int8_t>(input2), output_shape,
                 output_data)
           : reference_ops::EqualWithScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int8_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int8_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int8_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int8_t>(input2), output_shape,
                 output_data);
       break;
     default:
@@ -130,82 +130,82 @@ TfLiteStatus NotEqualEval(TfLiteContext* context, TfLiteNode* node) {
   const OpData* data = static_cast<const OpData*>(node->user_data);
 
   const TfLiteEvalTensor* input1 =
-      tflite::micro::GetEvalInput(context, node, kInputTensor1);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor1);
   const TfLiteEvalTensor* input2 =
-      tflite::micro::GetEvalInput(context, node, kInputTensor2);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor2);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
-  RuntimeShape input1_shape = tflite::micro::GetTensorShape(input1);
-  RuntimeShape input2_shape = tflite::micro::GetTensorShape(input2);
-  RuntimeShape output_shape = tflite::micro::GetTensorShape(output);
-  bool* output_data = tflite::micro::GetTensorData<bool>(output);
+  RuntimeShape input1_shape = tflite_micro::micro::GetTensorShape(input1);
+  RuntimeShape input2_shape = tflite_micro::micro::GetTensorShape(input2);
+  RuntimeShape output_shape = tflite_micro::micro::GetTensorShape(output);
+  bool* output_data = tflite_micro::micro::GetTensorData<bool>(output);
 
-  bool requires_broadcast = !tflite::micro::HaveSameShapes(input1, input2);
+  bool requires_broadcast = !tflite_micro::micro::HaveSameShapes(input1, input2);
   switch (input1->type) {
     case kTfLiteBool:
       requires_broadcast
           ? reference_ops::Broadcast4DSlowNotEqualNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<bool>(input1), input2_shape,
-                tflite::micro::GetTensorData<bool>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<bool>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<bool>(input2), output_shape,
                 output_data)
           : reference_ops::NotEqualNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<bool>(input1), input2_shape,
-                tflite::micro::GetTensorData<bool>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<bool>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<bool>(input2), output_shape,
                 output_data);
       break;
     case kTfLiteFloat32:
       requires_broadcast
           ? reference_ops::Broadcast4DSlowNotEqualNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<float>(input1), input2_shape,
-                tflite::micro::GetTensorData<float>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<float>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<float>(input2), output_shape,
                 output_data)
           : reference_ops::NotEqualNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<float>(input1), input2_shape,
-                tflite::micro::GetTensorData<float>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<float>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<float>(input2), output_shape,
                 output_data);
       break;
     case kTfLiteInt32:
       requires_broadcast
           ? reference_ops::Broadcast4DSlowNotEqualNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int32_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int32_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int32_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int32_t>(input2), output_shape,
                 output_data)
           : reference_ops::NotEqualNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int32_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int32_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int32_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int32_t>(input2), output_shape,
                 output_data);
       break;
     case kTfLiteInt64:
       requires_broadcast
           ? reference_ops::Broadcast4DSlowNotEqualNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int64_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int64_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int64_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int64_t>(input2), output_shape,
                 output_data)
           : reference_ops::NotEqualNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int64_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int64_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int64_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int64_t>(input2), output_shape,
                 output_data);
       break;
     case kTfLiteInt8:
       requires_broadcast
           ? reference_ops::Broadcast4DSlowNotEqualWithScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int8_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int8_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int8_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int8_t>(input2), output_shape,
                 output_data)
           : reference_ops::NotEqualWithScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int8_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int8_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int8_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int8_t>(input2), output_shape,
                 output_data);
       break;
     default:
@@ -221,69 +221,69 @@ TfLiteStatus GreaterEval(TfLiteContext* context, TfLiteNode* node) {
   const OpData* data = static_cast<const OpData*>(node->user_data);
 
   const TfLiteEvalTensor* input1 =
-      tflite::micro::GetEvalInput(context, node, kInputTensor1);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor1);
   const TfLiteEvalTensor* input2 =
-      tflite::micro::GetEvalInput(context, node, kInputTensor2);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor2);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
-  RuntimeShape input1_shape = tflite::micro::GetTensorShape(input1);
-  RuntimeShape input2_shape = tflite::micro::GetTensorShape(input2);
-  RuntimeShape output_shape = tflite::micro::GetTensorShape(output);
-  bool* output_data = tflite::micro::GetTensorData<bool>(output);
+  RuntimeShape input1_shape = tflite_micro::micro::GetTensorShape(input1);
+  RuntimeShape input2_shape = tflite_micro::micro::GetTensorShape(input2);
+  RuntimeShape output_shape = tflite_micro::micro::GetTensorShape(output);
+  bool* output_data = tflite_micro::micro::GetTensorData<bool>(output);
 
-  bool requires_broadcast = !tflite::micro::HaveSameShapes(input1, input2);
+  bool requires_broadcast = !tflite_micro::micro::HaveSameShapes(input1, input2);
   switch (input1->type) {
     case kTfLiteFloat32:
       requires_broadcast
           ? reference_ops::Broadcast4DSlowGreaterNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<float>(input1), input2_shape,
-                tflite::micro::GetTensorData<float>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<float>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<float>(input2), output_shape,
                 output_data)
           : reference_ops::GreaterNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<float>(input1), input2_shape,
-                tflite::micro::GetTensorData<float>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<float>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<float>(input2), output_shape,
                 output_data);
       break;
     case kTfLiteInt32:
       requires_broadcast
           ? reference_ops::Broadcast4DSlowGreaterNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int32_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int32_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int32_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int32_t>(input2), output_shape,
                 output_data)
           : reference_ops::GreaterNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int32_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int32_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int32_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int32_t>(input2), output_shape,
                 output_data);
       break;
     case kTfLiteInt64:
       requires_broadcast
           ? reference_ops::Broadcast4DSlowGreaterNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int64_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int64_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int64_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int64_t>(input2), output_shape,
                 output_data)
           : reference_ops::GreaterNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int64_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int64_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int64_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int64_t>(input2), output_shape,
                 output_data);
       break;
     case kTfLiteInt8:
       requires_broadcast
           ? reference_ops::Broadcast4DSlowGreaterWithScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int8_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int8_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int8_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int8_t>(input2), output_shape,
                 output_data)
           : reference_ops::GreaterWithScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int8_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int8_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int8_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int8_t>(input2), output_shape,
                 output_data);
       break;
     default:
@@ -299,69 +299,69 @@ TfLiteStatus GreaterEqualEval(TfLiteContext* context, TfLiteNode* node) {
   const OpData* data = static_cast<const OpData*>(node->user_data);
 
   const TfLiteEvalTensor* input1 =
-      tflite::micro::GetEvalInput(context, node, kInputTensor1);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor1);
   const TfLiteEvalTensor* input2 =
-      tflite::micro::GetEvalInput(context, node, kInputTensor2);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor2);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
-  RuntimeShape input1_shape = tflite::micro::GetTensorShape(input1);
-  RuntimeShape input2_shape = tflite::micro::GetTensorShape(input2);
-  RuntimeShape output_shape = tflite::micro::GetTensorShape(output);
-  bool* output_data = tflite::micro::GetTensorData<bool>(output);
+  RuntimeShape input1_shape = tflite_micro::micro::GetTensorShape(input1);
+  RuntimeShape input2_shape = tflite_micro::micro::GetTensorShape(input2);
+  RuntimeShape output_shape = tflite_micro::micro::GetTensorShape(output);
+  bool* output_data = tflite_micro::micro::GetTensorData<bool>(output);
 
-  bool requires_broadcast = !tflite::micro::HaveSameShapes(input1, input2);
+  bool requires_broadcast = !tflite_micro::micro::HaveSameShapes(input1, input2);
   switch (input1->type) {
     case kTfLiteFloat32:
       requires_broadcast
           ? reference_ops::Broadcast4DSlowGreaterEqualNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<float>(input1), input2_shape,
-                tflite::micro::GetTensorData<float>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<float>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<float>(input2), output_shape,
                 output_data)
           : reference_ops::GreaterEqualNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<float>(input1), input2_shape,
-                tflite::micro::GetTensorData<float>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<float>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<float>(input2), output_shape,
                 output_data);
       break;
     case kTfLiteInt32:
       requires_broadcast
           ? reference_ops::Broadcast4DSlowGreaterEqualNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int32_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int32_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int32_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int32_t>(input2), output_shape,
                 output_data)
           : reference_ops::GreaterEqualNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int32_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int32_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int32_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int32_t>(input2), output_shape,
                 output_data);
       break;
     case kTfLiteInt64:
       requires_broadcast
           ? reference_ops::Broadcast4DSlowGreaterEqualNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int64_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int64_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int64_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int64_t>(input2), output_shape,
                 output_data)
           : reference_ops::GreaterEqualNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int64_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int64_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int64_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int64_t>(input2), output_shape,
                 output_data);
       break;
     case kTfLiteInt8:
       requires_broadcast
           ? reference_ops::Broadcast4DSlowGreaterEqualWithScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int8_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int8_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int8_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int8_t>(input2), output_shape,
                 output_data)
           : reference_ops::GreaterEqualWithScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int8_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int8_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int8_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int8_t>(input2), output_shape,
                 output_data);
       break;
     default:
@@ -377,69 +377,69 @@ TfLiteStatus LessEval(TfLiteContext* context, TfLiteNode* node) {
   const OpData* data = static_cast<const OpData*>(node->user_data);
 
   const TfLiteEvalTensor* input1 =
-      tflite::micro::GetEvalInput(context, node, kInputTensor1);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor1);
   const TfLiteEvalTensor* input2 =
-      tflite::micro::GetEvalInput(context, node, kInputTensor2);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor2);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
-  RuntimeShape input1_shape = tflite::micro::GetTensorShape(input1);
-  RuntimeShape input2_shape = tflite::micro::GetTensorShape(input2);
-  RuntimeShape output_shape = tflite::micro::GetTensorShape(output);
-  bool* output_data = tflite::micro::GetTensorData<bool>(output);
+  RuntimeShape input1_shape = tflite_micro::micro::GetTensorShape(input1);
+  RuntimeShape input2_shape = tflite_micro::micro::GetTensorShape(input2);
+  RuntimeShape output_shape = tflite_micro::micro::GetTensorShape(output);
+  bool* output_data = tflite_micro::micro::GetTensorData<bool>(output);
 
-  bool requires_broadcast = !tflite::micro::HaveSameShapes(input1, input2);
+  bool requires_broadcast = !tflite_micro::micro::HaveSameShapes(input1, input2);
   switch (input1->type) {
     case kTfLiteFloat32:
       requires_broadcast
           ? reference_ops::Broadcast4DSlowLessNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<float>(input1), input2_shape,
-                tflite::micro::GetTensorData<float>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<float>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<float>(input2), output_shape,
                 output_data)
           : reference_ops::LessNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<float>(input1), input2_shape,
-                tflite::micro::GetTensorData<float>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<float>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<float>(input2), output_shape,
                 output_data);
       break;
     case kTfLiteInt32:
       requires_broadcast
           ? reference_ops::Broadcast4DSlowLessNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int32_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int32_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int32_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int32_t>(input2), output_shape,
                 output_data)
           : reference_ops::LessNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int32_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int32_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int32_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int32_t>(input2), output_shape,
                 output_data);
       break;
     case kTfLiteInt64:
       requires_broadcast
           ? reference_ops::Broadcast4DSlowLessNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int64_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int64_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int64_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int64_t>(input2), output_shape,
                 output_data)
           : reference_ops::LessNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int64_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int64_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int64_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int64_t>(input2), output_shape,
                 output_data);
       break;
     case kTfLiteInt8:
       requires_broadcast
           ? reference_ops::Broadcast4DSlowLessWithScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int8_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int8_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int8_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int8_t>(input2), output_shape,
                 output_data)
           : reference_ops::LessWithScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int8_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int8_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int8_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int8_t>(input2), output_shape,
                 output_data);
       break;
     default:
@@ -455,69 +455,69 @@ TfLiteStatus LessEqualEval(TfLiteContext* context, TfLiteNode* node) {
   const OpData* data = static_cast<const OpData*>(node->user_data);
 
   const TfLiteEvalTensor* input1 =
-      tflite::micro::GetEvalInput(context, node, kInputTensor1);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor1);
   const TfLiteEvalTensor* input2 =
-      tflite::micro::GetEvalInput(context, node, kInputTensor2);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor2);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
-  RuntimeShape input1_shape = tflite::micro::GetTensorShape(input1);
-  RuntimeShape input2_shape = tflite::micro::GetTensorShape(input2);
-  RuntimeShape output_shape = tflite::micro::GetTensorShape(output);
-  bool* output_data = tflite::micro::GetTensorData<bool>(output);
+  RuntimeShape input1_shape = tflite_micro::micro::GetTensorShape(input1);
+  RuntimeShape input2_shape = tflite_micro::micro::GetTensorShape(input2);
+  RuntimeShape output_shape = tflite_micro::micro::GetTensorShape(output);
+  bool* output_data = tflite_micro::micro::GetTensorData<bool>(output);
 
-  bool requires_broadcast = !tflite::micro::HaveSameShapes(input1, input2);
+  bool requires_broadcast = !tflite_micro::micro::HaveSameShapes(input1, input2);
   switch (input1->type) {
     case kTfLiteFloat32:
       requires_broadcast
           ? reference_ops::Broadcast4DSlowLessEqualNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<float>(input1), input2_shape,
-                tflite::micro::GetTensorData<float>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<float>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<float>(input2), output_shape,
                 output_data)
           : reference_ops::LessEqualNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<float>(input1), input2_shape,
-                tflite::micro::GetTensorData<float>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<float>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<float>(input2), output_shape,
                 output_data);
       break;
     case kTfLiteInt32:
       requires_broadcast
           ? reference_ops::Broadcast4DSlowLessEqualNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int32_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int32_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int32_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int32_t>(input2), output_shape,
                 output_data)
           : reference_ops::LessEqualNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int32_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int32_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int32_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int32_t>(input2), output_shape,
                 output_data);
       break;
     case kTfLiteInt64:
       requires_broadcast
           ? reference_ops::Broadcast4DSlowLessEqualNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int64_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int64_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int64_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int64_t>(input2), output_shape,
                 output_data)
           : reference_ops::LessEqualNoScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int64_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int64_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int64_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int64_t>(input2), output_shape,
                 output_data);
       break;
     case kTfLiteInt8:
       requires_broadcast
           ? reference_ops::Broadcast4DSlowLessEqualWithScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int8_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int8_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int8_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int8_t>(input2), output_shape,
                 output_data)
           : reference_ops::LessEqualWithScaling(
                 data->params, input1_shape,
-                tflite::micro::GetTensorData<int8_t>(input1), input2_shape,
-                tflite::micro::GetTensorData<int8_t>(input2), output_shape,
+                tflite_micro::micro::GetTensorData<int8_t>(input1), input2_shape,
+                tflite_micro::micro::GetTensorData<int8_t>(input2), output_shape,
                 output_data);
       break;
     default:
@@ -580,27 +580,27 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_EQUAL() {
-  return tflite::micro::RegisterOp(Init, Prepare, EqualEval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, EqualEval);
 }
 
 TFLMRegistration Register_NOT_EQUAL() {
-  return tflite::micro::RegisterOp(Init, Prepare, NotEqualEval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, NotEqualEval);
 }
 
 TFLMRegistration Register_GREATER() {
-  return tflite::micro::RegisterOp(Init, Prepare, GreaterEval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, GreaterEval);
 }
 
 TFLMRegistration Register_GREATER_EQUAL() {
-  return tflite::micro::RegisterOp(Init, Prepare, GreaterEqualEval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, GreaterEqualEval);
 }
 
 TFLMRegistration Register_LESS() {
-  return tflite::micro::RegisterOp(Init, Prepare, LessEval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, LessEval);
 }
 
 TFLMRegistration Register_LESS_EQUAL() {
-  return tflite::micro::RegisterOp(Init, Prepare, LessEqualEval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, LessEqualEval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/comparisons_test.cc b/tensorflow/lite/micro/kernels/comparisons_test.cc
index eec57d62..4b4533e4 100644
--- a/tensorflow/lite/micro/kernels/comparisons_test.cc
+++ b/tensorflow/lite/micro/kernels/comparisons_test.cc
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -128,7 +128,7 @@ void TestComparisonQuantizedInt8(const TFLMRegistration& registration,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -143,7 +143,7 @@ TF_LITE_MICRO_TEST(EqualBool) {
   int expected_dim[] = {4, 1, 1, 1, 4};
 
   bool output_data[4];
-  tflite::testing::TestComparisonBool(tflite::Register_EQUAL(), input1_dim,
+  tflite_micro::testing::TestComparisonBool(tflite_micro::Register_EQUAL(), input1_dim,
                                       input1_data, input2_dim, input2_data,
                                       expected_data, expected_dim, output_data);
 }
@@ -159,8 +159,8 @@ TF_LITE_MICRO_TEST(EqualFloat) {
   int expected_dim[] = {4, 1, 1, 1, 4};
 
   bool output_data[4];
-  tflite::testing::TestComparisonFloat(
-      tflite::Register_EQUAL(), input1_dim, input1_data, input2_dim,
+  tflite_micro::testing::TestComparisonFloat(
+      tflite_micro::Register_EQUAL(), input1_dim, input1_data, input2_dim,
       input2_data, expected_data, expected_dim, output_data);
 }
 
@@ -174,7 +174,7 @@ TF_LITE_MICRO_TEST(EqualInt) {
   bool expected_data[] = {false, false, true, false};
   int expected_dim[] = {4, 1, 1, 1, 4};
   bool output_data[4];
-  tflite::testing::TestComparisonInt(tflite::Register_EQUAL(), input1_dim,
+  tflite_micro::testing::TestComparisonInt(tflite_micro::Register_EQUAL(), input1_dim,
                                      input1_data, input2_dim, input2_data,
                                      expected_data, expected_dim, output_data);
 }
@@ -190,7 +190,7 @@ TF_LITE_MICRO_TEST(EqualBroadcast) {
   int expected_dim[] = {4, 1, 1, 1, 4};
 
   bool output_data[4];
-  tflite::testing::TestComparisonInt(tflite::Register_EQUAL(), input1_dim,
+  tflite_micro::testing::TestComparisonInt(tflite_micro::Register_EQUAL(), input1_dim,
                                      input1_data, input2_dim, input2_data,
                                      expected_data, expected_dim, output_data);
 }
@@ -207,7 +207,7 @@ TF_LITE_MICRO_TEST(EqualBroadcastTwoD) {
   int expected_dim[] = {4, 1, 1, 2, 4};
 
   bool output_data[8];
-  tflite::testing::TestComparisonInt(tflite::Register_EQUAL(), input1_dim,
+  tflite_micro::testing::TestComparisonInt(tflite_micro::Register_EQUAL(), input1_dim,
                                      input1_data, input2_dim, input2_data,
                                      expected_data, expected_dim, output_data);
 }
@@ -223,7 +223,7 @@ TF_LITE_MICRO_TEST(NotEqualBool) {
   int expected_dim[] = {4, 1, 1, 1, 4};
 
   bool output_data[4];
-  tflite::testing::TestComparisonBool(tflite::Register_NOT_EQUAL(), input1_dim,
+  tflite_micro::testing::TestComparisonBool(tflite_micro::Register_NOT_EQUAL(), input1_dim,
                                       input1_data, input2_dim, input2_data,
                                       expected_data, expected_dim, output_data);
 }
@@ -239,8 +239,8 @@ TF_LITE_MICRO_TEST(NotEqualFloat) {
   int expected_dim[] = {4, 1, 1, 1, 4};
 
   bool output_data[4];
-  tflite::testing::TestComparisonFloat(
-      tflite::Register_NOT_EQUAL(), input1_dim, input1_data, input2_dim,
+  tflite_micro::testing::TestComparisonFloat(
+      tflite_micro::Register_NOT_EQUAL(), input1_dim, input1_data, input2_dim,
       input2_data, expected_data, expected_dim, output_data);
 }
 
@@ -255,7 +255,7 @@ TF_LITE_MICRO_TEST(NotEqualInt) {
   int expected_dim[] = {4, 1, 1, 1, 4};
 
   bool output_data[4];
-  tflite::testing::TestComparisonInt(tflite::Register_NOT_EQUAL(), input1_dim,
+  tflite_micro::testing::TestComparisonInt(tflite_micro::Register_NOT_EQUAL(), input1_dim,
                                      input1_data, input2_dim, input2_data,
                                      expected_data, expected_dim, output_data);
 }
@@ -271,7 +271,7 @@ TF_LITE_MICRO_TEST(NotEqualBroadcast) {
   int expected_dim[] = {4, 1, 1, 1, 4};
 
   bool output_data[4];
-  tflite::testing::TestComparisonInt(tflite::Register_NOT_EQUAL(), input1_dim,
+  tflite_micro::testing::TestComparisonInt(tflite_micro::Register_NOT_EQUAL(), input1_dim,
                                      input1_data, input2_dim, input2_data,
                                      expected_data, expected_dim, output_data);
 }
@@ -287,7 +287,7 @@ TF_LITE_MICRO_TEST(NotEqualBroadcastTwoD) {
   int expected_dim[] = {4, 1, 1, 2, 4};
 
   bool output_data[8];
-  tflite::testing::TestComparisonInt(tflite::Register_NOT_EQUAL(), input1_dim,
+  tflite_micro::testing::TestComparisonInt(tflite_micro::Register_NOT_EQUAL(), input1_dim,
                                      input1_data, input2_dim, input2_data,
                                      expected_data, expected_dim, output_data);
 }
@@ -303,8 +303,8 @@ TF_LITE_MICRO_TEST(GreaterFloat) {
   int expected_dim[] = {4, 1, 1, 1, 4};
 
   bool output_data[4];
-  tflite::testing::TestComparisonFloat(
-      tflite::Register_GREATER(), input1_dim, input1_data, input2_dim,
+  tflite_micro::testing::TestComparisonFloat(
+      tflite_micro::Register_GREATER(), input1_dim, input1_data, input2_dim,
       input2_data, expected_data, expected_dim, output_data);
 }
 
@@ -319,7 +319,7 @@ TF_LITE_MICRO_TEST(GreaterInt) {
   int expected_dim[] = {4, 1, 1, 1, 4};
 
   bool output_data[4];
-  tflite::testing::TestComparisonInt(tflite::Register_GREATER(), input1_dim,
+  tflite_micro::testing::TestComparisonInt(tflite_micro::Register_GREATER(), input1_dim,
                                      input1_data, input2_dim, input2_data,
                                      expected_data, expected_dim, output_data);
 }
@@ -335,7 +335,7 @@ TF_LITE_MICRO_TEST(GreaterBroadcast) {
   int expected_dim[] = {4, 1, 1, 1, 4};
 
   bool output_data[4];
-  tflite::testing::TestComparisonInt(tflite::Register_GREATER(), input1_dim,
+  tflite_micro::testing::TestComparisonInt(tflite_micro::Register_GREATER(), input1_dim,
                                      input1_data, input2_dim, input2_data,
                                      expected_data, expected_dim, output_data);
 }
@@ -351,7 +351,7 @@ TF_LITE_MICRO_TEST(GreaterBroadcastTwoD) {
   int expected_dim[] = {4, 1, 1, 2, 4};
 
   bool output_data[8];
-  tflite::testing::TestComparisonInt(tflite::Register_GREATER(), input1_dim,
+  tflite_micro::testing::TestComparisonInt(tflite_micro::Register_GREATER(), input1_dim,
                                      input1_data, input2_dim, input2_data,
                                      expected_data, expected_dim, output_data);
 }
@@ -367,8 +367,8 @@ TF_LITE_MICRO_TEST(GreaterEqualFloat) {
   int expected_dim[] = {4, 1, 1, 1, 4};
 
   bool output_data[4];
-  tflite::testing::TestComparisonFloat(
-      tflite::Register_GREATER_EQUAL(), input1_dim, input1_data, input2_dim,
+  tflite_micro::testing::TestComparisonFloat(
+      tflite_micro::Register_GREATER_EQUAL(), input1_dim, input1_data, input2_dim,
       input2_data, expected_data, expected_dim, output_data);
 }
 
@@ -383,8 +383,8 @@ TF_LITE_MICRO_TEST(GreaterEqualInt) {
   int expected_dim[] = {4, 1, 1, 1, 4};
 
   bool output_data[4];
-  tflite::testing::TestComparisonInt(
-      tflite::Register_GREATER_EQUAL(), input1_dim, input1_data, input2_dim,
+  tflite_micro::testing::TestComparisonInt(
+      tflite_micro::Register_GREATER_EQUAL(), input1_dim, input1_data, input2_dim,
       input2_data, expected_data, expected_dim, output_data);
 }
 
@@ -399,8 +399,8 @@ TF_LITE_MICRO_TEST(GreaterEqualBroadcast) {
   int expected_dim[] = {4, 1, 1, 1, 4};
 
   bool output_data[4];
-  tflite::testing::TestComparisonInt(
-      tflite::Register_GREATER_EQUAL(), input1_dim, input1_data, input2_dim,
+  tflite_micro::testing::TestComparisonInt(
+      tflite_micro::Register_GREATER_EQUAL(), input1_dim, input1_data, input2_dim,
       input2_data, expected_data, expected_dim, output_data);
 }
 
@@ -415,8 +415,8 @@ TF_LITE_MICRO_TEST(GreaterEqualBroadcastTwoD) {
   int expected_dim[] = {4, 1, 1, 2, 4};
 
   bool output_data[8];
-  tflite::testing::TestComparisonInt(
-      tflite::Register_GREATER_EQUAL(), input1_dim, input1_data, input2_dim,
+  tflite_micro::testing::TestComparisonInt(
+      tflite_micro::Register_GREATER_EQUAL(), input1_dim, input1_data, input2_dim,
       input2_data, expected_data, expected_dim, output_data);
 }
 
@@ -431,8 +431,8 @@ TF_LITE_MICRO_TEST(LessFloat) {
   int expected_dim[] = {4, 1, 1, 1, 4};
 
   bool output_data[4];
-  tflite::testing::TestComparisonFloat(
-      tflite::Register_LESS(), input1_dim, input1_data, input2_dim, input2_data,
+  tflite_micro::testing::TestComparisonFloat(
+      tflite_micro::Register_LESS(), input1_dim, input1_data, input2_dim, input2_data,
       expected_data, expected_dim, output_data);
 }
 
@@ -447,7 +447,7 @@ TF_LITE_MICRO_TEST(LessInt) {
   int expected_dim[] = {4, 1, 1, 1, 4};
 
   bool output_data[4];
-  tflite::testing::TestComparisonInt(tflite::Register_LESS(), input1_dim,
+  tflite_micro::testing::TestComparisonInt(tflite_micro::Register_LESS(), input1_dim,
                                      input1_data, input2_dim, input2_data,
                                      expected_data, expected_dim, output_data);
 }
@@ -463,7 +463,7 @@ TF_LITE_MICRO_TEST(LessBroadcast) {
   int expected_dim[] = {4, 1, 1, 1, 4};
 
   bool output_data[4];
-  tflite::testing::TestComparisonInt(tflite::Register_LESS(), input1_dim,
+  tflite_micro::testing::TestComparisonInt(tflite_micro::Register_LESS(), input1_dim,
                                      input1_data, input2_dim, input2_data,
                                      expected_data, expected_dim, output_data);
 }
@@ -479,7 +479,7 @@ TF_LITE_MICRO_TEST(LessBroadcastTwoD) {
   int expected_dim[] = {4, 1, 1, 2, 4};
 
   bool output_data[8];
-  tflite::testing::TestComparisonInt(tflite::Register_LESS(), input1_dim,
+  tflite_micro::testing::TestComparisonInt(tflite_micro::Register_LESS(), input1_dim,
                                      input1_data, input2_dim, input2_data,
                                      expected_data, expected_dim, output_data);
 }
@@ -495,8 +495,8 @@ TF_LITE_MICRO_TEST(LessEqualFloat) {
   int expected_dim[] = {4, 1, 1, 1, 4};
 
   bool output_data[4];
-  tflite::testing::TestComparisonFloat(
-      tflite::Register_LESS_EQUAL(), input1_dim, input1_data, input2_dim,
+  tflite_micro::testing::TestComparisonFloat(
+      tflite_micro::Register_LESS_EQUAL(), input1_dim, input1_data, input2_dim,
       input2_data, expected_data, expected_dim, output_data);
 }
 
@@ -511,7 +511,7 @@ TF_LITE_MICRO_TEST(LessEqualInt) {
   int expected_dim[] = {4, 1, 1, 1, 4};
 
   bool output_data[4];
-  tflite::testing::TestComparisonInt(tflite::Register_LESS_EQUAL(), input1_dim,
+  tflite_micro::testing::TestComparisonInt(tflite_micro::Register_LESS_EQUAL(), input1_dim,
                                      input1_data, input2_dim, input2_data,
                                      expected_data, expected_dim, output_data);
 }
@@ -527,7 +527,7 @@ TF_LITE_MICRO_TEST(LessEqualBroadcast) {
   int expected_dim[] = {4, 1, 1, 1, 4};
 
   bool output_data[4];
-  tflite::testing::TestComparisonInt(tflite::Register_LESS_EQUAL(), input1_dim,
+  tflite_micro::testing::TestComparisonInt(tflite_micro::Register_LESS_EQUAL(), input1_dim,
                                      input1_data, input2_dim, input2_data,
                                      expected_data, expected_dim, output_data);
 }
@@ -543,7 +543,7 @@ TF_LITE_MICRO_TEST(LessEqualBroadcastTwoD) {
   int expected_dim[] = {4, 1, 1, 2, 4};
 
   bool output_data[8];
-  tflite::testing::TestComparisonInt(tflite::Register_LESS_EQUAL(), input1_dim,
+  tflite_micro::testing::TestComparisonInt(tflite_micro::Register_LESS_EQUAL(), input1_dim,
                                      input1_data, input2_dim, input2_data,
                                      expected_data, expected_dim, output_data);
 }
@@ -566,8 +566,8 @@ TF_LITE_MICRO_TEST(EqualQuantizedInt8) {
   int8_t input2_quantized[4];
 
   bool output_data[4];
-  tflite::testing::TestComparisonQuantizedInt8(
-      tflite::Register_EQUAL(), input1_dim, input1_data, input1_quantized,
+  tflite_micro::testing::TestComparisonQuantizedInt8(
+      tflite_micro::Register_EQUAL(), input1_dim, input1_data, input1_quantized,
       input1_scale, input1_zero_point, input2_dim, input2_data,
       input2_quantized, input2_scale, input2_zero_point, expected_data,
       expected_dim, output_data);
@@ -591,8 +591,8 @@ TF_LITE_MICRO_TEST(NotEqualQuantizedInt8) {
   int8_t input2_quantized[4];
 
   bool output_data[4];
-  tflite::testing::TestComparisonQuantizedInt8(
-      tflite::Register_NOT_EQUAL(), input1_dim, input1_data, input1_quantized,
+  tflite_micro::testing::TestComparisonQuantizedInt8(
+      tflite_micro::Register_NOT_EQUAL(), input1_dim, input1_data, input1_quantized,
       input1_scale, input1_zero_point, input2_dim, input2_data,
       input2_quantized, input2_scale, input2_zero_point, expected_data,
       expected_dim, output_data);
@@ -619,8 +619,8 @@ TF_LITE_MICRO_TEST(NotEqualQuantizedInt8WithBroadcast) {
     int8_t input2_quantized[6];
 
     bool output_data[6];
-    tflite::testing::TestComparisonQuantizedInt8(
-        tflite::Register_NOT_EQUAL(), input1_dim, input1_data, input1_quantized,
+    tflite_micro::testing::TestComparisonQuantizedInt8(
+        tflite_micro::Register_NOT_EQUAL(), input1_dim, input1_data, input1_quantized,
         input1_scale, input1_zero_point, input2_dim, input2_data,
         input2_quantized, input1_scale, input1_zero_point, expected_data,
         expected_dim, output_data);
@@ -648,8 +648,8 @@ TF_LITE_MICRO_TEST(GreaterQuantizedInt8WithBroadcast) {
     int8_t input2_quantized[6];
 
     bool output_data[6];
-    tflite::testing::TestComparisonQuantizedInt8(
-        tflite::Register_GREATER(), input1_dim, input1_data, input1_quantized,
+    tflite_micro::testing::TestComparisonQuantizedInt8(
+        tflite_micro::Register_GREATER(), input1_dim, input1_data, input1_quantized,
         input1_scale, input1_zero_point, input2_dim, input2_data,
         input2_quantized, input1_scale, input1_zero_point, expected_data,
         expected_dim, output_data);
@@ -677,8 +677,8 @@ TF_LITE_MICRO_TEST(GreaterEqualQuantizedInt8WithBroadcast) {
     int8_t input2_quantized[6];
 
     bool output_data[6];
-    tflite::testing::TestComparisonQuantizedInt8(
-        tflite::Register_GREATER_EQUAL(), input1_dim, input1_data,
+    tflite_micro::testing::TestComparisonQuantizedInt8(
+        tflite_micro::Register_GREATER_EQUAL(), input1_dim, input1_data,
         input1_quantized, input1_scale, input1_zero_point, input2_dim,
         input2_data, input2_quantized, input1_scale, input1_zero_point,
         expected_data, expected_dim, output_data);
@@ -706,8 +706,8 @@ TF_LITE_MICRO_TEST(LessQuantizedInt8WithBroadcast) {
     int8_t input2_quantized[6];
 
     bool output_data[6];
-    tflite::testing::TestComparisonQuantizedInt8(
-        tflite::Register_LESS(), input1_dim, input1_data, input1_quantized,
+    tflite_micro::testing::TestComparisonQuantizedInt8(
+        tflite_micro::Register_LESS(), input1_dim, input1_data, input1_quantized,
         input1_scale, input1_zero_point, input2_dim, input2_data,
         input2_quantized, input1_scale, input1_zero_point, expected_data,
         expected_dim, output_data);
@@ -735,8 +735,8 @@ TF_LITE_MICRO_TEST(LessEqualQuantizedInt8WithBroadcast) {
     int8_t input2_quantized[6];
 
     bool output_data[6];
-    tflite::testing::TestComparisonQuantizedInt8(
-        tflite::Register_LESS_EQUAL(), input1_dim, input1_data,
+    tflite_micro::testing::TestComparisonQuantizedInt8(
+        tflite_micro::Register_LESS_EQUAL(), input1_dim, input1_data,
         input1_quantized, input1_scale, input1_zero_point, input2_dim,
         input2_data, input2_quantized, input1_scale, input1_zero_point,
         expected_data, expected_dim, output_data);
diff --git a/tensorflow/lite/micro/kernels/concatenation.cc b/tensorflow/lite/micro/kernels/concatenation.cc
index b4a838f7..cbede447 100644
--- a/tensorflow/lite/micro/kernels/concatenation.cc
+++ b/tensorflow/lite/micro/kernels/concatenation.cc
@@ -24,11 +24,11 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
-constexpr int kMaxInputNum = 10;  // Maximum number of input tensors
+constexpr int kMaxInputNum = 40;  // Maximum number of input tensors
 constexpr int kOutputTensor = 0;
 
 struct OpData {
@@ -55,8 +55,8 @@ inline void GetAllInputTensorShapes(const TfLiteContext* context,
   TFLITE_DCHECK(context != nullptr);
   TFLITE_DCHECK(node != nullptr);
   for (int i = 0; i < node->inputs->size; ++i) {
-    const TfLiteEvalTensor* t = tflite::micro::GetEvalInput(context, node, i);
-    RuntimeShape shape = tflite::micro::GetTensorShape(t);
+    const TfLiteEvalTensor* t = tflite_micro::micro::GetEvalInput(context, node, i);
+    RuntimeShape shape = tflite_micro::micro::GetTensorShape(t);
     all_shapes[i].ReplaceWith(shape.DimensionsCount(), shape.DimsData());
   }
 }
@@ -77,8 +77,8 @@ inline void GetAllInputTensorData(const TfLiteContext* context,
   TFLITE_DCHECK(context != nullptr);
   TFLITE_DCHECK(node != nullptr);
   for (int i = 0; i < node->inputs->size; ++i) {
-    const TfLiteEvalTensor* t = tflite::micro::GetEvalInput(context, node, i);
-    all_data[i] = tflite::micro::GetTensorData<T>(t);
+    const TfLiteEvalTensor* t = tflite_micro::micro::GetEvalInput(context, node, i);
+    all_data[i] = tflite_micro::micro::GetTensorData<T>(t);
   }
 }
 
@@ -93,14 +93,14 @@ void EvalUnquantized(TfLiteContext* context, TfLiteNode* node) {
   GetAllInputTensorData(context, node, inputs_data);
 
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
   TFLITE_DCHECK(node->user_data != nullptr);
   const OpData* data = static_cast<const OpData*>(node->user_data);
 
   reference_ops::Concatenation(data->params, inputs_shape_ptr, inputs_data,
-                               tflite::micro::GetTensorShape(output),
-                               tflite::micro::GetTensorData<data_type>(output));
+                               tflite_micro::micro::GetTensorShape(output),
+                               tflite_micro::micro::GetTensorData<data_type>(output));
 }
 
 void* Init(TfLiteContext* context, const char* buffer, size_t length) {
@@ -216,7 +216,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* output_tensor =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
   TF_LITE_ENSURE(context, output_tensor != nullptr);
   TfLiteType output_type = output_tensor->type;
 
@@ -252,7 +252,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_CONCATENATION() {
-  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/concatenation_test.cc b/tensorflow/lite/micro/kernels/concatenation_test.cc
index ddbc74d4..6e5689dc 100644
--- a/tensorflow/lite/micro/kernels/concatenation_test.cc
+++ b/tensorflow/lite/micro/kernels/concatenation_test.cc
@@ -20,7 +20,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -96,7 +96,7 @@ void TestConcatenateTwoFloatInputs(
   TestConcatenateTwoInputs(input1_dims_data, input1_data, input2_dims_data,
                            input2_data, axis, output_dims_data, output_data);
 
-  TfLiteIntArray* dims = tflite::testing::IntArrayFromInts(output_dims_data);
+  TfLiteIntArray* dims = tflite_micro::testing::IntArrayFromInts(output_dims_data);
   const int output_dims_count = ElementCount(*dims);
   for (int i = 0; i < output_dims_count; ++i) {
     TF_LITE_MICRO_EXPECT_NEAR(expected_output_data[i], output_data[i], 1e-5f);
@@ -150,7 +150,7 @@ void TestConcatenateQuantizedTwoInputs(
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -161,11 +161,11 @@ TF_LITE_MICRO_TEST(BoolTypeOneInput) {
   int axis = 1;
 
   bool output_data[4];
-  tflite::testing::TestConcatenateOneInput(input_shape, input_value, axis,
+  tflite_micro::testing::TestConcatenateOneInput(input_shape, input_value, axis,
                                            output_shape, output_data);
 
-  TfLiteIntArray* dims = tflite::testing::IntArrayFromInts(output_shape);
-  const int output_dims_count = tflite::ElementCount(*dims);
+  TfLiteIntArray* dims = tflite_micro::testing::IntArrayFromInts(output_shape);
+  const int output_dims_count = tflite_micro::ElementCount(*dims);
   for (int i = 0; i < output_dims_count; ++i) {
     TF_LITE_MICRO_EXPECT_EQ(input_value[i], output_data[i]);
   }
@@ -186,12 +186,12 @@ TF_LITE_MICRO_TEST(BoolTypeTwoInputs) {
   int output_shape[] = {3, 2, 4, 2};
   bool output_data[16];
 
-  tflite::testing::TestConcatenateTwoInputs(input1_shape, input1_value,
+  tflite_micro::testing::TestConcatenateTwoInputs(input1_shape, input1_value,
                                             input2_shape, input2_value, axis,
                                             output_shape, output_data);
 
-  TfLiteIntArray* dims = tflite::testing::IntArrayFromInts(output_shape);
-  const int output_dims_count = tflite::ElementCount(*dims);
+  TfLiteIntArray* dims = tflite_micro::testing::IntArrayFromInts(output_shape);
+  const int output_dims_count = tflite_micro::ElementCount(*dims);
   for (int i = 0; i < output_dims_count; ++i) {
     TF_LITE_MICRO_EXPECT_EQ(expected_output[i], output_data[i]);
   }
@@ -217,22 +217,22 @@ TF_LITE_MICRO_TEST(TwoInputsAllAxesCombinations) {
   float output_data[12];
 
   // Axis = 0
-  tflite::testing::TestConcatenateTwoFloatInputs(
+  tflite_micro::testing::TestConcatenateTwoFloatInputs(
       input_shape, input1_value, input_shape, input2_value, /* axis */ 0,
       output_shape_axis0, output_value_axis0, output_data);
 
   // Axis = -2 (equivalent to axis = 0)
-  tflite::testing::TestConcatenateTwoFloatInputs(
+  tflite_micro::testing::TestConcatenateTwoFloatInputs(
       input_shape, input1_value, input_shape, input2_value, /* axis */ -2,
       output_shape_axis0, output_value_axis0, output_data);
 
   // Axis = 1
-  tflite::testing::TestConcatenateTwoFloatInputs(
+  tflite_micro::testing::TestConcatenateTwoFloatInputs(
       input_shape, input1_value, input_shape, input2_value, /* axis */ 1,
       output_shape_axis1, output_value_axis1, output_data);
 
   // Axis = -1 (equivalent to axis = 1)
-  tflite::testing::TestConcatenateTwoFloatInputs(
+  tflite_micro::testing::TestConcatenateTwoFloatInputs(
       input_shape, input1_value, input_shape, input2_value, /* axis */ -1,
       output_shape_axis1, output_value_axis1, output_data);
 }
@@ -254,7 +254,7 @@ TF_LITE_MICRO_TEST(TwoInputsQuantizedInt8) {
   const int8_t output_value[] = {1, 2, 5, 6, 3, 4, 7, 8};
 
   int8_t output_data[8];
-  tflite::testing::TestConcatenateQuantizedTwoInputs(
+  tflite_micro::testing::TestConcatenateQuantizedTwoInputs(
       input_shape, input1_values, input_shape, input2_values, input_scale,
       input_zero_point, axis, output_shape, output_value, output_scale,
       output_zero_point, output_data);
@@ -277,7 +277,7 @@ TF_LITE_MICRO_TEST(TwoInputsQuantizedInt16) {
   const int16_t output_value[] = {1, 2, 5, 6, 3, 4, 7, 8};
 
   int16_t output_data[8];
-  tflite::testing::TestConcatenateQuantizedTwoInputs(
+  tflite_micro::testing::TestConcatenateQuantizedTwoInputs(
       input_shape, input1_values, input_shape, input2_values, input_scale,
       input_zero_point, axis, output_shape, output_value, output_scale,
       output_zero_point, output_data);
@@ -298,7 +298,7 @@ TF_LITE_MICRO_TEST(ThreeDimensionalTwoInputsDifferentShapes) {
                                  9.0f, 10.0f, 11.0f, 12.0f};
 
   float output_data[16];
-  tflite::testing::TestConcatenateTwoFloatInputs(
+  tflite_micro::testing::TestConcatenateTwoFloatInputs(
       input1_shape, input1_values, input2_shape, input2_values, axis,
       output_shape, output_values, output_data);
 }
@@ -320,7 +320,7 @@ TF_LITE_MICRO_TEST(TwoInputsFiveDimensionsAllAxesCombinations) {
       1.0f,  2.0f,  3.0f,  4.0f,  5.0f,  6.0f,  7.0f,  8.0f,
       9.0f,  10.0f, 11.0f, 12.0f, 13.0f, 14.0f, 15.0f, 16.0f,
       17.0f, 18.0f, 19.0f, 20.0f, 21.0f, 22.0f, 23.0f, 24.0f};
-  tflite::testing::TestConcatenateTwoFloatInputs(
+  tflite_micro::testing::TestConcatenateTwoFloatInputs(
       input_shape, input1_value, input_shape, input2_value, /* axis */ 0,
       output_shape_axis0, output_value_axis0, output_data);
 
@@ -330,7 +330,7 @@ TF_LITE_MICRO_TEST(TwoInputsFiveDimensionsAllAxesCombinations) {
       1.0f,  2.0f,  3.0f,  13.0f, 14.0f, 15.0f, 4.0f,  5.0f,
       6.0f,  16.0f, 17.0f, 18.0f, 7.0f,  8.0f,  9.0f,  19.0f,
       20.0f, 21.0f, 10.0f, 11.0f, 12.0f, 22.0f, 23.0f, 24.0f};
-  tflite::testing::TestConcatenateTwoFloatInputs(
+  tflite_micro::testing::TestConcatenateTwoFloatInputs(
       input_shape, input1_value, input_shape, input2_value, /* axis */ 4,
       output_shape_axis4, output_value_axis4, output_data);
 
@@ -340,7 +340,7 @@ TF_LITE_MICRO_TEST(TwoInputsFiveDimensionsAllAxesCombinations) {
       1.0f,  2.0f,  3.0f,  13.0f, 14.0f, 15.0f, 4.0f,  5.0f,
       6.0f,  16.0f, 17.0f, 18.0f, 7.0f,  8.0f,  9.0f,  19.0f,
       20.0f, 21.0f, 10.0f, 11.0f, 12.0f, 22.0f, 23.0f, 24.0f};
-  tflite::testing::TestConcatenateTwoFloatInputs(
+  tflite_micro::testing::TestConcatenateTwoFloatInputs(
       input_shape, input1_value, input_shape, input2_value, /* axis */ -2,
       output_shape_axis_minus2, output_value_axis_minus2, output_data);
 }
@@ -363,7 +363,7 @@ TF_LITE_MICRO_TEST(TwoInputsQuantizedInt8FiveDimensions) {
   int output_shape[] = {5, 2, 1, 4, 1, 3};
   int8_t output_data[2 * kInputSize];
 
-  tflite::testing::TestConcatenateQuantizedTwoInputs(
+  tflite_micro::testing::TestConcatenateQuantizedTwoInputs(
       input_shape, input1_values, input_shape, input2_values, input_scale,
       input_zero_point, axis, output_shape, output_value, output_scale,
       output_zero_point, output_data);
diff --git a/tensorflow/lite/micro/kernels/conv.cc b/tensorflow/lite/micro/kernels/conv.cc
index 26300896..73782d4a 100644
--- a/tensorflow/lite/micro/kernels/conv.cc
+++ b/tensorflow/lite/micro/kernels/conv.cc
@@ -24,20 +24,20 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kConvInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kConvInputTensor);
   const TfLiteEvalTensor* filter =
-      tflite::micro::GetEvalInput(context, node, kConvWeightsTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kConvWeightsTensor);
   const TfLiteEvalTensor* bias =
       (NumInputs(node) == 3)
-          ? tflite::micro::GetEvalInput(context, node, kConvBiasTensor)
+          ? tflite_micro::micro::GetEvalInput(context, node, kConvBiasTensor)
           : nullptr;
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kConvOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kConvOutputTensor);
 
   TFLITE_DCHECK(node->builtin_data != nullptr);
   const auto& params =
@@ -47,16 +47,16 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 
   switch (input->type) {  // Already know in/out types are same.
     case kTfLiteFloat32: {
-      tflite::reference_ops::Conv(
-          ConvParamsFloat(params, data), tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<float>(input),
-          tflite::micro::GetTensorShape(filter),
-          tflite::micro::GetTensorData<float>(filter),
-          tflite::micro::GetTensorShape(bias),
-          tflite::micro::GetOptionalTensorData<float>(bias),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<float>(output),
-          tflite::micro::GetTensorShape(nullptr), nullptr);
+      tflite_micro::reference_ops::Conv(
+          ConvParamsFloat(params, data), tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<float>(input),
+          tflite_micro::micro::GetTensorShape(filter),
+          tflite_micro::micro::GetTensorData<float>(filter),
+          tflite_micro::micro::GetTensorShape(bias),
+          tflite_micro::micro::GetOptionalTensorData<float>(bias),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<float>(output),
+          tflite_micro::micro::GetTensorShape(nullptr), nullptr);
       break;
     }
     case kTfLiteInt16: {
@@ -64,26 +64,26 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
         reference_integer_ops::ConvPerChannel(
             ConvParamsQuantized(params, data),
             data.per_channel_output_multiplier, data.per_channel_output_shift,
-            tflite::micro::GetTensorShape(input),
-            tflite::micro::GetTensorData<int16_t>(input),
-            tflite::micro::GetTensorShape(filter),
-            tflite::micro::GetTensorData<int8_t>(filter),
-            tflite::micro::GetTensorShape(bias),
-            tflite::micro::GetOptionalTensorData<std::int32_t>(bias),
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<int16_t>(output));
+            tflite_micro::micro::GetTensorShape(input),
+            tflite_micro::micro::GetTensorData<int16_t>(input),
+            tflite_micro::micro::GetTensorShape(filter),
+            tflite_micro::micro::GetTensorData<int8_t>(filter),
+            tflite_micro::micro::GetTensorShape(bias),
+            tflite_micro::micro::GetOptionalTensorData<std::int32_t>(bias),
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int16_t>(output));
       } else if (bias->type == kTfLiteInt64) {
         reference_integer_ops::ConvPerChannel(
             ConvParamsQuantized(params, data),
             data.per_channel_output_multiplier, data.per_channel_output_shift,
-            tflite::micro::GetTensorShape(input),
-            tflite::micro::GetTensorData<int16_t>(input),
-            tflite::micro::GetTensorShape(filter),
-            tflite::micro::GetTensorData<int8_t>(filter),
-            tflite::micro::GetTensorShape(bias),
-            tflite::micro::GetOptionalTensorData<std::int64_t>(bias),
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<int16_t>(output));
+            tflite_micro::micro::GetTensorShape(input),
+            tflite_micro::micro::GetTensorData<int16_t>(input),
+            tflite_micro::micro::GetTensorShape(filter),
+            tflite_micro::micro::GetTensorData<int8_t>(filter),
+            tflite_micro::micro::GetTensorShape(bias),
+            tflite_micro::micro::GetOptionalTensorData<std::int64_t>(bias),
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int16_t>(output));
       } else {
         MicroPrintf("Bias type %s (%d) not supported.",
                     TfLiteTypeGetName(bias->type), bias->type);
@@ -96,34 +96,34 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
         case kTfLiteInt4: {
           int8_t* unpacked_filter_data = static_cast<int8_t*>(
               context->GetScratchBuffer(context, data.filter_buffer_index));
-          tflite::tensor_utils::UnpackDenseInt4IntoInt8(
-              tflite::micro::GetTensorData<int8_t>(filter),
-              tflite::micro::GetTensorShape(filter).FlatSize(),
+          tflite_micro::tensor_utils::UnpackDenseInt4IntoInt8(
+              tflite_micro::micro::GetTensorData<int8_t>(filter),
+              tflite_micro::micro::GetTensorShape(filter).FlatSize(),
               unpacked_filter_data);
           reference_integer_ops::ConvPerChannel(
               ConvParamsQuantized(params, data),
               data.per_channel_output_multiplier, data.per_channel_output_shift,
-              tflite::micro::GetTensorShape(input),
-              tflite::micro::GetTensorData<int8_t>(input),
-              tflite::micro::GetTensorShape(filter), unpacked_filter_data,
-              tflite::micro::GetTensorShape(bias),
-              tflite::micro::GetOptionalTensorData<int32_t>(bias),
-              tflite::micro::GetTensorShape(output),
-              tflite::micro::GetTensorData<int8_t>(output));
+              tflite_micro::micro::GetTensorShape(input),
+              tflite_micro::micro::GetTensorData<int8_t>(input),
+              tflite_micro::micro::GetTensorShape(filter), unpacked_filter_data,
+              tflite_micro::micro::GetTensorShape(bias),
+              tflite_micro::micro::GetOptionalTensorData<int32_t>(bias),
+              tflite_micro::micro::GetTensorShape(output),
+              tflite_micro::micro::GetTensorData<int8_t>(output));
           break;
         }
         case kTfLiteInt8: {
           reference_integer_ops::ConvPerChannel(
               ConvParamsQuantized(params, data),
               data.per_channel_output_multiplier, data.per_channel_output_shift,
-              tflite::micro::GetTensorShape(input),
-              tflite::micro::GetTensorData<int8_t>(input),
-              tflite::micro::GetTensorShape(filter),
-              tflite::micro::GetTensorData<int8_t>(filter),
-              tflite::micro::GetTensorShape(bias),
-              tflite::micro::GetOptionalTensorData<int32_t>(bias),
-              tflite::micro::GetTensorShape(output),
-              tflite::micro::GetTensorData<int8_t>(output));
+              tflite_micro::micro::GetTensorShape(input),
+              tflite_micro::micro::GetTensorData<int8_t>(input),
+              tflite_micro::micro::GetTensorShape(filter),
+              tflite_micro::micro::GetTensorData<int8_t>(filter),
+              tflite_micro::micro::GetTensorShape(bias),
+              tflite_micro::micro::GetOptionalTensorData<int32_t>(bias),
+              tflite_micro::micro::GetTensorShape(output),
+              tflite_micro::micro::GetTensorData<int8_t>(output));
           break;
         }
         default:
@@ -144,7 +144,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_CONV_2D() {
-  return tflite::micro::RegisterOp(ConvInit, ConvPrepare, Eval);
+  return tflite_micro::micro::RegisterOp(ConvInit, ConvPrepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/conv.h b/tensorflow/lite/micro/kernels/conv.h
index b8a034b8..45f3b406 100644
--- a/tensorflow/lite/micro/kernels/conv.h
+++ b/tensorflow/lite/micro/kernels/conv.h
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/types.h"
 #include "tensorflow/lite/micro/micro_common.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 struct OpDataConv {
   TfLitePaddingValues padding;
@@ -112,6 +112,6 @@ inline TFLMRegistration Register_CONV_2D_INT8() { return Register_CONV_2D(); }
 inline TFLMRegistration Register_CONV_2D_INT16() { return Register_CONV_2D(); }
 #endif  // defined(CMSIS_NN) || defined(XTENSA)
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_CONV_H_
diff --git a/tensorflow/lite/micro/kernels/conv_common.cc b/tensorflow/lite/micro/kernels/conv_common.cc
index 51c7a6ff..3c187de9 100644
--- a/tensorflow/lite/micro/kernels/conv_common.cc
+++ b/tensorflow/lite/micro/kernels/conv_common.cc
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/conv.h"
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 const int kConvInputTensor = 0;
 const int kConvWeightsTensor = 1;
@@ -39,7 +39,7 @@ ConvParams ConvParamsFloat(const TfLiteConvParams& params,
   ConvParams op_params;
   CalculateActivationRange(params.activation, &op_params.float_activation_min,
                            &op_params.float_activation_max);
-  op_params.padding_type = tflite::micro::RuntimePaddingType(params.padding);
+  op_params.padding_type = tflite_micro::micro::RuntimePaddingType(params.padding);
   op_params.padding_values.width = data.padding.width;
   op_params.padding_values.height = data.padding.height;
   op_params.stride_width = params.stride_width;
@@ -59,7 +59,7 @@ ConvParams ConvParamsQuantized(const TfLiteConvParams& params,
   op_params.output_offset = data.output_zero_point;
   op_params.output_multiplier = data.output_multiplier;
   op_params.output_shift = -data.output_shift;
-  op_params.padding_type = tflite::micro::RuntimePaddingType(params.padding);
+  op_params.padding_type = tflite_micro::micro::RuntimePaddingType(params.padding);
   op_params.padding_values.height = data.padding.height;
   op_params.padding_values.width = data.padding.width;
   op_params.stride_height = params.stride_height;
@@ -113,7 +113,7 @@ TfLiteStatus CalculateOpDataConv(TfLiteContext* context, TfLiteNode* node,
   if (data_type != kTfLiteFloat32) {
     int output_channels = filter->dims->data[kConvQuantizedDimension];
 
-    TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(
+    TF_LITE_ENSURE_STATUS(tflite_micro::PopulateConvolutionQuantizationParams(
         context, input, filter, bias, output, params.activation,
         &data->output_multiplier, &data->output_shift,
         &data->output_activation_min, &data->output_activation_max,
@@ -214,4 +214,4 @@ TfLiteStatus ConvPrepare(TfLiteContext* context, TfLiteNode* node) {
   micro_context->DeallocateTempTfLiteTensor(output);
   return kTfLiteOk;
 }
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/conv_test.cc b/tensorflow/lite/micro/kernels/conv_test.cc
index 0fb9411a..ef3d802f 100644
--- a/tensorflow/lite/micro/kernels/conv_test.cc
+++ b/tensorflow/lite/micro/kernels/conv_test.cc
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 // Common inputs and outputs.
@@ -58,7 +58,7 @@ static TfLiteConvParams common_conv_params = {
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -71,24 +71,24 @@ TF_LITE_MICRO_TEST(SimpleTestQuantized4bitPerChannel) {
   const int input_zero_point = 0;
   const int output_zero_point = 0;
 
-  int8_t input_quantized[tflite::testing::kInputElements];
-  int8_t filter_quantized[tflite::testing::kFilterElements];
-  int32_t bias_quantized[tflite::testing::kBiasElements];
-  int8_t golden_quantized[tflite::testing::kOutputElements];
-  int zero_points[tflite::testing::kBiasElements + 1];
-  float scales[tflite::testing::kBiasElements + 1];
+  int8_t input_quantized[tflite_micro::testing::kInputElements];
+  int8_t filter_quantized[tflite_micro::testing::kFilterElements];
+  int32_t bias_quantized[tflite_micro::testing::kBiasElements];
+  int8_t golden_quantized[tflite_micro::testing::kOutputElements];
+  int zero_points[tflite_micro::testing::kBiasElements + 1];
+  float scales[tflite_micro::testing::kBiasElements + 1];
 
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteOk,
-      tflite::testing::TestConvQuantizedPerChannel(
-          tflite::testing::kInputShape, tflite::testing::kInputData,
+      tflite_micro::testing::TestConvQuantizedPerChannel(
+          tflite_micro::testing::kInputShape, tflite_micro::testing::kInputData,
           input_quantized, input_scale, input_zero_point,
-          tflite::testing::kFilterShape, tflite::testing::kFilterData,
-          filter_quantized, tflite::testing::kBiasShape,
-          tflite::testing::kBiasData, bias_quantized, scales, zero_points,
-          tflite::testing::kOutputShape, tflite::testing::kGoldenData,
+          tflite_micro::testing::kFilterShape, tflite_micro::testing::kFilterData,
+          filter_quantized, tflite_micro::testing::kBiasShape,
+          tflite_micro::testing::kBiasData, bias_quantized, scales, zero_points,
+          tflite_micro::testing::kOutputShape, tflite_micro::testing::kGoldenData,
           golden_quantized, output_scale, output_zero_point,
-          &tflite::testing::common_conv_params, tflite::Register_CONV_2D(),
+          &tflite_micro::testing::common_conv_params, tflite_micro::Register_CONV_2D(),
           output_data, kTfLiteInt4));
 }
 
@@ -101,38 +101,38 @@ TF_LITE_MICRO_TEST(SimpleTestQuantizedPerChannel) {
   const int input_zero_point = 0;
   const int output_zero_point = 0;
 
-  int8_t input_quantized[tflite::testing::kInputElements];
-  int8_t filter_quantized[tflite::testing::kFilterElements];
-  int32_t bias_quantized[tflite::testing::kBiasElements];
-  int8_t golden_quantized[tflite::testing::kOutputElements];
-  int zero_points[tflite::testing::kBiasElements + 1];
-  float scales[tflite::testing::kBiasElements + 1];
+  int8_t input_quantized[tflite_micro::testing::kInputElements];
+  int8_t filter_quantized[tflite_micro::testing::kFilterElements];
+  int32_t bias_quantized[tflite_micro::testing::kBiasElements];
+  int8_t golden_quantized[tflite_micro::testing::kOutputElements];
+  int zero_points[tflite_micro::testing::kBiasElements + 1];
+  float scales[tflite_micro::testing::kBiasElements + 1];
 
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteOk,
-      tflite::testing::TestConvQuantizedPerChannel(
-          tflite::testing::kInputShape, tflite::testing::kInputData,
+      tflite_micro::testing::TestConvQuantizedPerChannel(
+          tflite_micro::testing::kInputShape, tflite_micro::testing::kInputData,
           input_quantized, input_scale, input_zero_point,
-          tflite::testing::kFilterShape, tflite::testing::kFilterData,
-          filter_quantized, tflite::testing::kBiasShape,
-          tflite::testing::kBiasData, bias_quantized, scales, zero_points,
-          tflite::testing::kOutputShape, tflite::testing::kGoldenData,
+          tflite_micro::testing::kFilterShape, tflite_micro::testing::kFilterData,
+          filter_quantized, tflite_micro::testing::kBiasShape,
+          tflite_micro::testing::kBiasData, bias_quantized, scales, zero_points,
+          tflite_micro::testing::kOutputShape, tflite_micro::testing::kGoldenData,
           golden_quantized, output_scale, output_zero_point,
-          &tflite::testing::common_conv_params, tflite::Register_CONV_2D(),
+          &tflite_micro::testing::common_conv_params, tflite_micro::Register_CONV_2D(),
           output_data));
 }
 
 TF_LITE_MICRO_TEST(SimpleTestFloat) {
-  float output_data[tflite::testing::kOutputElements];
+  float output_data[tflite_micro::testing::kOutputElements];
 
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteOk,
-      tflite::testing::TestConvFloat(
-          tflite::testing::kInputShape, tflite::testing::kInputData,
-          tflite::testing::kFilterShape, tflite::testing::kFilterData,
-          tflite::testing::kBiasShape, tflite::testing::kBiasData,
-          tflite::testing::kOutputShape, tflite::testing::kGoldenData,
-          &tflite::testing::common_conv_params, tflite::Register_CONV_2D(),
+      tflite_micro::testing::TestConvFloat(
+          tflite_micro::testing::kInputShape, tflite_micro::testing::kInputData,
+          tflite_micro::testing::kFilterShape, tflite_micro::testing::kFilterData,
+          tflite_micro::testing::kBiasShape, tflite_micro::testing::kBiasData,
+          tflite_micro::testing::kOutputShape, tflite_micro::testing::kGoldenData,
+          &tflite_micro::testing::common_conv_params, tflite_micro::Register_CONV_2D(),
           output_data));
 }
 
@@ -149,71 +149,71 @@ TF_LITE_MICRO_TEST(InputAndFilterSameWidthHeight) {
 
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteOk,
-      tflite::testing::TestConvFloat(
-          tflite::testing::kInputShape, tflite::testing::kInputData,
+      tflite_micro::testing::TestConvFloat(
+          tflite_micro::testing::kInputShape, tflite_micro::testing::kInputData,
           kFilterShape, filter_values, kBiasShape, bias_values, kOutputShape,
-          expected_output, &tflite::testing::common_conv_params,
-          tflite::Register_CONV_2D(), output_data));
+          expected_output, &tflite_micro::testing::common_conv_params,
+          tflite_micro::Register_CONV_2D(), output_data));
 }
 
 TF_LITE_MICRO_TEST(InputOutputDifferentTypeIsError) {
-  using tflite::testing::CreateQuantizedTensor;
-  using tflite::testing::CreateTensor;
-  using tflite::testing::IntArrayFromInts;
-
-  TfLiteIntArray* input_dims = IntArrayFromInts(tflite::testing::kInputShape);
-  TfLiteIntArray* filter_dims = IntArrayFromInts(tflite::testing::kFilterShape);
-  TfLiteIntArray* bias_dims = IntArrayFromInts(tflite::testing::kBiasShape);
-  TfLiteIntArray* output_dims = IntArrayFromInts(tflite::testing::kOutputShape);
-  const int output_dims_count = tflite::ElementCount(*output_dims);
+  using tflite_micro::testing::CreateQuantizedTensor;
+  using tflite_micro::testing::CreateTensor;
+  using tflite_micro::testing::IntArrayFromInts;
+
+  TfLiteIntArray* input_dims = IntArrayFromInts(tflite_micro::testing::kInputShape);
+  TfLiteIntArray* filter_dims = IntArrayFromInts(tflite_micro::testing::kFilterShape);
+  TfLiteIntArray* bias_dims = IntArrayFromInts(tflite_micro::testing::kBiasShape);
+  TfLiteIntArray* output_dims = IntArrayFromInts(tflite_micro::testing::kOutputShape);
+  const int output_dims_count = tflite_micro::ElementCount(*output_dims);
   constexpr int inputs_size = 3;
   constexpr int outputs_size = 1;
   constexpr int tensors_size = inputs_size + outputs_size;
 
-  int8_t output_data[tflite::testing::kOutputElements];
+  int8_t output_data[tflite_micro::testing::kOutputElements];
   TfLiteTensor tensors[tensors_size] = {
-      CreateTensor(tflite::testing::kInputData, input_dims),
-      CreateTensor(tflite::testing::kFilterData, filter_dims),
-      CreateTensor(tflite::testing::kBiasData, bias_dims),
+      CreateTensor(tflite_micro::testing::kInputData, input_dims),
+      CreateTensor(tflite_micro::testing::kFilterData, filter_dims),
+      CreateTensor(tflite_micro::testing::kBiasData, bias_dims),
       CreateQuantizedTensor(output_data, output_dims, /*scale=*/0.0f,
                             /*zero_point=*/0),
   };
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteError,
-      tflite::testing::InvokeConv(tensors, tensors_size, output_dims_count,
-                                  &tflite::testing::common_conv_params,
-                                  tflite::Register_CONV_2D(), output_data));
+      tflite_micro::testing::InvokeConv(tensors, tensors_size, output_dims_count,
+                                  &tflite_micro::testing::common_conv_params,
+                                  tflite_micro::Register_CONV_2D(), output_data));
 }
 
 TF_LITE_MICRO_TEST(HybridModeIsError) {
-  using tflite::testing::CreateQuantizedTensor;
-  using tflite::testing::CreateTensor;
-  using tflite::testing::IntArrayFromInts;
-
-  TfLiteIntArray* input_dims = IntArrayFromInts(tflite::testing::kInputShape);
-  TfLiteIntArray* filter_dims = IntArrayFromInts(tflite::testing::kFilterShape);
-  TfLiteIntArray* bias_dims = IntArrayFromInts(tflite::testing::kBiasShape);
-  TfLiteIntArray* output_dims = IntArrayFromInts(tflite::testing::kOutputShape);
-  const int output_dims_count = tflite::ElementCount(*output_dims);
+  using tflite_micro::testing::CreateQuantizedTensor;
+  using tflite_micro::testing::CreateTensor;
+  using tflite_micro::testing::IntArrayFromInts;
+
+  TfLiteIntArray* input_dims = IntArrayFromInts(tflite_micro::testing::kInputShape);
+  TfLiteIntArray* filter_dims = IntArrayFromInts(tflite_micro::testing::kFilterShape);
+  TfLiteIntArray* bias_dims = IntArrayFromInts(tflite_micro::testing::kBiasShape);
+  TfLiteIntArray* output_dims = IntArrayFromInts(tflite_micro::testing::kOutputShape);
+  const int output_dims_count = tflite_micro::ElementCount(*output_dims);
   constexpr int inputs_size = 3;
   constexpr int outputs_size = 1;
   constexpr int tensors_size = inputs_size + outputs_size;
 
-  int8_t filter_data[tflite::testing::kFilterElements] = {};
-  float output_data[tflite::testing::kOutputElements];
+  int8_t filter_data[tflite_micro::testing::kFilterElements] = {};
+  float output_data[tflite_micro::testing::kOutputElements];
   TfLiteTensor tensors[tensors_size] = {
-      CreateTensor(tflite::testing::kInputData, input_dims),
+      CreateTensor(tflite_micro::testing::kInputData, input_dims),
       CreateQuantizedTensor(filter_data, filter_dims,
                             /*scale=*/0.0f,
                             /*zero_point=*/0),
-      CreateTensor(tflite::testing::kBiasData, bias_dims),
+      CreateTensor(tflite_micro::testing::kBiasData, bias_dims),
       CreateTensor(output_data, output_dims),
   };
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteError,
-      tflite::testing::InvokeConv(tensors, tensors_size, output_dims_count,
-                                  &tflite::testing::common_conv_params,
-                                  tflite::Register_CONV_2D(), output_data));
+      tflite_micro::testing::InvokeConv(tensors, tensors_size, output_dims_count,
+                                  &tflite_micro::testing::common_conv_params,
+                                  tflite_micro::Register_CONV_2D(), output_data));
 }
 
 TF_LITE_MICRO_TEST(SimpleTestQuantized16x8PerChannel64bBias) {
@@ -225,24 +225,24 @@ TF_LITE_MICRO_TEST(SimpleTestQuantized16x8PerChannel64bBias) {
   const int input_zero_point = 0;
   const int output_zero_point = 0;
 
-  int16_t input_quantized[tflite::testing::kInputElements];
-  int8_t filter_quantized[tflite::testing::kFilterElements];
-  std::int64_t bias_quantized[tflite::testing::kBiasElements];
-  int16_t golden_quantized[tflite::testing::kOutputElements];
-  int zero_points[tflite::testing::kBiasElements + 1];
-  float scales[tflite::testing::kBiasElements + 1];
+  int16_t input_quantized[tflite_micro::testing::kInputElements];
+  int8_t filter_quantized[tflite_micro::testing::kFilterElements];
+  std::int64_t bias_quantized[tflite_micro::testing::kBiasElements];
+  int16_t golden_quantized[tflite_micro::testing::kOutputElements];
+  int zero_points[tflite_micro::testing::kBiasElements + 1];
+  float scales[tflite_micro::testing::kBiasElements + 1];
 
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteOk,
-      tflite::testing::TestConvQuantizedPerChannel(
-          tflite::testing::kInputShape, tflite::testing::kInputData,
+      tflite_micro::testing::TestConvQuantizedPerChannel(
+          tflite_micro::testing::kInputShape, tflite_micro::testing::kInputData,
           input_quantized, input_scale, input_zero_point,
-          tflite::testing::kFilterShape, tflite::testing::kFilterData,
-          filter_quantized, tflite::testing::kBiasShape,
-          tflite::testing::kBiasData, bias_quantized, scales, zero_points,
-          tflite::testing::kOutputShape, tflite::testing::kGoldenData,
+          tflite_micro::testing::kFilterShape, tflite_micro::testing::kFilterData,
+          filter_quantized, tflite_micro::testing::kBiasShape,
+          tflite_micro::testing::kBiasData, bias_quantized, scales, zero_points,
+          tflite_micro::testing::kOutputShape, tflite_micro::testing::kGoldenData,
           golden_quantized, output_scale, output_zero_point,
-          &tflite::testing::common_conv_params, tflite::Register_CONV_2D(),
+          &tflite_micro::testing::common_conv_params, tflite_micro::Register_CONV_2D(),
           output_data));
 }
 
@@ -255,24 +255,24 @@ TF_LITE_MICRO_TEST(SimpleTestQuantized16x8PerChannel32bBias) {
   const int input_zero_point = 0;
   const int output_zero_point = 0;
 
-  int16_t input_quantized[tflite::testing::kInputElements];
-  int8_t filter_quantized[tflite::testing::kFilterElements];
-  int32_t bias_quantized[tflite::testing::kBiasElements];
-  int16_t golden_quantized[tflite::testing::kOutputElements];
-  int zero_points[tflite::testing::kBiasElements + 1];
-  float scales[tflite::testing::kBiasElements + 1];
+  int16_t input_quantized[tflite_micro::testing::kInputElements];
+  int8_t filter_quantized[tflite_micro::testing::kFilterElements];
+  int32_t bias_quantized[tflite_micro::testing::kBiasElements];
+  int16_t golden_quantized[tflite_micro::testing::kOutputElements];
+  int zero_points[tflite_micro::testing::kBiasElements + 1];
+  float scales[tflite_micro::testing::kBiasElements + 1];
 
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteOk,
-      tflite::testing::TestConvQuantizedPerChannel(
-          tflite::testing::kInputShape, tflite::testing::kInputData,
+      tflite_micro::testing::TestConvQuantizedPerChannel(
+          tflite_micro::testing::kInputShape, tflite_micro::testing::kInputData,
           input_quantized, input_scale, input_zero_point,
-          tflite::testing::kFilterShape, tflite::testing::kFilterData,
-          filter_quantized, tflite::testing::kBiasShape,
-          tflite::testing::kBiasData, bias_quantized, scales, zero_points,
-          tflite::testing::kOutputShape, tflite::testing::kGoldenData,
+          tflite_micro::testing::kFilterShape, tflite_micro::testing::kFilterData,
+          filter_quantized, tflite_micro::testing::kBiasShape,
+          tflite_micro::testing::kBiasData, bias_quantized, scales, zero_points,
+          tflite_micro::testing::kOutputShape, tflite_micro::testing::kGoldenData,
           golden_quantized, output_scale, output_zero_point,
-          &tflite::testing::common_conv_params, tflite::Register_CONV_2D(),
+          &tflite_micro::testing::common_conv_params, tflite_micro::Register_CONV_2D(),
           output_data));
 }
 
@@ -298,26 +298,26 @@ TF_LITE_MICRO_TEST(SimpleTestDilatedQuantizedPerChannel) {
                                39, 7, 6, 50, 3, 4, 14, 4, -5, 15, 0, -7};
 
   int8_t input_quantized[input_elements];
-  int8_t filter_quantized[tflite::testing::kFilterElements];
-  int32_t bias_quantized[tflite::testing::kBiasElements];
+  int8_t filter_quantized[tflite_micro::testing::kFilterElements];
+  int32_t bias_quantized[tflite_micro::testing::kBiasElements];
   int8_t golden_quantized[output_elements];
-  int zero_points[tflite::testing::kBiasElements + 1];
-  float scales[tflite::testing::kBiasElements + 1];
+  int zero_points[tflite_micro::testing::kBiasElements + 1];
+  float scales[tflite_micro::testing::kBiasElements + 1];
 
-  TfLiteConvParams conv_params{tflite::testing::common_conv_params};
+  TfLiteConvParams conv_params{tflite_micro::testing::common_conv_params};
   conv_params.dilation_width_factor = 3;
   conv_params.dilation_height_factor = 2;
 
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteOk,
-      tflite::testing::TestConvQuantizedPerChannel(
+      tflite_micro::testing::TestConvQuantizedPerChannel(
           input_shape, input_data, input_quantized, input_scale,
-          input_zero_point, tflite::testing::kFilterShape,
-          tflite::testing::kFilterData, filter_quantized,
-          tflite::testing::kBiasShape, tflite::testing::kBiasData,
+          input_zero_point, tflite_micro::testing::kFilterShape,
+          tflite_micro::testing::kFilterData, filter_quantized,
+          tflite_micro::testing::kBiasShape, tflite_micro::testing::kBiasData,
           bias_quantized, scales, zero_points, output_shape, golden_data,
           golden_quantized, output_scale, output_zero_point, &conv_params,
-          tflite::Register_CONV_2D(), output_data));
+          tflite_micro::Register_CONV_2D(), output_data));
 }
 
 TF_LITE_MICRO_TEST(SimpleTestQuantizedPerChannelRelu6) {
@@ -332,23 +332,23 @@ TF_LITE_MICRO_TEST(SimpleTestQuantizedPerChannelRelu6) {
   const int input_zero_point = -128;
   const int output_zero_point = -128;
 
-  int8_t input_quantized[tflite::testing::kInputElements];
-  int8_t filter_quantized[tflite::testing::kFilterElements];
-  int32_t bias_quantized[tflite::testing::kBiasElements];
-  int8_t golden_quantized[tflite::testing::kOutputElements];
-  int zero_points[tflite::testing::kBiasElements + 1];
-  float scales[tflite::testing::kBiasElements + 1];
+  int8_t input_quantized[tflite_micro::testing::kInputElements];
+  int8_t filter_quantized[tflite_micro::testing::kFilterElements];
+  int32_t bias_quantized[tflite_micro::testing::kBiasElements];
+  int8_t golden_quantized[tflite_micro::testing::kOutputElements];
+  int zero_points[tflite_micro::testing::kBiasElements + 1];
+  float scales[tflite_micro::testing::kBiasElements + 1];
 
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteOk,
-      tflite::testing::TestConvQuantizedPerChannel(
-          tflite::testing::kInputShape, tflite::testing::kInputData,
+      tflite_micro::testing::TestConvQuantizedPerChannel(
+          tflite_micro::testing::kInputShape, tflite_micro::testing::kInputData,
           input_quantized, input_scale, input_zero_point,
-          tflite::testing::kFilterShape, tflite::testing::kFilterData,
-          filter_quantized, tflite::testing::kBiasShape, bias_values,
-          bias_quantized, scales, zero_points, tflite::testing::kOutputShape,
+          tflite_micro::testing::kFilterShape, tflite_micro::testing::kFilterData,
+          filter_quantized, tflite_micro::testing::kBiasShape, bias_values,
+          bias_quantized, scales, zero_points, tflite_micro::testing::kOutputShape,
           golden_data, golden_quantized, output_scale, output_zero_point,
-          &tflite::testing::common_conv_params, tflite::Register_CONV_2D(),
+          &tflite_micro::testing::common_conv_params, tflite_micro::Register_CONV_2D(),
           output_data));
 }
 
@@ -364,25 +364,25 @@ TF_LITE_MICRO_TEST(SimpleTestQuantized16x8PerChannelRelu664bBias) {
   const int input_zero_point = 0;
   const int output_zero_point = 0;
 
-  int16_t input_quantized[tflite::testing::kInputElements];
-  int8_t filter_quantized[tflite::testing::kFilterElements];
-  std::int64_t bias_quantized[tflite::testing::kBiasElements];
-  int16_t golden_quantized[tflite::testing::kOutputElements];
-  int zero_points[tflite::testing::kBiasElements + 1];
-  float scales[tflite::testing::kBiasElements + 1];
+  int16_t input_quantized[tflite_micro::testing::kInputElements];
+  int8_t filter_quantized[tflite_micro::testing::kFilterElements];
+  std::int64_t bias_quantized[tflite_micro::testing::kBiasElements];
+  int16_t golden_quantized[tflite_micro::testing::kOutputElements];
+  int zero_points[tflite_micro::testing::kBiasElements + 1];
+  float scales[tflite_micro::testing::kBiasElements + 1];
 
-  TfLiteConvParams conv_params{tflite::testing::common_conv_params};
+  TfLiteConvParams conv_params{tflite_micro::testing::common_conv_params};
   conv_params.activation = kTfLiteActRelu6;
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteOk,
-      tflite::testing::TestConvQuantizedPerChannel(
-          tflite::testing::kInputShape, tflite::testing::kInputData,
+      tflite_micro::testing::TestConvQuantizedPerChannel(
+          tflite_micro::testing::kInputShape, tflite_micro::testing::kInputData,
           input_quantized, input_scale, input_zero_point,
-          tflite::testing::kFilterShape, tflite::testing::kFilterData,
-          filter_quantized, tflite::testing::kBiasShape, bias_values,
-          bias_quantized, scales, zero_points, tflite::testing::kOutputShape,
+          tflite_micro::testing::kFilterShape, tflite_micro::testing::kFilterData,
+          filter_quantized, tflite_micro::testing::kBiasShape, bias_values,
+          bias_quantized, scales, zero_points, tflite_micro::testing::kOutputShape,
           golden_data, golden_quantized, output_scale, output_zero_point,
-          &conv_params, tflite::Register_CONV_2D(), output_data));
+          &conv_params, tflite_micro::Register_CONV_2D(), output_data));
 }
 
 TF_LITE_MICRO_TEST(SimpleTestQuantized16x8PerChannelRelu632bBias) {
@@ -397,25 +397,25 @@ TF_LITE_MICRO_TEST(SimpleTestQuantized16x8PerChannelRelu632bBias) {
   const int input_zero_point = 0;
   const int output_zero_point = 0;
 
-  int16_t input_quantized[tflite::testing::kInputElements];
-  int8_t filter_quantized[tflite::testing::kFilterElements];
-  int32_t bias_quantized[tflite::testing::kBiasElements];
-  int16_t golden_quantized[tflite::testing::kOutputElements];
-  int zero_points[tflite::testing::kBiasElements + 1];
-  float scales[tflite::testing::kBiasElements + 1];
+  int16_t input_quantized[tflite_micro::testing::kInputElements];
+  int8_t filter_quantized[tflite_micro::testing::kFilterElements];
+  int32_t bias_quantized[tflite_micro::testing::kBiasElements];
+  int16_t golden_quantized[tflite_micro::testing::kOutputElements];
+  int zero_points[tflite_micro::testing::kBiasElements + 1];
+  float scales[tflite_micro::testing::kBiasElements + 1];
 
-  TfLiteConvParams conv_params{tflite::testing::common_conv_params};
+  TfLiteConvParams conv_params{tflite_micro::testing::common_conv_params};
   conv_params.activation = kTfLiteActRelu6;
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteOk,
-      tflite::testing::TestConvQuantizedPerChannel(
-          tflite::testing::kInputShape, tflite::testing::kInputData,
+      tflite_micro::testing::TestConvQuantizedPerChannel(
+          tflite_micro::testing::kInputShape, tflite_micro::testing::kInputData,
           input_quantized, input_scale, input_zero_point,
-          tflite::testing::kFilterShape, tflite::testing::kFilterData,
-          filter_quantized, tflite::testing::kBiasShape, bias_values,
-          bias_quantized, scales, zero_points, tflite::testing::kOutputShape,
+          tflite_micro::testing::kFilterShape, tflite_micro::testing::kFilterData,
+          filter_quantized, tflite_micro::testing::kBiasShape, bias_values,
+          bias_quantized, scales, zero_points, tflite_micro::testing::kOutputShape,
           golden_data, golden_quantized, output_scale, output_zero_point,
-          &conv_params, tflite::Register_CONV_2D(), output_data));
+          &conv_params, tflite_micro::Register_CONV_2D(), output_data));
 }
 
 TF_LITE_MICRO_TEST(Kernel1x1QuantizedPerChannel) {
@@ -462,13 +462,13 @@ TF_LITE_MICRO_TEST(Kernel1x1QuantizedPerChannel) {
   float scales[bias_elements + 1];
 
   TF_LITE_MICRO_EXPECT_EQ(
-      kTfLiteOk, tflite::testing::TestConvQuantizedPerChannel(
+      kTfLiteOk, tflite_micro::testing::TestConvQuantizedPerChannel(
                      input_shape, input_data, input_quantized, input_scale,
                      input_zero_point, filter_shape, filter_data,
                      filter_quantized, bias_shape, bias_data, bias_quantized,
                      scales, zero_points, output_shape, golden_data,
                      golden_quantized, output_scale, output_zero_point,
-                     &conv_params, tflite::Register_CONV_2D(), output_data));
+                     &conv_params, tflite_micro::Register_CONV_2D(), output_data));
 }
 
 TF_LITE_MICRO_TEST(Kernel1x1QuantizedPerChannelRelu6) {
@@ -515,13 +515,13 @@ TF_LITE_MICRO_TEST(Kernel1x1QuantizedPerChannelRelu6) {
   float scales[bias_elements + 1];
 
   TF_LITE_MICRO_EXPECT_EQ(
-      kTfLiteOk, tflite::testing::TestConvQuantizedPerChannel(
+      kTfLiteOk, tflite_micro::testing::TestConvQuantizedPerChannel(
                      input_shape, input_data, input_quantized, input_scale,
                      input_zero_point, filter_shape, filter_data,
                      filter_quantized, bias_shape, bias_data, bias_quantized,
                      scales, zero_points, output_shape, golden_data,
                      golden_quantized, output_scale, output_zero_point,
-                     &conv_params, tflite::Register_CONV_2D(), output_data));
+                     &conv_params, tflite_micro::Register_CONV_2D(), output_data));
 }
 
 TF_LITE_MICRO_TEST(Kernel1x1Quantized16x8PerChannelRelu6) {
@@ -564,13 +564,13 @@ TF_LITE_MICRO_TEST(Kernel1x1Quantized16x8PerChannelRelu6) {
   float scales[bias_elements + 1];
 
   TF_LITE_MICRO_EXPECT_EQ(
-      kTfLiteOk, tflite::testing::TestConvQuantizedPerChannel(
+      kTfLiteOk, tflite_micro::testing::TestConvQuantizedPerChannel(
                      input_shape, input_data, input_quantized, input_scale,
                      input_zero_point, filter_shape, filter_data,
                      filter_quantized, bias_shape, bias_data, bias_quantized,
                      scales, zero_points, output_shape, golden_data,
                      golden_quantized, output_scale, output_zero_point,
-                     &conv_params, tflite::Register_CONV_2D(), output_data));
+                     &conv_params, tflite_micro::Register_CONV_2D(), output_data));
 }
 
 TF_LITE_MICRO_TEST(BroadcastPerLayerQuantizationToPerChannelShouldMatchGolden) {
@@ -581,63 +581,63 @@ TF_LITE_MICRO_TEST(BroadcastPerLayerQuantizationToPerChannelShouldMatchGolden) {
   const float filter_scale = 1.0f;
   const float output_scale = 1.0f;
 
-  int8_t input_quantized[tflite::testing::kInputElements];
-  int8_t filter_quantized[tflite::testing::kFilterElements];
-  int32_t bias_quantized[tflite::testing::kBiasElements];
-  int8_t golden_quantized[tflite::testing::kOutputElements];
+  int8_t input_quantized[tflite_micro::testing::kInputElements];
+  int8_t filter_quantized[tflite_micro::testing::kFilterElements];
+  int32_t bias_quantized[tflite_micro::testing::kBiasElements];
+  int8_t golden_quantized[tflite_micro::testing::kOutputElements];
 
   TfLiteIntArray* input_dims =
-      tflite::testing::IntArrayFromInts(tflite::testing::kInputShape);
+      tflite_micro::testing::IntArrayFromInts(tflite_micro::testing::kInputShape);
   TfLiteIntArray* filter_dims =
-      tflite::testing::IntArrayFromInts(tflite::testing::kFilterShape);
+      tflite_micro::testing::IntArrayFromInts(tflite_micro::testing::kFilterShape);
   TfLiteIntArray* bias_dims =
-      tflite::testing::IntArrayFromInts(tflite::testing::kBiasShape);
+      tflite_micro::testing::IntArrayFromInts(tflite_micro::testing::kBiasShape);
   TfLiteIntArray* output_dims =
-      tflite::testing::IntArrayFromInts(tflite::testing::kOutputShape);
+      tflite_micro::testing::IntArrayFromInts(tflite_micro::testing::kOutputShape);
 
   // Create per-layer quantized int8_t input tensor.
-  TfLiteTensor input_tensor = tflite::testing::CreateQuantizedTensor(
-      tflite::testing::kInputData, input_quantized, input_dims, input_scale, 0);
+  TfLiteTensor input_tensor = tflite_micro::testing::CreateQuantizedTensor(
+      tflite_micro::testing::kInputData, input_quantized, input_dims, input_scale, 0);
   int input_zero_points[2] = {1, 0};
   float input_scales[2] = {1, input_scale};
   TfLiteAffineQuantization input_quant = {
-      tflite::testing::FloatArrayFromFloats(input_scales),
-      tflite::testing::IntArrayFromInts(input_zero_points), 0};
+      tflite_micro::testing::FloatArrayFromFloats(input_scales),
+      tflite_micro::testing::IntArrayFromInts(input_zero_points), 0};
   input_tensor.quantization = {kTfLiteAffineQuantization, &input_quant};
 
   // Create per-layer quantized int8_t filter tensor.
-  TfLiteTensor filter_tensor = tflite::testing::CreateQuantizedTensor(
-      tflite::testing::kFilterData, filter_quantized, filter_dims, filter_scale,
+  TfLiteTensor filter_tensor = tflite_micro::testing::CreateQuantizedTensor(
+      tflite_micro::testing::kFilterData, filter_quantized, filter_dims, filter_scale,
       0);
   int filter_zero_points[2] = {1, 0};
   float filter_scales[2] = {1, filter_scale};
   TfLiteAffineQuantization filter_quant = {
-      tflite::testing::FloatArrayFromFloats(filter_scales),
-      tflite::testing::IntArrayFromInts(filter_zero_points), 0};
+      tflite_micro::testing::FloatArrayFromFloats(filter_scales),
+      tflite_micro::testing::IntArrayFromInts(filter_zero_points), 0};
   filter_tensor.quantization = {kTfLiteAffineQuantization, &filter_quant};
 
   // Create per-layer quantized int32_t bias tensor.
-  tflite::SymmetricQuantize(tflite::testing::kBiasData, bias_quantized,
-                            tflite::testing::kBiasElements,
+  tflite_micro::SymmetricQuantize(tflite_micro::testing::kBiasData, bias_quantized,
+                            tflite_micro::testing::kBiasElements,
                             input_scale * output_scale);
   TfLiteTensor bias_tensor =
-      tflite::testing::CreateTensor(bias_quantized, bias_dims);
+      tflite_micro::testing::CreateTensor(bias_quantized, bias_dims);
 
   int bias_zero_points[2] = {1, 0};
   float bias_scales[2] = {1, input_scale * filter_scale};
   TfLiteAffineQuantization bias_quant = {
-      tflite::testing::FloatArrayFromFloats(bias_scales),
-      tflite::testing::IntArrayFromInts(bias_zero_points), 0};
+      tflite_micro::testing::FloatArrayFromFloats(bias_scales),
+      tflite_micro::testing::IntArrayFromInts(bias_zero_points), 0};
   bias_tensor.quantization = {kTfLiteAffineQuantization, &bias_quant};
 
   // Create per-layer quantized int8_t output tensor.
-  TfLiteTensor output_tensor = tflite::testing::CreateQuantizedTensor(
+  TfLiteTensor output_tensor = tflite_micro::testing::CreateQuantizedTensor(
       output_data, output_dims, output_scale, 0 /* quantized dimension */);
   int output_zero_points[2] = {1, 0};
   float output_scales[2] = {1, output_scale};
   TfLiteAffineQuantization output_quant = {
-      tflite::testing::FloatArrayFromFloats(output_scales),
-      tflite::testing::IntArrayFromInts(output_zero_points), 0};
+      tflite_micro::testing::FloatArrayFromFloats(output_scales),
+      tflite_micro::testing::IntArrayFromInts(output_zero_points), 0};
   output_tensor.quantization = {kTfLiteAffineQuantization, &output_quant};
 
   constexpr int inputs_size = 3;
@@ -650,26 +650,26 @@ TF_LITE_MICRO_TEST(BroadcastPerLayerQuantizationToPerChannelShouldMatchGolden) {
       output_tensor,
   };
 
-  tflite::Quantize(tflite::testing::kGoldenData, golden_quantized,
+  tflite_micro::Quantize(tflite_micro::testing::kGoldenData, golden_quantized,
                    output_dims_count, output_scale, 0);
 
   TF_LITE_MICRO_EXPECT_EQ(
-      kTfLiteOk, tflite::testing::ValidateConvGoldens(
+      kTfLiteOk, tflite_micro::testing::ValidateConvGoldens(
                      tensors, tensors_size, golden_quantized, output_dims_count,
-                     &tflite::testing::common_conv_params,
-                     tflite::Register_CONV_2D(), output_data));
+                     &tflite_micro::testing::common_conv_params,
+                     tflite_micro::Register_CONV_2D(), output_data));
 }
 
 TF_LITE_MICRO_TEST(Int8Filter1x3x3x1ShouldMatchGoldenEvenInputPaddingSame) {
-  using tflite::ElementCount;
-  using tflite::kConvFilter1x3x3x1;
-  using tflite::kConvGoldenOutput4x4InputPaddingSame2x2;
-  using tflite::kConvInput1x4x4x1;
-  using tflite::kConvZeroBias;
-  using tflite::testing::CreateTensor;
-  using tflite::testing::FloatArrayFromFloats;
-  using tflite::testing::IntArrayFromInts;
-  using tflite::testing::ValidateConvGoldens;
+  using tflite_micro::ElementCount;
+  using tflite_micro::kConvFilter1x3x3x1;
+  using tflite_micro::kConvGoldenOutput4x4InputPaddingSame2x2;
+  using tflite_micro::kConvInput1x4x4x1;
+  using tflite_micro::kConvZeroBias;
+  using tflite_micro::testing::CreateTensor;
+  using tflite_micro::testing::FloatArrayFromFloats;
+  using tflite_micro::testing::IntArrayFromInts;
+  using tflite_micro::testing::ValidateConvGoldens;
 
   constexpr int kInDepth = 1;
   constexpr int kOutDepth = 1;
@@ -748,27 +748,27 @@ TF_LITE_MICRO_TEST(Int8Filter1x3x3x1ShouldMatchGoldenEvenInputPaddingSame) {
       output_tensor,
   };
 
-  TfLiteConvParams conv_params{tflite::testing::common_conv_params};
+  TfLiteConvParams conv_params{tflite_micro::testing::common_conv_params};
   conv_params.padding = kTfLitePaddingSame;
 
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteOk, ValidateConvGoldens(tensors, tensors_size,
                                      kConvGoldenOutput4x4InputPaddingSame2x2,
                                      output_dims_count, &conv_params,
-                                     tflite::Register_CONV_2D(), output_data,
+                                     tflite_micro::Register_CONV_2D(), output_data,
                                      1.0 /* tolerance */));
 }
 
 TF_LITE_MICRO_TEST(Int8Filter1x3x3x1ShouldMatchGoldenOddInputPaddingSame) {
-  using tflite::ElementCount;
-  using tflite::kConvFilter1x3x3x1;
-  using tflite::kConvGoldenOutput5x5InputPaddingSame3x3;
-  using tflite::kConvInput1x5x5x1;
-  using tflite::kConvZeroBias;
-  using tflite::testing::CreateTensor;
-  using tflite::testing::FloatArrayFromFloats;
-  using tflite::testing::IntArrayFromInts;
-  using tflite::testing::ValidateConvGoldens;
+  using tflite_micro::ElementCount;
+  using tflite_micro::kConvFilter1x3x3x1;
+  using tflite_micro::kConvGoldenOutput5x5InputPaddingSame3x3;
+  using tflite_micro::kConvInput1x5x5x1;
+  using tflite_micro::kConvZeroBias;
+  using tflite_micro::testing::CreateTensor;
+  using tflite_micro::testing::FloatArrayFromFloats;
+  using tflite_micro::testing::IntArrayFromInts;
+  using tflite_micro::testing::ValidateConvGoldens;
 
   constexpr int kInDepth = 1;
   constexpr int kOutDepth = 1;
@@ -847,14 +847,14 @@ TF_LITE_MICRO_TEST(Int8Filter1x3x3x1ShouldMatchGoldenOddInputPaddingSame) {
       output_tensor,
   };
 
-  TfLiteConvParams conv_params{tflite::testing::common_conv_params};
+  TfLiteConvParams conv_params{tflite_micro::testing::common_conv_params};
   conv_params.padding = kTfLitePaddingSame;
 
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteOk, ValidateConvGoldens(tensors, tensors_size,
                                      kConvGoldenOutput5x5InputPaddingSame3x3,
                                      output_dims_count, &conv_params,
-                                     tflite::Register_CONV_2D(), output_data,
+                                     tflite_micro::Register_CONV_2D(), output_data,
                                      1.0 /* tolerance */));
 }
 
@@ -865,45 +865,45 @@ TF_LITE_MICRO_TEST(FilterDimsNotMatchingAffineQuantization) {
   const float input_scale = 0.5f;
   const float output_scale = 1.0f;
 
-  int8_t input_quantized[tflite::testing::kInputElements];
-  int8_t filter_quantized[tflite::testing::kFilterElements];
-  int32_t bias_quantized[tflite::testing::kBiasElements];
-  int8_t golden_quantized[tflite::testing::kOutputElements];
-  int zero_points[tflite::testing::kBiasElements + 1];
-  float scales[tflite::testing::kBiasElements + 1];
+  int8_t input_quantized[tflite_micro::testing::kInputElements];
+  int8_t filter_quantized[tflite_micro::testing::kFilterElements];
+  int32_t bias_quantized[tflite_micro::testing::kBiasElements];
+  int8_t golden_quantized[tflite_micro::testing::kOutputElements];
+  int zero_points[tflite_micro::testing::kBiasElements + 1];
+  float scales[tflite_micro::testing::kBiasElements + 1];
 
   TfLiteIntArray* input_dims =
-      tflite::testing::IntArrayFromInts(tflite::testing::kInputShape);
+      tflite_micro::testing::IntArrayFromInts(tflite_micro::testing::kInputShape);
   TfLiteIntArray* filter_dims =
-      tflite::testing::IntArrayFromInts(tflite::testing::kFilterShape);
+      tflite_micro::testing::IntArrayFromInts(tflite_micro::testing::kFilterShape);
   TfLiteIntArray* bias_dims =
-      tflite::testing::IntArrayFromInts(tflite::testing::kBiasShape);
+      tflite_micro::testing::IntArrayFromInts(tflite_micro::testing::kBiasShape);
   TfLiteIntArray* output_dims =
-      tflite::testing::IntArrayFromInts(tflite::testing::kOutputShape);
+      tflite_micro::testing::IntArrayFromInts(tflite_micro::testing::kOutputShape);
 
   int filter_zero_points[5];
   float filter_scales[5];
   TfLiteAffineQuantization filter_quant;
   TfLiteAffineQuantization bias_quant;
-  TfLiteTensor input_tensor = tflite::testing::CreateQuantizedTensor(
-      tflite::testing::kInputData, input_quantized, input_dims, input_scale, 0);
+  TfLiteTensor input_tensor = tflite_micro::testing::CreateQuantizedTensor(
+      tflite_micro::testing::kInputData, input_quantized, input_dims, input_scale, 0);
   TfLiteTensor filter_tensor =
-      tflite::testing::CreateSymmetricPerChannelQuantizedTensor(
-          tflite::testing::kFilterData, filter_quantized, filter_dims,
+      tflite_micro::testing::CreateSymmetricPerChannelQuantizedTensor(
+          tflite_micro::testing::kFilterData, filter_quantized, filter_dims,
           filter_scales, filter_zero_points, &filter_quant,
           0 /* quantized dimension */);
   TfLiteTensor bias_tensor =
-      tflite::testing::CreatePerChannelQuantizedBiasTensor(
-          tflite::testing::kBiasData, bias_quantized, bias_dims, input_scale,
+      tflite_micro::testing::CreatePerChannelQuantizedBiasTensor(
+          tflite_micro::testing::kBiasData, bias_quantized, bias_dims, input_scale,
           &filter_scales[1], scales, zero_points, &bias_quant, 0);
-  TfLiteTensor output_tensor = tflite::testing::CreateQuantizedTensor(
+  TfLiteTensor output_tensor = tflite_micro::testing::CreateQuantizedTensor(
       output_data, output_dims, output_scale, 0 /* quantized dimension */);
 
   float input_scales[] = {1, input_scale};
   int input_zero_points[] = {1, 128};
   TfLiteAffineQuantization input_quant = {
-      tflite::testing::FloatArrayFromFloats(input_scales),
-      tflite::testing::IntArrayFromInts(input_zero_points), 0};
+      tflite_micro::testing::FloatArrayFromFloats(input_scales),
+      tflite_micro::testing::IntArrayFromInts(input_zero_points), 0};
   input_tensor.quantization = {kTfLiteAffineQuantization, &input_quant};
 
   constexpr int inputs_size = 3;
@@ -916,7 +916,7 @@ TF_LITE_MICRO_TEST(FilterDimsNotMatchingAffineQuantization) {
       output_tensor,
   };
 
-  tflite::Quantize(tflite::testing::kGoldenData, golden_quantized,
+  tflite_micro::Quantize(tflite_micro::testing::kGoldenData, golden_quantized,
                    output_dims_count, output_scale, 0);
 
   // Set filter quant to mismatched dimension.
@@ -927,19 +927,19 @@ TF_LITE_MICRO_TEST(FilterDimsNotMatchingAffineQuantization) {
   // (for broadcast case) nor the quantized dimension size.
   quant->scale->size = 2;
   TF_LITE_MICRO_EXPECT_EQ(
-      kTfLiteError, tflite::testing::ValidateConvGoldens(
+      kTfLiteError, tflite_micro::testing::ValidateConvGoldens(
                         tensors, tensors_size, golden_quantized,
-                        output_dims_count, &tflite::testing::common_conv_params,
-                        tflite::Register_CONV_2D(), output_data));
+                        output_dims_count, &tflite_micro::testing::common_conv_params,
+                        tflite_micro::Register_CONV_2D(), output_data));
 
   // Set scale back to correct dimension, and make zero point array too short.
-  quant->scale->size = tflite::testing::kFilterShape[0];
+  quant->scale->size = tflite_micro::testing::kFilterShape[0];
   quant->zero_point->size = 2;
   TF_LITE_MICRO_EXPECT_EQ(
-      kTfLiteError, tflite::testing::ValidateConvGoldens(
+      kTfLiteError, tflite_micro::testing::ValidateConvGoldens(
                         tensors, tensors_size, golden_quantized,
-                        output_dims_count, &tflite::testing::common_conv_params,
-                        tflite::Register_CONV_2D(), output_data));
+                        output_dims_count, &tflite_micro::testing::common_conv_params,
+                        tflite_micro::Register_CONV_2D(), output_data));
 }
 
 TF_LITE_MICRO_TEST(Int8Input32x1Filter32x32ShouldMatchGolden) {
@@ -983,11 +983,11 @@ TF_LITE_MICRO_TEST(Int8Input32x1Filter32x32ShouldMatchGolden) {
   conv_params.stride_width = 1;
   conv_params.padding = kTfLitePaddingValid;
 
-  TfLiteIntArray* input_dims = tflite::testing::IntArrayFromInts(input_shape);
-  TfLiteIntArray* filter_dims = tflite::testing::IntArrayFromInts(filter_shape);
-  TfLiteIntArray* bias_dims = tflite::testing::IntArrayFromInts(bias_shape);
-  TfLiteIntArray* output_dims = tflite::testing::IntArrayFromInts(output_shape);
-  const int output_dims_count = tflite::ElementCount(*output_dims);
+  TfLiteIntArray* input_dims = tflite_micro::testing::IntArrayFromInts(input_shape);
+  TfLiteIntArray* filter_dims = tflite_micro::testing::IntArrayFromInts(filter_shape);
+  TfLiteIntArray* bias_dims = tflite_micro::testing::IntArrayFromInts(bias_shape);
+  TfLiteIntArray* output_dims = tflite_micro::testing::IntArrayFromInts(output_shape);
+  const int output_dims_count = tflite_micro::ElementCount(*output_dims);
 
   // Quantization Parameters.  All scales except output are 1.0, and all zero
   // points are 0. This direct-maps the values to floating point and makes it
@@ -1002,55 +1002,55 @@ TF_LITE_MICRO_TEST(Int8Input32x1Filter32x32ShouldMatchGolden) {
 
   // Create per-tensor quantized int8_t input tensor.
   int8_t input_quantized[kSampleSize];
-  TfLiteTensor input_tensor = tflite::testing::CreateQuantizedTensor(
+  TfLiteTensor input_tensor = tflite_micro::testing::CreateQuantizedTensor(
       input_values, input_quantized, input_dims, input_scale, input_zero_point);
   // Set zero point and scale arrays with a single element for each.
   int input_zero_points[] = {1, input_zero_point};
   float input_scales[] = {1, input_scale};
   TfLiteAffineQuantization input_quant = {
-      tflite::testing::FloatArrayFromFloats(input_scales),
-      tflite::testing::IntArrayFromInts(input_zero_points), 0};
+      tflite_micro::testing::FloatArrayFromFloats(input_scales),
+      tflite_micro::testing::IntArrayFromInts(input_zero_points), 0};
   input_tensor.quantization = {kTfLiteAffineQuantization, &input_quant};
 
   // Create per-tensor quantized int8_t filter tensor.
   int8_t filter_quantized[kNumFilters * kSampleSize];
-  TfLiteTensor filter_tensor = tflite::testing::CreateQuantizedTensor(
+  TfLiteTensor filter_tensor = tflite_micro::testing::CreateQuantizedTensor(
       filter_values, filter_quantized, filter_dims, filter_scale,
       filter_zero_point);
   // Set zero point and scale arrays with a single element for each.
   int filter_zero_points[] = {1, filter_zero_point};
   float filter_scales[] = {1, filter_scale};
   TfLiteAffineQuantization filter_quant = {
-      tflite::testing::FloatArrayFromFloats(filter_scales),
-      tflite::testing::IntArrayFromInts(filter_zero_points), 0};
+      tflite_micro::testing::FloatArrayFromFloats(filter_scales),
+      tflite_micro::testing::IntArrayFromInts(filter_zero_points), 0};
   filter_tensor.quantization = {kTfLiteAffineQuantization, &filter_quant};
 
   // Create per-tensor quantized int32_t bias tensor.
   int32_t bias_quantized[kSampleSize];
-  tflite::SymmetricQuantize(bias_values, bias_quantized, kSampleSize,
+  tflite_micro::SymmetricQuantize(bias_values, bias_quantized, kSampleSize,
                             input_scale * output_scale);
   TfLiteTensor bias_tensor =
-      tflite::testing::CreateTensor(bias_quantized, bias_dims);
+      tflite_micro::testing::CreateTensor(bias_quantized, bias_dims);
 
   // There is a single zero point of 0, and a single scale of
   // input_scale * filter_scale.
   int bias_zero_points[] = {1, 0};
   float bias_scales[] = {1, input_scale * filter_scale};
   TfLiteAffineQuantization bias_quant = {
-      tflite::testing::FloatArrayFromFloats(bias_scales),
-      tflite::testing::IntArrayFromInts(bias_zero_points), 0};
+      tflite_micro::testing::FloatArrayFromFloats(bias_scales),
+      tflite_micro::testing::IntArrayFromInts(bias_zero_points), 0};
   bias_tensor.quantization = {kTfLiteAffineQuantization, &bias_quant};
 
   // Create per-tensor quantized int8_t output tensor.
   int8_t output_quantized[kSampleSize];
-  TfLiteTensor output_tensor = tflite::testing::CreateQuantizedTensor(
+  TfLiteTensor output_tensor = tflite_micro::testing::CreateQuantizedTensor(
       output_quantized, output_dims, output_scale, output_zero_point);
   // Set zero point and scale arrays with a single element for each.
   int output_zero_points[] = {1, output_zero_point};
   float output_scales[] = {1, output_scale};
   TfLiteAffineQuantization output_quant = {
-      tflite::testing::FloatArrayFromFloats(output_scales),
-      tflite::testing::IntArrayFromInts(output_zero_points), 0};
+      tflite_micro::testing::FloatArrayFromFloats(output_scales),
+      tflite_micro::testing::IntArrayFromInts(output_zero_points), 0};
   output_tensor.quantization = {kTfLiteAffineQuantization, &output_quant};
 
   // The 3 inputs include the input, filter and bias tensors.
@@ -1065,16 +1065,16 @@ TF_LITE_MICRO_TEST(Int8Input32x1Filter32x32ShouldMatchGolden) {
   };
 
   int8_t golden_quantized[kSampleSize];
-  tflite::Quantize(expected_output, golden_quantized, output_dims_count,
+  tflite_micro::Quantize(expected_output, golden_quantized, output_dims_count,
                    output_scale, output_zero_point);
 
   // Rounding errors due to quantization should not exceed 1.
   constexpr int kQuantizationTolerance = 1;
 
   TF_LITE_MICRO_EXPECT_EQ(
-      kTfLiteOk, tflite::testing::ValidateConvGoldens(
+      kTfLiteOk, tflite_micro::testing::ValidateConvGoldens(
                      tensors, kTensorsSize, golden_quantized, output_dims_count,
-                     &conv_params, tflite::Register_CONV_2D(), output_quantized,
+                     &conv_params, tflite_micro::Register_CONV_2D(), output_quantized,
                      kQuantizationTolerance));
 }
 
@@ -1084,15 +1084,15 @@ TF_LITE_MICRO_TEST(Int8Input32x1Filter32x32ShouldMatchGolden) {
 // Filter tensor is of dimension 8x3x3x3 with different scales per output
 // channel. Some arbitrary parameters come from the above issue.
 TF_LITE_MICRO_TEST(Int8Filter8x3x3x3PerChannelScaleRelu6ShouldMatchGolden) {
-  using tflite::ElementCount;
-  using tflite::kConvBiasQuantized8;
-  using tflite::kConvFilter8x3x3x3;
-  using tflite::kConvGoldenOutput1x16x16x8;
-  using tflite::kConvInput1x32x32x3;
-  using tflite::testing::CreateTensor;
-  using tflite::testing::FloatArrayFromFloats;
-  using tflite::testing::IntArrayFromInts;
-  using tflite::testing::ValidateConvGoldens;
+  using tflite_micro::ElementCount;
+  using tflite_micro::kConvBiasQuantized8;
+  using tflite_micro::kConvFilter8x3x3x3;
+  using tflite_micro::kConvGoldenOutput1x16x16x8;
+  using tflite_micro::kConvInput1x32x32x3;
+  using tflite_micro::testing::CreateTensor;
+  using tflite_micro::testing::FloatArrayFromFloats;
+  using tflite_micro::testing::IntArrayFromInts;
+  using tflite_micro::testing::ValidateConvGoldens;
 
   constexpr int kInDepth = 3;
   constexpr int kOutDepth = 8;
@@ -1178,14 +1178,14 @@ TF_LITE_MICRO_TEST(Int8Filter8x3x3x3PerChannelScaleRelu6ShouldMatchGolden) {
       output_tensor,
   };
 
-  TfLiteConvParams conv_params{tflite::testing::common_conv_params};
+  TfLiteConvParams conv_params{tflite_micro::testing::common_conv_params};
   conv_params.activation = kTfLiteActRelu6;
 
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteOk,
       ValidateConvGoldens(tensors, tensors_size, kConvGoldenOutput1x16x16x8,
                           output_dims_count, &conv_params,
-                          tflite::Register_CONV_2D(), output_data,
+                          tflite_micro::Register_CONV_2D(), output_data,
                           1.0 /* tolerance */));
 }
 
diff --git a/tensorflow/lite/micro/kernels/conv_test.h b/tensorflow/lite/micro/kernels/conv_test.h
index c655f043..d23f18c9 100644
--- a/tensorflow/lite/micro/kernels/conv_test.h
+++ b/tensorflow/lite/micro/kernels/conv_test.h
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 
 TfLiteStatus InvokeConv(TfLiteTensor* tensors, int tensors_size,
@@ -89,6 +89,6 @@ TfLiteStatus TestConvQuantizedPerChannel(
     TFLMRegistration registration, int16_t* output_data);
 
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_CONV_TEST_H_
diff --git a/tensorflow/lite/micro/kernels/conv_test_common.cc b/tensorflow/lite/micro/kernels/conv_test_common.cc
index a0f733b8..9f3a8949 100644
--- a/tensorflow/lite/micro/kernels/conv_test_common.cc
+++ b/tensorflow/lite/micro/kernels/conv_test_common.cc
@@ -15,7 +15,7 @@ limitations under the License.
 
 #include "tensorflow/lite/micro/kernels/conv_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 
 template <typename T>
@@ -177,7 +177,7 @@ TfLiteStatus TestConvQuantizedPerChannel(
       output_tensor,
   };
 
-  tflite::Quantize(expected_output_data, expected_output_data_quantized,
+  tflite_micro::Quantize(expected_output_data, expected_output_data_quantized,
                    output_dims_count, output_scale, output_zero_point);
   return ValidateConvGoldens(
       tensors, tensors_size, expected_output_data_quantized, output_dims_count,
@@ -244,4 +244,4 @@ TfLiteStatus TestConvQuantizedPerChannel(
 }
 
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/cumsum.cc b/tensorflow/lite/micro/kernels/cumsum.cc
index f62f2a51..26669471 100644
--- a/tensorflow/lite/micro/kernels/cumsum.cc
+++ b/tensorflow/lite/micro/kernels/cumsum.cc
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 constexpr int kInputTensor = 0;
@@ -110,17 +110,17 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   const TfLiteEvalTensor* axis_tensor =
-      tflite::micro::GetEvalInput(context, node, kAxisTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kAxisTensor);
 
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
   auto* cs_params = static_cast<TfLiteCumsumParams*>(node->builtin_data);
-  auto input_shape = tflite::micro::GetTensorShape(input);
+  auto input_shape = tflite_micro::micro::GetTensorShape(input);
 
-  int32_t axis = *tflite::micro::GetTensorData<int32_t>(axis_tensor);
+  int32_t axis = *tflite_micro::micro::GetTensorData<int32_t>(axis_tensor);
   if (axis < 0) axis += input_shape.DimensionsCount();
 
   if (axis < 0 || axis >= input_shape.DimensionsCount()) {
@@ -130,10 +130,10 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 
   switch (input->type) {
     case kTfLiteFloat32: {
-      reference_ops::CumSum(tflite::micro::GetTensorData<float>(input),
+      reference_ops::CumSum(tflite_micro::micro::GetTensorData<float>(input),
                             input_shape, axis, cs_params->exclusive,
                             cs_params->reverse,
-                            tflite::micro::GetTensorData<float>(output));
+                            tflite_micro::micro::GetTensorData<float>(output));
       return kTfLiteOk;
     } break;
 
@@ -149,10 +149,10 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
       params.output_shift = data->output_shift;
       SetActivationParams(data->output_activation_min,
                           data->output_activation_max, &params);
-      reference_ops::CumSum(params, tflite::micro::GetTensorData<int8_t>(input),
+      reference_ops::CumSum(params, tflite_micro::micro::GetTensorData<int8_t>(input),
                             input_shape, axis, cs_params->exclusive,
                             cs_params->reverse,
-                            tflite::micro::GetTensorData<int8_t>(output));
+                            tflite_micro::micro::GetTensorData<int8_t>(output));
       return kTfLiteOk;
     } break;
 
@@ -169,7 +169,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_CUMSUM() {
-  return tflite::micro::RegisterOp(nullptr, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/cumsum_test.cc b/tensorflow/lite/micro/kernels/cumsum_test.cc
index 1d85f696..f52aa86e 100644
--- a/tensorflow/lite/micro/kernels/cumsum_test.cc
+++ b/tensorflow/lite/micro/kernels/cumsum_test.cc
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -43,7 +43,7 @@ void ExecuteCumSumTest(CumSumTestParams& test_params, TfLiteTensor* tensors,
   params.exclusive = test_params.exclusive;
   params.reverse = test_params.reverse;
 
-  const TFLMRegistration registration = tflite::Register_CUMSUM();
+  const TFLMRegistration registration = tflite_micro::Register_CUMSUM();
   micro::KernelRunner runner(registration, tensors, tensors_count, inputs_array,
                              outputs_array, static_cast<void*>(&params));
 
@@ -132,7 +132,7 @@ void TestCumSumQuantized(CumSumTestParams& test_params,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -144,10 +144,10 @@ TF_LITE_MICRO_TEST(CumSumOpTestSimpleTest) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
 
-  tflite::testing::CumSumTestParams test_params;
+  tflite_micro::testing::CumSumTestParams test_params;
   test_params.axis = 1;
 
-  tflite::testing::TestCumSum(test_params, kDims, kInput, kDims, kExpect,
+  tflite_micro::testing::TestCumSum(test_params, kDims, kInput, kDims, kExpect,
                               output_data);
 }
 
@@ -159,10 +159,10 @@ TF_LITE_MICRO_TEST(CumSumOpTestSimpleAxis0Test) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
 
-  tflite::testing::CumSumTestParams test_params;
+  tflite_micro::testing::CumSumTestParams test_params;
   test_params.axis = 0;
 
-  tflite::testing::TestCumSum(test_params, kDims, kInput, kDims, kExpect,
+  tflite_micro::testing::TestCumSum(test_params, kDims, kInput, kDims, kExpect,
                               output_data);
 }
 
@@ -174,10 +174,10 @@ TF_LITE_MICRO_TEST(CumSumOpTestSimple1DTest) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
 
-  tflite::testing::CumSumTestParams test_params;
+  tflite_micro::testing::CumSumTestParams test_params;
   test_params.axis = 0;
 
-  tflite::testing::TestCumSum(test_params, kDims, kInput, kDims, kExpect,
+  tflite_micro::testing::TestCumSum(test_params, kDims, kInput, kDims, kExpect,
                               output_data);
 }
 
@@ -189,11 +189,11 @@ TF_LITE_MICRO_TEST(CumSumOpTestSimpleReverseTest) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
 
-  tflite::testing::CumSumTestParams test_params;
+  tflite_micro::testing::CumSumTestParams test_params;
   test_params.axis = 1;
   test_params.reverse = true;
 
-  tflite::testing::TestCumSum(test_params, kDims, kInput, kDims, kExpect,
+  tflite_micro::testing::TestCumSum(test_params, kDims, kInput, kDims, kExpect,
                               output_data);
 }
 
@@ -205,11 +205,11 @@ TF_LITE_MICRO_TEST(CumSumOpTestSimpleExclusiveTest) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
 
-  tflite::testing::CumSumTestParams test_params;
+  tflite_micro::testing::CumSumTestParams test_params;
   test_params.axis = 1;
   test_params.exclusive = true;
 
-  tflite::testing::TestCumSum(test_params, kDims, kInput, kDims, kExpect,
+  tflite_micro::testing::TestCumSum(test_params, kDims, kInput, kDims, kExpect,
                               output_data);
 }
 
@@ -221,12 +221,12 @@ TF_LITE_MICRO_TEST(CumSumOpTestSimpleReverseExclusiveTest) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
 
-  tflite::testing::CumSumTestParams test_params;
+  tflite_micro::testing::CumSumTestParams test_params;
   test_params.axis = -1;
   test_params.exclusive = true;
   test_params.reverse = true;
 
-  tflite::testing::TestCumSum(test_params, kDims, kInput, kDims, kExpect,
+  tflite_micro::testing::TestCumSum(test_params, kDims, kInput, kDims, kExpect,
                               output_data);
 }
 
@@ -238,14 +238,14 @@ TF_LITE_MICRO_TEST(CumSumOpTestSimpleTestInt8) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
 
-  tflite::testing::CumSumTestParams test_params;
+  tflite_micro::testing::CumSumTestParams test_params;
   test_params.axis = 1;
 
-  tflite::testing::TestQuantParams<int8_t, kOutputCount> params = {};
+  tflite_micro::testing::TestQuantParams<int8_t, kOutputCount> params = {};
   params.data_min = -26.0f;
   params.data_max = 26.0f;
 
-  tflite::testing::TestCumSumQuantized<int8_t, kOutputCount>(
+  tflite_micro::testing::TestCumSumQuantized<int8_t, kOutputCount>(
       test_params, &params, kDims, kInput, kDims, kExpect, output_data);
 }
 
@@ -257,14 +257,14 @@ TF_LITE_MICRO_TEST(CumSumOpTestSimpleAxis0TestInt8) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
 
-  tflite::testing::CumSumTestParams test_params;
+  tflite_micro::testing::CumSumTestParams test_params;
   test_params.axis = 0;
 
-  tflite::testing::TestQuantParams<int8_t, kOutputCount> params = {};
+  tflite_micro::testing::TestQuantParams<int8_t, kOutputCount> params = {};
   params.data_min = -12.0f;
   params.data_max = 12.0f;
 
-  tflite::testing::TestCumSumQuantized<int8_t, kOutputCount>(
+  tflite_micro::testing::TestCumSumQuantized<int8_t, kOutputCount>(
       test_params, &params, kDims, kInput, kDims, kExpect, output_data);
 }
 
@@ -276,14 +276,14 @@ TF_LITE_MICRO_TEST(CumSumOpTestSimple1DTestInt8) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
 
-  tflite::testing::CumSumTestParams test_params;
+  tflite_micro::testing::CumSumTestParams test_params;
   test_params.axis = 0;
 
-  tflite::testing::TestQuantParams<int8_t, kOutputCount> params = {};
+  tflite_micro::testing::TestQuantParams<int8_t, kOutputCount> params = {};
   params.data_min = -36.0f;
   params.data_max = 36.0f;
 
-  tflite::testing::TestCumSumQuantized<int8_t, kOutputCount>(
+  tflite_micro::testing::TestCumSumQuantized<int8_t, kOutputCount>(
       test_params, &params, kDims, kInput, kDims, kExpect, output_data);
 }
 
@@ -295,15 +295,15 @@ TF_LITE_MICRO_TEST(CumSumOpTestSimpleReverseTestInt8) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
 
-  tflite::testing::CumSumTestParams test_params;
+  tflite_micro::testing::CumSumTestParams test_params;
   test_params.axis = 1;
   test_params.reverse = true;
 
-  tflite::testing::TestQuantParams<int8_t, kOutputCount> params = {};
+  tflite_micro::testing::TestQuantParams<int8_t, kOutputCount> params = {};
   params.data_min = -26.0f;
   params.data_max = 26.0f;
 
-  tflite::testing::TestCumSumQuantized<int8_t, kOutputCount>(
+  tflite_micro::testing::TestCumSumQuantized<int8_t, kOutputCount>(
       test_params, &params, kDims, kInput, kDims, kExpect, output_data);
 }
 
@@ -315,15 +315,15 @@ TF_LITE_MICRO_TEST(CumSumOpTestSimpleExclusiveTestInt8) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
 
-  tflite::testing::CumSumTestParams test_params;
+  tflite_micro::testing::CumSumTestParams test_params;
   test_params.axis = 1;
   test_params.exclusive = true;
 
-  tflite::testing::TestQuantParams<int8_t, kOutputCount> params = {};
+  tflite_micro::testing::TestQuantParams<int8_t, kOutputCount> params = {};
   params.data_min = -18.0f;
   params.data_max = 18.0f;
 
-  tflite::testing::TestCumSumQuantized<int8_t, kOutputCount>(
+  tflite_micro::testing::TestCumSumQuantized<int8_t, kOutputCount>(
       test_params, &params, kDims, kInput, kDims, kExpect, output_data);
 }
 
@@ -335,16 +335,16 @@ TF_LITE_MICRO_TEST(CumSumOpTestSimpleReverseExclusiveTestInt8) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
 
-  tflite::testing::CumSumTestParams test_params;
+  tflite_micro::testing::CumSumTestParams test_params;
   test_params.axis = -1;
   test_params.exclusive = true;
   test_params.reverse = true;
 
-  tflite::testing::TestQuantParams<int8_t, kOutputCount> params = {};
+  tflite_micro::testing::TestQuantParams<int8_t, kOutputCount> params = {};
   params.data_min = -21.0f;
   params.data_max = 21.0f;
 
-  tflite::testing::TestCumSumQuantized<int8_t, kOutputCount>(
+  tflite_micro::testing::TestCumSumQuantized<int8_t, kOutputCount>(
       test_params, &params, kDims, kInput, kDims, kExpect, output_data);
 }
 TF_LITE_MICRO_TESTS_END
diff --git a/tensorflow/lite/micro/kernels/depth_to_space.cc b/tensorflow/lite/micro/kernels/depth_to_space.cc
index 7e0a8fa5..86f9bfe0 100644
--- a/tensorflow/lite/micro/kernels/depth_to_space.cc
+++ b/tensorflow/lite/micro/kernels/depth_to_space.cc
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 constexpr int kInputTensor = 0;
@@ -79,8 +79,8 @@ TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node) {
   // is a temporary allocation.  We must therefore relocate the dims
   // from the FlatBuffer to the persistent storage arena.
   TfLiteEvalTensor* output_eval =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
-  TF_LITE_ENSURE_OK(context, tflite::micro::CreateWritableTensorDimsWithCopy(
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
+  TF_LITE_ENSURE_OK(context, tflite_micro::micro::CreateWritableTensorDimsWithCopy(
                                  context, output, output_eval));
   output->dims->data[kBatchRank] = input->dims->data[kBatchRank];
   output->dims->data[kHeightRank] = output_height;
@@ -102,27 +102,27 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
       reinterpret_cast<TfLiteDepthToSpaceParams*>(node->builtin_data);
 
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
-  tflite::DepthToSpaceParams op_params;
+  tflite_micro::DepthToSpaceParams op_params;
   op_params.block_size = static_cast<int32_t>(params->block_size);
 
   switch (input->type) {  // Already know in/out types are same.
     case kTfLiteFloat32:
       reference_ops::DepthToSpace(op_params,
-                                  tflite::micro::GetTensorShape(input),
-                                  tflite::micro::GetTensorData<float>(input),
-                                  tflite::micro::GetTensorShape(output),
-                                  tflite::micro::GetTensorData<float>(output));
+                                  tflite_micro::micro::GetTensorShape(input),
+                                  tflite_micro::micro::GetTensorData<float>(input),
+                                  tflite_micro::micro::GetTensorShape(output),
+                                  tflite_micro::micro::GetTensorData<float>(output));
       break;
     case kTfLiteInt8:
       reference_ops::DepthToSpace(op_params,
-                                  tflite::micro::GetTensorShape(input),
-                                  tflite::micro::GetTensorData<int8_t>(input),
-                                  tflite::micro::GetTensorShape(output),
-                                  tflite::micro::GetTensorData<int8_t>(output));
+                                  tflite_micro::micro::GetTensorShape(input),
+                                  tflite_micro::micro::GetTensorData<int8_t>(input),
+                                  tflite_micro::micro::GetTensorShape(output),
+                                  tflite_micro::micro::GetTensorData<int8_t>(output));
       break;
     default:
       MicroPrintf("DEPTH_TO_SPACE only supports FLOAT32 and INT8, got %s.",
@@ -136,7 +136,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_DEPTH_TO_SPACE() {
-  return tflite::micro::RegisterOp(nullptr, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/depth_to_space_test.cc b/tensorflow/lite/micro/kernels/depth_to_space_test.cc
index 4053436d..3bad065a 100644
--- a/tensorflow/lite/micro/kernels/depth_to_space_test.cc
+++ b/tensorflow/lite/micro/kernels/depth_to_space_test.cc
@@ -20,7 +20,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -42,7 +42,7 @@ void ExecuteDepthToSpaceTest(const DepthToSpaceTestParams& params,
   TfLiteDepthToSpaceParams op_params = {};
   op_params.block_size = params.block_size;
 
-  const TFLMRegistration registration = tflite::Register_DEPTH_TO_SPACE();
+  const TFLMRegistration registration = tflite_micro::Register_DEPTH_TO_SPACE();
   micro::KernelRunner runner(registration, tensors, tensors_count, inputs_array,
                              outputs_array, static_cast<void*>(&op_params));
 
@@ -136,7 +136,7 @@ void TestDepthToSpaceQuantized(DepthToSpaceTestParams& params,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -147,10 +147,10 @@ TF_LITE_MICRO_TEST(DepthToSpaceOpModelFloat32_1114_2) {
   constexpr float kExpect[] = {1.4, 2.3, 3.2, 4.1};
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
-  tflite::testing::DepthToSpaceTestParams params;
+  tflite_micro::testing::DepthToSpaceTestParams params;
   params.block_size = 2;
 
-  tflite::testing::TestDepthToSpace(params, kInputDims, kInput, kExpectDims,
+  tflite_micro::testing::TestDepthToSpace(params, kInputDims, kInput, kExpectDims,
                                     kExpect, output_data);
 }
 
@@ -161,10 +161,10 @@ TF_LITE_MICRO_TEST(DepthToSpaceOpModelFloat32_1124_2) {
   constexpr float kExpect[] = {1, 2, 5, 6, 3, 4, 7, 8};
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
-  tflite::testing::DepthToSpaceTestParams params;
+  tflite_micro::testing::DepthToSpaceTestParams params;
   params.block_size = 2;
 
-  tflite::testing::TestDepthToSpace(params, kInputDims, kInput, kExpectDims,
+  tflite_micro::testing::TestDepthToSpace(params, kInputDims, kInput, kExpectDims,
                                     kExpect, output_data);
 }
 
@@ -175,10 +175,10 @@ TF_LITE_MICRO_TEST(DepthToSpaceOpModelFloat32_1214_2) {
   constexpr float kExpect[] = {1, 2, 3, 4, 5, 6, 7, 8};
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
-  tflite::testing::DepthToSpaceTestParams params;
+  tflite_micro::testing::DepthToSpaceTestParams params;
   params.block_size = 2;
 
-  tflite::testing::TestDepthToSpace(params, kInputDims, kInput, kExpectDims,
+  tflite_micro::testing::TestDepthToSpace(params, kInputDims, kInput, kExpectDims,
                                     kExpect, output_data);
 }
 
@@ -191,10 +191,10 @@ TF_LITE_MICRO_TEST(DepthToSpaceOpModelFloat32_1224_2) {
                                9, 10, 13, 14, 11, 12, 15, 16};
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
-  tflite::testing::DepthToSpaceTestParams params;
+  tflite_micro::testing::DepthToSpaceTestParams params;
   params.block_size = 2;
 
-  tflite::testing::TestDepthToSpace(params, kInputDims, kInput, kExpectDims,
+  tflite_micro::testing::TestDepthToSpace(params, kInputDims, kInput, kExpectDims,
                                     kExpect, output_data);
 }
 
@@ -205,10 +205,10 @@ TF_LITE_MICRO_TEST(DepthToSpaceOpModelFloat32_1111_1) {
   constexpr float kExpect[] = {4};
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
-  tflite::testing::DepthToSpaceTestParams params;
+  tflite_micro::testing::DepthToSpaceTestParams params;
   params.block_size = 1;
 
-  tflite::testing::TestDepthToSpace(params, kInputDims, kInput, kExpectDims,
+  tflite_micro::testing::TestDepthToSpace(params, kInputDims, kInput, kExpectDims,
                                     kExpect, output_data);
 }
 
@@ -219,13 +219,13 @@ TF_LITE_MICRO_TEST(DepthToSpaceOpModelInt8_1114_2) {
   constexpr float kExpect[] = {1.4, 2.3, 3.2, 4.1};
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
-  tflite::testing::DepthToSpaceTestParams params;
+  tflite_micro::testing::DepthToSpaceTestParams params;
   params.block_size = 2;
-  tflite::testing::TestQuantParams<int8_t, kOutputCount> quant_params = {};
+  tflite_micro::testing::TestQuantParams<int8_t, kOutputCount> quant_params = {};
   quant_params.data_min = 0.0;
   quant_params.data_max = 5.0;
 
-  tflite::testing::TestDepthToSpaceQuantized<int8_t, kOutputCount>(
+  tflite_micro::testing::TestDepthToSpaceQuantized<int8_t, kOutputCount>(
       params, &quant_params, kInputDims, kInput, kExpectDims, kExpect,
       output_data);
 }
@@ -237,13 +237,13 @@ TF_LITE_MICRO_TEST(DepthToSpaceOpModelInt8_1124_2) {
   constexpr float kExpect[] = {1, 2, 5, 6, 3, 4, 7, 8};
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
-  tflite::testing::DepthToSpaceTestParams params;
+  tflite_micro::testing::DepthToSpaceTestParams params;
   params.block_size = 2;
-  tflite::testing::TestQuantParams<int8_t, kOutputCount> quant_params = {};
+  tflite_micro::testing::TestQuantParams<int8_t, kOutputCount> quant_params = {};
   quant_params.data_min = 0.0;
   quant_params.data_max = 9.0;
 
-  tflite::testing::TestDepthToSpaceQuantized<int8_t, kOutputCount>(
+  tflite_micro::testing::TestDepthToSpaceQuantized<int8_t, kOutputCount>(
       params, &quant_params, kInputDims, kInput, kExpectDims, kExpect,
       output_data);
 }
@@ -255,13 +255,13 @@ TF_LITE_MICRO_TEST(DepthToSpaceOpModelInt8_1214_2) {
   constexpr float kExpect[] = {1, 2, 3, 4, 5, 6, 7, 8};
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
-  tflite::testing::DepthToSpaceTestParams params;
+  tflite_micro::testing::DepthToSpaceTestParams params;
   params.block_size = 2;
-  tflite::testing::TestQuantParams<int8_t, kOutputCount> quant_params = {};
+  tflite_micro::testing::TestQuantParams<int8_t, kOutputCount> quant_params = {};
   quant_params.data_min = 0.0;
   quant_params.data_max = 9.0;
 
-  tflite::testing::TestDepthToSpaceQuantized<int8_t, kOutputCount>(
+  tflite_micro::testing::TestDepthToSpaceQuantized<int8_t, kOutputCount>(
       params, &quant_params, kInputDims, kInput, kExpectDims, kExpect,
       output_data);
 }
@@ -275,13 +275,13 @@ TF_LITE_MICRO_TEST(DepthToSpaceOpModelInt8_1224_2) {
                                9, 10, 13, 14, 11, 12, 15, 16};
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
-  tflite::testing::DepthToSpaceTestParams params;
+  tflite_micro::testing::DepthToSpaceTestParams params;
   params.block_size = 2;
-  tflite::testing::TestQuantParams<int8_t, kOutputCount> quant_params = {};
+  tflite_micro::testing::TestQuantParams<int8_t, kOutputCount> quant_params = {};
   quant_params.data_min = 0.0;
   quant_params.data_max = 17.0;
 
-  tflite::testing::TestDepthToSpaceQuantized<int8_t, kOutputCount>(
+  tflite_micro::testing::TestDepthToSpaceQuantized<int8_t, kOutputCount>(
       params, &quant_params, kInputDims, kInput, kExpectDims, kExpect,
       output_data);
 }
@@ -293,13 +293,13 @@ TF_LITE_MICRO_TEST(DepthToSpaceOpModelInt8_1111_1) {
   constexpr float kExpect[] = {4};
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
-  tflite::testing::DepthToSpaceTestParams params;
+  tflite_micro::testing::DepthToSpaceTestParams params;
   params.block_size = 1;
-  tflite::testing::TestQuantParams<int8_t, kOutputCount> quant_params = {};
+  tflite_micro::testing::TestQuantParams<int8_t, kOutputCount> quant_params = {};
   quant_params.data_min = 3.0;
   quant_params.data_max = 5.0;
 
-  tflite::testing::TestDepthToSpaceQuantized<int8_t, kOutputCount>(
+  tflite_micro::testing::TestDepthToSpaceQuantized<int8_t, kOutputCount>(
       params, &quant_params, kInputDims, kInput, kExpectDims, kExpect,
       output_data);
 }
diff --git a/tensorflow/lite/micro/kernels/depthwise_conv.cc b/tensorflow/lite/micro/kernels/depthwise_conv.cc
index 398f8cd0..65f9868f 100644
--- a/tensorflow/lite/micro/kernels/depthwise_conv.cc
+++ b/tensorflow/lite/micro/kernels/depthwise_conv.cc
@@ -24,7 +24,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 void* Init(TfLiteContext* context, const char* buffer, size_t length) {
@@ -41,28 +41,28 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const OpDataConv& data = *(static_cast<const OpDataConv*>(node->user_data));
 
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kDepthwiseConvOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kDepthwiseConvOutputTensor);
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kDepthwiseConvInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kDepthwiseConvInputTensor);
   const TfLiteEvalTensor* filter =
-      tflite::micro::GetEvalInput(context, node, kDepthwiseConvWeightsTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kDepthwiseConvWeightsTensor);
   const TfLiteEvalTensor* bias =
       (NumInputs(node) == 3)
-          ? tflite::micro::GetEvalInput(context, node, kDepthwiseConvBiasTensor)
+          ? tflite_micro::micro::GetEvalInput(context, node, kDepthwiseConvBiasTensor)
           : nullptr;
 
   switch (input->type) {  // Already know in/out types are same.
     case kTfLiteFloat32: {
-      tflite::reference_ops::DepthwiseConv(
+      tflite_micro::reference_ops::DepthwiseConv(
           DepthwiseConvParamsFloat(params, data),
-          tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<float>(input),
-          tflite::micro::GetTensorShape(filter),
-          tflite::micro::GetTensorData<float>(filter),
-          tflite::micro::GetTensorShape(bias),
-          tflite::micro::GetOptionalTensorData<float>(bias),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<float>(output));
+          tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<float>(input),
+          tflite_micro::micro::GetTensorShape(filter),
+          tflite_micro::micro::GetTensorData<float>(filter),
+          tflite_micro::micro::GetTensorShape(bias),
+          tflite_micro::micro::GetOptionalTensorData<float>(bias),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<float>(output));
       break;
     }
     case kTfLiteInt8: {
@@ -70,34 +70,34 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
         case kTfLiteInt4: {
           int8_t* unpacked_filter_data = static_cast<int8_t*>(
               context->GetScratchBuffer(context, data.filter_buffer_index));
-          tflite::tensor_utils::UnpackDenseInt4IntoInt8(
-              tflite::micro::GetTensorData<int8_t>(filter),
-              tflite::micro::GetTensorShape(filter).FlatSize(),
+          tflite_micro::tensor_utils::UnpackDenseInt4IntoInt8(
+              tflite_micro::micro::GetTensorData<int8_t>(filter),
+              tflite_micro::micro::GetTensorShape(filter).FlatSize(),
               unpacked_filter_data);
           reference_integer_ops::DepthwiseConvPerChannel(
               DepthwiseConvParamsQuantized(params, data),
               data.per_channel_output_multiplier, data.per_channel_output_shift,
-              tflite::micro::GetTensorShape(input),
-              tflite::micro::GetTensorData<int8_t>(input),
-              tflite::micro::GetTensorShape(filter), unpacked_filter_data,
-              tflite::micro::GetTensorShape(bias),
-              tflite::micro::GetOptionalTensorData<int32_t>(bias),
-              tflite::micro::GetTensorShape(output),
-              tflite::micro::GetTensorData<int8_t>(output));
+              tflite_micro::micro::GetTensorShape(input),
+              tflite_micro::micro::GetTensorData<int8_t>(input),
+              tflite_micro::micro::GetTensorShape(filter), unpacked_filter_data,
+              tflite_micro::micro::GetTensorShape(bias),
+              tflite_micro::micro::GetOptionalTensorData<int32_t>(bias),
+              tflite_micro::micro::GetTensorShape(output),
+              tflite_micro::micro::GetTensorData<int8_t>(output));
           break;
         }
         case kTfLiteInt8: {
           reference_integer_ops::DepthwiseConvPerChannel(
               DepthwiseConvParamsQuantized(params, data),
               data.per_channel_output_multiplier, data.per_channel_output_shift,
-              tflite::micro::GetTensorShape(input),
-              tflite::micro::GetTensorData<int8_t>(input),
-              tflite::micro::GetTensorShape(filter),
-              tflite::micro::GetTensorData<int8_t>(filter),
-              tflite::micro::GetTensorShape(bias),
-              tflite::micro::GetOptionalTensorData<int32_t>(bias),
-              tflite::micro::GetTensorShape(output),
-              tflite::micro::GetTensorData<int8_t>(output));
+              tflite_micro::micro::GetTensorShape(input),
+              tflite_micro::micro::GetTensorData<int8_t>(input),
+              tflite_micro::micro::GetTensorShape(filter),
+              tflite_micro::micro::GetTensorData<int8_t>(filter),
+              tflite_micro::micro::GetTensorShape(bias),
+              tflite_micro::micro::GetOptionalTensorData<int32_t>(bias),
+              tflite_micro::micro::GetTensorShape(output),
+              tflite_micro::micro::GetTensorData<int8_t>(output));
           break;
         }
         default:
@@ -114,14 +114,14 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
           reference_integer_ops::DepthwiseConvPerChannel(
               DepthwiseConvParamsQuantized(params, data),
               data.per_channel_output_multiplier, data.per_channel_output_shift,
-              tflite::micro::GetTensorShape(input),
-              tflite::micro::GetTensorData<int16_t>(input),
-              tflite::micro::GetTensorShape(filter),
-              tflite::micro::GetTensorData<int8_t>(filter),
-              tflite::micro::GetTensorShape(bias),
-              tflite::micro::GetOptionalTensorData<int64_t>(bias),
-              tflite::micro::GetTensorShape(output),
-              tflite::micro::GetTensorData<int16_t>(output));
+              tflite_micro::micro::GetTensorShape(input),
+              tflite_micro::micro::GetTensorData<int16_t>(input),
+              tflite_micro::micro::GetTensorShape(filter),
+              tflite_micro::micro::GetTensorData<int8_t>(filter),
+              tflite_micro::micro::GetTensorShape(bias),
+              tflite_micro::micro::GetOptionalTensorData<int64_t>(bias),
+              tflite_micro::micro::GetTensorShape(output),
+              tflite_micro::micro::GetTensorData<int16_t>(output));
           break;
         }
         default:
@@ -143,7 +143,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_DEPTHWISE_CONV_2D() {
-  return tflite::micro::RegisterOp(Init, DepthwiseConvPrepare, Eval);
+  return tflite_micro::micro::RegisterOp(Init, DepthwiseConvPrepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/depthwise_conv.h b/tensorflow/lite/micro/kernels/depthwise_conv.h
index d8cc78db..a035d580 100644
--- a/tensorflow/lite/micro/kernels/depthwise_conv.h
+++ b/tensorflow/lite/micro/kernels/depthwise_conv.h
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/types.h"
 #include "tensorflow/lite/micro/kernels/conv.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 extern const int kDepthwiseConvInputTensor;
 extern const int kDepthwiseConvWeightsTensor;
@@ -75,6 +75,6 @@ inline TFLMRegistration Register_DEPTHWISE_CONV_2D_INT16() {
 }
 #endif
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_DEPTHWISE_CONV_H_
diff --git a/tensorflow/lite/micro/kernels/depthwise_conv_common.cc b/tensorflow/lite/micro/kernels/depthwise_conv_common.cc
index 52804de3..41e2b7b6 100644
--- a/tensorflow/lite/micro/kernels/depthwise_conv_common.cc
+++ b/tensorflow/lite/micro/kernels/depthwise_conv_common.cc
@@ -25,7 +25,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/depthwise_conv.h"
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 const int kDepthwiseConvInputTensor = 0;
 const int kDepthwiseConvWeightsTensor = 1;
@@ -43,7 +43,7 @@ DepthwiseParams DepthwiseConvParamsFloat(
   DepthwiseParams op_params;
   CalculateActivationRange(params.activation, &op_params.float_activation_min,
                            &op_params.float_activation_max);
-  op_params.padding_type = tflite::micro::RuntimePaddingType(params.padding);
+  op_params.padding_type = tflite_micro::micro::RuntimePaddingType(params.padding);
   op_params.padding_values.width = data.padding.width;
   op_params.padding_values.height = data.padding.height;
   op_params.stride_width = params.stride_width;
@@ -64,7 +64,7 @@ DepthwiseParams DepthwiseConvParamsQuantized(
   op_params.output_offset = data.output_zero_point;
   op_params.output_multiplier = data.output_multiplier;
   op_params.output_shift = -data.output_shift;
-  op_params.padding_type = tflite::micro::RuntimePaddingType(params.padding);
+  op_params.padding_type = tflite_micro::micro::RuntimePaddingType(params.padding);
   op_params.padding_values.height = data.padding.height;
   op_params.padding_values.width = data.padding.width;
   op_params.stride_height = params.stride_height;
@@ -113,7 +113,7 @@ TfLiteStatus CalculateOpDataDepthwiseConv(
   if (data_type != kTfLiteFloat32) {
     int output_channels = filter->dims->data[kDepthwiseConvQuantizedDimension];
 
-    TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(
+    TF_LITE_ENSURE_STATUS(tflite_micro::PopulateConvolutionQuantizationParams(
         context, input, filter, bias, output, params.activation,
         &data->output_multiplier, &data->output_shift,
         &data->output_activation_min, &data->output_activation_max,
@@ -216,4 +216,4 @@ TfLiteStatus DepthwiseConvPrepare(TfLiteContext* context, TfLiteNode* node) {
   return kTfLiteOk;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/depthwise_conv_test.cc b/tensorflow/lite/micro/kernels/depthwise_conv_test.cc
index b50b40ae..5dca2627 100644
--- a/tensorflow/lite/micro/kernels/depthwise_conv_test.cc
+++ b/tensorflow/lite/micro/kernels/depthwise_conv_test.cc
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -69,7 +69,7 @@ TfLiteStatus ValidateDepthwiseConvGoldens(
   }
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, runner.Invoke());
 
-  const T* output_data = tflite::GetTensorData<T>(&tensors[kOutputTensorIndex]);
+  const T* output_data = tflite_micro::GetTensorData<T>(&tensors[kOutputTensorIndex]);
 
   for (int i = 0; i < output_length; ++i) {
     TF_LITE_MICRO_EXPECT_NEAR(expected_output_data[i], output_data[i],
@@ -219,7 +219,7 @@ void TestDepthwiseConvFloat(int* input_dims_data, const float* input_data,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -248,7 +248,7 @@ TF_LITE_MICRO_TEST(SimpleTest) {
   conv_params.stride_height = 1;
   conv_params.stride_width = 1;
 
-  tflite::testing::TestDepthwiseConvFloat(
+  tflite_micro::testing::TestDepthwiseConvFloat(
       input_shape, input_values, filter_shape, filter_values, bias_shape,
       bias_values, golden, output_shape, &conv_params, output_data);
 }
@@ -273,7 +273,7 @@ TF_LITE_MICRO_TEST(SimpleTestRelu) {
   conv_params.stride_height = 1;
   conv_params.stride_width = 1;
 
-  tflite::testing::TestDepthwiseConvFloat(
+  tflite_micro::testing::TestDepthwiseConvFloat(
       input_shape, input_values, filter_shape, filter_values, bias_shape,
       bias_values, golden_relu, output_shape, &conv_params, output_data);
 }
@@ -316,7 +316,7 @@ TF_LITE_MICRO_TEST(SimpleTestQuantizedPerChannelDepthMultiplier1) {
   conv_params.stride_height = 1;
   conv_params.stride_width = 1;
 
-  tflite::testing::TestDepthwiseConvQuantizedPerChannel(
+  tflite_micro::testing::TestDepthwiseConvQuantizedPerChannel(
       input_shape, input_values, input_quantized, input_scale, input_zero_point,
       filter_shape, filter_values, filter_quantized, bias_shape, bias_values,
       bias_quantized, output_shape, golden, golden_quantized, output_data,
@@ -359,7 +359,7 @@ TF_LITE_MICRO_TEST(TestQuantizedPerChannelDepthMultiplier1Relu6) {
   conv_params.stride_height = 1;
   conv_params.stride_width = 1;
 
-  tflite::testing::TestDepthwiseConvQuantizedPerChannel(
+  tflite_micro::testing::TestDepthwiseConvQuantizedPerChannel(
       input_shape, input_values, input_quantized, input_scale, input_zero_point,
       filter_shape, filter_values, filter_quantized, bias_shape, bias_values,
       bias_quantized, output_shape, golden, golden_quantized, output_data,
@@ -405,7 +405,7 @@ TF_LITE_MICRO_TEST(SimpleTestDilatedQuantizedPerChannel) {
   conv_params.stride_height = 1;
   conv_params.stride_width = 1;
 
-  tflite::testing::TestDepthwiseConvQuantizedPerChannel(
+  tflite_micro::testing::TestDepthwiseConvQuantizedPerChannel(
       input_shape, input_values, input_quantized, input_scale, input_zero_point,
       filter_shape, filter_values, filter_quantized, bias_shape, bias_values,
       bias_quantized, output_shape, golden, golden_quantized, output_data,
@@ -445,13 +445,13 @@ TF_LITE_MICRO_TEST(TestQuantizedPerChannelCompareWithFloat) {
   conv_params.stride_height = 1;
   conv_params.stride_width = 1;
 
-  tflite::testing::TestDepthwiseConvQuantizedPerChannel(
+  tflite_micro::testing::TestDepthwiseConvQuantizedPerChannel(
       input_dims, input_data, input_quantized, input_scale, input_zero_point,
       filter_dims, filter_data, filter_quantized, bias_dims, bias_data,
       bias_quantized, output_dims, golden, golden_quantized, output_data,
       output_scale, output_zero_point, &conv_params);
 
-  tflite::testing::TestDepthwiseConvFloat(
+  tflite_micro::testing::TestDepthwiseConvFloat(
       input_dims, input_data, filter_dims, filter_data, bias_dims, bias_data,
       golden, output_dims, &conv_params, output_float);
 }
@@ -484,52 +484,52 @@ TF_LITE_MICRO_TEST(PerChannelBroadcastQuantizationParams) {
   int32_t bias_quantized[bias_elements];
   int8_t golden_quantized[output_elements];
 
-  TfLiteIntArray* input_dims = tflite::testing::IntArrayFromInts(input_shape);
-  TfLiteIntArray* filter_dims = tflite::testing::IntArrayFromInts(filter_shape);
-  TfLiteIntArray* bias_dims = tflite::testing::IntArrayFromInts(bias_shape);
-  TfLiteIntArray* output_dims = tflite::testing::IntArrayFromInts(output_shape);
+  TfLiteIntArray* input_dims = tflite_micro::testing::IntArrayFromInts(input_shape);
+  TfLiteIntArray* filter_dims = tflite_micro::testing::IntArrayFromInts(filter_shape);
+  TfLiteIntArray* bias_dims = tflite_micro::testing::IntArrayFromInts(bias_shape);
+  TfLiteIntArray* output_dims = tflite_micro::testing::IntArrayFromInts(output_shape);
 
   // Create per-layer quantized int8_t input tensor.
-  TfLiteTensor input_tensor = tflite::testing::CreateQuantizedTensor(
+  TfLiteTensor input_tensor = tflite_micro::testing::CreateQuantizedTensor(
       input_values, input_quantized, input_dims, input_scale, 0);
   int input_zero_points[2] = {1, 0};
   float input_scales[2] = {1, input_scale};
   TfLiteAffineQuantization input_quant = {
-      tflite::testing::FloatArrayFromFloats(input_scales),
-      tflite::testing::IntArrayFromInts(input_zero_points), 0};
+      tflite_micro::testing::FloatArrayFromFloats(input_scales),
+      tflite_micro::testing::IntArrayFromInts(input_zero_points), 0};
   input_tensor.quantization = {kTfLiteAffineQuantization, &input_quant};
 
   // Create per-layer quantized int8_t filter tensor.
-  TfLiteTensor filter_tensor = tflite::testing::CreateQuantizedTensor(
+  TfLiteTensor filter_tensor = tflite_micro::testing::CreateQuantizedTensor(
       filter_values, filter_quantized, filter_dims, filter_scale, 0);
   int filter_zero_points[2] = {1, 0};
   float filter_scales[2] = {1, filter_scale};
   TfLiteAffineQuantization filter_quant = {
-      tflite::testing::FloatArrayFromFloats(filter_scales),
-      tflite::testing::IntArrayFromInts(filter_zero_points), 0};
+      tflite_micro::testing::FloatArrayFromFloats(filter_scales),
+      tflite_micro::testing::IntArrayFromInts(filter_zero_points), 0};
   filter_tensor.quantization = {kTfLiteAffineQuantization, &filter_quant};
 
   // Create per-layer quantized int32_t bias tensor.
-  tflite::SymmetricQuantize(bias_values, bias_quantized, bias_elements,
+  tflite_micro::SymmetricQuantize(bias_values, bias_quantized, bias_elements,
                             input_scale * output_scale);
   TfLiteTensor bias_tensor =
-      tflite::testing::CreateTensor(bias_quantized, bias_dims);
+      tflite_micro::testing::CreateTensor(bias_quantized, bias_dims);
 
   int bias_zero_points[2] = {1, 0};
   float bias_scales[2] = {1, input_scale * filter_scale};
   TfLiteAffineQuantization bias_quant = {
-      tflite::testing::FloatArrayFromFloats(bias_scales),
-      tflite::testing::IntArrayFromInts(bias_zero_points), 0};
+      tflite_micro::testing::FloatArrayFromFloats(bias_scales),
+      tflite_micro::testing::IntArrayFromInts(bias_zero_points), 0};
   bias_tensor.quantization = {kTfLiteAffineQuantization, &bias_quant};
 
   // Create per-layer quantized int8_t output tensor.
-  TfLiteTensor output_tensor = tflite::testing::CreateQuantizedTensor(
+  TfLiteTensor output_tensor = tflite_micro::testing::CreateQuantizedTensor(
       output_data, output_dims, output_scale, 0);
   int output_zero_points[2] = {1, 0};
   float output_scales[2] = {1, output_scale};
   TfLiteAffineQuantization output_quant = {
-      tflite::testing::FloatArrayFromFloats(output_scales),
-      tflite::testing::IntArrayFromInts(output_zero_points), 0};
+      tflite_micro::testing::FloatArrayFromFloats(output_scales),
+      tflite_micro::testing::IntArrayFromInts(output_zero_points), 0};
   output_tensor.quantization = {kTfLiteAffineQuantization, &output_quant};
 
   constexpr int inputs_size = 3;
@@ -542,7 +542,7 @@ TF_LITE_MICRO_TEST(PerChannelBroadcastQuantizationParams) {
       output_tensor,
   };
 
-  tflite::Quantize(golden, golden_quantized, output_dims_count, output_scale,
+  tflite_micro::Quantize(golden, golden_quantized, output_dims_count, output_scale,
                    0);
 
   TfLiteDepthwiseConvParams conv_params;
@@ -553,7 +553,7 @@ TF_LITE_MICRO_TEST(PerChannelBroadcastQuantizationParams) {
   conv_params.stride_width = 1;
 
   TF_LITE_MICRO_EXPECT_EQ(
-      kTfLiteOk, tflite::testing::ValidateDepthwiseConvGoldens(
+      kTfLiteOk, tflite_micro::testing::ValidateDepthwiseConvGoldens(
                      golden_quantized, output_dims_count, &conv_params, 1e-5,
                      tensors_size, tensors));
 }
@@ -586,33 +586,33 @@ TF_LITE_MICRO_TEST(FilterDimsNotMatchingAffineQuantization) {
   const int input_zero_point = 0;
   const int output_zero_point = 0;
 
-  TfLiteIntArray* input_dims = tflite::testing::IntArrayFromInts(input_shape);
-  TfLiteIntArray* filter_dims = tflite::testing::IntArrayFromInts(filter_shape);
-  TfLiteIntArray* bias_dims = tflite::testing::IntArrayFromInts(bias_shape);
-  TfLiteIntArray* output_dims = tflite::testing::IntArrayFromInts(output_shape);
+  TfLiteIntArray* input_dims = tflite_micro::testing::IntArrayFromInts(input_shape);
+  TfLiteIntArray* filter_dims = tflite_micro::testing::IntArrayFromInts(filter_shape);
+  TfLiteIntArray* bias_dims = tflite_micro::testing::IntArrayFromInts(bias_shape);
+  TfLiteIntArray* output_dims = tflite_micro::testing::IntArrayFromInts(output_shape);
 
   int filter_zero_points[5];
   float filter_scales[5];
   TfLiteAffineQuantization filter_quant;
   TfLiteAffineQuantization bias_quant;
-  TfLiteTensor input_tensor = tflite::testing::CreateQuantizedTensor(
+  TfLiteTensor input_tensor = tflite_micro::testing::CreateQuantizedTensor(
       input_data, input_quantized, input_dims, input_scale, input_zero_point);
   TfLiteTensor filter_tensor =
-      tflite::testing::CreateSymmetricPerChannelQuantizedTensor(
+      tflite_micro::testing::CreateSymmetricPerChannelQuantizedTensor(
           filter_data, filter_quantized, filter_dims, filter_scales,
           filter_zero_points, &filter_quant, 0 /* quantized dimension */);
   TfLiteTensor bias_tensor =
-      tflite::testing::CreatePerChannelQuantizedBiasTensor(
+      tflite_micro::testing::CreatePerChannelQuantizedBiasTensor(
           bias_data, bias_quantized, bias_dims, input_scale, &filter_scales[1],
           scales, zero_points, &bias_quant, 0);
-  TfLiteTensor output_tensor = tflite::testing::CreateQuantizedTensor(
+  TfLiteTensor output_tensor = tflite_micro::testing::CreateQuantizedTensor(
       output_data, output_dims, output_scale, output_zero_point);
 
   float input_scales[] = {1, input_scale};
   int input_zero_points[] = {1, input_zero_point};
   TfLiteAffineQuantization input_quant = {
-      tflite::testing::FloatArrayFromFloats(input_scales),
-      tflite::testing::IntArrayFromInts(input_zero_points), 0};
+      tflite_micro::testing::FloatArrayFromFloats(input_scales),
+      tflite_micro::testing::IntArrayFromInts(input_zero_points), 0};
   input_tensor.quantization = {kTfLiteAffineQuantization, &input_quant};
 
   constexpr int inputs_size = 3;
@@ -637,7 +637,7 @@ TF_LITE_MICRO_TEST(FilterDimsNotMatchingAffineQuantization) {
       filter_tensor.quantization.params);
   quant->scale->size = 2;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteError,
-                          tflite::testing::ValidateDepthwiseConvGoldens(
+                          tflite_micro::testing::ValidateDepthwiseConvGoldens(
                               golden_quantized, output_size, &conv_params, 1e-5,
                               tensors_size, tensors));
 
@@ -645,7 +645,7 @@ TF_LITE_MICRO_TEST(FilterDimsNotMatchingAffineQuantization) {
   quant->scale->size = filter_shape[0];
   quant->zero_point->size = 2;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteError,
-                          tflite::testing::ValidateDepthwiseConvGoldens(
+                          tflite_micro::testing::ValidateDepthwiseConvGoldens(
                               golden_quantized, output_size, &conv_params, 1e-5,
                               tensors_size, tensors));
 }
@@ -713,65 +713,65 @@ TF_LITE_MICRO_TEST(Int8Input32x4Filter32x4ShouldMatchGolden) {
   const int input_zero_point = -128;
   const int output_zero_point = 0;
 
-  TfLiteIntArray* input_dims = tflite::testing::IntArrayFromInts(input_shape);
-  TfLiteIntArray* filter_dims = tflite::testing::IntArrayFromInts(filter_shape);
-  TfLiteIntArray* bias_dims = tflite::testing::IntArrayFromInts(bias_shape);
-  TfLiteIntArray* output_dims = tflite::testing::IntArrayFromInts(output_shape);
+  TfLiteIntArray* input_dims = tflite_micro::testing::IntArrayFromInts(input_shape);
+  TfLiteIntArray* filter_dims = tflite_micro::testing::IntArrayFromInts(filter_shape);
+  TfLiteIntArray* bias_dims = tflite_micro::testing::IntArrayFromInts(bias_shape);
+  TfLiteIntArray* output_dims = tflite_micro::testing::IntArrayFromInts(output_shape);
 
   // Create per-tensor quantized int8_t input tensor.
   int8_t input_quantized[input_elements];
-  TfLiteTensor input_tensor = tflite::testing::CreateQuantizedTensor(
+  TfLiteTensor input_tensor = tflite_micro::testing::CreateQuantizedTensor(
       input_values, input_quantized, input_dims, input_scale, input_zero_point);
 
   // Set zero point and scale arrays with a single element for each.
   int input_zero_points[] = {1, input_zero_point};
   float input_scales[] = {1, input_scale};
   TfLiteAffineQuantization input_quant = {
-      tflite::testing::FloatArrayFromFloats(input_scales),
-      tflite::testing::IntArrayFromInts(input_zero_points), 0};
+      tflite_micro::testing::FloatArrayFromFloats(input_scales),
+      tflite_micro::testing::IntArrayFromInts(input_zero_points), 0};
   input_tensor.quantization = {kTfLiteAffineQuantization, &input_quant};
 
   // Create per-tensor quantized int8_t filter tensor.
   int8_t filter_quantized[filter_elements];
-  TfLiteTensor filter_tensor = tflite::testing::CreateQuantizedTensor(
+  TfLiteTensor filter_tensor = tflite_micro::testing::CreateQuantizedTensor(
       filter_values, filter_quantized, filter_dims, filter_scale, 0);
 
   // Set zero point and scale arrays with a single element for each.
   int filter_zero_points[] = {1, 0};
   float filter_scales[] = {1, filter_scale};
   TfLiteAffineQuantization filter_quant = {
-      tflite::testing::FloatArrayFromFloats(filter_scales),
-      tflite::testing::IntArrayFromInts(filter_zero_points), 0};
+      tflite_micro::testing::FloatArrayFromFloats(filter_scales),
+      tflite_micro::testing::IntArrayFromInts(filter_zero_points), 0};
   filter_tensor.quantization = {kTfLiteAffineQuantization, &filter_quant};
 
   // Create per-tensor quantized int32_t bias tensor.
   int32_t bias_quantized[bias_elements];
   // See https://www.tensorflow.org/lite/performance/quantization_spec for a
   // detailed explanation of why bias scale is input_scale * filter_scale.
-  tflite::SymmetricQuantize(bias_values, bias_quantized, bias_elements,
+  tflite_micro::SymmetricQuantize(bias_values, bias_quantized, bias_elements,
                             input_scale * output_scale);
   TfLiteTensor bias_tensor =
-      tflite::testing::CreateTensor(bias_quantized, bias_dims);
+      tflite_micro::testing::CreateTensor(bias_quantized, bias_dims);
 
   // Set zero point and scale arrays with a single element for each.
   int bias_zero_points[] = {1, 0};
   float bias_scales[] = {1, input_scale * filter_scale};
   TfLiteAffineQuantization bias_quant = {
-      tflite::testing::FloatArrayFromFloats(bias_scales),
-      tflite::testing::IntArrayFromInts(bias_zero_points), 0};
+      tflite_micro::testing::FloatArrayFromFloats(bias_scales),
+      tflite_micro::testing::IntArrayFromInts(bias_zero_points), 0};
   bias_tensor.quantization = {kTfLiteAffineQuantization, &bias_quant};
 
   // Create per-tensor quantized int8_t output tensor.
   int8_t output_quantized[output_elements];
-  TfLiteTensor output_tensor = tflite::testing::CreateQuantizedTensor(
+  TfLiteTensor output_tensor = tflite_micro::testing::CreateQuantizedTensor(
       output_quantized, output_dims, output_scale, output_zero_point);
 
   // Set zero point and scale arrays with a single element for each.
   int output_zero_points[] = {1, output_zero_point};
   float output_scales[] = {1, output_scale};
   TfLiteAffineQuantization output_quant = {
-      tflite::testing::FloatArrayFromFloats(output_scales),
-      tflite::testing::IntArrayFromInts(output_zero_points), 0};
+      tflite_micro::testing::FloatArrayFromFloats(output_scales),
+      tflite_micro::testing::IntArrayFromInts(output_zero_points), 0};
   output_tensor.quantization = {kTfLiteAffineQuantization, &output_quant};
 
   // The 3 inputs include the input, filter and bias tensors.
@@ -786,7 +786,7 @@ TF_LITE_MICRO_TEST(Int8Input32x4Filter32x4ShouldMatchGolden) {
   };
 
   int8_t golden_quantized[output_elements];
-  tflite::Quantize(golden, golden_quantized, output_elements, output_scale, 0);
+  tflite_micro::Quantize(golden, golden_quantized, output_elements, output_scale, 0);
 
   // Errors due to quantization should not exceed 1.
   constexpr int kQuantizationTolerance = 1;
@@ -797,7 +797,7 @@ TF_LITE_MICRO_TEST(Int8Input32x4Filter32x4ShouldMatchGolden) {
   conv_params.dilation_height_factor = 1;
   conv_params.stride_height = 1;
   conv_params.stride_width = 1;
-  tflite::testing::ValidateDepthwiseConvGoldens(
+  tflite_micro::testing::ValidateDepthwiseConvGoldens(
       golden_quantized, output_elements, &conv_params, kQuantizationTolerance,
       kTensorsSize, tensors);
 }
@@ -841,65 +841,65 @@ TF_LITE_MICRO_TEST(Int8Input32x1Filter32x1ShouldMatchGolden) {
   const int input_zero_point = -128;
   const int output_zero_point = 0;
 
-  TfLiteIntArray* input_dims = tflite::testing::IntArrayFromInts(input_shape);
-  TfLiteIntArray* filter_dims = tflite::testing::IntArrayFromInts(filter_shape);
-  TfLiteIntArray* bias_dims = tflite::testing::IntArrayFromInts(bias_shape);
-  TfLiteIntArray* output_dims = tflite::testing::IntArrayFromInts(output_shape);
+  TfLiteIntArray* input_dims = tflite_micro::testing::IntArrayFromInts(input_shape);
+  TfLiteIntArray* filter_dims = tflite_micro::testing::IntArrayFromInts(filter_shape);
+  TfLiteIntArray* bias_dims = tflite_micro::testing::IntArrayFromInts(bias_shape);
+  TfLiteIntArray* output_dims = tflite_micro::testing::IntArrayFromInts(output_shape);
 
   // Create per-tensor quantized int8_t input tensor.
   int8_t input_quantized[input_elements];
-  TfLiteTensor input_tensor = tflite::testing::CreateQuantizedTensor(
+  TfLiteTensor input_tensor = tflite_micro::testing::CreateQuantizedTensor(
       input_values, input_quantized, input_dims, input_scale, input_zero_point);
 
   // Set zero point and scale arrays with a single element for each.
   int input_zero_points[] = {1, input_zero_point};
   float input_scales[] = {1, input_scale};
   TfLiteAffineQuantization input_quant = {
-      tflite::testing::FloatArrayFromFloats(input_scales),
-      tflite::testing::IntArrayFromInts(input_zero_points), 0};
+      tflite_micro::testing::FloatArrayFromFloats(input_scales),
+      tflite_micro::testing::IntArrayFromInts(input_zero_points), 0};
   input_tensor.quantization = {kTfLiteAffineQuantization, &input_quant};
 
   // Create per-tensor quantized int8_t filter tensor.
   int8_t filter_quantized[filter_elements];
-  TfLiteTensor filter_tensor = tflite::testing::CreateQuantizedTensor(
+  TfLiteTensor filter_tensor = tflite_micro::testing::CreateQuantizedTensor(
       filter_values, filter_quantized, filter_dims, filter_scale, 0);
 
   // Set zero point and scale arrays with a single element for each.
   int filter_zero_points[] = {1, 0};
   float filter_scales[] = {1, filter_scale};
   TfLiteAffineQuantization filter_quant = {
-      tflite::testing::FloatArrayFromFloats(filter_scales),
-      tflite::testing::IntArrayFromInts(filter_zero_points), 0};
+      tflite_micro::testing::FloatArrayFromFloats(filter_scales),
+      tflite_micro::testing::IntArrayFromInts(filter_zero_points), 0};
   filter_tensor.quantization = {kTfLiteAffineQuantization, &filter_quant};
 
   // Create per-tensor quantized int32_t bias tensor.
   int32_t bias_quantized[bias_elements];
   // See https://www.tensorflow.org/lite/performance/quantization_spec for a
   // detailed explanation of why bias scale is input_scale * filter_scale.
-  tflite::SymmetricQuantize(bias_values, bias_quantized, bias_elements,
+  tflite_micro::SymmetricQuantize(bias_values, bias_quantized, bias_elements,
                             input_scale * output_scale);
   TfLiteTensor bias_tensor =
-      tflite::testing::CreateTensor(bias_quantized, bias_dims);
+      tflite_micro::testing::CreateTensor(bias_quantized, bias_dims);
 
   // Set zero point and scale arrays with a single element for each.
   int bias_zero_points[] = {1, 0};
   float bias_scales[] = {1, input_scale * filter_scale};
   TfLiteAffineQuantization bias_quant = {
-      tflite::testing::FloatArrayFromFloats(bias_scales),
-      tflite::testing::IntArrayFromInts(bias_zero_points), 0};
+      tflite_micro::testing::FloatArrayFromFloats(bias_scales),
+      tflite_micro::testing::IntArrayFromInts(bias_zero_points), 0};
   bias_tensor.quantization = {kTfLiteAffineQuantization, &bias_quant};
 
   // Create per-tensor quantized int8_t output tensor.
   int8_t output_quantized[output_elements];
-  TfLiteTensor output_tensor = tflite::testing::CreateQuantizedTensor(
+  TfLiteTensor output_tensor = tflite_micro::testing::CreateQuantizedTensor(
       output_quantized, output_dims, output_scale, output_zero_point);
 
   // Set zero point and scale arrays with a single element for each.
   int output_zero_points[] = {1, output_zero_point};
   float output_scales[] = {1, output_scale};
   TfLiteAffineQuantization output_quant = {
-      tflite::testing::FloatArrayFromFloats(output_scales),
-      tflite::testing::IntArrayFromInts(output_zero_points), 0};
+      tflite_micro::testing::FloatArrayFromFloats(output_scales),
+      tflite_micro::testing::IntArrayFromInts(output_zero_points), 0};
   output_tensor.quantization = {kTfLiteAffineQuantization, &output_quant};
 
   // The 3 inputs include the input, filter and bias tensors.
@@ -914,7 +914,7 @@ TF_LITE_MICRO_TEST(Int8Input32x1Filter32x1ShouldMatchGolden) {
   };
 
   int8_t golden_quantized[output_elements];
-  tflite::Quantize(golden, golden_quantized, output_elements, output_scale, 0);
+  tflite_micro::Quantize(golden, golden_quantized, output_elements, output_scale, 0);
 
   // Errors due to quantization should not exceed 1.
   constexpr int kQuantizationTolerance = 1;
@@ -926,7 +926,7 @@ TF_LITE_MICRO_TEST(Int8Input32x1Filter32x1ShouldMatchGolden) {
   conv_params.stride_height = 2;
   conv_params.stride_width = 2;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::testing::ValidateDepthwiseConvGoldens(
+                          tflite_micro::testing::ValidateDepthwiseConvGoldens(
                               golden_quantized, output_elements, &conv_params,
                               kQuantizationTolerance, kTensorsSize, tensors));
 }
@@ -975,7 +975,7 @@ TF_LITE_MICRO_TEST(SimpleTestQuantizedPerChannelInt4Filter) {
   conv_params.stride_height = 1;
   conv_params.stride_width = 1;
 
-  tflite::testing::TestDepthwiseConvQuantizedPerChannel(
+  tflite_micro::testing::TestDepthwiseConvQuantizedPerChannel(
       input_shape, input_values, input_quantized, input_scale, input_zero_point,
       filter_shape, filter_values, filter_quantized, bias_shape, bias_values,
       bias_quantized, output_shape, golden, golden_quantized, output_data,
@@ -1018,7 +1018,7 @@ TF_LITE_MICRO_TEST(SimpleTestQuantizedPerChannel) {
   conv_params.stride_height = 1;
   conv_params.stride_width = 1;
 
-  tflite::testing::TestDepthwiseConvQuantizedPerChannel(
+  tflite_micro::testing::TestDepthwiseConvQuantizedPerChannel(
       input_shape, input_values, input_quantized, input_scale, input_zero_point,
       filter_shape, filter_values, filter_quantized, bias_shape, bias_values,
       bias_quantized, output_shape, golden, golden_quantized, output_data,
@@ -1062,7 +1062,7 @@ TF_LITE_MICRO_TEST(SimpleTestQuantizedPerChannelInt16InputInt8Filter) {
   conv_params.stride_height = 1;
   conv_params.stride_width = 1;
 
-  tflite::testing::TestDepthwiseConvQuantizedPerChannel(
+  tflite_micro::testing::TestDepthwiseConvQuantizedPerChannel(
       input_shape, input_values, input_quantized, input_scale, input_zero_point,
       filter_shape, filter_values, filter_quantized, bias_shape, bias_values,
       bias_quantized, output_shape, golden, golden_quantized, output_data,
diff --git a/tensorflow/lite/micro/kernels/dequantize.cc b/tensorflow/lite/micro/kernels/dequantize.cc
index 428c866a..f41a5b6c 100644
--- a/tensorflow/lite/micro/kernels/dequantize.cc
+++ b/tensorflow/lite/micro/kernels/dequantize.cc
@@ -26,7 +26,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 void* DequantizeInit(TfLiteContext* context, const char* buffer,
                      size_t length) {
@@ -38,8 +38,8 @@ TfLiteStatus DequantizeEval(TfLiteContext* context, TfLiteNode* node) {
   TFLITE_DCHECK(node->user_data != nullptr);
   DequantizeOpData* data = static_cast<DequantizeOpData*>(node->user_data);
 
-  const TfLiteEvalTensor* input = tflite::micro::GetEvalInput(context, node, 0);
-  TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, 0);
+  const TfLiteEvalTensor* input = tflite_micro::micro::GetEvalInput(context, node, 0);
+  TfLiteEvalTensor* output = tflite_micro::micro::GetEvalOutput(context, node, 0);
 
   // Output type ensured to be kTfLiteFloat32 at the Prepare stage
   TFLITE_DCHECK(output->type == kTfLiteFloat32);
@@ -47,24 +47,24 @@ TfLiteStatus DequantizeEval(TfLiteContext* context, TfLiteNode* node) {
   switch (input->type) {
     case kTfLiteInt8:
       reference_ops::Dequantize(data->quantization_params,
-                                tflite::micro::GetTensorShape(input),
-                                tflite::micro::GetTensorData<int8_t>(input),
-                                tflite::micro::GetTensorShape(output),
-                                tflite::micro::GetTensorData<float>(output));
+                                tflite_micro::micro::GetTensorShape(input),
+                                tflite_micro::micro::GetTensorData<int8_t>(input),
+                                tflite_micro::micro::GetTensorShape(output),
+                                tflite_micro::micro::GetTensorData<float>(output));
       break;
     case kTfLiteInt16:
       reference_ops::Dequantize(data->quantization_params,
-                                tflite::micro::GetTensorShape(input),
-                                tflite::micro::GetTensorData<int16_t>(input),
-                                tflite::micro::GetTensorShape(output),
-                                tflite::micro::GetTensorData<float>(output));
+                                tflite_micro::micro::GetTensorShape(input),
+                                tflite_micro::micro::GetTensorData<int16_t>(input),
+                                tflite_micro::micro::GetTensorShape(output),
+                                tflite_micro::micro::GetTensorData<float>(output));
       break;
     case kTfLiteUInt8:
       reference_ops::Dequantize(data->quantization_params,
-                                tflite::micro::GetTensorShape(input),
-                                tflite::micro::GetTensorData<uint8_t>(input),
-                                tflite::micro::GetTensorShape(output),
-                                tflite::micro::GetTensorData<float>(output));
+                                tflite_micro::micro::GetTensorShape(input),
+                                tflite_micro::micro::GetTensorData<uint8_t>(input),
+                                tflite_micro::micro::GetTensorShape(output),
+                                tflite_micro::micro::GetTensorData<float>(output));
       break;
     default:
       MicroPrintf("Input %s, output %s not supported.",
@@ -77,8 +77,8 @@ TfLiteStatus DequantizeEval(TfLiteContext* context, TfLiteNode* node) {
 }
 
 TFLMRegistration Register_DEQUANTIZE() {
-  return tflite::micro::RegisterOp(DequantizeInit, DequantizePrepare,
+  return tflite_micro::micro::RegisterOp(DequantizeInit, DequantizePrepare,
                                    DequantizeEval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/dequantize.h b/tensorflow/lite/micro/kernels/dequantize.h
index fe6ec169..5915cea1 100644
--- a/tensorflow/lite/micro/kernels/dequantize.h
+++ b/tensorflow/lite/micro/kernels/dequantize.h
@@ -20,10 +20,10 @@ limitations under the License.
 #include "tensorflow/lite/c/common.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 struct DequantizeOpData {
-  tflite::DequantizationParams quantization_params;
+  tflite_micro::DequantizationParams quantization_params;
   // The scaling factor from input to output (aka the 'real multiplier') can
   // be represented as a fixed point multiplier plus a left shift.
   int32_t output_multiplier;
@@ -33,6 +33,6 @@ struct DequantizeOpData {
 
 TfLiteStatus DequantizePrepare(TfLiteContext* context, TfLiteNode* node);
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_DEQUANTIZE_H_
diff --git a/tensorflow/lite/micro/kernels/dequantize_common.cc b/tensorflow/lite/micro/kernels/dequantize_common.cc
index 08e448ee..419f80fd 100644
--- a/tensorflow/lite/micro/kernels/dequantize_common.cc
+++ b/tensorflow/lite/micro/kernels/dequantize_common.cc
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/dequantize.h"
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 TfLiteStatus DequantizePrepare(TfLiteContext* context, TfLiteNode* node) {
   TFLITE_DCHECK(node->user_data != nullptr);
@@ -55,4 +55,4 @@ TfLiteStatus DequantizePrepare(TfLiteContext* context, TfLiteNode* node) {
   return kTfLiteOk;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/dequantize_test.cc b/tensorflow/lite/micro/kernels/dequantize_test.cc
index 3a34b8dd..88081e63 100644
--- a/tensorflow/lite/micro/kernels/dequantize_test.cc
+++ b/tensorflow/lite/micro/kernels/dequantize_test.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -32,7 +32,7 @@ void ValidateDequantizeGoldens(TfLiteTensor* tensors, int tensors_size,
   int outputs_array_data[] = {1, 1};
   TfLiteIntArray* outputs_array = IntArrayFromInts(outputs_array_data);
 
-  const TFLMRegistration registration = tflite::Register_DEQUANTIZE();
+  const TFLMRegistration registration = tflite_micro::Register_DEQUANTIZE();
   micro::KernelRunner runner(registration, tensors, tensors_size, inputs_array,
                              outputs_array,
                              /*builtin_data=*/nullptr);
@@ -69,7 +69,7 @@ void TestDequantizeToFloat(int* input_dims_data, const float* input_data,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -82,7 +82,7 @@ TF_LITE_MICRO_TEST(DequantizeOpTestInt8) {
   const int zero_point = -1;
   int8_t input_quantized[length];
   float output[length];
-  tflite::testing::TestDequantizeToFloat(dims, values, input_quantized, scale,
+  tflite_micro::testing::TestDequantizeToFloat(dims, values, input_quantized, scale,
                                          zero_point, dims, values, output);
 }
 
@@ -95,7 +95,7 @@ TF_LITE_MICRO_TEST(DequantizeOpTestInt16) {
   const int zero_point = -1;
   int16_t input_quantized[length];
   float output[length];
-  tflite::testing::TestDequantizeToFloat(dims, values, input_quantized, scale,
+  tflite_micro::testing::TestDequantizeToFloat(dims, values, input_quantized, scale,
                                          zero_point, dims, values, output);
 }
 
@@ -108,7 +108,7 @@ TF_LITE_MICRO_TEST(DequantizeOpTestUint8) {
   const int zero_point = 127;
   uint8_t input_quantized[length];
   float output[length];
-  tflite::testing::TestDequantizeToFloat(dims, values, input_quantized, scale,
+  tflite_micro::testing::TestDequantizeToFloat(dims, values, input_quantized, scale,
                                          zero_point, dims, values, output);
 }
 
diff --git a/tensorflow/lite/micro/kernels/detection_postprocess.cc b/tensorflow/lite/micro/kernels/detection_postprocess.cc
index e807f35d..7aa6de56 100644
--- a/tensorflow/lite/micro/kernels/detection_postprocess.cc
+++ b/tensorflow/lite/micro/kernels/detection_postprocess.cc
@@ -28,7 +28,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 /**
@@ -242,13 +242,13 @@ class Dequantizer {
 
 template <class T>
 T ReInterpretTensor(const TfLiteEvalTensor* tensor) {
-  const float* tensor_base = tflite::micro::GetTensorData<float>(tensor);
+  const float* tensor_base = tflite_micro::micro::GetTensorData<float>(tensor);
   return reinterpret_cast<T>(tensor_base);
 }
 
 template <class T>
 T ReInterpretTensor(TfLiteEvalTensor* tensor) {
-  float* tensor_base = tflite::micro::GetTensorData<float>(tensor);
+  float* tensor_base = tflite_micro::micro::GetTensorData<float>(tensor);
   return reinterpret_cast<T>(tensor_base);
 }
 
@@ -256,12 +256,12 @@ TfLiteStatus DecodeCenterSizeBoxes(TfLiteContext* context, TfLiteNode* node,
                                    OpData* op_data) {
   // Parse input tensor boxencodings
   const TfLiteEvalTensor* input_box_encodings =
-      tflite::micro::GetEvalInput(context, node, kInputTensorBoxEncodings);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensorBoxEncodings);
   TF_LITE_ENSURE_EQ(context, input_box_encodings->dims->data[0], kBatchSize);
   const int num_boxes = input_box_encodings->dims->data[1];
   TF_LITE_ENSURE(context, input_box_encodings->dims->data[2] >= kNumCoordBox);
   const TfLiteEvalTensor* input_anchors =
-      tflite::micro::GetEvalInput(context, node, kInputTensorAnchors);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensorAnchors);
 
   // Decode the boxes to get (ymin, xmin, ymax, xmax) based on the anchors
   CenterSizeEncoding box_centersize;
@@ -273,7 +273,7 @@ TfLiteStatus DecodeCenterSizeBoxes(TfLiteContext* context, TfLiteNode* node,
       case kTfLiteFloat32: {
         // Please see DequantizeBoxEncodings function for the support detail.
         const int box_encoding_idx = idx * input_box_encodings->dims->data[2];
-        const float* boxes = &(tflite::micro::GetTensorData<float>(
+        const float* boxes = &(tflite_micro::micro::GetTensorData<float>(
             input_box_encodings)[box_encoding_idx]);
         box_centersize = *reinterpret_cast<const CenterSizeEncoding*>(boxes);
         anchor =
@@ -430,7 +430,7 @@ TfLiteStatus NonMaxSuppressionSingleClassHelper(
     const float* scores, int* selected, int* selected_size,
     int max_detections) {
   const TfLiteEvalTensor* input_box_encodings =
-      tflite::micro::GetEvalInput(context, node, kInputTensorBoxEncodings);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensorBoxEncodings);
   const int num_boxes = input_box_encodings->dims->data[1];
   const float non_max_suppression_score_threshold =
       op_data->non_max_suppression_score_threshold;
@@ -519,17 +519,17 @@ TfLiteStatus NonMaxSuppressionMultiClassRegularHelper(TfLiteContext* context,
                                                       OpData* op_data,
                                                       const float* scores) {
   const TfLiteEvalTensor* input_box_encodings =
-      tflite::micro::GetEvalInput(context, node, kInputTensorBoxEncodings);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensorBoxEncodings);
   const TfLiteEvalTensor* input_class_predictions =
-      tflite::micro::GetEvalInput(context, node, kInputTensorClassPredictions);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensorClassPredictions);
   TfLiteEvalTensor* detection_boxes =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensorDetectionBoxes);
-  TfLiteEvalTensor* detection_classes = tflite::micro::GetEvalOutput(
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensorDetectionBoxes);
+  TfLiteEvalTensor* detection_classes = tflite_micro::micro::GetEvalOutput(
       context, node, kOutputTensorDetectionClasses);
   TfLiteEvalTensor* detection_scores =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensorDetectionScores);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensorDetectionScores);
   TfLiteEvalTensor* num_detections =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensorNumDetections);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensorNumDetections);
 
   const int num_boxes = input_box_encodings->dims->data[1];
   const int num_classes = op_data->num_classes;
@@ -618,23 +618,23 @@ TfLiteStatus NonMaxSuppressionMultiClassRegularHelper(TfLiteContext* context,
       ReInterpretTensor<BoxCornerEncoding*>(detection_boxes)[output_box_index] =
           reinterpret_cast<BoxCornerEncoding*>(decoded_boxes)[anchor_index];
       // detection_classes
-      tflite::micro::GetTensorData<float>(detection_classes)[output_box_index] =
+      tflite_micro::micro::GetTensorData<float>(detection_classes)[output_box_index] =
           class_index;
       // detection_scores
-      tflite::micro::GetTensorData<float>(detection_scores)[output_box_index] =
+      tflite_micro::micro::GetTensorData<float>(detection_scores)[output_box_index] =
           selected_score;
     } else {
       ReInterpretTensor<BoxCornerEncoding*>(
           detection_boxes)[output_box_index] = {0.0f, 0.0f, 0.0f, 0.0f};
       // detection_classes
-      tflite::micro::GetTensorData<float>(detection_classes)[output_box_index] =
+      tflite_micro::micro::GetTensorData<float>(detection_classes)[output_box_index] =
           0.0f;
       // detection_scores
-      tflite::micro::GetTensorData<float>(detection_scores)[output_box_index] =
+      tflite_micro::micro::GetTensorData<float>(detection_scores)[output_box_index] =
           0.0f;
     }
   }
-  tflite::micro::GetTensorData<float>(num_detections)[0] =
+  tflite_micro::micro::GetTensorData<float>(num_detections)[0] =
       size_of_sorted_indices;
 
   return kTfLiteOk;
@@ -652,18 +652,18 @@ TfLiteStatus NonMaxSuppressionMultiClassFastHelper(TfLiteContext* context,
                                                    OpData* op_data,
                                                    const float* scores) {
   const TfLiteEvalTensor* input_box_encodings =
-      tflite::micro::GetEvalInput(context, node, kInputTensorBoxEncodings);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensorBoxEncodings);
   const TfLiteEvalTensor* input_class_predictions =
-      tflite::micro::GetEvalInput(context, node, kInputTensorClassPredictions);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensorClassPredictions);
   TfLiteEvalTensor* detection_boxes =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensorDetectionBoxes);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensorDetectionBoxes);
 
-  TfLiteEvalTensor* detection_classes = tflite::micro::GetEvalOutput(
+  TfLiteEvalTensor* detection_classes = tflite_micro::micro::GetEvalOutput(
       context, node, kOutputTensorDetectionClasses);
   TfLiteEvalTensor* detection_scores =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensorDetectionScores);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensorDetectionScores);
   TfLiteEvalTensor* num_detections =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensorNumDetections);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensorNumDetections);
 
   const int num_boxes = input_box_encodings->dims->data[1];
   const int num_classes = op_data->num_classes;
@@ -719,18 +719,18 @@ TfLiteStatus NonMaxSuppressionMultiClassFastHelper(TfLiteContext* context,
           reinterpret_cast<BoxCornerEncoding*>(decoded_boxes)[selected_index];
 
       // detection_classes
-      tflite::micro::GetTensorData<float>(detection_classes)[box_offset] =
+      tflite_micro::micro::GetTensorData<float>(detection_classes)[box_offset] =
           class_indices[col];
 
       // detection_scores
-      tflite::micro::GetTensorData<float>(detection_scores)[box_offset] =
+      tflite_micro::micro::GetTensorData<float>(detection_scores)[box_offset] =
           box_scores[class_indices[col]];
 
       output_box_index++;
     }
   }
 
-  tflite::micro::GetTensorData<float>(num_detections)[0] = output_box_index;
+  tflite_micro::micro::GetTensorData<float>(num_detections)[0] = output_box_index;
   return kTfLiteOk;
 }
 
@@ -738,9 +738,9 @@ TfLiteStatus NonMaxSuppressionMultiClass(TfLiteContext* context,
                                          TfLiteNode* node, OpData* op_data) {
   // Get the input tensors
   const TfLiteEvalTensor* input_box_encodings =
-      tflite::micro::GetEvalInput(context, node, kInputTensorBoxEncodings);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensorBoxEncodings);
   const TfLiteEvalTensor* input_class_predictions =
-      tflite::micro::GetEvalInput(context, node, kInputTensorClassPredictions);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensorClassPredictions);
   const int num_boxes = input_box_encodings->dims->data[1];
   const int num_classes = op_data->num_classes;
 
@@ -756,7 +756,7 @@ TfLiteStatus NonMaxSuppressionMultiClass(TfLiteContext* context,
   const float* scores;
   switch (input_class_predictions->type) {
     case kTfLiteFloat32:
-      scores = tflite::micro::GetTensorData<float>(input_class_predictions);
+      scores = tflite_micro::micro::GetTensorData<float>(input_class_predictions);
       break;
     default:
       // Unsupported type.
@@ -800,8 +800,8 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration* Register_DETECTION_POSTPROCESS() {
-  static TFLMRegistration r = tflite::micro::RegisterOp(Init, Prepare, Eval);
+  static TFLMRegistration r = tflite_micro::micro::RegisterOp(Init, Prepare, Eval);
   return &r;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/detection_postprocess_test.cc b/tensorflow/lite/micro/kernels/detection_postprocess_test.cc
index 728e2e71..fa6ee9b6 100644
--- a/tensorflow/lite/micro/kernels/detection_postprocess_test.cc
+++ b/tensorflow/lite/micro/kernels/detection_postprocess_test.cc
@@ -24,7 +24,7 @@ limitations under the License.
 // See: tensorflow/lite/micro/kernels/detection_postprocess_test/README.md
 #include "tensorflow/lite/micro/kernels/detection_postprocess_flexbuffers_generated_data.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -161,7 +161,7 @@ void TestDetectionPostprocess(int* input_dims_data1, const float* input_data1,
 }
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -171,16 +171,16 @@ TF_LITE_MICRO_TEST(DetectionPostprocessFloatFastNMS) {
   float output_data3[3];
   float output_data4[1];
 
-  tflite::testing::TestDetectionPostprocess(
-      tflite::testing::kInputShape1, tflite::testing::kInputData1,
-      tflite::testing::kInputShape2, tflite::testing::kInputData2,
-      tflite::testing::kInputShape3, tflite::testing::kInputData3,
-      tflite::testing::kOutputShape1, output_data1,
-      tflite::testing::kOutputShape2, output_data2,
-      tflite::testing::kOutputShape3, output_data3,
-      tflite::testing::kOutputShape4, output_data4, tflite::testing::kGolden1,
-      tflite::testing::kGolden2, tflite::testing::kGolden3,
-      tflite::testing::kGolden4,
+  tflite_micro::testing::TestDetectionPostprocess(
+      tflite_micro::testing::kInputShape1, tflite_micro::testing::kInputData1,
+      tflite_micro::testing::kInputShape2, tflite_micro::testing::kInputData2,
+      tflite_micro::testing::kInputShape3, tflite_micro::testing::kInputData3,
+      tflite_micro::testing::kOutputShape1, output_data1,
+      tflite_micro::testing::kOutputShape2, output_data2,
+      tflite_micro::testing::kOutputShape3, output_data3,
+      tflite_micro::testing::kOutputShape4, output_data4, tflite_micro::testing::kGolden1,
+      tflite_micro::testing::kGolden2, tflite_micro::testing::kGolden3,
+      tflite_micro::testing::kGolden4,
       /* tolerance */ 0, /* Use regular NMS: */ false);
 }
 
@@ -194,15 +194,15 @@ TF_LITE_MICRO_TEST(DetectionPostprocessFloatRegularNMS) {
   const float kGolden3[] = {0.95, 0.9, 0.0};
   const float kGolden4[] = {2.0};
 
-  tflite::testing::TestDetectionPostprocess(
-      tflite::testing::kInputShape1, tflite::testing::kInputData1,
-      tflite::testing::kInputShape2, tflite::testing::kInputData2,
-      tflite::testing::kInputShape3, tflite::testing::kInputData3,
-      tflite::testing::kOutputShape1, output_data1,
-      tflite::testing::kOutputShape2, output_data2,
-      tflite::testing::kOutputShape3, output_data3,
-      tflite::testing::kOutputShape4, output_data4, kGolden1,
-      tflite::testing::kGolden2, kGolden3, kGolden4,
+  tflite_micro::testing::TestDetectionPostprocess(
+      tflite_micro::testing::kInputShape1, tflite_micro::testing::kInputData1,
+      tflite_micro::testing::kInputShape2, tflite_micro::testing::kInputData2,
+      tflite_micro::testing::kInputShape3, tflite_micro::testing::kInputData3,
+      tflite_micro::testing::kOutputShape1, output_data1,
+      tflite_micro::testing::kOutputShape2, output_data2,
+      tflite_micro::testing::kOutputShape3, output_data3,
+      tflite_micro::testing::kOutputShape4, output_data4, kGolden1,
+      tflite_micro::testing::kGolden2, kGolden3, kGolden4,
       /* tolerance */ 1e-1, /* Use regular NMS: */ true);
 }
 
@@ -230,15 +230,15 @@ TF_LITE_MICRO_TEST(
   float output_data3[3];
   float output_data4[1];
 
-  tflite::testing::TestDetectionPostprocess(
+  tflite_micro::testing::TestDetectionPostprocess(
       kInputShape1, kInputData1, kInputShape2, kInputData2,
-      tflite::testing::kInputShape3, tflite::testing::kInputData3,
-      tflite::testing::kOutputShape1, output_data1,
-      tflite::testing::kOutputShape2, output_data2,
-      tflite::testing::kOutputShape3, output_data3,
-      tflite::testing::kOutputShape4, output_data4, tflite::testing::kGolden1,
-      tflite::testing::kGolden2, tflite::testing::kGolden3,
-      tflite::testing::kGolden4,
+      tflite_micro::testing::kInputShape3, tflite_micro::testing::kInputData3,
+      tflite_micro::testing::kOutputShape1, output_data1,
+      tflite_micro::testing::kOutputShape2, output_data2,
+      tflite_micro::testing::kOutputShape3, output_data3,
+      tflite_micro::testing::kOutputShape4, output_data4, tflite_micro::testing::kGolden1,
+      tflite_micro::testing::kGolden2, tflite_micro::testing::kGolden3,
+      tflite_micro::testing::kGolden4,
       /* tolerance */ 0, /* Use regular NMS: */ false);
 }
 
@@ -260,14 +260,14 @@ TF_LITE_MICRO_TEST(
   float output_data3[3];
   float output_data4[1];
 
-  tflite::testing::TestDetectionPostprocess(
-      tflite::testing::kInputShape1, tflite::testing::kInputData1, kInputShape2,
-      kInputData2, tflite::testing::kInputShape3, tflite::testing::kInputData3,
-      tflite::testing::kOutputShape1, output_data1,
-      tflite::testing::kOutputShape2, output_data2,
-      tflite::testing::kOutputShape3, output_data3,
-      tflite::testing::kOutputShape4, output_data4, kGolden1,
-      tflite::testing::kGolden2, kGolden3, kGolden4,
+  tflite_micro::testing::TestDetectionPostprocess(
+      tflite_micro::testing::kInputShape1, tflite_micro::testing::kInputData1, kInputShape2,
+      kInputData2, tflite_micro::testing::kInputShape3, tflite_micro::testing::kInputData3,
+      tflite_micro::testing::kOutputShape1, output_data1,
+      tflite_micro::testing::kOutputShape2, output_data2,
+      tflite_micro::testing::kOutputShape3, output_data3,
+      tflite_micro::testing::kOutputShape4, output_data4, kGolden1,
+      tflite_micro::testing::kGolden2, kGolden3, kGolden4,
       /* tolerance */ 1e-1, /* Use regular NMS: */ true);
 }
 
@@ -290,15 +290,15 @@ TF_LITE_MICRO_TEST(
   float output_data3[3];
   float output_data4[1];
 
-  tflite::testing::TestDetectionPostprocess(
-      kInputShape1, kInputData1, tflite::testing::kInputShape2,
-      tflite::testing::kInputData2, tflite::testing::kInputShape3,
-      tflite::testing::kInputData3, tflite::testing::kOutputShape1,
-      output_data1, tflite::testing::kOutputShape2, output_data2,
-      tflite::testing::kOutputShape3, output_data3,
-      tflite::testing::kOutputShape4, output_data4, tflite::testing::kGolden1,
-      tflite::testing::kGolden2, tflite::testing::kGolden3,
-      tflite::testing::kGolden4,
+  tflite_micro::testing::TestDetectionPostprocess(
+      kInputShape1, kInputData1, tflite_micro::testing::kInputShape2,
+      tflite_micro::testing::kInputData2, tflite_micro::testing::kInputShape3,
+      tflite_micro::testing::kInputData3, tflite_micro::testing::kOutputShape1,
+      output_data1, tflite_micro::testing::kOutputShape2, output_data2,
+      tflite_micro::testing::kOutputShape3, output_data3,
+      tflite_micro::testing::kOutputShape4, output_data4, tflite_micro::testing::kGolden1,
+      tflite_micro::testing::kGolden2, tflite_micro::testing::kGolden3,
+      tflite_micro::testing::kGolden4,
       /* tolerance */ 0, /* Use regular NMS: */ false);
 }
 
@@ -329,14 +329,14 @@ TF_LITE_MICRO_TEST(
   const float kGolden2[] = {0, 0, 0};
   const float kGolden3[] = {0.0196078, 0.0156863, 0.00392157};
 
-  tflite::testing::TestDetectionPostprocess(
+  tflite_micro::testing::TestDetectionPostprocess(
       kInputShape1, kInputData1, kInputShape2, kInputData2,
-      tflite::testing::kInputShape3, tflite::testing::kInputData3,
-      tflite::testing::kOutputShape1, output_data1,
-      tflite::testing::kOutputShape2, output_data2,
-      tflite::testing::kOutputShape3, output_data3,
-      tflite::testing::kOutputShape4, output_data4, tflite::testing::kGolden1,
-      kGolden2, kGolden3, tflite::testing::kGolden4,
+      tflite_micro::testing::kInputShape3, tflite_micro::testing::kInputData3,
+      tflite_micro::testing::kOutputShape1, output_data1,
+      tflite_micro::testing::kOutputShape2, output_data2,
+      tflite_micro::testing::kOutputShape3, output_data3,
+      tflite_micro::testing::kOutputShape4, output_data4, tflite_micro::testing::kGolden1,
+      kGolden2, kGolden3, tflite_micro::testing::kGolden4,
       /* tolerance */ 3e-1, /* Use regular NMS: */ false);
 }
 
diff --git a/tensorflow/lite/micro/kernels/div.cc b/tensorflow/lite/micro/kernels/div.cc
index b29686ac..b9749bbc 100644
--- a/tensorflow/lite/micro/kernels/div.cc
+++ b/tensorflow/lite/micro/kernels/div.cc
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 constexpr int kInputTensor1 = 0;
@@ -109,7 +109,7 @@ template <typename T>
 void EvalDiv(TfLiteContext* context, TfLiteNode* node, TfLiteDivParams* params,
              const OpDataDiv* data, const TfLiteEvalTensor* input1,
              const TfLiteEvalTensor* input2, TfLiteEvalTensor* output) {
-  tflite::ArithmeticParams op_params = {};
+  tflite_micro::ArithmeticParams op_params = {};
 
 #define TF_LITE_DIV(type, opname, data_type)                           \
   data_type output_activation_min, output_activation_max;              \
@@ -117,16 +117,16 @@ void EvalDiv(TfLiteContext* context, TfLiteNode* node, TfLiteDivParams* params,
                            &output_activation_max);                    \
   SetActivationParams(output_activation_min, output_activation_max,    \
                       &op_params);                                     \
-  type::opname(op_params, tflite::micro::GetTensorShape(input1),       \
-               tflite::micro::GetTensorData<data_type>(input1),        \
-               tflite::micro::GetTensorShape(input2),                  \
-               tflite::micro::GetTensorData<data_type>(input2),        \
-               tflite::micro::GetTensorShape(output),                  \
-               tflite::micro::GetTensorData<data_type>(output))
+  type::opname(op_params, tflite_micro::micro::GetTensorShape(input1),       \
+               tflite_micro::micro::GetTensorData<data_type>(input1),        \
+               tflite_micro::micro::GetTensorShape(input2),                  \
+               tflite_micro::micro::GetTensorData<data_type>(input2),        \
+               tflite_micro::micro::GetTensorShape(output),                  \
+               tflite_micro::micro::GetTensorData<data_type>(output))
 
   bool requires_broadcast = reference_ops::ProcessBroadcastShapes(
-      tflite::micro::GetTensorShape(input1),
-      tflite::micro::GetTensorShape(input2), &op_params);
+      tflite_micro::micro::GetTensorShape(input1),
+      tflite_micro::micro::GetTensorShape(input2), &op_params);
 
   if (requires_broadcast) {
     TF_LITE_DIV(reference_ops, BroadcastDivSlow, T);
@@ -141,15 +141,15 @@ TfLiteStatus EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                            const TfLiteEvalTensor* input1,
                            const TfLiteEvalTensor* input2,
                            TfLiteEvalTensor* output) {
-  tflite::ArithmeticParams op_params = {};
+  tflite_micro::ArithmeticParams op_params = {};
 
 #define TF_LITE_DIV(type, opname, dtype)                         \
-  type::opname(op_params, tflite::micro::GetTensorShape(input1), \
-               tflite::micro::GetTensorData<dtype>(input1),      \
-               tflite::micro::GetTensorShape(input2),            \
-               tflite::micro::GetTensorData<dtype>(input2),      \
-               tflite::micro::GetTensorShape(output),            \
-               tflite::micro::GetTensorData<dtype>(output))
+  type::opname(op_params, tflite_micro::micro::GetTensorShape(input1), \
+               tflite_micro::micro::GetTensorData<dtype>(input1),      \
+               tflite_micro::micro::GetTensorShape(input2),            \
+               tflite_micro::micro::GetTensorData<dtype>(input2),      \
+               tflite_micro::micro::GetTensorShape(output),            \
+               tflite_micro::micro::GetTensorData<dtype>(output))
 
   if (input1->type == kTfLiteInt8 && input2->type == kTfLiteInt8 &&
       output->type == kTfLiteInt8) {
@@ -162,8 +162,8 @@ TfLiteStatus EvalQuantized(TfLiteContext* context, TfLiteNode* node,
     op_params.output_shift = data->output_shift;
 
     bool requires_broadcast = reference_ops::ProcessBroadcastShapes(
-        tflite::micro::GetTensorShape(input1),
-        tflite::micro::GetTensorShape(input2), &op_params);
+        tflite_micro::micro::GetTensorShape(input1),
+        tflite_micro::micro::GetTensorShape(input2), &op_params);
 
     if (requires_broadcast) {
       TF_LITE_DIV(reference_ops, BroadcastDivSlow, int8_t);
@@ -186,11 +186,11 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   auto* data = static_cast<OpDataDiv*>(node->user_data);
 
   const TfLiteEvalTensor* input1 =
-      tflite::micro::GetEvalInput(context, node, kInputTensor1);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor1);
   const TfLiteEvalTensor* input2 =
-      tflite::micro::GetEvalInput(context, node, kInputTensor2);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor2);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
   if (output->type == kTfLiteFloat32) {
     EvalDiv<float>(context, node, params, data, input1, input2, output);
@@ -213,7 +213,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_DIV() {
-  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/div_test.cc b/tensorflow/lite/micro/kernels/div_test.cc
index ef35d0c0..49501379 100644
--- a/tensorflow/lite/micro/kernels/div_test.cc
+++ b/tensorflow/lite/micro/kernels/div_test.cc
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -35,7 +35,7 @@ void ExecuteDivTest(TfLiteTensor* tensors, int tensors_count,
   int kOutputArrayData[] = {1, 2};
   TfLiteIntArray* outputs_array = IntArrayFromInts(kOutputArrayData);
 
-  const TFLMRegistration registration = tflite::Register_DIV();
+  const TFLMRegistration registration = tflite_micro::Register_DIV();
   micro::KernelRunner runner(registration, tensors, tensors_count, inputs_array,
                              outputs_array, static_cast<void*>(&builtin_data));
 
@@ -175,7 +175,7 @@ void TestDivMultiBroadcastQuant(int** shapes, const int shapes_count,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -187,7 +187,7 @@ TF_LITE_MICRO_TEST(FloatDivOpTestActNone) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
 
-  tflite::testing::TestDiv(kDims, kInput1, kDims, kInput2, kDims, kExpect,
+  tflite_micro::testing::TestDiv(kDims, kInput1, kDims, kInput2, kDims, kExpect,
                            output_data, kTfLiteActNone);
 }
 
@@ -199,7 +199,7 @@ TF_LITE_MICRO_TEST(FloatDivOpTestActReluN1To1) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
 
-  tflite::testing::TestDiv(kDims, kInput1, kDims, kInput2, kDims, kExpect,
+  tflite_micro::testing::TestDiv(kDims, kInput1, kDims, kInput2, kDims, kExpect,
                            output_data, kTfLiteActReluN1To1);
 }
 
@@ -217,7 +217,7 @@ TF_LITE_MICRO_TEST(FloatDivOpTestMultiShape) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
 
-  tflite::testing::TestDivMultiShape(kDims, kDimsCount, kInput1, kInput2,
+  tflite_micro::testing::TestDivMultiShape(kDims, kDimsCount, kInput1, kInput2,
                                      kExpect, output_data, kTfLiteActNone);
 }
 
@@ -236,7 +236,7 @@ TF_LITE_MICRO_TEST(FloatDivOpTestBroadcast) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
 
-  tflite::testing::TestDivMultiBroadcast(kDims, kDimsCount, kInput1, kInput2,
+  tflite_micro::testing::TestDivMultiBroadcast(kDims, kDimsCount, kInput1, kInput2,
                                          kExpect, output_data, kTfLiteActNone);
 }
 
@@ -252,7 +252,7 @@ TF_LITE_MICRO_TEST(FloatDivOpTestBroadcast5D) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
 
-  tflite::testing::TestDivMultiBroadcast(kDims, kDimsCount, kInput1, kInput2,
+  tflite_micro::testing::TestDivMultiBroadcast(kDims, kDimsCount, kInput1, kInput2,
                                          kExpect, output_data, kTfLiteActNone);
 }
 
@@ -268,14 +268,14 @@ TF_LITE_MICRO_TEST(QuantizedDivOpTestActNone) {
   int8_t q_output_data[kOutputCount];
   int8_t q_input1_data[kOutputCount];
   int8_t q_input2_data[kOutputCount];
-  tflite::testing::TestQuantParams<int8_t> params = {};
+  tflite_micro::testing::TestQuantParams<int8_t> params = {};
   params.data_min = -1.0;
   params.data_max = 1.0;
   params.input1_data = q_input1_data;
   params.input2_data = q_input2_data;
   params.output_data = q_output_data;
 
-  tflite::testing::TestDivQuantized(kDims, kInput1, kDims, kInput2, kDims,
+  tflite_micro::testing::TestDivQuantized(kDims, kInput1, kDims, kInput2, kDims,
                                     kExpect, output_data, kTfLiteActNone,
                                     &params);
 }
@@ -292,14 +292,14 @@ TF_LITE_MICRO_TEST(QuantizedDivOpTestActReluN1To1) {
   int8_t q_output_data[kOutputCount];
   int8_t q_input1_data[kOutputCount];
   int8_t q_input2_data[kOutputCount];
-  tflite::testing::TestQuantParams<int8_t> params = {};
+  tflite_micro::testing::TestQuantParams<int8_t> params = {};
   params.data_min = -1.0;
   params.data_max = 1.0;
   params.input1_data = q_input1_data;
   params.input2_data = q_input2_data;
   params.output_data = q_output_data;
 
-  tflite::testing::TestDivQuantized(kDims, kInput1, kDims, kInput2, kDims,
+  tflite_micro::testing::TestDivQuantized(kDims, kInput1, kDims, kInput2, kDims,
                                     kExpect1, output_data, kTfLiteActReluN1To1,
                                     &params);
 
@@ -307,7 +307,7 @@ TF_LITE_MICRO_TEST(QuantizedDivOpTestActReluN1To1) {
   constexpr float kInput4[] = {0.6, 0.5, -0.8, 0.5};
   constexpr float kExpect2[] = {-0.833, 0.4, -0.75, 0.6};
 
-  tflite::testing::TestDivQuantized(kDims, kInput3, kDims, kInput4, kDims,
+  tflite_micro::testing::TestDivQuantized(kDims, kInput3, kDims, kInput4, kDims,
                                     kExpect2, output_data, kTfLiteActReluN1To1,
                                     &params);
 }
@@ -330,14 +330,14 @@ TF_LITE_MICRO_TEST(QuantizedDivOpTestMultiShape) {
   int8_t q_output_data[kOutputCount];
   int8_t q_input1_data[kOutputCount];
   int8_t q_input2_data[kOutputCount];
-  tflite::testing::TestQuantParams<int8_t> params = {};
+  tflite_micro::testing::TestQuantParams<int8_t> params = {};
   params.data_min = -3.0;
   params.data_max = 3.0;
   params.input1_data = q_input1_data;
   params.input2_data = q_input2_data;
   params.output_data = q_output_data;
 
-  tflite::testing::TestDivMultiShapeQuant(kDims, kDimsCount, kInput1, kInput2,
+  tflite_micro::testing::TestDivMultiShapeQuant(kDims, kDimsCount, kInput1, kInput2,
                                           kExpect, output_data, kTfLiteActNone,
                                           &params);
 }
@@ -362,14 +362,14 @@ TF_LITE_MICRO_TEST(QuantizedDivOpTestBroadcast) {
   int8_t q_output_data[kOutputCount];
   int8_t q_input1_data[kOutputCount];
   int8_t q_input2_data[kOutputCount];
-  tflite::testing::TestQuantParams<int8_t> params = {};
+  tflite_micro::testing::TestQuantParams<int8_t> params = {};
   params.data_min = -3.0;
   params.data_max = 3.0;
   params.input1_data = q_input1_data;
   params.input2_data = q_input2_data;
   params.output_data = q_output_data;
 
-  tflite::testing::TestDivMultiBroadcastQuant(kDims, kDimsCount, kInput1,
+  tflite_micro::testing::TestDivMultiBroadcastQuant(kDims, kDimsCount, kInput1,
                                               kInput2, kExpect, output_data,
                                               kTfLiteActNone, &params);
 }
@@ -382,7 +382,7 @@ TF_LITE_MICRO_TEST(IntegerDivOpTestNoActivation) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   int32_t output_data[kOutputCount];
 
-  tflite::testing::TestDiv(kDims, kInput1, kDims, kInput2, kDims, kExpect,
+  tflite_micro::testing::TestDiv(kDims, kInput1, kDims, kInput2, kDims, kExpect,
                            output_data, kTfLiteActNone);
 }
 
@@ -394,7 +394,7 @@ TF_LITE_MICRO_TEST(IntegerDivOpTestActivationRELU_N1_TO_1) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   int32_t output_data[kOutputCount];
 
-  tflite::testing::TestDiv(kDims, kInput1, kDims, kInput2, kDims, kExpect,
+  tflite_micro::testing::TestDiv(kDims, kInput1, kDims, kInput2, kDims, kExpect,
                            output_data, kTfLiteActReluN1To1);
 }
 
@@ -412,7 +412,7 @@ TF_LITE_MICRO_TEST(IntegerDivOpTestVariousInputShapes) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   int32_t output_data[kOutputCount];
 
-  tflite::testing::TestDivMultiShape(kDims, kDimsCount, kInput1, kInput2,
+  tflite_micro::testing::TestDivMultiShape(kDims, kDimsCount, kInput1, kInput2,
                                      kExpect, output_data, kTfLiteActNone);
 }
 
@@ -431,7 +431,7 @@ TF_LITE_MICRO_TEST(IntegerDivOpTestWithBroadcast) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   int32_t output_data[kOutputCount];
 
-  tflite::testing::TestDivMultiBroadcast(kDims, kDimsCount, kInput1, kInput2,
+  tflite_micro::testing::TestDivMultiBroadcast(kDims, kDimsCount, kInput1, kInput2,
                                          kExpect, output_data, kTfLiteActNone);
 }
 
diff --git a/tensorflow/lite/micro/kernels/elementwise.cc b/tensorflow/lite/micro/kernels/elementwise.cc
index a33c3406..a64e58d6 100644
--- a/tensorflow/lite/micro/kernels/elementwise.cc
+++ b/tensorflow/lite/micro/kernels/elementwise.cc
@@ -24,7 +24,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_log.h"
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 constexpr int kAbsNameId = 0;
@@ -171,12 +171,12 @@ inline TfLiteStatus EvalImplQuantized(
     T func(TfLiteContext*, TfLiteNode*, T),
     TfLiteStatus validate_input_func(TfLiteContext*, TfLiteNode*, T),
     TfLiteType expected_type) {
-  const TfLiteEvalTensor* input = tflite::micro::GetEvalInput(context, node, 0);
-  TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, 0);
+  const TfLiteEvalTensor* input = tflite_micro::micro::GetEvalInput(context, node, 0);
+  TfLiteEvalTensor* output = tflite_micro::micro::GetEvalOutput(context, node, 0);
   TF_LITE_ENSURE_TYPES_EQ(context, input->type, expected_type);
   const size_t num_elements = ElementCount(*input->dims);
-  const T* in_data = tflite::micro::GetTensorData<T>(input);
-  T* out_data = tflite::micro::GetTensorData<T>(output);
+  const T* in_data = tflite_micro::micro::GetTensorData<T>(input);
+  T* out_data = tflite_micro::micro::GetTensorData<T>(output);
   for (size_t i = 0; i < num_elements; ++i) {
     if (validate_input_func) {
       TF_LITE_ENSURE_OK(context,
@@ -196,12 +196,12 @@ template <typename T>
 inline TfLiteStatus EvalImpl(TfLiteContext* context, TfLiteNode* node,
                              T func(T), TfLiteStatus validate_input_func(T),
                              TfLiteType expected_type) {
-  const TfLiteEvalTensor* input = tflite::micro::GetEvalInput(context, node, 0);
-  TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, 0);
+  const TfLiteEvalTensor* input = tflite_micro::micro::GetEvalInput(context, node, 0);
+  TfLiteEvalTensor* output = tflite_micro::micro::GetEvalOutput(context, node, 0);
   TF_LITE_ENSURE_TYPES_EQ(context, input->type, expected_type);
   const size_t num_elements = ElementCount(*input->dims);
-  const T* in_data = tflite::micro::GetTensorData<T>(input);
-  T* out_data = tflite::micro::GetTensorData<T>(output);
+  const T* in_data = tflite_micro::micro::GetTensorData<T>(input);
+  T* out_data = tflite_micro::micro::GetTensorData<T>(output);
   for (size_t i = 0; i < num_elements; ++i) {
     if (validate_input_func) {
       TF_LITE_ENSURE_OK(context, validate_input_func(in_data[i]));
@@ -244,7 +244,7 @@ inline T AbsEvalQuantized(TfLiteContext* context, TfLiteNode* node, T i) {
                  static_cast<long int>(kMax)));
   }
 
-  const int32_t output = tflite::MultiplyByQuantizedMultiplier(
+  const int32_t output = tflite_micro::MultiplyByQuantizedMultiplier(
                              value, op_data->multiplier, op_data->shift) +
                          op_data->output_offset;
   return static_cast<T>(std::min(
@@ -268,10 +268,10 @@ inline T RsqrtEvalQuantized(TfLiteContext* context, TfLiteNode* node, T i) {
   int inv_sqrt_shift;
   GetInvSqrtQuantizedMultiplierExp(value, kReverseShift, &inv_sqrt_multiplier,
                                    &inv_sqrt_shift);
-  const int32_t data = tflite::MultiplyByQuantizedMultiplier(
+  const int32_t data = tflite_micro::MultiplyByQuantizedMultiplier(
       static_cast<int32_t>(1), inv_sqrt_multiplier, inv_sqrt_shift + kShift);
   const int32_t output =
-      tflite::MultiplyByQuantizedMultiplier(data, op_data->multiplier,
+      tflite_micro::MultiplyByQuantizedMultiplier(data, op_data->multiplier,
                                             op_data->shift - kShift) +
       op_data->output_offset;
   return static_cast<T>(std::min(
@@ -372,45 +372,45 @@ TfLiteStatus LogicalNotEval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_ABS() {
-  return tflite::micro::RegisterOp(
+  return tflite_micro::micro::RegisterOp(
       ElementWiseAbsRsqrtInit, PrepareAbsRsqrt<IsAbsSupportedType, kAbsNameId>,
       AbsEval);
 }
 
 TFLMRegistration Register_SIN() {
-  return tflite::micro::RegisterOp(
+  return tflite_micro::micro::RegisterOp(
       nullptr, GenericPrepare<IsNumericSupportedType>, SinEval);
 }
 
 TFLMRegistration Register_COS() {
-  return tflite::micro::RegisterOp(
+  return tflite_micro::micro::RegisterOp(
       nullptr, GenericPrepare<IsNumericSupportedType>, CosEval);
 }
 
 TFLMRegistration Register_LOG() {
-  return tflite::micro::RegisterOp(
+  return tflite_micro::micro::RegisterOp(
       nullptr, GenericPrepare<IsNumericSupportedType>, LogEval);
 }
 
 TFLMRegistration Register_SQRT() {
-  return tflite::micro::RegisterOp(
+  return tflite_micro::micro::RegisterOp(
       nullptr, GenericPrepare<IsNumericSupportedType>, SqrtEval);
 }
 
 TFLMRegistration Register_RSQRT() {
-  return tflite::micro::RegisterOp(
+  return tflite_micro::micro::RegisterOp(
       ElementWiseAbsRsqrtInit,
       PrepareAbsRsqrt<IsRsqrtSupportedType, kRsrqtNameId>, RsqrtEval);
 }
 
 TFLMRegistration Register_SQUARE() {
-  return tflite::micro::RegisterOp(
+  return tflite_micro::micro::RegisterOp(
       nullptr, GenericPrepare<IsNumericSupportedType>, SquareEval);
 }
 
 TFLMRegistration Register_LOGICAL_NOT() {
-  return tflite::micro::RegisterOp(
+  return tflite_micro::micro::RegisterOp(
       nullptr, GenericPrepare<IsLogicalSupportedType>, LogicalNotEval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/elementwise_test.cc b/tensorflow/lite/micro/kernels/elementwise_test.cc
index 10e96c35..2abca41a 100644
--- a/tensorflow/lite/micro/kernels/elementwise_test.cc
+++ b/tensorflow/lite/micro/kernels/elementwise_test.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 
 void TestElementwiseFloat(const TFLMRegistration& registration,
@@ -85,15 +85,15 @@ void TestElementwiseQuantized(const TFLMRegistration& registration,
   int input_zero_points[2] = {1, input_zero_point};
   float input_scales[2] = {1, input_scale};
   TfLiteAffineQuantization input_quant = {
-      tflite::testing::FloatArrayFromFloats(input_scales),
-      tflite::testing::IntArrayFromInts(input_zero_points), 0};
+      tflite_micro::testing::FloatArrayFromFloats(input_scales),
+      tflite_micro::testing::IntArrayFromInts(input_zero_points), 0};
   tensors[0].quantization = {kTfLiteAffineQuantization, &input_quant};
 
   int output_zero_points[2] = {1, output_zero_point};
   float output_scales[2] = {1, output_scale};
   TfLiteAffineQuantization output_quant = {
-      tflite::testing::FloatArrayFromFloats(output_scales),
-      tflite::testing::IntArrayFromInts(output_zero_points), 0};
+      tflite_micro::testing::FloatArrayFromFloats(output_scales),
+      tflite_micro::testing::IntArrayFromInts(output_zero_points), 0};
   tensors[1].quantization = {kTfLiteAffineQuantization, &output_quant};
 
   static int inputs_array_data[] = {1, 0};
@@ -152,7 +152,7 @@ void TestElementwiseBool(const TFLMRegistration& registration,
 }
 
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -162,7 +162,7 @@ TF_LITE_MICRO_TEST(Abs) {
   const float input[] = {0.01, -0.01, 10, -10};
   const float golden[] = {0.01, 0.01, 10, 10};
   float output_data[output_dims_count];
-  tflite::testing::TestElementwiseFloat(tflite::Register_ABS(), shape, input,
+  tflite_micro::testing::TestElementwiseFloat(tflite_micro::Register_ABS(), shape, input,
                                         shape, golden, output_data);
 }
 
@@ -182,8 +182,8 @@ TF_LITE_MICRO_TEST(AbsInt8) {
   const float output_scale = abs_max / 255.0f;
   const int input_zero_point = 127 - data_max;
   const int output_zero_point = -128;
-  tflite::testing::TestElementwiseQuantized<int8_t>(
-      tflite::Register_ABS(), shape, input_data, input_quantized, input_scale,
+  tflite_micro::testing::TestElementwiseQuantized<int8_t>(
+      tflite_micro::Register_ABS(), shape, input_data, input_quantized, input_scale,
       input_zero_point, shape, golden, output_quantized, output_scale,
       output_zero_point);
 }
@@ -201,8 +201,8 @@ TF_LITE_MICRO_TEST(AbsInt8SameScale) {
   const float data_max = 113;
   const float scale = (data_max - data_min) / 255.0f;
   const int zero_point = 127 - data_max;
-  tflite::testing::TestElementwiseQuantized(
-      tflite::Register_ABS(), shape, input_data, input_quantized, scale,
+  tflite_micro::testing::TestElementwiseQuantized(
+      tflite_micro::Register_ABS(), shape, input_data, input_quantized, scale,
       zero_point, shape, golden, output_quantized, scale, -128);
 }
 
@@ -219,8 +219,8 @@ TF_LITE_MICRO_TEST(AbsInt16) {
   const float output_max = 150;
   const float input_scale = input_max / std::numeric_limits<int16_t>::max();
   const float output_scale = output_max / std::numeric_limits<int16_t>::max();
-  tflite::testing::TestElementwiseQuantized(
-      tflite::Register_ABS(), shape, input_data, input_quantized, input_scale,
+  tflite_micro::testing::TestElementwiseQuantized(
+      tflite_micro::Register_ABS(), shape, input_data, input_quantized, input_scale,
       /*input_zero_point*/ 0, shape, golden, output_quantized, output_scale,
       /*output_zero_point*/ 0);
 }
@@ -231,7 +231,7 @@ TF_LITE_MICRO_TEST(Sin) {
   const float input[] = {0, 3.1415926, -3.1415926, 1};
   const float golden[] = {0, 0, 0, 0.84147};
   float output_data[output_dims_count];
-  tflite::testing::TestElementwiseFloat(tflite::Register_SIN(), shape, input,
+  tflite_micro::testing::TestElementwiseFloat(tflite_micro::Register_SIN(), shape, input,
                                         shape, golden, output_data);
 }
 
@@ -241,7 +241,7 @@ TF_LITE_MICRO_TEST(Cos) {
   const float input[] = {0, 3.1415926, -3.1415926, 1};
   const float golden[] = {1, -1, -1, 0.54030};
   float output_data[output_dims_count];
-  tflite::testing::TestElementwiseFloat(tflite::Register_COS(), shape, input,
+  tflite_micro::testing::TestElementwiseFloat(tflite_micro::Register_COS(), shape, input,
                                         shape, golden, output_data);
 }
 
@@ -251,7 +251,7 @@ TF_LITE_MICRO_TEST(Log) {
   const float input[] = {1, 2.7182818, 0.5, 2};
   const float golden[] = {0, 1, -0.6931472, 0.6931472};
   float output_data[output_dims_count];
-  tflite::testing::TestElementwiseFloat(tflite::Register_LOG(), shape, input,
+  tflite_micro::testing::TestElementwiseFloat(tflite_micro::Register_LOG(), shape, input,
                                         shape, golden, output_data);
 }
 
@@ -261,7 +261,7 @@ TF_LITE_MICRO_TEST(Sqrt) {
   const float input[] = {0, 1, 2, 4};
   const float golden[] = {0, 1, 1.41421, 2};
   float output_data[output_dims_count];
-  tflite::testing::TestElementwiseFloat(tflite::Register_SQRT(), shape, input,
+  tflite_micro::testing::TestElementwiseFloat(tflite_micro::Register_SQRT(), shape, input,
                                         shape, golden, output_data);
 }
 
@@ -271,7 +271,7 @@ TF_LITE_MICRO_TEST(Rsqrt) {
   const float input[] = {1, 2, 4, 9};
   const float golden[] = {1, 0.7071, 0.5, 0.33333};
   float output_data[output_dims_count];
-  tflite::testing::TestElementwiseFloat(tflite::Register_RSQRT(), shape, input,
+  tflite_micro::testing::TestElementwiseFloat(tflite_micro::Register_RSQRT(), shape, input,
                                         shape, golden, output_data);
 }
 
@@ -290,8 +290,8 @@ TF_LITE_MICRO_TEST(RsqrtInt8) {
   const float output_scale = 1.0 / 255.0;
   const int input_zero_point = 127 - data_max;
   const int output_zero_point = -128;
-  tflite::testing::TestElementwiseQuantized<int8_t>(
-      tflite::Register_RSQRT(), shape, input_data, input_quantized, input_scale,
+  tflite_micro::testing::TestElementwiseQuantized<int8_t>(
+      tflite_micro::Register_RSQRT(), shape, input_data, input_quantized, input_scale,
       input_zero_point, shape, golden, output_quantized, output_scale,
       output_zero_point);
 }
@@ -310,8 +310,8 @@ TF_LITE_MICRO_TEST(RsqrtInt16) {
   const float output_scale = 1.0 / 32768.0;
   const int input_zero_point = 0;
   const int output_zero_point = 0;
-  tflite::testing::TestElementwiseQuantized<int16_t>(
-      tflite::Register_RSQRT(), shape, input_data, input_quantized, input_scale,
+  tflite_micro::testing::TestElementwiseQuantized<int16_t>(
+      tflite_micro::Register_RSQRT(), shape, input_data, input_quantized, input_scale,
       input_zero_point, shape, golden, output_quantized, output_scale,
       output_zero_point);
 }
@@ -331,8 +331,8 @@ TF_LITE_MICRO_TEST(RsqrtCloseTo0Int8) {
   const float output_scale = 3.16 / 255.0;
   const int input_zero_point = 127 - data_max;
   const int output_zero_point = -128;
-  tflite::testing::TestElementwiseQuantized<int8_t>(
-      tflite::Register_RSQRT(), shape, input_data, input_quantized, input_scale,
+  tflite_micro::testing::TestElementwiseQuantized<int8_t>(
+      tflite_micro::Register_RSQRT(), shape, input_data, input_quantized, input_scale,
       input_zero_point, shape, golden, output_quantized, output_scale,
       output_zero_point);
 }
@@ -353,8 +353,8 @@ TF_LITE_MICRO_TEST(RsqrtNanInt8) {
   const int input_zero_point = 127 - data_max;
   const int output_zero_point = -128;
 
-  tflite::testing::TestElementwiseQuantized<int8_t>(
-      tflite::Register_RSQRT(), shape, input_data, input_quantized, input_scale,
+  tflite_micro::testing::TestElementwiseQuantized<int8_t>(
+      tflite_micro::Register_RSQRT(), shape, input_data, input_quantized, input_scale,
       input_zero_point, shape, golden, output_quantized, output_scale,
       output_zero_point, kTfLiteError);
 }
@@ -365,7 +365,7 @@ TF_LITE_MICRO_TEST(Square) {
   const float input[] = {1, 2, 0.5, -3.0};
   const float golden[] = {1, 4.0, 0.25, 9.0};
   float output_data[output_dims_count];
-  tflite::testing::TestElementwiseFloat(tflite::Register_SQUARE(), shape, input,
+  tflite_micro::testing::TestElementwiseFloat(tflite_micro::Register_SQUARE(), shape, input,
                                         shape, golden, output_data);
 }
 
@@ -375,7 +375,7 @@ TF_LITE_MICRO_TEST(LogicalNot) {
   const bool input[] = {true, false, false, true};
   const bool golden[] = {false, true, true, false};
   bool output_data[output_dims_count];
-  tflite::testing::TestElementwiseBool(tflite::Register_LOGICAL_NOT(), shape,
+  tflite_micro::testing::TestElementwiseBool(tflite_micro::Register_LOGICAL_NOT(), shape,
                                        input, shape, golden, output_data);
 }
 
diff --git a/tensorflow/lite/micro/kernels/elu.cc b/tensorflow/lite/micro/kernels/elu.cc
index aacd21e1..9e22f3ad 100644
--- a/tensorflow/lite/micro/kernels/elu.cc
+++ b/tensorflow/lite/micro/kernels/elu.cc
@@ -27,7 +27,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 // Input/output tensor index.
@@ -69,10 +69,10 @@ void PopulateLookupTable(const TfLiteTensor* input, const TfLiteTensor* output,
 // OLD-TODO(b/143696793): move this to optimized_ops.
 void EvalUsingLookupTable(const OpData* data, const TfLiteEvalTensor* input,
                           TfLiteEvalTensor* output) {
-  const int size = MatchingFlatSize(tflite::micro::GetTensorShape(input),
-                                    tflite::micro::GetTensorShape(output));
-  int8_t* output_data = tflite::micro::GetTensorData<int8_t>(output);
-  const int8_t* input_data = tflite::micro::GetTensorData<int8_t>(input);
+  const int size = MatchingFlatSize(tflite_micro::micro::GetTensorShape(input),
+                                    tflite_micro::micro::GetTensorShape(output));
+  int8_t* output_data = tflite_micro::micro::GetTensorData<int8_t>(output);
+  const int8_t* input_data = tflite_micro::micro::GetTensorData<int8_t>(input);
 
   for (int i = 0; i < size; ++i) {
     output_data[i] = data->table[static_cast<uint8_t>(input_data[i])];
@@ -119,15 +119,15 @@ TfLiteStatus EluPrepare(TfLiteContext* context, TfLiteNode* node) {
 
 TfLiteStatus EluEval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
   switch (input->type) {
     case kTfLiteFloat32: {
-      reference_ops::Elu(tflite::micro::GetTensorShape(input),
-                         tflite::micro::GetTensorData<float>(input),
-                         tflite::micro::GetTensorShape(output),
-                         tflite::micro::GetTensorData<float>(output));
+      reference_ops::Elu(tflite_micro::micro::GetTensorShape(input),
+                         tflite_micro::micro::GetTensorData<float>(input),
+                         tflite_micro::micro::GetTensorShape(output),
+                         tflite_micro::micro::GetTensorData<float>(output));
       return kTfLiteOk;
     }
     case kTfLiteInt8: {
@@ -145,7 +145,7 @@ TfLiteStatus EluEval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_ELU() {
-  return tflite::micro::RegisterOp(EluInit, EluPrepare, EluEval);
+  return tflite_micro::micro::RegisterOp(EluInit, EluPrepare, EluEval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/elu_test.cc b/tensorflow/lite/micro/kernels/elu_test.cc
index e8fa378f..cee31795 100644
--- a/tensorflow/lite/micro/kernels/elu_test.cc
+++ b/tensorflow/lite/micro/kernels/elu_test.cc
@@ -20,7 +20,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -59,7 +59,7 @@ void ExecuteEluTest(TfLiteTensor* tensors, int tensors_count) {
   int kOutputArrayData[] = {1, 1};
   TfLiteIntArray* outputs_array = IntArrayFromInts(kOutputArrayData);
 
-  const TFLMRegistration registration = tflite::Register_ELU();
+  const TFLMRegistration registration = tflite_micro::Register_ELU();
   micro::KernelRunner runner(registration, tensors, tensors_count, inputs_array,
                              outputs_array, nullptr);
 
@@ -117,7 +117,7 @@ void TestEluQuantized(const TestEluParams<T>& params, int* input_dims_data,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -134,7 +134,7 @@ TF_LITE_MICRO_TEST(FloatActivationsOpTestElu) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
 
-  tflite::testing::TestElu(kDims, kInput, kDims, kExpect, output_data);
+  tflite_micro::testing::TestElu(kDims, kInput, kDims, kExpect, output_data);
 }
 
 TF_LITE_MICRO_TEST(QuantizedActivationsOpTestEluInt8) {
@@ -155,14 +155,14 @@ TF_LITE_MICRO_TEST(QuantizedActivationsOpTestEluInt8) {
   int8_t q_input_data[kOutputCount];
   constexpr float kMin = -1;
   constexpr float kMax = 127.f / 128.f;
-  tflite::testing::TestEluParams<int8_t> params = {};
+  tflite_micro::testing::TestEluParams<int8_t> params = {};
   params.data_min = 8 * kMin;
   params.data_max = 8 * kMax;
   params.input_data = q_input_data;
   params.output_data = q_output_data;
-  params.tolerance = tflite::testing::kQuantizedTolerance;
+  params.tolerance = tflite_micro::testing::kQuantizedTolerance;
 
-  tflite::testing::TestEluQuantized(params, kDims, kInput, kDims, kExpect,
+  tflite_micro::testing::TestEluQuantized(params, kDims, kInput, kDims, kExpect,
                                     output_data);
 }
 
diff --git a/tensorflow/lite/micro/kernels/embedding_lookup.cc b/tensorflow/lite/micro/kernels/embedding_lookup.cc
index 77ac0e0c..2e168efd 100644
--- a/tensorflow/lite/micro/kernels/embedding_lookup.cc
+++ b/tensorflow/lite/micro/kernels/embedding_lookup.cc
@@ -36,7 +36,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_log.h"
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 constexpr int kInputTensor_0 = 0;
@@ -98,8 +98,8 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   TF_LITE_ENSURE(context, output->dims->size >= NumDimensions(value));
   // make the output tensor dimensions mutable
   TfLiteEvalTensor* output_eval =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
-  TF_LITE_ENSURE_OK(context, tflite::micro::CreateWritableTensorDimsWithCopy(
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
+  TF_LITE_ENSURE_OK(context, tflite_micro::micro::CreateWritableTensorDimsWithCopy(
                                  context, output, output_eval));
   // set the new output dimensions
   output->dims->data[0] = SizeOfDimension(lookup, 0);
@@ -130,9 +130,9 @@ TfLiteStatus EvalSimple(const OpData& op_data, const TfLiteEvalTensor* lookup,
   }
   const size_t row_bytes = op_data.num_columns * TfLiteTypeGetSize(value->type);
 
-  int8_t* output_raw = tflite::micro::GetTensorData<int8_t>(output);
-  const int8_t* value_raw = tflite::micro::GetTensorData<int8_t>(value);
-  const int32_t* lookup_data = tflite::micro::GetTensorData<int32_t>(lookup);
+  int8_t* output_raw = tflite_micro::micro::GetTensorData<int8_t>(output);
+  const int8_t* value_raw = tflite_micro::micro::GetTensorData<int8_t>(value);
+  const int32_t* lookup_data = tflite_micro::micro::GetTensorData<int32_t>(lookup);
   for (int i = 0; i < lookup->dims->data[0]; i++) {
     int32_t idx = lookup_data[i];
     if (idx >= num_rows || idx < 0) {
@@ -156,9 +156,9 @@ TfLiteStatus EvalHybrid(const OpData& op_data, const TfLiteEvalTensor* lookup,
   const int num_rows = value->dims->data[0];
   const size_t num_colums = op_data.num_columns;
 
-  float* output_ptr = tflite::micro::GetTensorData<float>(output);
-  const int8_t* value_ptr = tflite::micro::GetTensorData<int8_t>(value);
-  const int32_t* lookup_data = tflite::micro::GetTensorData<int32_t>(lookup);
+  float* output_ptr = tflite_micro::micro::GetTensorData<float>(output);
+  const int8_t* value_ptr = tflite_micro::micro::GetTensorData<int8_t>(value);
+  const int32_t* lookup_data = tflite_micro::micro::GetTensorData<int32_t>(lookup);
 
   for (int i = 0; i < lookup->dims->data[0]; i++) {
     int32_t idx = lookup_data[i];
@@ -180,11 +180,11 @@ TfLiteStatus EvalHybrid(const OpData& op_data, const TfLiteEvalTensor* lookup,
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* lookup =
-      tflite::micro::GetEvalInput(context, node, kInputTensor_0);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor_0);
   const TfLiteEvalTensor* value =
-      tflite::micro::GetEvalInput(context, node, kInputTensor_1);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor_1);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
   OpData& op_data = *static_cast<OpData*>(node->user_data);
 
@@ -207,7 +207,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_EMBEDDING_LOOKUP() {
-  return tflite::micro::RegisterOp(nullptr, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/embedding_lookup_test.cc b/tensorflow/lite/micro/kernels/embedding_lookup_test.cc
index c94cebbd..9604212a 100644
--- a/tensorflow/lite/micro/kernels/embedding_lookup_test.cc
+++ b/tensorflow/lite/micro/kernels/embedding_lookup_test.cc
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -50,7 +50,7 @@ void ExecuteEmbeddingLookupTest(TfLiteTensor* tensors, int tensors_count) {
   int kOutputArrayData[] = {kNumOutputs, kOutputTensorIndex};
   TfLiteIntArray* outputs_array = IntArrayFromInts(kOutputArrayData);
 
-  const TFLMRegistration registration = tflite::Register_EMBEDDING_LOOKUP();
+  const TFLMRegistration registration = tflite_micro::Register_EMBEDDING_LOOKUP();
   micro::KernelRunner runner(registration, tensors, tensors_count, inputs_array,
                              outputs_array, nullptr);
 
@@ -130,14 +130,14 @@ void TestEmbeddingLookup(int* input_dims_data[kNumInputs],
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(EmbeddingLookupOpTestSimpleFloat) {
   int kInputDims_0[] = {1, 3};
   int kInputDims_1[] = {3, 3, 2, 4};
-  int* kInputDims[tflite::testing::kNumInputs] = {kInputDims_0, kInputDims_1};
+  int* kInputDims[tflite_micro::testing::kNumInputs] = {kInputDims_0, kInputDims_1};
   int kOutputDims[] = {3, 3, 2, 4};
 
   constexpr int32_t kInput_0[] = {1, 0, 2};
@@ -154,14 +154,14 @@ TF_LITE_MICRO_TEST(EmbeddingLookupOpTestSimpleFloat) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
 
-  tflite::testing::TestEmbeddingLookup(kInputDims, kInput_0, kInput_1,
+  tflite_micro::testing::TestEmbeddingLookup(kInputDims, kInput_0, kInput_1,
                                        kOutputDims, kExpect, output_data);
 }
 
 TF_LITE_MICRO_TEST(HybridEmbeddingLookupHybridOpTestSimple2DTestInt8) {
   int kInputDims_0[] = {1, 3};
   int kInputDims_1[] = {2, 3, 8};
-  int* kInputDims[tflite::testing::kNumInputs] = {kInputDims_0, kInputDims_1};
+  int* kInputDims[tflite_micro::testing::kNumInputs] = {kInputDims_0, kInputDims_1};
   int kOutputDims[] = {2, 3, 8};
 
   constexpr int32_t kInput_0[] = {1, 0, 2};
@@ -179,12 +179,12 @@ TF_LITE_MICRO_TEST(HybridEmbeddingLookupHybridOpTestSimple2DTestInt8) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
 
-  tflite::testing::TestEmbeddingLookupParams<kInputCount_1> params = {};
+  tflite_micro::testing::TestEmbeddingLookupParams<kInputCount_1> params = {};
   auto minmax = std::minmax_element(std::begin(kInput_1), std::end(kInput_1));
   params.data_max = *minmax.second;
   params.data_min = *minmax.first;
 
-  tflite::testing::TestEmbeddingLookupQuantized(params, kInputDims, kInput_0,
+  tflite_micro::testing::TestEmbeddingLookupQuantized(params, kInputDims, kInput_0,
                                                 kInput_1, kOutputDims, kExpect,
                                                 output_data);
 }
@@ -192,7 +192,7 @@ TF_LITE_MICRO_TEST(HybridEmbeddingLookupHybridOpTestSimple2DTestInt8) {
 TF_LITE_MICRO_TEST(HybridEmbeddingLookupHybridOpTestSimple3DTestInt8) {
   int kInputDims_0[] = {1, 3};
   int kInputDims_1[] = {3, 3, 2, 4};
-  int* kInputDims[tflite::testing::kNumInputs] = {kInputDims_0, kInputDims_1};
+  int* kInputDims[tflite_micro::testing::kNumInputs] = {kInputDims_0, kInputDims_1};
   int kOutputDims[] = {3, 3, 2, 4};
 
   constexpr int32_t kInput_0[] = {1, 0, 2};
@@ -210,12 +210,12 @@ TF_LITE_MICRO_TEST(HybridEmbeddingLookupHybridOpTestSimple3DTestInt8) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
 
-  tflite::testing::TestEmbeddingLookupParams<kInputCount_1> params = {};
+  tflite_micro::testing::TestEmbeddingLookupParams<kInputCount_1> params = {};
   auto minmax = std::minmax_element(std::begin(kInput_1), std::end(kInput_1));
   params.data_max = *minmax.second;
   params.data_min = *minmax.first;
 
-  tflite::testing::TestEmbeddingLookupQuantized(params, kInputDims, kInput_0,
+  tflite_micro::testing::TestEmbeddingLookupQuantized(params, kInputDims, kInput_0,
                                                 kInput_1, kOutputDims, kExpect,
                                                 output_data);
 }
@@ -223,7 +223,7 @@ TF_LITE_MICRO_TEST(HybridEmbeddingLookupHybridOpTestSimple3DTestInt8) {
 TF_LITE_MICRO_TEST(HybridEmbeddingLookupHybridOpTestSimple4DTestInt8) {
   int kInputDims_0[] = {1, 3};
   int kInputDims_1[] = {4, 3, 2, 2, 2};
-  int* kInputDims[tflite::testing::kNumInputs] = {kInputDims_0, kInputDims_1};
+  int* kInputDims[tflite_micro::testing::kNumInputs] = {kInputDims_0, kInputDims_1};
   int kOutputDims[] = {4, 3, 2, 2, 2};
 
   constexpr int32_t kInput_0[] = {1, 0, 2};
@@ -241,12 +241,12 @@ TF_LITE_MICRO_TEST(HybridEmbeddingLookupHybridOpTestSimple4DTestInt8) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
 
-  tflite::testing::TestEmbeddingLookupParams<kInputCount_1> params = {};
+  tflite_micro::testing::TestEmbeddingLookupParams<kInputCount_1> params = {};
   auto minmax = std::minmax_element(std::begin(kInput_1), std::end(kInput_1));
   params.data_max = *minmax.second;
   params.data_min = *minmax.first;
 
-  tflite::testing::TestEmbeddingLookupQuantized(params, kInputDims, kInput_0,
+  tflite_micro::testing::TestEmbeddingLookupQuantized(params, kInputDims, kInput_0,
                                                 kInput_1, kOutputDims, kExpect,
                                                 output_data);
 }
@@ -254,7 +254,7 @@ TF_LITE_MICRO_TEST(HybridEmbeddingLookupHybridOpTestSimple4DTestInt8) {
 TF_LITE_MICRO_TEST(EmbeddingLookupOpTestSimpleInt8) {
   int kInputDims_0[] = {1, 3};
   int kInputDims_1[] = {3, 3, 2, 4};
-  int* kInputDims[tflite::testing::kNumInputs] = {kInputDims_0, kInputDims_1};
+  int* kInputDims[tflite_micro::testing::kNumInputs] = {kInputDims_0, kInputDims_1};
   int kOutputDims[] = {3, 3, 2, 4};
 
   constexpr int32_t kInput_0[] = {1, 0, 2};
@@ -271,7 +271,7 @@ TF_LITE_MICRO_TEST(EmbeddingLookupOpTestSimpleInt8) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   int8_t output_data[kOutputCount];
 
-  tflite::testing::TestEmbeddingLookup(kInputDims, kInput_0, kInput_1,
+  tflite_micro::testing::TestEmbeddingLookup(kInputDims, kInput_0, kInput_1,
                                        kOutputDims, kExpect, output_data);
 }
 
diff --git a/tensorflow/lite/micro/kernels/ethos_u/ethosu.cc b/tensorflow/lite/micro/kernels/ethos_u/ethosu.cc
index b167b6bf..2fc1546e 100644
--- a/tensorflow/lite/micro/kernels/ethos_u/ethosu.cc
+++ b/tensorflow/lite/micro/kernels/ethos_u/ethosu.cc
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_context.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 constexpr uint8_t CO_TYPE_ETHOSU = 1;
@@ -164,10 +164,10 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration* Register_ETHOSU() {
-  static TFLMRegistration r = tflite::micro::RegisterOp(Init, Prepare, Eval);
+  static TFLMRegistration r = tflite_micro::micro::RegisterOp(Init, Prepare, Eval);
   return &r;
 }
 
 const char* GetString_ETHOSU() { return "ethos-u"; }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/ethosu.cc b/tensorflow/lite/micro/kernels/ethosu.cc
index 56203599..8116c521 100644
--- a/tensorflow/lite/micro/kernels/ethosu.cc
+++ b/tensorflow/lite/micro/kernels/ethosu.cc
@@ -18,10 +18,10 @@ limitations under the License.
 //
 #include "tensorflow/lite/micro/micro_common.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 TFLMRegistration* Register_ETHOSU() { return nullptr; }
 
 const char* GetString_ETHOSU() { return ""; }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/ethosu.h b/tensorflow/lite/micro/kernels/ethosu.h
index 5b863032..da91066b 100644
--- a/tensorflow/lite/micro/kernels/ethosu.h
+++ b/tensorflow/lite/micro/kernels/ethosu.h
@@ -17,12 +17,12 @@ limitations under the License.
 
 #include "tensorflow/lite/c/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 TFLMRegistration* Register_ETHOSU();
 
 const char* GetString_ETHOSU();
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_ETHOSU_H_
diff --git a/tensorflow/lite/micro/kernels/exp.cc b/tensorflow/lite/micro/kernels/exp.cc
index 1a2e00ca..5c152c73 100644
--- a/tensorflow/lite/micro/kernels/exp.cc
+++ b/tensorflow/lite/micro/kernels/exp.cc
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 constexpr int kInputTensor = 0;
@@ -53,16 +53,16 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
-  int flat_size = MatchingFlatSize(tflite::micro::GetTensorShape(input),
-                                   tflite::micro::GetTensorShape(output));
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
+  int flat_size = MatchingFlatSize(tflite_micro::micro::GetTensorShape(input),
+                                   tflite_micro::micro::GetTensorShape(output));
 
   if (input->type == kTfLiteFloat32) {
-    reference_ops::Exp(tflite::micro::GetTensorData<float>(input),
+    reference_ops::Exp(tflite_micro::micro::GetTensorData<float>(input),
                        static_cast<size_t>(flat_size),
-                       tflite::micro::GetTensorData<float>(output));
+                       tflite_micro::micro::GetTensorData<float>(output));
   } else {
     MicroPrintf("Type %s (%d) currently not supported by Exp.",
                 TfLiteTypeGetName(input->type), input->type);
@@ -73,7 +73,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_EXP() {
-  return tflite::micro::RegisterOp(nullptr, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/exp_test.cc b/tensorflow/lite/micro/kernels/exp_test.cc
index ff18106d..9afb2d16 100644
--- a/tensorflow/lite/micro/kernels/exp_test.cc
+++ b/tensorflow/lite/micro/kernels/exp_test.cc
@@ -20,7 +20,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -55,7 +55,7 @@ void TestExp(int* input_dims_data, const float* input_data,
 }
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -70,7 +70,7 @@ TF_LITE_MICRO_TEST(SingleDim) {
     golden[i] = std::exp(input_values[i]);
   }
 
-  tflite::testing::TestExp(input_dims, input_values, golden, output_data);
+  tflite_micro::testing::TestExp(input_dims, input_values, golden, output_data);
 }
 
 TF_LITE_MICRO_TESTS_END
diff --git a/tensorflow/lite/micro/kernels/expand_dims.cc b/tensorflow/lite/micro/kernels/expand_dims.cc
index 0c4c6ff3..f4c66007 100644
--- a/tensorflow/lite/micro/kernels/expand_dims.cc
+++ b/tensorflow/lite/micro/kernels/expand_dims.cc
@@ -20,7 +20,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_log.h"
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 constexpr int kInputTensor = 0;
@@ -30,14 +30,14 @@ constexpr int kOutputTensor = 0;
 TfLiteStatus GetAxisValueFromTensor(TfLiteContext* context,
                                     const TfLiteTensor* axis,
                                     int32_t* axis_value) {
-  const int axis_dims = (tflite::GetTensorShape(axis)).DimensionsCount();
+  const int axis_dims = (tflite_micro::GetTensorShape(axis)).DimensionsCount();
   if (axis_dims > 1) {
     MicroPrintf("Axis has only one element for Expand_Dims.", axis_dims);
     return kTfLiteError;
   }
 
   if (kTfLiteInt32 == (axis->type)) {
-    const int32_t* axis_ptr = tflite::GetTensorData<int32_t>(axis);
+    const int32_t* axis_ptr = tflite_micro::GetTensorData<int32_t>(axis);
     *axis_value = axis_ptr[0];
     return kTfLiteOk;
   } else {
@@ -57,7 +57,7 @@ TfLiteStatus VerifyTensorDim(TfLiteContext* context, const TfLiteTensor* input,
   TF_LITE_ENSURE_OK(context,
                     GetAxisValueFromTensor(context, axis_tensor, &axis_value));
 
-  tflite::RuntimeShape input_shape = tflite::GetTensorShape(input);
+  tflite_micro::RuntimeShape input_shape = tflite_micro::GetTensorShape(input);
   if (axis_value < 0) {
     axis_value = input_shape.DimensionsCount() + 1 + axis_value;
   }
@@ -66,7 +66,7 @@ TfLiteStatus VerifyTensorDim(TfLiteContext* context, const TfLiteTensor* input,
   // TFLM only supports fixed dimension tensor and assumes that the output shape
   // is fully specified in the model. As such, TFLM directly use the pointer to
   // the dimension array in the model buffer.
-  tflite::RuntimeShape output_shape = tflite::GetTensorShape(output);
+  tflite_micro::RuntimeShape output_shape = tflite_micro::GetTensorShape(output);
 
   TF_LITE_ENSURE(context, output_shape.DimensionsCount() ==
                               input_shape.DimensionsCount() + 1);
@@ -118,19 +118,19 @@ void memCopyN(T* out, const T* in, const int num_elements) {
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
   const int flat_size = ElementCount(*input->dims);
 
   switch (input->type) {
     case kTfLiteFloat32: {
-      memCopyN(tflite::micro::GetTensorData<float>(output),
-               tflite::micro::GetTensorData<float>(input), flat_size);
+      memCopyN(tflite_micro::micro::GetTensorData<float>(output),
+               tflite_micro::micro::GetTensorData<float>(input), flat_size);
     } break;
     case kTfLiteInt8: {
-      memCopyN(tflite::micro::GetTensorData<int8_t>(output),
-               tflite::micro::GetTensorData<int8_t>(input), flat_size);
+      memCopyN(tflite_micro::micro::GetTensorData<int8_t>(output),
+               tflite_micro::micro::GetTensorData<int8_t>(input), flat_size);
     } break;
     default:
       MicroPrintf(
@@ -143,7 +143,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_EXPAND_DIMS() {
-  return tflite::micro::RegisterOp(nullptr, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/expand_dims_test.cc b/tensorflow/lite/micro/kernels/expand_dims_test.cc
index d8e217e5..0da3351f 100644
--- a/tensorflow/lite/micro/kernels/expand_dims_test.cc
+++ b/tensorflow/lite/micro/kernels/expand_dims_test.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -92,7 +92,7 @@ void TestExpandDims(int* input_dims, const T* input_data, int* axis_dims,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -105,7 +105,7 @@ TF_LITE_MICRO_TEST(ExpandDimsPositiveAxisTest0) {
   const int32_t axis_data[] = {0};
   int golden_dims[] = {1, 2, 2};
   int output_dims[] = {3, 1, 2, 2};
-  tflite::testing::TestExpandDims<int8_t>(input_dims, input_data, axis_dims,
+  tflite_micro::testing::TestExpandDims<int8_t>(input_dims, input_data, axis_dims,
                                           axis_data, golden_dims, output_dims,
                                           golden_data, output_data);
 }
@@ -119,7 +119,7 @@ TF_LITE_MICRO_TEST(ExpandDimsPositiveAxisTest1) {
   const int32_t axis_data[] = {1};
   int golden_dims[] = {2, 1, 2};
   int output_dims[] = {3, 2, 1, 2};
-  tflite::testing::TestExpandDims<float>(input_dims, input_data, axis_dims,
+  tflite_micro::testing::TestExpandDims<float>(input_dims, input_data, axis_dims,
                                          axis_data, golden_dims, output_dims,
                                          golden_data, output_data);
 }
@@ -133,7 +133,7 @@ TF_LITE_MICRO_TEST(ExpandDimsPositiveAxisTest2) {
   const int32_t axis_data[] = {2};
   int golden_dims[] = {2, 2, 1};
   int output_dims[] = {3, 2, 2, 1};
-  tflite::testing::TestExpandDims<int8_t>(input_dims, input_data, axis_dims,
+  tflite_micro::testing::TestExpandDims<int8_t>(input_dims, input_data, axis_dims,
                                           axis_data, golden_dims, output_dims,
                                           golden_data, output_data);
 }
@@ -147,7 +147,7 @@ TF_LITE_MICRO_TEST(ExpandDimsNegativeAxisTest4) {
   const int32_t axis_data[] = {-4};
   int golden_dims[] = {1, 3, 1, 2};
   int output_dims[] = {4, 1, 3, 1, 2};
-  tflite::testing::TestExpandDims<int8_t>(input_dims, input_data, axis_dims,
+  tflite_micro::testing::TestExpandDims<int8_t>(input_dims, input_data, axis_dims,
                                           axis_data, golden_dims, output_dims,
                                           golden_data, output_data);
 }
@@ -161,7 +161,7 @@ TF_LITE_MICRO_TEST(ExpandDimsNegativeAxisTest3) {
   const int32_t axis_data[] = {-3};
   int golden_dims[] = {3, 1, 1, 2};
   int output_dims[] = {4, 3, 1, 1, 2};
-  tflite::testing::TestExpandDims<float>(input_dims, input_data, axis_dims,
+  tflite_micro::testing::TestExpandDims<float>(input_dims, input_data, axis_dims,
                                          axis_data, golden_dims, output_dims,
                                          golden_data, output_data);
 }
@@ -175,7 +175,7 @@ TF_LITE_MICRO_TEST(ExpandDimsNegativeAxisTest2) {
   const int32_t axis_data[] = {-2};
   int golden_dims[] = {1, 2, 1, 3};
   int output_dims[] = {4, 1, 2, 1, 3};
-  tflite::testing::TestExpandDims<int8_t>(input_dims, input_data, axis_dims,
+  tflite_micro::testing::TestExpandDims<int8_t>(input_dims, input_data, axis_dims,
                                           axis_data, golden_dims, output_dims,
                                           golden_data, output_data);
 }
@@ -189,7 +189,7 @@ TF_LITE_MICRO_TEST(ExpandDimsNegativeAxisTest1) {
   const int32_t axis_data[] = {-1};
   int golden_dims[] = {1, 3, 2, 1};
   int output_dims[] = {4, 1, 3, 2, 1};
-  tflite::testing::TestExpandDims<float>(input_dims, input_data, axis_dims,
+  tflite_micro::testing::TestExpandDims<float>(input_dims, input_data, axis_dims,
                                          axis_data, golden_dims, output_dims,
                                          golden_data, output_data);
 }
@@ -206,8 +206,8 @@ TF_LITE_MICRO_TEST(ExpandDimsInputOutputDimsMismatchShallFail) {
   // op would fail at prepare.
   int output_dims[] = {4, 1, 3, 1, 2};
 
-  tflite::micro::KernelRunner runner =
-      tflite::testing::CreateExpandDimsKernelRunner(input_dims, input_data,
+  tflite_micro::micro::KernelRunner runner =
+      tflite_micro::testing::CreateExpandDimsKernelRunner(input_dims, input_data,
                                                     axis_dims, axis_data,
                                                     output_dims, output_data);
 
@@ -224,8 +224,8 @@ TF_LITE_MICRO_TEST(ExpandDimsAxisOutOfRangeShallFail) {
   const int32_t axis_data[] = {4};
   int output_dims[] = {4, 1, 3, 2, 1};
 
-  tflite::micro::KernelRunner runner =
-      tflite::testing::CreateExpandDimsKernelRunner(input_dims, input_data,
+  tflite_micro::micro::KernelRunner runner =
+      tflite_micro::testing::CreateExpandDimsKernelRunner(input_dims, input_data,
                                                     axis_dims, axis_data,
                                                     output_dims, output_data);
 
diff --git a/tensorflow/lite/micro/kernels/fill.cc b/tensorflow/lite/micro/kernels/fill.cc
index b1b366eb..eecb4d2d 100644
--- a/tensorflow/lite/micro/kernels/fill.cc
+++ b/tensorflow/lite/micro/kernels/fill.cc
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -134,7 +134,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_FILL() {
-  return tflite::micro::RegisterOp(nullptr, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/fill_test.cc b/tensorflow/lite/micro/kernels/fill_test.cc
index 70355402..d3329487 100644
--- a/tensorflow/lite/micro/kernels/fill_test.cc
+++ b/tensorflow/lite/micro/kernels/fill_test.cc
@@ -21,8 +21,8 @@ limitations under the License.
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
 namespace {
-using ::tflite::testing::CreateTensor;
-using ::tflite::testing::IntArrayFromInts;
+using ::tflite_micro::testing::CreateTensor;
+using ::tflite_micro::testing::IntArrayFromInts;
 
 // The layout of tensors is fixed.
 constexpr int kDimsIndex = 0;
@@ -33,7 +33,7 @@ constexpr int kOutputsTensor[] = {1, kOutputIndex};
 
 // This function is NOT thread safe.
 template <typename DimsType, typename ValueType, typename OutputType>
-tflite::micro::KernelRunner CreateFillTestRunner(
+tflite_micro::micro::KernelRunner CreateFillTestRunner(
     int* dims_shape, DimsType* dims_data, int* value_shape,
     ValueType* value_data, int* output_shape, OutputType* output_data) {
   // Some targets do not support dynamic memory (i.e., no malloc or new), thus,
@@ -51,8 +51,8 @@ tflite::micro::KernelRunner CreateFillTestRunner(
   TF_LITE_MICRO_EXPECT_EQ(tensors[kOutputIndex].type,
                           tensors[kValueIndex].type);
 
-  registration = tflite::Register_FILL();
-  tflite::micro::KernelRunner runner = tflite::micro::KernelRunner(
+  registration = tflite_micro::Register_FILL();
+  tflite_micro::micro::KernelRunner runner = tflite_micro::micro::KernelRunner(
       registration, tensors, sizeof(tensors) / sizeof(TfLiteTensor),
       IntArrayFromInts(const_cast<int*>(kInputsTensor)),
       IntArrayFromInts(const_cast<int*>(kOutputsTensor)),
@@ -64,7 +64,7 @@ template <typename DimsType, typename ValueType, typename OutputType>
 void TestFill(int* dims_shape, DimsType* dims_data, int* value_shape,
               ValueType* value_data, int* output_shape,
               OutputType* output_data) {
-  tflite::micro::KernelRunner runner =
+  tflite_micro::micro::KernelRunner runner =
       CreateFillTestRunner(dims_shape, dims_data, value_shape, value_data,
                            output_shape, output_data);
 
@@ -86,7 +86,7 @@ void TestFill(int* dims_shape, DimsType* dims_data, int* value_shape,
   }
 
   // The output elements contain the fill value.
-  const auto elements = tflite::ElementCount(*IntArrayFromInts(output_shape));
+  const auto elements = tflite_micro::ElementCount(*IntArrayFromInts(output_shape));
   for (int i = 0; i < elements; ++i) {
     TF_LITE_MICRO_EXPECT_EQ(output_data[i], value_data[0]);
   }
@@ -226,7 +226,7 @@ TF_LITE_MICRO_TEST(FillInputDimsMismatchWithOutputShallFail) {
   int output_shape[] = {3, kDim1 + 1, kDim2, kDim3};
   int8_t output_data[(kDim1 + 1) * kDim2 * kDim3];
 
-  tflite::micro::KernelRunner runner =
+  tflite_micro::micro::KernelRunner runner =
       CreateFillTestRunner(dims_shape, dims_data, value_shape, value_data,
                            output_shape, output_data);
 
diff --git a/tensorflow/lite/micro/kernels/floor.cc b/tensorflow/lite/micro/kernels/floor.cc
index 094c8b55..3a6df0f8 100644
--- a/tensorflow/lite/micro/kernels/floor.cc
+++ b/tensorflow/lite/micro/kernels/floor.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/tensor_ctypes.h"
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -28,21 +28,21 @@ constexpr int kOutputTensor = 0;
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   TF_LITE_ENSURE_TYPES_EQ(context, input->type, kTfLiteFloat32);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
-  reference_ops::Floor(tflite::micro::GetTensorShape(input),
-                       tflite::micro::GetTensorData<float>(input),
-                       tflite::micro::GetTensorShape(output),
-                       tflite::micro::GetTensorData<float>(output));
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
+  reference_ops::Floor(tflite_micro::micro::GetTensorShape(input),
+                       tflite_micro::micro::GetTensorData<float>(input),
+                       tflite_micro::micro::GetTensorShape(output),
+                       tflite_micro::micro::GetTensorData<float>(output));
   return kTfLiteOk;
 }
 
 }  // namespace
 
 TFLMRegistration Register_FLOOR() {
-  return tflite::micro::RegisterOp(nullptr, nullptr, Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, nullptr, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/floor_div.cc b/tensorflow/lite/micro/kernels/floor_div.cc
index 5c008085..cb50ccb1 100644
--- a/tensorflow/lite/micro/kernels/floor_div.cc
+++ b/tensorflow/lite/micro/kernels/floor_div.cc
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_log.h"
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 // Input/output tensor index.
@@ -70,32 +70,32 @@ TfLiteStatus EvalFloorDiv(TfLiteContext* context,
                           const TfLiteEvalTensor* input1,
                           const TfLiteEvalTensor* input2,
                           TfLiteEvalTensor* output) {
-  const T* denominator_data = tflite::micro::GetTensorData<T>(input2);
+  const T* denominator_data = tflite_micro::micro::GetTensorData<T>(input2);
 
   // Validate the denominator.
-  for (int i = 0; i < tflite::ElementCount(*input2->dims); ++i) {
+  for (int i = 0; i < tflite_micro::ElementCount(*input2->dims); ++i) {
     if (std::equal_to<T>()(denominator_data[i], 0)) {
       MicroPrintf("Division by 0");
       return kTfLiteError;
     }
   }
 
-  bool requires_broadcast = !tflite::micro::HaveSameShapes(input1, input2);
+  bool requires_broadcast = !tflite_micro::micro::HaveSameShapes(input1, input2);
 
   if (requires_broadcast) {
     reference_ops::BroadcastBinaryFunction4DSlow<T, T, T>(
-        tflite::micro::GetTensorShape(input1),
-        tflite::micro::GetTensorData<T>(input1),
-        tflite::micro::GetTensorShape(input2), denominator_data,
-        tflite::micro::GetTensorShape(output),
-        tflite::micro::GetTensorData<T>(output), reference_ops::FloorDiv<T>);
+        tflite_micro::micro::GetTensorShape(input1),
+        tflite_micro::micro::GetTensorData<T>(input1),
+        tflite_micro::micro::GetTensorShape(input2), denominator_data,
+        tflite_micro::micro::GetTensorShape(output),
+        tflite_micro::micro::GetTensorData<T>(output), reference_ops::FloorDiv<T>);
   } else {
     reference_ops::BinaryFunction<T, T, T>(
-        tflite::micro::GetTensorShape(input1),
-        tflite::micro::GetTensorData<T>(input1),
-        tflite::micro::GetTensorShape(input2), denominator_data,
-        tflite::micro::GetTensorShape(output),
-        tflite::micro::GetTensorData<T>(output), reference_ops::FloorDiv<T>);
+        tflite_micro::micro::GetTensorShape(input1),
+        tflite_micro::micro::GetTensorData<T>(input1),
+        tflite_micro::micro::GetTensorShape(input2), denominator_data,
+        tflite_micro::micro::GetTensorShape(output),
+        tflite_micro::micro::GetTensorData<T>(output), reference_ops::FloorDiv<T>);
   }
 
   return kTfLiteOk;
@@ -103,11 +103,11 @@ TfLiteStatus EvalFloorDiv(TfLiteContext* context,
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input1 =
-      tflite::micro::GetEvalInput(context, node, kInputTensor1);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor1);
   const TfLiteEvalTensor* input2 =
-      tflite::micro::GetEvalInput(context, node, kInputTensor2);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor2);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
   switch (input1->type) {
     case kTfLiteFloat32: {
@@ -124,7 +124,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_FLOOR_DIV() {
-  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/floor_div_test.cc b/tensorflow/lite/micro/kernels/floor_div_test.cc
index 531ee639..6ee28275 100644
--- a/tensorflow/lite/micro/kernels/floor_div_test.cc
+++ b/tensorflow/lite/micro/kernels/floor_div_test.cc
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -31,7 +31,7 @@ void ExecuteFloorDivTest(TfLiteTensor* tensors, int tensors_count) {
   int kOutputArrayData[] = {1, 2};
   TfLiteIntArray* outputs_array = IntArrayFromInts(kOutputArrayData);
 
-  const TFLMRegistration registration = tflite::Register_FLOOR_DIV();
+  const TFLMRegistration registration = tflite_micro::Register_FLOOR_DIV();
   micro::KernelRunner runner(registration, tensors, tensors_count, inputs_array,
                              outputs_array, nullptr);
 
@@ -64,7 +64,7 @@ void TestFloorDiv(int* input1_dims_data, const T* input1_data,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -76,7 +76,7 @@ TF_LITE_MICRO_TEST(FloorDivTestSimpleFloat) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
 
-  tflite::testing::TestFloorDiv(kDims, kInput1, kDims, kInput2, kDims, kExpect,
+  tflite_micro::testing::TestFloorDiv(kDims, kInput1, kDims, kInput2, kDims, kExpect,
                                 output_data);
 }
 
@@ -88,7 +88,7 @@ TF_LITE_MICRO_TEST(FloorDivTestNegativeValueFloat) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
 
-  tflite::testing::TestFloorDiv(kDims, kInput1, kDims, kInput2, kDims, kExpect,
+  tflite_micro::testing::TestFloorDiv(kDims, kInput1, kDims, kInput2, kDims, kExpect,
                                 output_data);
 }
 
@@ -101,7 +101,7 @@ TF_LITE_MICRO_TEST(FloorDivTestBroadcastFloat) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
 
-  tflite::testing::TestFloorDiv(kDims1, kInput1, kDims2, kInput2, kDims1,
+  tflite_micro::testing::TestFloorDiv(kDims1, kInput1, kDims2, kInput2, kDims1,
                                 kExpect, output_data);
 }
 
diff --git a/tensorflow/lite/micro/kernels/floor_mod.cc b/tensorflow/lite/micro/kernels/floor_mod.cc
index f4598920..ef79a030 100644
--- a/tensorflow/lite/micro/kernels/floor_mod.cc
+++ b/tensorflow/lite/micro/kernels/floor_mod.cc
@@ -26,7 +26,7 @@ limitations under the License.
 
 // OLD-TODO(b/117523611): We should factor out a binary_op and put binary ops
 // there.
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 // Input/output tensor index.
@@ -75,22 +75,22 @@ TfLiteStatus EvalFloorMod(TfLiteContext* context, bool requires_broadcast,
                           const TfLiteEvalTensor* input1,
                           const TfLiteEvalTensor* input2,
                           TfLiteEvalTensor* output) {
-  const T* denominator_data = tflite::micro::GetTensorData<T>(input2);
+  const T* denominator_data = tflite_micro::micro::GetTensorData<T>(input2);
 
   if (requires_broadcast) {
     reference_ops::BroadcastBinaryFunction4DSlow<T, T, T>(
-        tflite::micro::GetTensorShape(input1),
-        tflite::micro::GetTensorData<T>(input1),
-        tflite::micro::GetTensorShape(input2), denominator_data,
-        tflite::micro::GetTensorShape(output),
-        tflite::micro::GetTensorData<T>(output), reference_ops::FloorMod<T>);
+        tflite_micro::micro::GetTensorShape(input1),
+        tflite_micro::micro::GetTensorData<T>(input1),
+        tflite_micro::micro::GetTensorShape(input2), denominator_data,
+        tflite_micro::micro::GetTensorShape(output),
+        tflite_micro::micro::GetTensorData<T>(output), reference_ops::FloorMod<T>);
   } else {
     reference_ops::BinaryFunction<T, T, T>(
-        tflite::micro::GetTensorShape(input1),
-        tflite::micro::GetTensorData<T>(input1),
-        tflite::micro::GetTensorShape(input2), denominator_data,
-        tflite::micro::GetTensorShape(output),
-        tflite::micro::GetTensorData<T>(output), reference_ops::FloorMod<T>);
+        tflite_micro::micro::GetTensorShape(input1),
+        tflite_micro::micro::GetTensorData<T>(input1),
+        tflite_micro::micro::GetTensorShape(input2), denominator_data,
+        tflite_micro::micro::GetTensorShape(output),
+        tflite_micro::micro::GetTensorData<T>(output), reference_ops::FloorMod<T>);
   }
 
   return kTfLiteOk;
@@ -98,13 +98,13 @@ TfLiteStatus EvalFloorMod(TfLiteContext* context, bool requires_broadcast,
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input1 =
-      tflite::micro::GetEvalInput(context, node, kInputTensor1);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor1);
   const TfLiteEvalTensor* input2 =
-      tflite::micro::GetEvalInput(context, node, kInputTensor2);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor2);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
-  bool requires_broadcast = !tflite::micro::HaveSameShapes(input1, input2);
+  bool requires_broadcast = !tflite_micro::micro::HaveSameShapes(input1, input2);
 
   switch (input1->type) {
     case kTfLiteFloat32: {
@@ -122,7 +122,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_FLOOR_MOD() {
-  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/floor_mod_test.cc b/tensorflow/lite/micro/kernels/floor_mod_test.cc
index dd0d1796..8350a5da 100644
--- a/tensorflow/lite/micro/kernels/floor_mod_test.cc
+++ b/tensorflow/lite/micro/kernels/floor_mod_test.cc
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -31,7 +31,7 @@ void ExecuteFloorModTest(TfLiteTensor* tensors, int tensors_count) {
   int kOutputArrayData[] = {1, 2};
   TfLiteIntArray* outputs_array = IntArrayFromInts(kOutputArrayData);
 
-  const TFLMRegistration registration = tflite::Register_FLOOR_MOD();
+  const TFLMRegistration registration = tflite_micro::Register_FLOOR_MOD();
   micro::KernelRunner runner(registration, tensors, tensors_count, inputs_array,
                              outputs_array, nullptr);
 
@@ -64,7 +64,7 @@ void TestFloorMod(int* input1_dims_data, const T* input1_data,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -76,7 +76,7 @@ TF_LITE_MICRO_TEST(FloorModFloatSimple) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
 
-  tflite::testing::TestFloorMod(kDims, kInput1, kDims, kInput2, kDims, kExpect,
+  tflite_micro::testing::TestFloorMod(kDims, kInput1, kDims, kInput2, kDims, kExpect,
                                 output_data);
 }
 
@@ -88,7 +88,7 @@ TF_LITE_MICRO_TEST(FloorModFloatNegativeValue) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
 
-  tflite::testing::TestFloorMod(kDims, kInput1, kDims, kInput2, kDims, kExpect,
+  tflite_micro::testing::TestFloorMod(kDims, kInput1, kDims, kInput2, kDims, kExpect,
                                 output_data);
 }
 
@@ -101,7 +101,7 @@ TF_LITE_MICRO_TEST(FloorModFloatBroadcast) {
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
 
-  tflite::testing::TestFloorMod(kDims1, kInput1, kDims2, kInput2, kDims1,
+  tflite_micro::testing::TestFloorMod(kDims1, kInput1, kDims2, kInput2, kDims1,
                                 kExpect, output_data);
 }
 
diff --git a/tensorflow/lite/micro/kernels/floor_test.cc b/tensorflow/lite/micro/kernels/floor_test.cc
index 466b3352..7e269731 100644
--- a/tensorflow/lite/micro/kernels/floor_test.cc
+++ b/tensorflow/lite/micro/kernels/floor_test.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -56,7 +56,7 @@ void TestFloor(int* input_dims_data, const float* input_data,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -65,7 +65,7 @@ TF_LITE_MICRO_TEST(FloorOpSingleDimFloat32) {
   const float input[] = {8.5f, 0.0f};
   const float golden[] = {8, 0};
   float output_data[2];
-  tflite::testing::TestFloor(dims, input, golden, dims, output_data);
+  tflite_micro::testing::TestFloor(dims, input, golden, dims, output_data);
 }
 
 TF_LITE_MICRO_TEST(FloorOpMultiDimFloat32) {
@@ -75,7 +75,7 @@ TF_LITE_MICRO_TEST(FloorOpMultiDimFloat32) {
   const float golden[] = {0.0f,  8.0f,  0.0f,  9.0f,   0.0f,
                           -1.0f, -9.0f, -1.0f, -10.0f, -1.0f};
   float output_data[10];
-  tflite::testing::TestFloor(dims, input, golden, dims, output_data);
+  tflite_micro::testing::TestFloor(dims, input, golden, dims, output_data);
 }
 
 TF_LITE_MICRO_TESTS_END
diff --git a/tensorflow/lite/micro/kernels/fully_connected.cc b/tensorflow/lite/micro/kernels/fully_connected.cc
index 54576faf..a60b9d83 100644
--- a/tensorflow/lite/micro/kernels/fully_connected.cc
+++ b/tensorflow/lite/micro/kernels/fully_connected.cc
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 void* Init(TfLiteContext* context, const char* buffer, size_t length) {
@@ -93,13 +93,13 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
       static_cast<const TfLiteFullyConnectedParams*>(node->builtin_data);
 
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kFullyConnectedInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kFullyConnectedInputTensor);
   const TfLiteEvalTensor* filter =
-      tflite::micro::GetEvalInput(context, node, kFullyConnectedWeightsTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kFullyConnectedWeightsTensor);
   const TfLiteEvalTensor* bias =
-      tflite::micro::GetEvalInput(context, node, kFullyConnectedBiasTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kFullyConnectedBiasTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kFullyConnectedOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kFullyConnectedOutputTensor);
 
   TFLITE_DCHECK(node->user_data != nullptr);
 
@@ -109,16 +109,16 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   // Checks in Prepare ensure input, output and filter types are all the same.
   switch (input->type) {
     case kTfLiteFloat32: {
-      tflite::reference_ops::FullyConnected(
+      tflite_micro::reference_ops::FullyConnected(
           FullyConnectedParamsFloat(params->activation),
-          tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<float>(input),
-          tflite::micro::GetTensorShape(filter),
-          tflite::micro::GetTensorData<float>(filter),
-          tflite::micro::GetTensorShape(bias),
-          tflite::micro::GetOptionalTensorData<float>(bias),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<float>(output));
+          tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<float>(input),
+          tflite_micro::micro::GetTensorShape(filter),
+          tflite_micro::micro::GetTensorData<float>(filter),
+          tflite_micro::micro::GetTensorShape(bias),
+          tflite_micro::micro::GetOptionalTensorData<float>(bias),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<float>(output));
       break;
     }
 
@@ -127,32 +127,32 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
         case kTfLiteInt4: {
           int8_t* unpacked_filter_data = static_cast<int8_t*>(
               context->GetScratchBuffer(context, data.filter_buffer_index));
-          tflite::tensor_utils::UnpackDenseInt4IntoInt8(
-              tflite::micro::GetTensorData<int8_t>(filter),
-              tflite::micro::GetTensorShape(filter).FlatSize(),
+          tflite_micro::tensor_utils::UnpackDenseInt4IntoInt8(
+              tflite_micro::micro::GetTensorData<int8_t>(filter),
+              tflite_micro::micro::GetTensorShape(filter).FlatSize(),
               unpacked_filter_data);
-          tflite::reference_integer_ops::FullyConnected(
+          tflite_micro::reference_integer_ops::FullyConnected(
               FullyConnectedParamsQuantized(data),
-              tflite::micro::GetTensorShape(input),
-              tflite::micro::GetTensorData<int8_t>(input),
-              tflite::micro::GetTensorShape(filter), unpacked_filter_data,
-              tflite::micro::GetTensorShape(bias),
-              tflite::micro::GetOptionalTensorData<int32_t>(bias),
-              tflite::micro::GetTensorShape(output),
-              tflite::micro::GetTensorData<int8_t>(output));
+              tflite_micro::micro::GetTensorShape(input),
+              tflite_micro::micro::GetTensorData<int8_t>(input),
+              tflite_micro::micro::GetTensorShape(filter), unpacked_filter_data,
+              tflite_micro::micro::GetTensorShape(bias),
+              tflite_micro::micro::GetOptionalTensorData<int32_t>(bias),
+              tflite_micro::micro::GetTensorShape(output),
+              tflite_micro::micro::GetTensorData<int8_t>(output));
           break;
         }
         case kTfLiteInt8: {
-          tflite::reference_integer_ops::FullyConnected(
+          tflite_micro::reference_integer_ops::FullyConnected(
               FullyConnectedParamsQuantized(data),
-              tflite::micro::GetTensorShape(input),
-              tflite::micro::GetTensorData<int8_t>(input),
-              tflite::micro::GetTensorShape(filter),
-              tflite::micro::GetTensorData<int8_t>(filter),
-              tflite::micro::GetTensorShape(bias),
-              tflite::micro::GetOptionalTensorData<int32_t>(bias),
-              tflite::micro::GetTensorShape(output),
-              tflite::micro::GetTensorData<int8_t>(output));
+              tflite_micro::micro::GetTensorShape(input),
+              tflite_micro::micro::GetTensorData<int8_t>(input),
+              tflite_micro::micro::GetTensorShape(filter),
+              tflite_micro::micro::GetTensorData<int8_t>(filter),
+              tflite_micro::micro::GetTensorShape(bias),
+              tflite_micro::micro::GetOptionalTensorData<int32_t>(bias),
+              tflite_micro::micro::GetTensorShape(output),
+              tflite_micro::micro::GetTensorData<int8_t>(output));
           break;
         }
         default: {
@@ -167,16 +167,16 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
     case kTfLiteInt16: {
       switch (filter->type) {
         case kTfLiteInt8: {
-          tflite::reference_integer_ops::FullyConnected(
+          tflite_micro::reference_integer_ops::FullyConnected(
               FullyConnectedParamsQuantized(data),
-              tflite::micro::GetTensorShape(input),
-              tflite::micro::GetTensorData<int16_t>(input),
-              tflite::micro::GetTensorShape(filter),
-              tflite::micro::GetTensorData<int8_t>(filter),
-              tflite::micro::GetTensorShape(bias),
-              tflite::micro::GetOptionalTensorData<int64_t>(bias),
-              tflite::micro::GetTensorShape(output),
-              tflite::micro::GetTensorData<int16_t>(output));
+              tflite_micro::micro::GetTensorShape(input),
+              tflite_micro::micro::GetTensorData<int16_t>(input),
+              tflite_micro::micro::GetTensorShape(filter),
+              tflite_micro::micro::GetTensorData<int8_t>(filter),
+              tflite_micro::micro::GetTensorShape(bias),
+              tflite_micro::micro::GetOptionalTensorData<int32_t>(bias),
+              tflite_micro::micro::GetTensorShape(output),
+              tflite_micro::micro::GetTensorData<int16_t>(output));
           break;
         }
         default: {
@@ -200,11 +200,11 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_FULLY_CONNECTED() {
-  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, Eval);
 }
 
 TFLMInferenceRegistration RegisterInference_FULLY_CONNECTED() {
-  return tflite::micro::RegisterOp(Eval);
+  return tflite_micro::micro::RegisterOp(Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/fully_connected.h b/tensorflow/lite/micro/kernels/fully_connected.h
index 3fa6060c..9a7adb8a 100644
--- a/tensorflow/lite/micro/kernels/fully_connected.h
+++ b/tensorflow/lite/micro/kernels/fully_connected.h
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/types.h"
 #include "tensorflow/lite/micro/micro_common.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 struct OpDataFullyConnected {
   // The scaling factor from input to output (aka the 'real multiplier') can
@@ -107,6 +107,6 @@ inline TFLMRegistration Register_FULLY_CONNECTED_INT16() {
 
 #endif
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_FULLY_CONNECTED_H_
diff --git a/tensorflow/lite/micro/kernels/fully_connected_common.cc b/tensorflow/lite/micro/kernels/fully_connected_common.cc
index 5a8d312d..a98d19cf 100644
--- a/tensorflow/lite/micro/kernels/fully_connected_common.cc
+++ b/tensorflow/lite/micro/kernels/fully_connected_common.cc
@@ -24,7 +24,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/fully_connected.h"
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 const int kFullyConnectedInputTensor = 0;
 const int kFullyConnectedWeightsTensor = 1;
@@ -81,4 +81,4 @@ TfLiteStatus CalculateOpDataFullyConnected(
   return kTfLiteOk;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/fully_connected_test.cc b/tensorflow/lite/micro/kernels/fully_connected_test.cc
index 2ad13205..fb523151 100644
--- a/tensorflow/lite/micro/kernels/fully_connected_test.cc
+++ b/tensorflow/lite/micro/kernels/fully_connected_test.cc
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -375,34 +375,34 @@ TfLiteStatus TestFullyConnectedQuantized(
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(SimpleTest) {
-  float output_data[tflite::testing::simple_output_size];
+  float output_data[tflite_micro::testing::simple_output_size];
   TF_LITE_MICRO_EXPECT_EQ(
-      tflite::testing::TestFullyConnectedFloat(
-          tflite::testing::simple_input_dims,
-          tflite::testing::simple_input_data,
-          tflite::testing::simple_weights_dims,
-          tflite::testing::simple_weights_data,
-          tflite::testing::simple_bias_dims, tflite::testing::simple_bias_data,
-          tflite::testing::simple_golden, tflite::testing::simple_output_dims,
+      tflite_micro::testing::TestFullyConnectedFloat(
+          tflite_micro::testing::simple_input_dims,
+          tflite_micro::testing::simple_input_data,
+          tflite_micro::testing::simple_weights_dims,
+          tflite_micro::testing::simple_weights_data,
+          tflite_micro::testing::simple_bias_dims, tflite_micro::testing::simple_bias_data,
+          tflite_micro::testing::simple_golden, tflite_micro::testing::simple_output_dims,
           kTfLiteActNone, output_data),
       kTfLiteOk);
 }
 
 TF_LITE_MICRO_TEST(SimpleTestNullBias) {
-  float output_data[tflite::testing::simple_output_size];
+  float output_data[tflite_micro::testing::simple_output_size];
   TF_LITE_MICRO_EXPECT_EQ(
-      tflite::testing::TestFullyConnectedFloat(
-          tflite::testing::simple_input_dims,
-          tflite::testing::simple_input_data,
-          tflite::testing::simple_weights_dims,
-          tflite::testing::simple_weights_data, nullptr, nullptr,
-          tflite::testing::simple_golden_null_bias,
-          tflite::testing::simple_output_dims, kTfLiteActNone, output_data),
+      tflite_micro::testing::TestFullyConnectedFloat(
+          tflite_micro::testing::simple_input_dims,
+          tflite_micro::testing::simple_input_data,
+          tflite_micro::testing::simple_weights_dims,
+          tflite_micro::testing::simple_weights_data, nullptr, nullptr,
+          tflite_micro::testing::simple_golden_null_bias,
+          tflite_micro::testing::simple_output_dims, kTfLiteActNone, output_data),
       kTfLiteOk);
 }
 
@@ -414,22 +414,22 @@ TF_LITE_MICRO_TEST(SimpleTestQuantizedInt8) {
   const float output_scale = 0.5f;
   const int output_zero_point = -1;
 
-  int8_t input_quantized[tflite::testing::simple_input_size];
-  int8_t weights_quantized[tflite::testing::simple_weights_size];
-  int32_t bias_quantized[tflite::testing::simple_output_size];
-  int8_t golden_quantized[tflite::testing::simple_output_size];
-  int8_t output_data[tflite::testing::simple_output_size];
+  int8_t input_quantized[tflite_micro::testing::simple_input_size];
+  int8_t weights_quantized[tflite_micro::testing::simple_weights_size];
+  int32_t bias_quantized[tflite_micro::testing::simple_output_size];
+  int8_t golden_quantized[tflite_micro::testing::simple_output_size];
+  int8_t output_data[tflite_micro::testing::simple_output_size];
 
   TF_LITE_MICRO_EXPECT_EQ(
-      tflite::testing::TestFullyConnectedQuantized(
-          tflite::testing::simple_input_dims,
-          tflite::testing::simple_input_data, input_quantized, input_scale,
-          input_zero_point, tflite::testing::simple_weights_dims,
-          tflite::testing::simple_weights_data, weights_quantized,
-          weights_scale, weights_zero_point, tflite::testing::simple_bias_dims,
-          tflite::testing::simple_bias_data, bias_quantized,
-          tflite::testing::simple_golden, golden_quantized,
-          tflite::testing::simple_output_dims, output_scale, output_zero_point,
+      tflite_micro::testing::TestFullyConnectedQuantized(
+          tflite_micro::testing::simple_input_dims,
+          tflite_micro::testing::simple_input_data, input_quantized, input_scale,
+          input_zero_point, tflite_micro::testing::simple_weights_dims,
+          tflite_micro::testing::simple_weights_data, weights_quantized,
+          weights_scale, weights_zero_point, tflite_micro::testing::simple_bias_dims,
+          tflite_micro::testing::simple_bias_data, bias_quantized,
+          tflite_micro::testing::simple_golden, golden_quantized,
+          tflite_micro::testing::simple_output_dims, output_scale, output_zero_point,
           kTfLiteActNone, output_data),
       kTfLiteOk);
 }
@@ -444,21 +444,21 @@ TF_LITE_MICRO_TEST(SimpleTestQuantizedInt16) {
   const int output_zero_point = 0;
 
   const float simple_golden[] = {24, 25, 26, 58, 59, 60};
-  int16_t input_quantized[tflite::testing::simple_input_size];
-  int8_t weights_quantized[tflite::testing::simple_weights_size];
-  int64_t bias_quantized[tflite::testing::simple_output_size];
-  int16_t golden_quantized[tflite::testing::simple_output_size];
-  int16_t output_data[tflite::testing::simple_output_size];
+  int16_t input_quantized[tflite_micro::testing::simple_input_size];
+  int8_t weights_quantized[tflite_micro::testing::simple_weights_size];
+  int64_t bias_quantized[tflite_micro::testing::simple_output_size];
+  int16_t golden_quantized[tflite_micro::testing::simple_output_size];
+  int16_t output_data[tflite_micro::testing::simple_output_size];
 
   TF_LITE_MICRO_EXPECT_EQ(
-      tflite::testing::TestFullyConnectedQuantized(
-          tflite::testing::simple_input_dims,
-          tflite::testing::simple_input_data, input_quantized, input_scale,
-          input_zero_point, tflite::testing::simple_weights_dims,
-          tflite::testing::simple_weights_data, weights_quantized,
-          weights_scale, weights_zero_point, tflite::testing::simple_bias_dims,
-          tflite::testing::simple_bias_data, bias_quantized, simple_golden,
-          golden_quantized, tflite::testing::simple_output_dims, output_scale,
+      tflite_micro::testing::TestFullyConnectedQuantized(
+          tflite_micro::testing::simple_input_dims,
+          tflite_micro::testing::simple_input_data, input_quantized, input_scale,
+          input_zero_point, tflite_micro::testing::simple_weights_dims,
+          tflite_micro::testing::simple_weights_data, weights_quantized,
+          weights_scale, weights_zero_point, tflite_micro::testing::simple_bias_dims,
+          tflite_micro::testing::simple_bias_data, bias_quantized, simple_golden,
+          golden_quantized, tflite_micro::testing::simple_output_dims, output_scale,
           output_zero_point, kTfLiteActNone, output_data),
       kTfLiteOk);
 }
@@ -475,21 +475,21 @@ TF_LITE_MICRO_TEST(SimpleTest4DInputQuantizedInt8) {
 
   int input_dims_4d[] = {4, 1, 1, 2, 10};
 
-  int8_t input_quantized[tflite::testing::simple_input_size];
-  int8_t weights_quantized[tflite::testing::simple_weights_size];
-  int32_t bias_quantized[tflite::testing::simple_output_size];
-  int8_t golden_quantized[tflite::testing::simple_output_size];
-  int8_t output_data[tflite::testing::simple_output_size];
+  int8_t input_quantized[tflite_micro::testing::simple_input_size];
+  int8_t weights_quantized[tflite_micro::testing::simple_weights_size];
+  int32_t bias_quantized[tflite_micro::testing::simple_output_size];
+  int8_t golden_quantized[tflite_micro::testing::simple_output_size];
+  int8_t output_data[tflite_micro::testing::simple_output_size];
 
   TF_LITE_MICRO_EXPECT_EQ(
-      tflite::testing::TestFullyConnectedQuantized(
-          input_dims_4d, tflite::testing::simple_input_data, input_quantized,
-          input_scale, input_zero_point, tflite::testing::simple_weights_dims,
-          tflite::testing::simple_weights_data, weights_quantized,
-          weights_scale, weights_zero_point, tflite::testing::simple_bias_dims,
-          tflite::testing::simple_bias_data, bias_quantized,
-          tflite::testing::simple_golden, golden_quantized,
-          tflite::testing::simple_output_dims, output_scale, output_zero_point,
+      tflite_micro::testing::TestFullyConnectedQuantized(
+          input_dims_4d, tflite_micro::testing::simple_input_data, input_quantized,
+          input_scale, input_zero_point, tflite_micro::testing::simple_weights_dims,
+          tflite_micro::testing::simple_weights_data, weights_quantized,
+          weights_scale, weights_zero_point, tflite_micro::testing::simple_bias_dims,
+          tflite_micro::testing::simple_bias_data, bias_quantized,
+          tflite_micro::testing::simple_golden, golden_quantized,
+          tflite_micro::testing::simple_output_dims, output_scale, output_zero_point,
           kTfLiteActNone, output_data),
       kTfLiteOk);
 }
@@ -503,22 +503,22 @@ TF_LITE_MICRO_TEST(SimpleTestQuantizedInt8Relu) {
   const float output_scale = 0.5f;
   const int output_zero_point = -128;
 
-  int8_t input_quantized[tflite::testing::relu_input_size];
-  int8_t weights_quantized[tflite::testing::relu_weights_size];
-  int32_t bias_quantized[tflite::testing::relu_output_size];
-  int8_t golden_quantized[tflite::testing::relu_output_size];
-  int8_t output_data[tflite::testing::relu_output_size];
+  int8_t input_quantized[tflite_micro::testing::relu_input_size];
+  int8_t weights_quantized[tflite_micro::testing::relu_weights_size];
+  int32_t bias_quantized[tflite_micro::testing::relu_output_size];
+  int8_t golden_quantized[tflite_micro::testing::relu_output_size];
+  int8_t output_data[tflite_micro::testing::relu_output_size];
 
   TF_LITE_MICRO_EXPECT_EQ(
-      tflite::testing::TestFullyConnectedQuantized(
-          tflite::testing::relu_input_dims, tflite::testing::relu_input_data,
+      tflite_micro::testing::TestFullyConnectedQuantized(
+          tflite_micro::testing::relu_input_dims, tflite_micro::testing::relu_input_data,
           input_quantized, input_scale, input_zero_point,
-          tflite::testing::relu_weights_dims,
-          tflite::testing::relu_weights_data, weights_quantized, weights_scale,
-          weights_zero_point, tflite::testing::relu_bias_dims,
-          tflite::testing::relu_bias_data, bias_quantized,
-          tflite::testing::relu_golden, golden_quantized,
-          tflite::testing::relu_output_dims, output_scale, output_zero_point,
+          tflite_micro::testing::relu_weights_dims,
+          tflite_micro::testing::relu_weights_data, weights_quantized, weights_scale,
+          weights_zero_point, tflite_micro::testing::relu_bias_dims,
+          tflite_micro::testing::relu_bias_data, bias_quantized,
+          tflite_micro::testing::relu_golden, golden_quantized,
+          tflite_micro::testing::relu_output_dims, output_scale, output_zero_point,
           kTfLiteActRelu, output_data),
       kTfLiteOk);
 }
@@ -526,32 +526,32 @@ TF_LITE_MICRO_TEST(SimpleTestQuantizedInt8Relu) {
 TF_LITE_MICRO_TEST(SimpleTest4DInput) {
   int input_dims_4d[] = {4, 1, 1, 2, 10};
 
-  float output_data[tflite::testing::simple_output_size];
+  float output_data[tflite_micro::testing::simple_output_size];
 
   TF_LITE_MICRO_EXPECT_EQ(
-      tflite::testing::TestFullyConnectedFloat(
-          input_dims_4d, tflite::testing::simple_input_data,
-          tflite::testing::simple_weights_dims,
-          tflite::testing::simple_weights_data,
-          tflite::testing::simple_bias_dims, tflite::testing::simple_bias_data,
-          tflite::testing::simple_golden, tflite::testing::simple_output_dims,
+      tflite_micro::testing::TestFullyConnectedFloat(
+          input_dims_4d, tflite_micro::testing::simple_input_data,
+          tflite_micro::testing::simple_weights_dims,
+          tflite_micro::testing::simple_weights_data,
+          tflite_micro::testing::simple_bias_dims, tflite_micro::testing::simple_bias_data,
+          tflite_micro::testing::simple_golden, tflite_micro::testing::simple_output_dims,
           kTfLiteActNone, output_data),
       kTfLiteOk);
 }
 
 TF_LITE_MICRO_TEST(Representative1x64Input1x16Output) {
-  float output_data[tflite::testing::representative_64x16_output_size];
+  float output_data[tflite_micro::testing::representative_64x16_output_size];
 
   TF_LITE_MICRO_EXPECT_EQ(
-      tflite::testing::TestFullyConnectedFloat(
-          tflite::testing::representative_64x16_input_dims,
-          tflite::testing::representative_64x16_input_data,
-          tflite::testing::representative_64x16_weights_dims,
-          tflite::testing::representative_64x16_weights_data,
-          tflite::testing::representative_64x16_bias_dims,
-          tflite::testing::representative_64x16_bias_data,
-          tflite::testing::representative_64x16_golden,
-          tflite::testing::representative_64x16_output_dims, kTfLiteActNone,
+      tflite_micro::testing::TestFullyConnectedFloat(
+          tflite_micro::testing::representative_64x16_input_dims,
+          tflite_micro::testing::representative_64x16_input_data,
+          tflite_micro::testing::representative_64x16_weights_dims,
+          tflite_micro::testing::representative_64x16_weights_data,
+          tflite_micro::testing::representative_64x16_bias_dims,
+          tflite_micro::testing::representative_64x16_bias_data,
+          tflite_micro::testing::representative_64x16_golden,
+          tflite_micro::testing::representative_64x16_output_dims, kTfLiteActNone,
           output_data),
       kTfLiteOk);
 }
@@ -565,24 +565,24 @@ TF_LITE_MICRO_TEST(Representative1x64Input1x16OutputQuantizedInt8) {
   const float output_scale = 0.069785;
   const int output_zero_point = -9;
 
-  int8_t input_quantized[tflite::testing::representative_64x16_input_size];
-  int8_t weights_quantized[tflite::testing::representative_64x16_weights_size];
-  int32_t bias_quantized[tflite::testing::representative_64x16_output_size];
-  int8_t golden_quantized[tflite::testing::representative_64x16_output_size];
-  int8_t output_data[tflite::testing::representative_64x16_output_size];
+  int8_t input_quantized[tflite_micro::testing::representative_64x16_input_size];
+  int8_t weights_quantized[tflite_micro::testing::representative_64x16_weights_size];
+  int32_t bias_quantized[tflite_micro::testing::representative_64x16_output_size];
+  int8_t golden_quantized[tflite_micro::testing::representative_64x16_output_size];
+  int8_t output_data[tflite_micro::testing::representative_64x16_output_size];
 
   TF_LITE_MICRO_EXPECT_EQ(
-      tflite::testing::TestFullyConnectedQuantized(
-          tflite::testing::representative_64x16_input_dims,
-          tflite::testing::representative_64x16_input_data, input_quantized,
+      tflite_micro::testing::TestFullyConnectedQuantized(
+          tflite_micro::testing::representative_64x16_input_dims,
+          tflite_micro::testing::representative_64x16_input_data, input_quantized,
           input_scale, input_zero_point,
-          tflite::testing::representative_64x16_weights_dims,
-          tflite::testing::representative_64x16_weights_data, weights_quantized,
+          tflite_micro::testing::representative_64x16_weights_dims,
+          tflite_micro::testing::representative_64x16_weights_data, weights_quantized,
           weights_scale, weights_zero_point,
-          tflite::testing::representative_64x16_bias_dims,
-          tflite::testing::representative_64x16_bias_data, bias_quantized,
-          tflite::testing::representative_64x16_golden, golden_quantized,
-          tflite::testing::representative_64x16_output_dims, output_scale,
+          tflite_micro::testing::representative_64x16_bias_dims,
+          tflite_micro::testing::representative_64x16_bias_data, bias_quantized,
+          tflite_micro::testing::representative_64x16_golden, golden_quantized,
+          tflite_micro::testing::representative_64x16_output_dims, output_scale,
           output_zero_point, kTfLiteActNone, output_data),
       kTfLiteOk);
 }
@@ -595,21 +595,21 @@ TF_LITE_MICRO_TEST(SimpleTestQuantizedInt8NullBias) {
   const float output_scale = 0.5f;
   const int output_zero_point = -1;
 
-  int8_t input_quantized[tflite::testing::simple_input_size];
-  int8_t weights_quantized[tflite::testing::simple_weights_size];
-  int8_t golden_quantized[tflite::testing::simple_output_size];
-  int8_t output_data[tflite::testing::simple_output_size];
+  int8_t input_quantized[tflite_micro::testing::simple_input_size];
+  int8_t weights_quantized[tflite_micro::testing::simple_weights_size];
+  int8_t golden_quantized[tflite_micro::testing::simple_output_size];
+  int8_t output_data[tflite_micro::testing::simple_output_size];
 
   TF_LITE_MICRO_EXPECT_EQ(
-      tflite::testing::TestFullyConnectedQuantized(
-          tflite::testing::simple_input_dims,
-          tflite::testing::simple_input_data, input_quantized, input_scale,
-          input_zero_point, tflite::testing::simple_weights_dims,
-          tflite::testing::simple_weights_data, weights_quantized,
+      tflite_micro::testing::TestFullyConnectedQuantized(
+          tflite_micro::testing::simple_input_dims,
+          tflite_micro::testing::simple_input_data, input_quantized, input_scale,
+          input_zero_point, tflite_micro::testing::simple_weights_dims,
+          tflite_micro::testing::simple_weights_data, weights_quantized,
           weights_scale, weights_zero_point, nullptr, nullptr,
           static_cast<int32_t*>(nullptr),
-          tflite::testing::simple_golden_null_bias, golden_quantized,
-          tflite::testing::simple_output_dims, output_scale, output_zero_point,
+          tflite_micro::testing::simple_golden_null_bias, golden_quantized,
+          tflite_micro::testing::simple_output_dims, output_scale, output_zero_point,
           kTfLiteActNone, output_data),
       kTfLiteOk);
 }
@@ -628,21 +628,21 @@ TF_LITE_MICRO_TEST(SimpleTestQuantizedInt4Weights) {
   const float output_scale = 0.5f;
   const int output_zero_point = -1;
 
-  int8_t input_quantized[tflite::testing::simple_input_size];
-  int8_t weights_quantized[tflite::testing::simple_weights_size];
-  int8_t golden_quantized[tflite::testing::simple_output_size];
-  int8_t output_data[tflite::testing::simple_output_size];
+  int8_t input_quantized[tflite_micro::testing::simple_input_size];
+  int8_t weights_quantized[tflite_micro::testing::simple_weights_size];
+  int8_t golden_quantized[tflite_micro::testing::simple_output_size];
+  int8_t output_data[tflite_micro::testing::simple_output_size];
 
   TF_LITE_MICRO_EXPECT_EQ(
-      tflite::testing::TestFullyConnectedQuantized(
-          tflite::testing::simple_input_dims,
-          tflite::testing::simple_input_data, input_quantized, input_scale,
-          input_zero_point, tflite::testing::simple_weights_dims,
-          tflite::testing::simple_int4_weights_data, weights_quantized,
+      tflite_micro::testing::TestFullyConnectedQuantized(
+          tflite_micro::testing::simple_input_dims,
+          tflite_micro::testing::simple_input_data, input_quantized, input_scale,
+          input_zero_point, tflite_micro::testing::simple_weights_dims,
+          tflite_micro::testing::simple_int4_weights_data, weights_quantized,
           weights_scale, weights_zero_point, nullptr, nullptr,
           static_cast<int32_t*>(nullptr),
-          tflite::testing::simple_golden_null_bias_int4_weights,
-          golden_quantized, tflite::testing::simple_output_dims, output_scale,
+          tflite_micro::testing::simple_golden_null_bias_int4_weights,
+          golden_quantized, tflite_micro::testing::simple_output_dims, output_scale,
           output_zero_point, kTfLiteActNone, output_data, kTfLiteInt4),
       kTfLiteOk);
 }
diff --git a/tensorflow/lite/micro/kernels/gather.cc b/tensorflow/lite/micro/kernels/gather.cc
index 99556011..69c8854c 100644
--- a/tensorflow/lite/micro/kernels/gather.cc
+++ b/tensorflow/lite/micro/kernels/gather.cc
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_log.h"
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 constexpr int kInputTensor = 0;
@@ -32,9 +32,9 @@ template <typename InputT, typename CoordsT = int32_t>
 TfLiteStatus Gather(const TfLiteGatherParams* params,
                     const TfLiteEvalTensor* input,
                     const TfLiteEvalTensor* coords, TfLiteEvalTensor* output) {
-  const InputT* input_data = tflite::micro::GetTensorData<InputT>(input);
-  const CoordsT* coords_data = tflite::micro::GetTensorData<CoordsT>(coords);
-  InputT* output_data = tflite::micro::GetTensorData<InputT>(output);
+  const InputT* input_data = tflite_micro::micro::GetTensorData<InputT>(input);
+  const CoordsT* coords_data = tflite_micro::micro::GetTensorData<CoordsT>(coords);
+  InputT* output_data = tflite_micro::micro::GetTensorData<InputT>(output);
   const TfLiteIntArray* input_dims = input->dims;
   const int input_dims_size = input_dims->size;
   int axis = params->axis;
@@ -163,8 +163,8 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   // MicroInterpreter is a temporary allocation. We must therefore relocate the
   // dims from the FlatBuffer to the persistent storage arena.
   TfLiteEvalTensor* output_eval =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
-  TF_LITE_ENSURE_OK(context, tflite::micro::CreateWritableTensorDimsWithCopy(
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
+  TF_LITE_ENSURE_OK(context, tflite_micro::micro::CreateWritableTensorDimsWithCopy(
                                  context, output, output_eval));
 
   TfLiteIntArray* output_shape = output->dims;
@@ -192,11 +192,11 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const auto* params =
       reinterpret_cast<const TfLiteGatherParams*>(node->builtin_data);
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   const TfLiteEvalTensor* coords =
-      tflite::micro::GetEvalInput(context, node, kInputPositions);
+      tflite_micro::micro::GetEvalInput(context, node, kInputPositions);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
   if (coords->type == kTfLiteInt32) {
     switch (input->type) {
@@ -218,7 +218,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_GATHER() {
-  return tflite::micro::RegisterOp(nullptr, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/gather_nd.cc b/tensorflow/lite/micro/kernels/gather_nd.cc
index 3774dddb..ef361bc8 100644
--- a/tensorflow/lite/micro/kernels/gather_nd.cc
+++ b/tensorflow/lite/micro/kernels/gather_nd.cc
@@ -20,7 +20,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_log.h"
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 constexpr int kParams = 0;
@@ -89,8 +89,8 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   // The tensor output dims must be relocated
   // from the FlatBuffer to the persistent storage arena.
   TfLiteEvalTensor* output_eval =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
-  TF_LITE_ENSURE_OK(context, tflite::micro::CreateWritableTensorDimsWithCopy(
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
+  TF_LITE_ENSURE_OK(context, tflite_micro::micro::CreateWritableTensorDimsWithCopy(
                                  context, output, output_eval));
 
   // TFLM gather_nd does not create the output tensor, but it needs to ensure
@@ -119,9 +119,9 @@ TfLiteStatus GatherNd(const TfLiteEvalTensor* params,
   const int indices_dims = indices->dims->size;
   const int indices_nd = indices->dims->data[indices_dims - 1];
   const int params_dims = params->dims->size;
-  const IndicesT* index_data = tflite::micro::GetTensorData<IndicesT>(indices);
-  const ParamsT* param_data = tflite::micro::GetTensorData<ParamsT>(params);
-  ParamsT* output_data = tflite::micro::GetTensorData<ParamsT>(output);
+  const IndicesT* index_data = tflite_micro::micro::GetTensorData<IndicesT>(indices);
+  const ParamsT* param_data = tflite_micro::micro::GetTensorData<ParamsT>(params);
+  ParamsT* output_data = tflite_micro::micro::GetTensorData<ParamsT>(output);
 
   int n_slices = 1;
   for (int i = 0; i < indices_dims - 1; ++i) {
@@ -187,11 +187,11 @@ TfLiteStatus EvalGatherNd(TfLiteContext* context,
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* params =
-      tflite::micro::GetEvalInput(context, node, kParams);
+      tflite_micro::micro::GetEvalInput(context, node, kParams);
   const TfLiteEvalTensor* indices =
-      tflite::micro::GetEvalInput(context, node, kIndices);
+      tflite_micro::micro::GetEvalInput(context, node, kIndices);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
   switch (indices->type) {
     case kTfLiteInt32:
@@ -206,7 +206,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_GATHER_ND() {
-  return tflite::micro::RegisterOp(nullptr, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/gather_nd_test.cc b/tensorflow/lite/micro/kernels/gather_nd_test.cc
index 39dd3377..f4c0fc4d 100644
--- a/tensorflow/lite/micro/kernels/gather_nd_test.cc
+++ b/tensorflow/lite/micro/kernels/gather_nd_test.cc
@@ -18,7 +18,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -63,7 +63,7 @@ void TestGatherNd(int* param_dims, const ParamType* param_data, int* index_dims,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -77,7 +77,7 @@ TF_LITE_MICRO_TEST(GatherNd_ElementIndexingIntoMatrix) {
   const float golden_data[] = {1.1, 2.2};
   float output_data[2];
   int output_dims[] = {1, 0};
-  tflite::testing::TestGatherNd<float, int32_t>(
+  tflite_micro::testing::TestGatherNd<float, int32_t>(
       input_dims, input_data, index_dims, index_data, output_dims, output_data,
       golden_data);
 }
@@ -92,7 +92,7 @@ TF_LITE_MICRO_TEST(GatherNd_SliceIndexingIntoMatrix) {
   const float golden_data[] = {2.1, 2.2, 1.1, 1.2};
   float output_data[4];
   int output_dims[] = {2, 0, 0};
-  tflite::testing::TestGatherNd<float, int32_t>(
+  tflite_micro::testing::TestGatherNd<float, int32_t>(
       input_dims, input_data, index_dims, index_data, output_dims, output_data,
       golden_data);
 }
@@ -107,7 +107,7 @@ TF_LITE_MICRO_TEST(GatherNd_BatchedIndexingIntoMatrix1) {
   const float golden_data[] = {2.1, 2.2, 1.1, 1.2};
   float output_data[4];
   int output_dims[] = {3, 0, 0, 0};
-  tflite::testing::TestGatherNd<float, int32_t>(
+  tflite_micro::testing::TestGatherNd<float, int32_t>(
       input_dims, input_data, index_dims, index_data, output_dims, output_data,
       golden_data);
 }
@@ -122,7 +122,7 @@ TF_LITE_MICRO_TEST(GatherNd_BatchedIndexingIntoMatrix2) {
   const float golden_data[] = {1.1, 2.2};
   float output_data[2];
   int output_dims[] = {3, 0, 0, 0};
-  tflite::testing::TestGatherNd<float, int32_t>(
+  tflite_micro::testing::TestGatherNd<float, int32_t>(
       input_dims, input_data, index_dims, index_data, output_dims, output_data,
       golden_data);
 }
@@ -137,7 +137,7 @@ TF_LITE_MICRO_TEST(GatherNd_DuplicateIndexingIntoMatrix) {
   const float golden_data[] = {1.1, 1.1};
   float output_data[2];
   int output_dims[] = {1, 0};
-  tflite::testing::TestGatherNd<float, int32_t>(
+  tflite_micro::testing::TestGatherNd<float, int32_t>(
       input_dims, input_data, index_dims, index_data, output_dims, output_data,
       golden_data);
 }
@@ -154,7 +154,7 @@ TF_LITE_MICRO_TEST(GatherNd_ElementIndexingIntoRank3Tensor) {
   const float golden_data[] = {-1.2, -4.1};
   float output_data[2];
   int output_dims[] = {2, 0, 0};
-  tflite::testing::TestGatherNd<float, int32_t>(
+  tflite_micro::testing::TestGatherNd<float, int32_t>(
       input_dims, input_data, index_dims, index_data, output_dims, output_data,
       golden_data);
 }
@@ -172,7 +172,7 @@ TF_LITE_MICRO_TEST(GatherNd_SliceIndexingIntoRank3Tensor) {
                                5.1, -5.2, 5.3, 6.1,  -6.2, 6.3};
   float output_data[12];
   int output_dims[] = {3, 0, 0, 0};
-  tflite::testing::TestGatherNd<float, int32_t>(
+  tflite_micro::testing::TestGatherNd<float, int32_t>(
       input_dims, input_data, index_dims, index_data, output_dims, output_data,
       golden_data);
 }
@@ -189,7 +189,7 @@ TF_LITE_MICRO_TEST(GatherNd_BatchedIndexingIntoRank3Tensor1) {
   const float golden_data[] = {-1.2, -4.1};
   float output_data[2];
   int output_dims[] = {2, 0, 0};
-  tflite::testing::TestGatherNd<float, int32_t>(
+  tflite_micro::testing::TestGatherNd<float, int32_t>(
       input_dims, input_data, index_dims, index_data, output_dims, output_data,
       golden_data);
 }
@@ -208,7 +208,7 @@ TF_LITE_MICRO_TEST(GatherNd_BatchedIndexingIntoRank3Tensor2) {
                                1.1, -1.2, 1.3,  -2.1, 2.2,  2.3};
   float output_data[18];
   int output_dims[] = {4, 0, 0, 0, 0};
-  tflite::testing::TestGatherNd<float, int32_t>(
+  tflite_micro::testing::TestGatherNd<float, int32_t>(
       input_dims, input_data, index_dims, index_data, output_dims, output_data,
       golden_data);
 }
@@ -226,7 +226,7 @@ TF_LITE_MICRO_TEST(GatherNd_BatchedIndexingIntoRank3Tensor3) {
                                1.1,  -1.2, 1.3, 6.1, -6.2, 6.3};
   float output_data[12];
   int output_dims[] = {3, 0, 0, 0};
-  tflite::testing::TestGatherNd<float, int32_t>(
+  tflite_micro::testing::TestGatherNd<float, int32_t>(
       input_dims, input_data, index_dims, index_data, output_dims, output_data,
       golden_data);
 }
@@ -243,7 +243,7 @@ TF_LITE_MICRO_TEST(GatherNd_BatchedIndexingIntoRank3Tensor4) {
   const float golden_data[] = {-1.2, 3.2, 4.3, 6.3};
   float output_data[4];
   int output_dims[] = {2, 0, 0};
-  tflite::testing::TestGatherNd<float, int32_t>(
+  tflite_micro::testing::TestGatherNd<float, int32_t>(
       input_dims, input_data, index_dims, index_data, output_dims, output_data,
       golden_data);
 }
@@ -260,7 +260,7 @@ TF_LITE_MICRO_TEST(GatherNd_DuplicateIndexingIntoRank3Tensor) {
   const float golden_data[] = {-2.1, 2.2, 2.3, -2.1, 2.2, 2.3};
   float output_data[6];
   int output_dims[] = {2, 0, 0};
-  tflite::testing::TestGatherNd<float, int32_t>(
+  tflite_micro::testing::TestGatherNd<float, int32_t>(
       input_dims, input_data, index_dims, index_data, output_dims, output_data,
       golden_data);
 }
@@ -277,7 +277,7 @@ TF_LITE_MICRO_TEST(GatherNd_Float32Int32) {
   const float golden_data[] = {-2.1, 2.2, 2.3, 3.1, 3.2, -3.3};
   float output_data[6];
   int output_dims[] = {2, 0, 0};
-  tflite::testing::TestGatherNd<float, int32_t>(
+  tflite_micro::testing::TestGatherNd<float, int32_t>(
       input_dims, input_data, index_dims, index_data, output_dims, output_data,
       golden_data);
 }
@@ -294,7 +294,7 @@ TF_LITE_MICRO_TEST(GatherNd_Int8Int32) {
   const int8_t golden_data[] = {-2, 2, 2, 3, 3, -3};
   int8_t output_data[6];
   int output_dims[] = {2, 0, 0};
-  tflite::testing::TestGatherNd<int8_t, int32_t>(
+  tflite_micro::testing::TestGatherNd<int8_t, int32_t>(
       input_dims, input_data, index_dims, index_data, output_dims, output_data,
       golden_data);
 }
@@ -308,7 +308,7 @@ TF_LITE_MICRO_TEST(GatherNd_ReadOOB) {
   const int8_t input_data[] = {1, -1, 1, -2};
   int8_t output_data;
   int output_dims[] = {1, 0, 0};
-  tflite::testing::TestGatherNd<int8_t, int32_t>(
+  tflite_micro::testing::TestGatherNd<int8_t, int32_t>(
       input_dims, input_data, index_dims, index_data, output_dims, &output_data,
       nullptr, kTfLiteError);
 }
@@ -322,7 +322,7 @@ TF_LITE_MICRO_TEST(GatherNd_ReadOOBNegative) {
   const int8_t input_data[] = {1, -1, 1, -2};
   int8_t output_data;
   int output_dims[] = {1, 0, 0};
-  tflite::testing::TestGatherNd<int8_t, int32_t>(
+  tflite_micro::testing::TestGatherNd<int8_t, int32_t>(
       input_dims, input_data, index_dims, index_data, output_dims, &output_data,
       nullptr, kTfLiteError);
 }
diff --git a/tensorflow/lite/micro/kernels/gather_test.cc b/tensorflow/lite/micro/kernels/gather_test.cc
index 91c010b0..4972edb3 100644
--- a/tensorflow/lite/micro/kernels/gather_test.cc
+++ b/tensorflow/lite/micro/kernels/gather_test.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -71,7 +71,7 @@ void TestGather(int* input_dims, const InType* input_data, int* positions_dims,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -90,7 +90,7 @@ TF_LITE_MICRO_TEST(GatherOp_Shuffle) {
   // against golden_dims[0] onward.
   const int golden_dims[] = {2, 2};
   int output_dims[] = {2, 0, 0};
-  tflite::testing::TestGather<float, int32_t>(
+  tflite_micro::testing::TestGather<float, int32_t>(
       input_dims, input_data, positions_dims, positions_data, output_dims,
       output_data, golden_dims, golden_data);
 }
@@ -109,7 +109,7 @@ TF_LITE_MICRO_TEST(GatherOp_Test0DIndex) {
   // against golden_dims[0] onward.
   const int golden_dims[] = {2};
   int output_dims[] = {1, 0};
-  tflite::testing::TestGather<float, int32_t>(
+  tflite_micro::testing::TestGather<float, int32_t>(
       input_dims, input_data, positions_dims, positions_data, output_dims,
       output_data, golden_dims, golden_data);
 }
@@ -130,7 +130,7 @@ TF_LITE_MICRO_TEST(GatherOp_Test0DIndexWith0DResult) {
   // against golden_dims[0] onward.
   const int golden_dims[] = {0};
   int output_dims[] = {1, 0};
-  tflite::testing::TestGather<float, int32_t>(
+  tflite_micro::testing::TestGather<float, int32_t>(
       input_dims, input_data, positions_dims, positions_data, output_dims,
       output_data, golden_dims, golden_data);
 }
@@ -149,7 +149,7 @@ TF_LITE_MICRO_TEST(GatherOp_Test1DInput1DIndex) {
   // against golden_dims[0] onward.
   const int golden_dims[] = {1};
   int output_dims[] = {1, 0};
-  tflite::testing::TestGather<float, int32_t>(
+  tflite_micro::testing::TestGather<float, int32_t>(
       input_dims, input_data, positions_dims, positions_data, output_dims,
       output_data, golden_dims, golden_data);
 }
@@ -168,7 +168,7 @@ TF_LITE_MICRO_TEST(GatherOp_Test2DIndexWith2DResult) {
   // against golden_dims[0] onward.
   const int golden_dims[] = {1, 2};
   int output_dims[] = {2, 0, 0};
-  tflite::testing::TestGather<float, int32_t>(
+  tflite_micro::testing::TestGather<float, int32_t>(
       input_dims, input_data, positions_dims, positions_data, output_dims,
       output_data, golden_dims, golden_data);
 }
@@ -187,7 +187,7 @@ TF_LITE_MICRO_TEST(GatherOp_Duplicate) {
   // against golden_dims[0] onward.
   const int golden_dims[] = {2, 2, 2};
   int output_dims[] = {3, 0, 0, 0};
-  tflite::testing::TestGather<float, int32_t>(
+  tflite_micro::testing::TestGather<float, int32_t>(
       input_dims, input_data, positions_dims, positions_data, output_dims,
       output_data, golden_dims, golden_data);
 }
@@ -206,7 +206,7 @@ TF_LITE_MICRO_TEST(GatherOp_Slice) {
   // against golden_dims[0] onward.
   const int golden_dims[] = {2, 1};
   int output_dims[] = {2, 0, 0};
-  tflite::testing::TestGather<float, int32_t>(
+  tflite_micro::testing::TestGather<float, int32_t>(
       input_dims, input_data, positions_dims, positions_data, output_dims,
       output_data, golden_dims, golden_data);
 }
@@ -226,7 +226,7 @@ TF_LITE_MICRO_TEST(GatherOp_Axis1) {
   // against golden_dims[0] onward.
   const int golden_dims[] = {1, 2, 3};
   int output_dims[] = {3, 0, 0, 0};
-  tflite::testing::TestGather<float, int32_t>(
+  tflite_micro::testing::TestGather<float, int32_t>(
       input_dims, input_data, positions_dims, positions_data, output_dims,
       output_data, golden_dims, golden_data, axis);
 }
@@ -246,7 +246,7 @@ TF_LITE_MICRO_TEST(GatherOp_Axis1_0DIndex) {
   // against golden_dims[0] onward.
   const int golden_dims[] = {1, 2};
   int output_dims[] = {2, 0, 0};
-  tflite::testing::TestGather<float, int32_t>(
+  tflite_micro::testing::TestGather<float, int32_t>(
       input_dims, input_data, positions_dims, positions_data, output_dims,
       output_data, golden_dims, golden_data, axis);
 }
@@ -266,7 +266,7 @@ TF_LITE_MICRO_TEST(GatherOp_Axis1Slice) {
   // against golden_dims[0] onward.
   const int golden_dims[] = {1, 2, 2};
   int output_dims[] = {3, 0, 0, 0};
-  tflite::testing::TestGather<float, int32_t>(
+  tflite_micro::testing::TestGather<float, int32_t>(
       input_dims, input_data, positions_dims, positions_data, output_dims,
       output_data, golden_dims, golden_data, axis);
 }
@@ -286,7 +286,7 @@ TF_LITE_MICRO_TEST(GatherOp_LastAxis) {
   // against golden_dims[0] onward.
   const int golden_dims[] = {1, 2, 2};
   int output_dims[] = {3, 0, 0, 0};
-  tflite::testing::TestGather<float, int32_t>(
+  tflite_micro::testing::TestGather<float, int32_t>(
       input_dims, input_data, positions_dims, positions_data, output_dims,
       output_data, golden_dims, golden_data, axis);
 }
@@ -306,7 +306,7 @@ TF_LITE_MICRO_TEST(GatherOp_LastAxis0DIndex) {
   // against golden_dims[0] onward.
   const int golden_dims[] = {1, 2};
   int output_dims[] = {2, 0, 0};
-  tflite::testing::TestGather<float, int32_t>(
+  tflite_micro::testing::TestGather<float, int32_t>(
       input_dims, input_data, positions_dims, positions_data, output_dims,
       output_data, golden_dims, golden_data, axis);
 }
@@ -325,7 +325,7 @@ TF_LITE_MICRO_TEST(GatherOp_Float32Int32) {
   // against golden_dims[0] onward.
   const int golden_dims[] = {2, 2};
   int output_dims[] = {2, 0, 0};
-  tflite::testing::TestGather<float, int32_t>(
+  tflite_micro::testing::TestGather<float, int32_t>(
       input_dims, input_data, positions_dims, positions_data, output_dims,
       output_data, golden_dims, golden_data);
 }
@@ -344,7 +344,7 @@ TF_LITE_MICRO_TEST(GatherOp_Int8Int32) {
   // against golden_dims[0] onward.
   const int golden_dims[] = {2, 2};
   int output_dims[] = {2, 0, 0};
-  tflite::testing::TestGather<int8_t, int32_t>(
+  tflite_micro::testing::TestGather<int8_t, int32_t>(
       input_dims, input_data, positions_dims, positions_data, output_dims,
       output_data, golden_dims, golden_data);
 }
@@ -372,7 +372,7 @@ TF_LITE_MICRO_TEST(GatherOp_BatchDims2) {
   // against golden_dims[0] onward.
   const int golden_dims[] = {2, 2, 2, 5};
   int output_dims[] = {4, 0, 0, 0, 0};
-  tflite::testing::TestGather<float, int32_t>(
+  tflite_micro::testing::TestGather<float, int32_t>(
       input_dims, input_data, positions_dims, positions_data, output_dims,
       output_data, golden_dims, golden_data, axis, batch_dims);
 }
@@ -402,7 +402,7 @@ TF_LITE_MICRO_TEST(GatherOp_BatchDims1) {
   // against golden_dims[0] onward.
   const int golden_dims[] = {2, 2, 2, 2, 5};
   int output_dims[] = {5, 0, 0, 0, 0, 0};
-  tflite::testing::TestGather<int8_t, int32_t>(
+  tflite_micro::testing::TestGather<int8_t, int32_t>(
       input_dims, input_data, positions_dims, positions_data, output_dims,
       output_data, golden_dims, golden_data, axis, batch_dims);
 }
@@ -432,7 +432,7 @@ TF_LITE_MICRO_TEST(GatherOp_NegativeBatchDims) {
   // against golden_dims[0] onward.
   const int golden_dims[] = {2, 2, 2, 2, 5};
   int output_dims[] = {5, 0, 0, 0, 0, 0};
-  tflite::testing::TestGather<int8_t, int32_t>(
+  tflite_micro::testing::TestGather<int8_t, int32_t>(
       input_dims, input_data, positions_dims, positions_data, output_dims,
       output_data, golden_dims, golden_data, axis, batch_dims);
 }
@@ -456,7 +456,7 @@ TF_LITE_MICRO_TEST(GatherOp_BatchDimsEqualIndexDims) {
   // against golden_dims[0] onward.
   const int golden_dims[] = {2, 2, 2};
   int output_dims[] = {3, 0, 0, 0};
-  tflite::testing::TestGather<int8_t, int32_t>(
+  tflite_micro::testing::TestGather<int8_t, int32_t>(
       input_dims, input_data, positions_dims, positions_data, output_dims,
       output_data, golden_dims, golden_data, axis, batch_dims);
 }
diff --git a/tensorflow/lite/micro/kernels/hard_swish.cc b/tensorflow/lite/micro/kernels/hard_swish.cc
index f7f49ec4..f62528bd 100644
--- a/tensorflow/lite/micro/kernels/hard_swish.cc
+++ b/tensorflow/lite/micro/kernels/hard_swish.cc
@@ -28,7 +28,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_log.h"
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 void* HardSwishInit(TfLiteContext* context, const char* buffer, size_t length) {
   TFLITE_DCHECK(context->AllocatePersistentBuffer != nullptr);
@@ -37,25 +37,25 @@ void* HardSwishInit(TfLiteContext* context, const char* buffer, size_t length) {
 
 TfLiteStatus HardSwishEval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kHardSwishInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kHardSwishInputTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kHardSwishOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kHardSwishOutputTensor);
   HardSwishParams* params = static_cast<HardSwishParams*>(node->user_data);
 
   switch (input->type) {
     case kTfLiteFloat32: {
-      tflite::reference_ops::HardSwish<float>(
-          tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<float>(input),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<float>(output));
+      tflite_micro::reference_ops::HardSwish<float>(
+          tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<float>(input),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<float>(output));
     } break;
     case kTfLiteInt8: {
-      tflite::reference_ops::HardSwish<int8_t>(
-          *params, tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<int8_t>(input),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<int8_t>(output));
+      tflite_micro::reference_ops::HardSwish<int8_t>(
+          *params, tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<int8_t>(input),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<int8_t>(output));
     } break;
     default: {
       MicroPrintf("Unsupported type %s", TfLiteTypeGetName(input->type));
@@ -68,8 +68,8 @@ TfLiteStatus HardSwishEval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_HARD_SWISH() {
-  return tflite::micro::RegisterOp(HardSwishInit, tflite::HardSwishPrepare,
+  return tflite_micro::micro::RegisterOp(HardSwishInit, tflite_micro::HardSwishPrepare,
                                    HardSwishEval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/hard_swish.h b/tensorflow/lite/micro/kernels/hard_swish.h
index 3ffe60dc..18bbf3bb 100644
--- a/tensorflow/lite/micro/kernels/hard_swish.h
+++ b/tensorflow/lite/micro/kernels/hard_swish.h
@@ -19,12 +19,12 @@ limitations under the License.
 #include "tensorflow/lite/c/builtin_op_data.h"
 #include "tensorflow/lite/c/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 extern const int kHardSwishInputTensor;
 extern const int kHardSwishOutputTensor;
 
 TfLiteStatus HardSwishPrepare(TfLiteContext* context, TfLiteNode* node);
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_HARD_SWISH_H_
diff --git a/tensorflow/lite/micro/kernels/hard_swish_common.cc b/tensorflow/lite/micro/kernels/hard_swish_common.cc
index 8f846522..4c12d8a6 100644
--- a/tensorflow/lite/micro/kernels/hard_swish_common.cc
+++ b/tensorflow/lite/micro/kernels/hard_swish_common.cc
@@ -26,7 +26,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 const int kHardSwishInputTensor = 0;
 const int kHardSwishOutputTensor = 0;
@@ -83,4 +83,4 @@ TfLiteStatus HardSwishPrepare(TfLiteContext* context, TfLiteNode* node) {
   return kTfLiteOk;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/hard_swish_test.cc b/tensorflow/lite/micro/kernels/hard_swish_test.cc
index 2a33deb0..76e2e470 100644
--- a/tensorflow/lite/micro/kernels/hard_swish_test.cc
+++ b/tensorflow/lite/micro/kernels/hard_swish_test.cc
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -104,7 +104,7 @@ void TestHardSwishQuantized(int size, const T* output_data,
   int outputs_array_data[] = {1, 1};
   TfLiteIntArray* outputs_array = IntArrayFromInts(outputs_array_data);
 
-  const TFLMRegistration registration = tflite::Register_HARD_SWISH();
+  const TFLMRegistration registration = tflite_micro::Register_HARD_SWISH();
   micro::KernelRunner runner(registration, tensors, tensors_size, inputs_array,
                              outputs_array, /*builtin_data=*/nullptr);
 
@@ -182,7 +182,7 @@ void TestHardSwishQuantizedBias(const int size, const T* output_data,
   int outputs_array_data[] = {1, 1};
   TfLiteIntArray* outputs_array = IntArrayFromInts(outputs_array_data);
 
-  const TFLMRegistration registration = tflite::Register_HARD_SWISH();
+  const TFLMRegistration registration = tflite_micro::Register_HARD_SWISH();
   micro::KernelRunner runner(registration, tensors, tensors_size, inputs_array,
                              outputs_array, /*builtin_data=*/nullptr);
 
@@ -233,7 +233,7 @@ void TestHardSwishFloat(const int size, float* output_data,
   int outputs_array_data[] = {1, 1};
   TfLiteIntArray* outputs_array = IntArrayFromInts(outputs_array_data);
 
-  const TFLMRegistration registration = tflite::Register_HARD_SWISH();
+  const TFLMRegistration registration = tflite_micro::Register_HARD_SWISH();
   micro::KernelRunner runner(registration, tensors, tensors_size, inputs_array,
                              outputs_array, /*builtin_data=*/nullptr);
 
@@ -248,7 +248,7 @@ void TestHardSwishFloat(const int size, float* output_data,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -259,7 +259,7 @@ TF_LITE_MICRO_TEST(SimpleHardSwishTestFloat) {
   float input_values[size] = {0.f};
   float output_values[size] = {0.f};
 
-  tflite::testing::TestHardSwishFloat(size, output_data, &random_engine,
+  tflite_micro::testing::TestHardSwishFloat(size, output_data, &random_engine,
                                       input_values, output_values);
 }
 
@@ -282,7 +282,7 @@ TF_LITE_MICRO_TEST(SimpleHardSwishTestInt8) {
       float output_min = minmax_pairs[y][0];
       float output_max = minmax_pairs[y][1];
 
-      tflite::testing::TestHardSwishQuantized<int8_t>(
+      tflite_micro::testing::TestHardSwishQuantized<int8_t>(
           size, output_data, input_data_quantized, dequantized_output,
           input_min, input_max, output_min, output_max, &random_engine,
           input_values, output_values);
diff --git a/tensorflow/lite/micro/kernels/if.cc b/tensorflow/lite/micro/kernels/if.cc
index 9143c9c6..ee95fc9a 100644
--- a/tensorflow/lite/micro/kernels/if.cc
+++ b/tensorflow/lite/micro/kernels/if.cc
@@ -27,7 +27,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_graph.h"
 #include "tensorflow/lite/schema/schema_generated.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -53,7 +53,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   TF_LITE_ENSURE(context, node->inputs->size > 0);
 
   // The first input is the condition.
-  tflite::MicroContext* micro_context = tflite::GetMicroContext(context);
+  tflite_micro::MicroContext* micro_context = tflite_micro::GetMicroContext(context);
   TfLiteTensor* cond =
       micro_context->AllocateTempInputTensor(node, kCondTensor);
 
@@ -88,9 +88,9 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const OpData* op_data = reinterpret_cast<OpData*>(node->user_data);
 
-  tflite::MicroContext* micro_context = tflite::GetMicroContext(context);
+  tflite_micro::MicroContext* micro_context = tflite_micro::GetMicroContext(context);
   const TfLiteEvalTensor* cond =
-      tflite::micro::GetEvalInput(context, node, kCondTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kCondTensor);
   TF_LITE_ENSURE(context, cond != nullptr);
   const bool cond_value = cond->data.b[0];
 
@@ -100,7 +100,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
       cond_value ? op_data->then_subgraph_index : op_data->else_subgraph_index;
 
   TF_LITE_ENSURE_OK(context,
-                    tflite::micro::CopyOpInputsToSubgraphInputs(
+                    tflite_micro::micro::CopyOpInputsToSubgraphInputs(
                         context, node, graph_info, active_branch_subgraph_index,
                         /*first_tensor_idx=*/1));
 
@@ -108,7 +108,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
                     graph_info->InvokeSubgraph(active_branch_subgraph_index));
 
   TF_LITE_ENSURE_OK(
-      context, tflite::micro::CopySubgraphOutputsToOpOutputs(
+      context, tflite_micro::micro::CopySubgraphOutputsToOpOutputs(
                    context, node, graph_info, active_branch_subgraph_index));
 
   return kTfLiteOk;
@@ -117,7 +117,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace.
 
 TFLMRegistration Register_IF() {
-  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/if_test.cc b/tensorflow/lite/micro/kernels/if_test.cc
index 6e33941d..7c16169c 100644
--- a/tensorflow/lite/micro/kernels/if_test.cc
+++ b/tensorflow/lite/micro/kernels/if_test.cc
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -55,7 +55,7 @@ void TestIf(int* input1_dims_data, const bool* input1_data,
   params.then_subgraph_index = 1;
   params.else_subgraph_index = 2;
 
-  const TFLMRegistration registration = tflite::Register_IF();
+  const TFLMRegistration registration = tflite_micro::Register_IF();
   micro::KernelRunner runner(registration, tensors, tensors_size, inputs_array,
                              outputs_array, &params);
 
@@ -80,7 +80,7 @@ void TestIf(int* input1_dims_data, const bool* input1_data,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -91,7 +91,7 @@ TF_LITE_MICRO_TEST(IfShouldInvokeSubgraphWithMockModelConditionTrue) {
   float input[] = {5.0, 2.0};
   const float golden[] = {5.0, 2.0};
   float output_data[2] = {0};
-  tflite::testing::TestIf(condition_shape, condition, shape, input, shape,
+  tflite_micro::testing::TestIf(condition_shape, condition, shape, input, shape,
                           golden, 1, 0, output_data);
 }
 
@@ -102,7 +102,7 @@ TF_LITE_MICRO_TEST(IfShouldInvokeSubgraphWithMockModelConditionFalse) {
   float input[] = {5.0, 2.0};
   const float golden[] = {5.0, 2.0};
   float output_data[2] = {0};
-  tflite::testing::TestIf(condition_shape, condition, shape, input, shape,
+  tflite_micro::testing::TestIf(condition_shape, condition, shape, input, shape,
                           golden, 0, 1, output_data);
 }
 
@@ -110,13 +110,13 @@ TF_LITE_MICRO_TEST(IfShouldInvokeSubgraphConditionTrue) {
   constexpr int kArenaSize = 5000;
   uint8_t arena[kArenaSize];
 
-  const tflite::Model* model =
-      tflite::testing::GetSimpleModelWithSubgraphsAndIf();
-  tflite::MicroMutableOpResolver<3> resolver;
+  const tflite_micro::Model* model =
+      tflite_micro::testing::GetSimpleModelWithSubgraphsAndIf();
+  tflite_micro::MicroMutableOpResolver<3> resolver;
   resolver.AddIf();
   resolver.AddAdd();
   resolver.AddMul();
-  tflite::MicroInterpreter interpreter(model, resolver, arena, kArenaSize);
+  tflite_micro::MicroInterpreter interpreter(model, resolver, arena, kArenaSize);
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, interpreter.AllocateTensors());
   TfLiteTensor* condition = interpreter.input(0);
   TfLiteTensor* input1 = interpreter.input(1);
@@ -138,13 +138,13 @@ TF_LITE_MICRO_TEST(IfShouldInvokeSubgraphConditionFalse) {
   constexpr int kArenaSize = 5000;
   uint8_t arena[kArenaSize];
 
-  const tflite::Model* model =
-      tflite::testing::GetSimpleModelWithSubgraphsAndIf();
-  tflite::MicroMutableOpResolver<3> resolver;
+  const tflite_micro::Model* model =
+      tflite_micro::testing::GetSimpleModelWithSubgraphsAndIf();
+  tflite_micro::MicroMutableOpResolver<3> resolver;
   resolver.AddIf();
   resolver.AddAdd();
   resolver.AddMul();
-  tflite::MicroInterpreter interpreter(model, resolver, arena, kArenaSize);
+  tflite_micro::MicroInterpreter interpreter(model, resolver, arena, kArenaSize);
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, interpreter.AllocateTensors());
   TfLiteTensor* condition = interpreter.input(0);
   TfLiteTensor* input1 = interpreter.input(1);
@@ -166,14 +166,14 @@ TF_LITE_MICRO_TEST(IfShouldNotOverwriteTensorAcrossSubgraphs) {
   constexpr int kArenaSize = 5000;
   uint8_t arena[kArenaSize];
 
-  const tflite::Model* model =
-      tflite::testing::GetModelWithIfAndSubgraphInputTensorOverlap();
+  const tflite_micro::Model* model =
+      tflite_micro::testing::GetModelWithIfAndSubgraphInputTensorOverlap();
 
-  tflite::testing::TestingOpResolver op_resolver;
+  tflite_micro::testing::TestingOpResolver op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::testing::GetTestingOpResolver(op_resolver));
+                          tflite_micro::testing::GetTestingOpResolver(op_resolver));
 
-  tflite::MicroInterpreter interpreter(model, op_resolver, arena, kArenaSize);
+  tflite_micro::MicroInterpreter interpreter(model, op_resolver, arena, kArenaSize);
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, interpreter.AllocateTensors());
 
   TfLiteTensor* condition = interpreter.input(0);
@@ -181,7 +181,7 @@ TF_LITE_MICRO_TEST(IfShouldNotOverwriteTensorAcrossSubgraphs) {
   TfLiteTensor* input2 = interpreter.input(2);
   TfLiteTensor* output = interpreter.output(0);
   constexpr int32_t block_size =
-      tflite::MicroArenaBufferAlignment() / sizeof(int32_t);
+      tflite_micro::MicroArenaBufferAlignment() / sizeof(int32_t);
   int32_t input1_data[2 * block_size] = {1, 1, 1, 1, 2, 2, 2, 2};
   int32_t input2_data[4 * block_size] = {3, 3, 3, 3, 4, 4, 4, 4,
                                          5, 5, 5, 5, 6, 6, 6, 6};
diff --git a/tensorflow/lite/micro/kernels/kernel_runner.cc b/tensorflow/lite/micro/kernels/kernel_runner.cc
index 602778d7..1fb34212 100644
--- a/tensorflow/lite/micro/kernels/kernel_runner.cc
+++ b/tensorflow/lite/micro/kernels/kernel_runner.cc
@@ -20,7 +20,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_log.h"
 #include "tensorflow/lite/micro/test_helpers.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace micro {
 
 // TODO(b/161841696): Consider moving away from global arena buffers:
@@ -50,7 +50,7 @@ KernelRunner::KernelRunner(const TFLMRegistration& registration,
   context_.recommended_num_threads = 1;
   context_.GetTensor = MicroContextGetTensor;
   context_.GetEvalTensor = MicroContextGetEvalTensor;
-  tflite::micro::ClearBufferApi(&context_);
+  tflite_micro::micro::ClearBufferApi(&context_);
   context_.AllocatePersistentBuffer = MicroContextAllocatePersistentBuffer;
 
   context_.recommended_num_threads = 0;
@@ -69,7 +69,7 @@ bool KernelRunner::ValidateTempBufferDeallocated() {
 TfLiteStatus KernelRunner::InitAndPrepare(const char* init_data,
                                           size_t length) {
   if (registration_.init) {
-    tflite::micro::ClearBufferApi(&context_);
+    tflite_micro::micro::ClearBufferApi(&context_);
     context_.AllocatePersistentBuffer = MicroContextAllocatePersistentBuffer;
     node_.user_data = registration_.init(&context_, init_data, length);
   }
@@ -91,7 +91,7 @@ TfLiteStatus KernelRunner::InitAndPrepare(const char* init_data,
 }
 
 TfLiteStatus KernelRunner::Invoke() {
-  tflite::micro::ClearBufferApi(&context_);
+  tflite_micro::micro::ClearBufferApi(&context_);
   context_.GetScratchBuffer = MicroContextGetScratchBuffer;
 
   if (registration_.invoke == nullptr) {
@@ -107,7 +107,7 @@ TfLiteStatus KernelRunner::Invoke() {
 }
 
 TfLiteStatus KernelRunner::Reset() {
-  tflite::micro::ClearBufferApi(&context_);
+  tflite_micro::micro::ClearBufferApi(&context_);
   context_.GetScratchBuffer = MicroContextGetScratchBuffer;
 
   if (registration_.reset == nullptr) {
@@ -120,7 +120,7 @@ TfLiteStatus KernelRunner::Reset() {
 }
 
 TfLiteStatus KernelRunner::Free() {
-  tflite::micro::ClearBufferApi(&context_);
+  tflite_micro::micro::ClearBufferApi(&context_);
   context_.GetScratchBuffer = MicroContextGetScratchBuffer;
 
   if (registration_.free == nullptr) {
@@ -132,4 +132,4 @@ TfLiteStatus KernelRunner::Free() {
   return kTfLiteOk;
 }
 }  // namespace micro
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/kernel_runner.h b/tensorflow/lite/micro/kernels/kernel_runner.h
index 25b97c11..a0be324f 100644
--- a/tensorflow/lite/micro/kernels/kernel_runner.h
+++ b/tensorflow/lite/micro/kernels/kernel_runner.h
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/fake_micro_context.h"
 #include "tensorflow/lite/micro/mock_micro_graph.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace micro {
 
 // Helper class to perform a simulated kernel (i.e. TFLMRegistration)
@@ -81,6 +81,6 @@ class KernelRunner {
 };
 
 }  // namespace micro
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_KERNEL_RUNNER_H_
diff --git a/tensorflow/lite/micro/kernels/kernel_util.cc b/tensorflow/lite/micro/kernels/kernel_util.cc
index 12adf361..d02f7855 100644
--- a/tensorflow/lite/micro/kernels/kernel_util.cc
+++ b/tensorflow/lite/micro/kernels/kernel_util.cc
@@ -20,7 +20,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/memory_helpers.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace micro {
 
 namespace {
@@ -162,8 +162,8 @@ TfLiteStatus CopyOpInputsToOpOutputs(TfLiteContext* context, TfLiteNode* node) {
   TF_LITE_ENSURE(context, node->inputs->size == node->outputs->size);
   for (int i = 0; i < node->inputs->size; i++) {
     const TfLiteEvalTensor* input =
-        tflite::micro::GetEvalInput(context, node, i);
-    TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, i);
+        tflite_micro::micro::GetEvalInput(context, node, i);
+    TfLiteEvalTensor* output = tflite_micro::micro::GetEvalOutput(context, node, i);
     int bytes = ValidateAndGetTensorSizes(input, output);
     TF_LITE_ENSURE(context, bytes >= 0);
     memcpy(output->data.raw, input->data.raw, bytes);
@@ -200,14 +200,14 @@ void PrintNBytes(const int8_t* tensor_data, int n_bytes, const char* prefix) {
 // TfLiteEvalTensor*
 void PrintNBytes(const TfLiteEvalTensor* tensor, int n_bytes,
                  const char* prefix) {
-  const int8_t* tensor_data = tflite::micro::GetTensorData<int8_t>(tensor);
+  const int8_t* tensor_data = tflite_micro::micro::GetTensorData<int8_t>(tensor);
   PrintNBytes(tensor_data, n_bytes, prefix);
 }
 
 // same as the PrintNBytes above but the buffer needs to be extracted out of the
 // TfLiteEvalTensor*
 void PrintNBytes(const TfLiteTensor* tensor, int n_bytes, const char* prefix) {
-  const int8_t* tensor_data = tflite::GetTensorData<int8_t>(tensor);
+  const int8_t* tensor_data = tflite_micro::GetTensorData<int8_t>(tensor);
   PrintNBytes(tensor_data, n_bytes, prefix);
 }
 
@@ -221,7 +221,7 @@ TfLiteStatus CopyOpInputsToSubgraphInputs(TfLiteContext* context,
                      graph_info->NumSubgraphInputs(subgraph_idx));
   for (int i = 0; i < node->inputs->size - first_tensor_idx; i++) {
     const TfLiteEvalTensor* input =
-        tflite::micro::GetEvalInput(context, node, i + first_tensor_idx);
+        tflite_micro::micro::GetEvalInput(context, node, i + first_tensor_idx);
     TfLiteEvalTensor* subgraph_input =
         graph_info->GetSubgraphInput(subgraph_idx, i);
     int bytes = ValidateAndGetTensorSizes(input, subgraph_input);
@@ -238,7 +238,7 @@ TfLiteStatus CopyOpOutputsToSubgraphInputs(TfLiteContext* context,
   TF_LITE_ENSURE(context, static_cast<size_t>(node->outputs->size) ==
                               graph_info->NumSubgraphInputs(subgraph_idx));
   for (int i = 0; i < node->outputs->size; i++) {
-    TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, i);
+    TfLiteEvalTensor* output = tflite_micro::micro::GetEvalOutput(context, node, i);
     TfLiteEvalTensor* subgraph_input =
         graph_info->GetSubgraphInput(subgraph_idx, i);
     int bytes = ValidateAndGetTensorSizes(output, subgraph_input);
@@ -255,7 +255,7 @@ TfLiteStatus CopySubgraphOutputsToOpOutputs(TfLiteContext* context,
   TF_LITE_ENSURE(context, static_cast<size_t>(node->outputs->size) ==
                               graph_info->NumSubgraphOutputs(subgraph_idx));
   for (int i = 0; i < node->outputs->size; i++) {
-    TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, i);
+    TfLiteEvalTensor* output = tflite_micro::micro::GetEvalOutput(context, node, i);
     TfLiteEvalTensor* subgraph_output =
         graph_info->GetSubgraphOutput(subgraph_idx, i);
     int bytes = ValidateAndGetTensorSizes(output, subgraph_output);
@@ -277,12 +277,12 @@ TfLiteEvalTensor MakeUnpackedInt4Tensor(TfLiteContext* context,
       context->GetScratchBuffer(context, scratch_buffer_index));
   new_tensor.dims = tensor->dims;
   new_tensor.type = kTfLiteInt8;
-  tflite::tensor_utils::UnpackDenseInt4IntoInt8(
-      tflite::micro::GetTensorData<int8_t>(tensor),
-      tflite::micro::GetTensorShape(tensor).FlatSize(),
-      tflite::micro::GetTensorData<int8_t>(&new_tensor));
+  tflite_micro::tensor_utils::UnpackDenseInt4IntoInt8(
+      tflite_micro::micro::GetTensorData<int8_t>(tensor),
+      tflite_micro::micro::GetTensorShape(tensor).FlatSize(),
+      tflite_micro::micro::GetTensorData<int8_t>(&new_tensor));
   return new_tensor;
 }
 
 }  // namespace micro
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/kernel_util.h b/tensorflow/lite/micro/kernels/kernel_util.h
index f14c9271..f8a534fa 100644
--- a/tensorflow/lite/micro/kernels/kernel_util.h
+++ b/tensorflow/lite/micro/kernels/kernel_util.h
@@ -25,7 +25,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/types.h"
 #include "tensorflow/lite/micro/micro_context.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace micro {
 
 TFLMRegistration RegisterOp(
@@ -145,6 +145,6 @@ TfLiteEvalTensor MakeUnpackedInt4Tensor(TfLiteContext* context,
                                         int scratch_buffer_index,
                                         const TfLiteEvalTensor* tensor);
 }  // namespace micro
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_KERNEL_UTIL_H_
diff --git a/tensorflow/lite/micro/kernels/l2_pool_2d.cc b/tensorflow/lite/micro/kernels/l2_pool_2d.cc
index c5eb70f9..8c9612a0 100644
--- a/tensorflow/lite/micro/kernels/l2_pool_2d.cc
+++ b/tensorflow/lite/micro/kernels/l2_pool_2d.cc
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 // Input/output tensor index.
@@ -78,8 +78,8 @@ TfLiteStatus L2Prepare(TfLiteContext* context, TfLiteNode* node) {
   // is a temporary allocation.  We must therefore relocate the dims
   // from the FlatBuffer to the persistent storage arena.
   TfLiteEvalTensor* output_eval =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
-  TF_LITE_ENSURE_OK(context, tflite::micro::CreateWritableTensorDimsWithCopy(
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
+  TF_LITE_ENSURE_OK(context, tflite_micro::micro::CreateWritableTensorDimsWithCopy(
                                  context, output, output_eval));
   output->dims->data[kBatchRank] = batches;
   output->dims->data[kHeightRank] = out_height;
@@ -93,27 +93,27 @@ TfLiteStatus L2Prepare(TfLiteContext* context, TfLiteNode* node) {
 }
 
 void L2EvalFloat(const TfLitePoolParams& params, const TfLiteEvalTensor& input,
-                 tflite::PoolParams* op_params, TfLiteEvalTensor* output) {
+                 tflite_micro::PoolParams* op_params, TfLiteEvalTensor* output) {
   float activation_min, activation_max;
   CalculateActivationRange(params.activation, &activation_min, &activation_max);
 
   op_params->float_activation_min = activation_min;
   op_params->float_activation_max = activation_max;
-  reference_ops::L2Pool(*op_params, tflite::micro::GetTensorShape(&input),
-                        tflite::micro::GetTensorData<float>(&input),
-                        tflite::micro::GetTensorShape(output),
-                        tflite::micro::GetTensorData<float>(output));
+  reference_ops::L2Pool(*op_params, tflite_micro::micro::GetTensorShape(&input),
+                        tflite_micro::micro::GetTensorData<float>(&input),
+                        tflite_micro::micro::GetTensorShape(output),
+                        tflite_micro::micro::GetTensorData<float>(output));
 }
 
 TfLiteStatus L2Eval(TfLiteContext* context, TfLiteNode* node) {
   auto* params = static_cast<const TfLitePoolParams*>(node->builtin_data);
 
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
 
-  tflite::PoolParams op_params;
+  tflite_micro::PoolParams op_params;
   op_params.stride_height = params->stride_height;
   op_params.stride_width = params->stride_width;
   op_params.filter_height = params->filter_height;
@@ -136,7 +136,7 @@ TfLiteStatus L2Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_L2_POOL_2D() {
-  return tflite::micro::RegisterOp(nullptr, L2Prepare, L2Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, L2Prepare, L2Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/l2_pool_2d_test.cc b/tensorflow/lite/micro/kernels/l2_pool_2d_test.cc
index a8c20b20..c876a3d9 100644
--- a/tensorflow/lite/micro/kernels/l2_pool_2d_test.cc
+++ b/tensorflow/lite/micro/kernels/l2_pool_2d_test.cc
@@ -20,7 +20,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -55,7 +55,7 @@ void ExecuteL2Pool2DTest(const L2Pool2DTestParams& params,
   op_params.stride_height = params.stride_height;
   op_params.stride_width = params.stride_width;
 
-  const TFLMRegistration registration = tflite::Register_L2_POOL_2D();
+  const TFLMRegistration registration = tflite_micro::Register_L2_POOL_2D();
   micro::KernelRunner runner(registration, tensors, tensors_count, inputs_array,
                              outputs_array, static_cast<void*>(&op_params));
 
@@ -92,7 +92,7 @@ void TestL2Pool2D(L2Pool2DTestParams& params, int* input_dims_data,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -106,9 +106,9 @@ TF_LITE_MICRO_TEST(FloatPoolingOpTestL2Pool) {
   constexpr float kExpect[] = {3.5, 6.5};
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
-  tflite::testing::L2Pool2DTestParams params;
+  tflite_micro::testing::L2Pool2DTestParams params;
 
-  tflite::testing::TestL2Pool2D(params, kInputDims, kInput, kExpectDims,
+  tflite_micro::testing::TestL2Pool2D(params, kInputDims, kInput, kExpectDims,
                                 kExpect, output_data);
 }
 
@@ -122,10 +122,10 @@ TF_LITE_MICRO_TEST(FloatPoolingOpTestL2PoolActivationRelu) {
   constexpr float kExpect[] = {3.53553, 6.5};
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
-  tflite::testing::L2Pool2DTestParams params;
+  tflite_micro::testing::L2Pool2DTestParams params;
   params.activation = kTfLiteActRelu;
 
-  tflite::testing::TestL2Pool2D(params, kInputDims, kInput, kExpectDims,
+  tflite_micro::testing::TestL2Pool2D(params, kInputDims, kInput, kExpectDims,
                                 kExpect, output_data);
 }
 
@@ -139,10 +139,10 @@ TF_LITE_MICRO_TEST(FloatPoolingOpTestL2PoolActivationRelu1) {
   constexpr float kExpect[] = {0.353553, 1.0};
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
-  tflite::testing::L2Pool2DTestParams params;
+  tflite_micro::testing::L2Pool2DTestParams params;
   params.activation = kTfLiteActReluN1To1;
 
-  tflite::testing::TestL2Pool2D(params, kInputDims, kInput, kExpectDims,
+  tflite_micro::testing::TestL2Pool2D(params, kInputDims, kInput, kExpectDims,
                                 kExpect, output_data);
 }
 
@@ -156,10 +156,10 @@ TF_LITE_MICRO_TEST(FloatPoolingOpTestL2PoolActivationRelu6) {
   constexpr float kExpect[] = {0.353553, 6.0};
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
-  tflite::testing::L2Pool2DTestParams params;
+  tflite_micro::testing::L2Pool2DTestParams params;
   params.activation = kTfLiteActRelu6;
 
-  tflite::testing::TestL2Pool2D(params, kInputDims, kInput, kExpectDims,
+  tflite_micro::testing::TestL2Pool2D(params, kInputDims, kInput, kExpectDims,
                                 kExpect, output_data);
 }
 
@@ -173,10 +173,10 @@ TF_LITE_MICRO_TEST(FloatPoolingOpTestL2PoolPaddingSame) {
   constexpr float kExpect[] = {3.5, 6.5};
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
-  tflite::testing::L2Pool2DTestParams params;
+  tflite_micro::testing::L2Pool2DTestParams params;
   params.padding = kTfLitePaddingSame;
 
-  tflite::testing::TestL2Pool2D(params, kInputDims, kInput, kExpectDims,
+  tflite_micro::testing::TestL2Pool2D(params, kInputDims, kInput, kExpectDims,
                                 kExpect, output_data);
 }
 
@@ -191,13 +191,13 @@ TF_LITE_MICRO_TEST(FloatPoolingOpTestL2PoolPaddingSameStride1) {
                                2.54951, 7.2111, 8.63134, 7.0};
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
-  tflite::testing::L2Pool2DTestParams params;
+  tflite_micro::testing::L2Pool2DTestParams params;
   params.padding = kTfLitePaddingSame;
   params.compare_tolerance = 1e-4;
   params.stride_width = 1;
   params.stride_height = 1;
 
-  tflite::testing::TestL2Pool2D(params, kInputDims, kInput, kExpectDims,
+  tflite_micro::testing::TestL2Pool2D(params, kInputDims, kInput, kExpectDims,
                                 kExpect, output_data);
 }
 
@@ -211,11 +211,11 @@ TF_LITE_MICRO_TEST(FloatPoolingOpTestL2PoolPaddingValidStride1) {
   constexpr float kExpect[] = {3.5, 6.0, 6.5};
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
-  tflite::testing::L2Pool2DTestParams params;
+  tflite_micro::testing::L2Pool2DTestParams params;
   params.stride_width = 1;
   params.stride_height = 1;
 
-  tflite::testing::TestL2Pool2D(params, kInputDims, kInput, kExpectDims,
+  tflite_micro::testing::TestL2Pool2D(params, kInputDims, kInput, kExpectDims,
                                 kExpect, output_data);
 }
 
diff --git a/tensorflow/lite/micro/kernels/l2norm.cc b/tensorflow/lite/micro/kernels/l2norm.cc
index fa3601bf..942c2611 100644
--- a/tensorflow/lite/micro/kernels/l2norm.cc
+++ b/tensorflow/lite/micro/kernels/l2norm.cc
@@ -20,7 +20,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -84,9 +84,9 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
       *(static_cast<const L2NormalizationParams*>(node->user_data));
 
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
   // TODO(b/143912164): instead of hardcode the epsilon here, we should read it
   // from tensorflow, i.e., adding a params.
@@ -103,14 +103,14 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   // So we don't even need to do handle the epsilon for quantized kernel case.
   const float epsilon = 1e-6f;
   if (output->type == kTfLiteFloat32) {
-    reference_ops::L2Normalization(data, tflite::micro::GetTensorShape(input),
-                                   tflite::micro::GetTensorData<float>(input),
-                                   tflite::micro::GetTensorShape(output),
-                                   tflite::micro::GetTensorData<float>(output),
+    reference_ops::L2Normalization(data, tflite_micro::micro::GetTensorShape(input),
+                                   tflite_micro::micro::GetTensorData<float>(input),
+                                   tflite_micro::micro::GetTensorShape(output),
+                                   tflite_micro::micro::GetTensorData<float>(output),
                                    epsilon);
   } else if (output->type == kTfLiteInt8) {
-    const auto input_shape = tflite::micro::GetTensorShape(input);
-    const auto output_shape = tflite::micro::GetTensorShape(output);
+    const auto input_shape = tflite_micro::micro::GetTensorShape(input);
+    const auto output_shape = tflite_micro::micro::GetTensorShape(output);
     const int trailing_dim = input_shape.DimensionsCount() - 1;
     const int depth =
         MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);
@@ -118,8 +118,8 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
         MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);
     reference_integer_ops::L2Normalization(
         data.input_zero_point, outer_size, depth,
-        tflite::micro::GetTensorData<int8_t>(input),
-        tflite::micro::GetTensorData<int8_t>(output));
+        tflite_micro::micro::GetTensorData<int8_t>(input),
+        tflite_micro::micro::GetTensorData<int8_t>(output));
   } else {
     MicroPrintf("Output type is %s, requires float.",
                 TfLiteTypeGetName(output->type));
@@ -132,9 +132,9 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_L2NORM_REF() {
-  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, Eval);
 }
 
 TFLMRegistration Register_L2_NORMALIZATION() { return Register_L2NORM_REF(); }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/l2norm_test.cc b/tensorflow/lite/micro/kernels/l2norm_test.cc
index 435f2f00..7efa7b8e 100644
--- a/tensorflow/lite/micro/kernels/l2norm_test.cc
+++ b/tensorflow/lite/micro/kernels/l2norm_test.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -74,7 +74,7 @@ void TestL2Normalization(int* input_dims_data, const T* input_data,
       .activation = kTfLiteActNone,
   };
 
-  const TFLMRegistration registration = tflite::Register_L2_NORMALIZATION();
+  const TFLMRegistration registration = tflite_micro::Register_L2_NORMALIZATION();
   micro::KernelRunner runner(registration, tensors, tensors_size, inputs_array,
                              outputs_array,
                              reinterpret_cast<void*>(&builtin_data));
@@ -89,7 +89,7 @@ void TestL2Normalization(int* input_dims_data, const T* input_data,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -101,7 +101,7 @@ TF_LITE_MICRO_TEST(SimpleFloatTest) {
                                                    0.6,   -0.35, 0.05};
   float output_data[data_length];
 
-  tflite::testing::TestL2Normalization<float>(
+  tflite_micro::testing::TestL2Normalization<float>(
       input_dims, input_data, expected_output_data, output_data);
 }
 
@@ -112,7 +112,7 @@ TF_LITE_MICRO_TEST(ZerosVectorFloatTest) {
   const float expected_output_data[data_length] = {0, 0, 0, 0, 0, 0};
   float output_data[data_length];
 
-  tflite::testing::TestL2Normalization<float>(
+  tflite_micro::testing::TestL2Normalization<float>(
       input_dims, input_data, expected_output_data, output_data);
 }
 
@@ -124,7 +124,7 @@ TF_LITE_MICRO_TEST(SimpleFloatWithRankLessThanFourTest) {
                                                    0.6,   -0.35, 0.05};
   float output_data[data_length];
 
-  tflite::testing::TestL2Normalization<float>(
+  tflite_micro::testing::TestL2Normalization<float>(
       input_dims, input_data, expected_output_data, output_data);
 }
 
@@ -143,7 +143,7 @@ TF_LITE_MICRO_TEST(MultipleBatchFloatTest) {
   };
   float output_data[data_length];
 
-  tflite::testing::TestL2Normalization<float>(
+  tflite_micro::testing::TestL2Normalization<float>(
       input_dims, input_data, expected_output_data, output_data);
 }
 
@@ -154,7 +154,7 @@ TF_LITE_MICRO_TEST(SimpleInt8Test) {
   const int8_t expected_output[data_length] = {-70, 38, 45, 77, -45, 6};
   int8_t output_data[data_length];
 
-  tflite::testing::TestL2Normalization<int8_t>(input_dims, input_data,
+  tflite_micro::testing::TestL2Normalization<int8_t>(input_dims, input_data,
                                                expected_output, output_data);
 }
 
@@ -165,7 +165,7 @@ TF_LITE_MICRO_TEST(ZerosVectorInt8Test) {
   const int8_t expected_output[data_length] = {0, 0, 0, 0, 0, 0};
   int8_t output_data[data_length];
 
-  tflite::testing::TestL2Normalization<int8_t>(input_dims, input_data,
+  tflite_micro::testing::TestL2Normalization<int8_t>(input_dims, input_data,
                                                expected_output, output_data);
 }
 
@@ -184,7 +184,7 @@ TF_LITE_MICRO_TEST(MultipleBatchInt8Test) {
   };
   int8_t output_data[data_length];
 
-  tflite::testing::TestL2Normalization<int8_t>(input_dims, input_data,
+  tflite_micro::testing::TestL2Normalization<int8_t>(input_dims, input_data,
                                                expected_output, output_data);
 }
 
diff --git a/tensorflow/lite/micro/kernels/leaky_relu.cc b/tensorflow/lite/micro/kernels/leaky_relu.cc
index ee86f195..6e5c3937 100644
--- a/tensorflow/lite/micro/kernels/leaky_relu.cc
+++ b/tensorflow/lite/micro/kernels/leaky_relu.cc
@@ -24,7 +24,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/leaky_relu.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 template <typename T>
 void QuantizeLeakyRelu(const LeakyReluOpData& data,
@@ -39,10 +39,10 @@ void QuantizeLeakyRelu(const LeakyReluOpData& data,
   op_params.output_multiplier_identity = data.output_multiplier_identity;
   op_params.output_shift_identity = data.output_shift_identity;
   reference_ops::QuantizeLeakyRelu(op_params,
-                                   tflite::micro::GetTensorShape(input),
-                                   tflite::micro::GetTensorData<T>(input),
-                                   tflite::micro::GetTensorShape(output),
-                                   tflite::micro::GetTensorData<T>(output));
+                                   tflite_micro::micro::GetTensorShape(input),
+                                   tflite_micro::micro::GetTensorData<T>(input),
+                                   tflite_micro::micro::GetTensorShape(output),
+                                   tflite_micro::micro::GetTensorData<T>(output));
 }
 
 void* LeakyReluInit(TfLiteContext* context, const char* buffer, size_t length) {
@@ -52,9 +52,9 @@ void* LeakyReluInit(TfLiteContext* context, const char* buffer, size_t length) {
 
 TfLiteStatus LeakyReluEval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
   const LeakyReluOpData& data = *static_cast<LeakyReluOpData*>(node->user_data);
 
   switch (input->type) {
@@ -64,10 +64,10 @@ TfLiteStatus LeakyReluEval(TfLiteContext* context, TfLiteNode* node) {
           static_cast<TfLiteLeakyReluParams*>(node->builtin_data);
 
       op_params.alpha = params->alpha;
-      reference_ops::LeakyRelu(op_params, tflite::micro::GetTensorShape(input),
-                               tflite::micro::GetTensorData<float>(input),
-                               tflite::micro::GetTensorShape(output),
-                               tflite::micro::GetTensorData<float>(output));
+      reference_ops::LeakyRelu(op_params, tflite_micro::micro::GetTensorShape(input),
+                               tflite_micro::micro::GetTensorData<float>(input),
+                               tflite_micro::micro::GetTensorShape(output),
+                               tflite_micro::micro::GetTensorData<float>(output));
       return kTfLiteOk;
     } break;
     case kTfLiteInt8: {
@@ -88,8 +88,8 @@ TfLiteStatus LeakyReluEval(TfLiteContext* context, TfLiteNode* node) {
 }
 
 TFLMRegistration Register_LEAKY_RELU() {
-  return tflite::micro::RegisterOp(LeakyReluInit, LeakyReluPrepare,
+  return tflite_micro::micro::RegisterOp(LeakyReluInit, LeakyReluPrepare,
                                    LeakyReluEval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/leaky_relu.h b/tensorflow/lite/micro/kernels/leaky_relu.h
index dfcd6e93..5a05869a 100644
--- a/tensorflow/lite/micro/kernels/leaky_relu.h
+++ b/tensorflow/lite/micro/kernels/leaky_relu.h
@@ -18,7 +18,7 @@ limitations under the License.
 
 #include "tensorflow/lite/c/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // Input/output tensor index.
 extern const int kInputTensor;
@@ -38,6 +38,6 @@ TfLiteStatus CalculateOpDataLeakyRelu(TfLiteContext* context, TfLiteNode* node);
 
 TfLiteStatus LeakyReluPrepare(TfLiteContext* context, TfLiteNode* node);
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_LEAKY_RELU_H_
diff --git a/tensorflow/lite/micro/kernels/leaky_relu_common.cc b/tensorflow/lite/micro/kernels/leaky_relu_common.cc
index 3d1ffebb..ca0a8425 100644
--- a/tensorflow/lite/micro/kernels/leaky_relu_common.cc
+++ b/tensorflow/lite/micro/kernels/leaky_relu_common.cc
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/kernels/leaky_relu.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // Input/output tensor index.
 const int kInputTensor = 0;
@@ -75,4 +75,4 @@ TfLiteStatus LeakyReluPrepare(TfLiteContext* context, TfLiteNode* node) {
   return CalculateOpDataLeakyRelu(context, node);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/leaky_relu_test.cc b/tensorflow/lite/micro/kernels/leaky_relu_test.cc
index 3c5df3f2..bcb692db 100644
--- a/tensorflow/lite/micro/kernels/leaky_relu_test.cc
+++ b/tensorflow/lite/micro/kernels/leaky_relu_test.cc
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -50,7 +50,7 @@ void ExecuteLeakyReluTest(const float alpha, const int tensors_count,
   int kOutputArrayData[] = {1, 1};
   TfLiteIntArray* outputs_array = IntArrayFromInts(kOutputArrayData);
 
-  const TFLMRegistration registration = tflite::Register_LEAKY_RELU();
+  const TFLMRegistration registration = tflite_micro::Register_LEAKY_RELU();
   micro::KernelRunner runner(registration, tensors, tensors_count, inputs_array,
                              outputs_array, static_cast<void*>(&builtin_data));
 
@@ -167,7 +167,7 @@ void QuantizedActivationsOpTestLeakyRelu() {
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -182,20 +182,20 @@ TF_LITE_MICRO_TEST(QuantizedActivationsOpTestLeakyReluInt8_1) {
   int8_t q_output_data[kOutputCount];
   int8_t q_input_data[kOutputCount];
 
-  tflite::testing::TestLeakyReluParams<int8_t> params = {};
+  tflite_micro::testing::TestLeakyReluParams<int8_t> params = {};
   params.alpha = 0.5f;
   params.scale = 0.1f;
   params.zero_point = 0;
   params.input_data = q_input_data;
   params.output_data = q_output_data;
-  params.tolerance = tflite::testing::kQuantizedTolerance;
+  params.tolerance = tflite_micro::testing::kQuantizedTolerance;
 
-  tflite::testing::TestLeakyReluQuantized(params, kDims, kInput, kDims, kExpect,
+  tflite_micro::testing::TestLeakyReluQuantized(params, kDims, kInput, kDims, kExpect,
                                           output_data);
 }
 
 TF_LITE_MICRO_TEST(QuantizedActivationsOpTestLeakyReluInt8_2) {
-  tflite::testing::QuantizedActivationsOpTestLeakyRelu<int8_t>();
+  tflite_micro::testing::QuantizedActivationsOpTestLeakyRelu<int8_t>();
 }
 
 TF_LITE_MICRO_TEST(QuantizedActivationsOpTestLeakyReluInt16_1) {
@@ -209,20 +209,20 @@ TF_LITE_MICRO_TEST(QuantizedActivationsOpTestLeakyReluInt16_1) {
   int16_t q_output_data[kOutputCount];
   int16_t q_input_data[kOutputCount];
 
-  tflite::testing::TestLeakyReluParams<int16_t> params = {};
+  tflite_micro::testing::TestLeakyReluParams<int16_t> params = {};
   params.alpha = 0.5f;
   params.scale = 0.01f;
   params.zero_point = 0;
   params.input_data = q_input_data;
   params.output_data = q_output_data;
-  params.tolerance = tflite::testing::kQuantizedTolerance;
+  params.tolerance = tflite_micro::testing::kQuantizedTolerance;
 
-  tflite::testing::TestLeakyReluQuantized(params, kDims, kInput, kDims, kExpect,
+  tflite_micro::testing::TestLeakyReluQuantized(params, kDims, kInput, kDims, kExpect,
                                           output_data);
 }
 
 TF_LITE_MICRO_TEST(QuantizedActivationsOpTestLeakyReluInt16_2) {
-  tflite::testing::QuantizedActivationsOpTestLeakyRelu<int16_t>();
+  tflite_micro::testing::QuantizedActivationsOpTestLeakyRelu<int16_t>();
 }
 
 TF_LITE_MICRO_TEST(FloatActivationsOpTestLeakyRelu) {
@@ -231,10 +231,10 @@ TF_LITE_MICRO_TEST(FloatActivationsOpTestLeakyRelu) {
   constexpr float kExpect[] = {0.0f, 1.0f, 3.0f, 1.0f, -0.5f, -1.0f};
   constexpr int kOutputCount = std::extent<decltype(kExpect)>::value;
   float output_data[kOutputCount];
-  tflite::testing::TestLeakyReluParams<float> params = {};
+  tflite_micro::testing::TestLeakyReluParams<float> params = {};
   params.alpha = 0.5f;
 
-  tflite::testing::TestLeakyRelu(params, kDims, kInput, kDims, kExpect,
+  tflite_micro::testing::TestLeakyRelu(params, kDims, kInput, kDims, kExpect,
                                  output_data);
 }
 
diff --git a/tensorflow/lite/micro/kernels/log_softmax.cc b/tensorflow/lite/micro/kernels/log_softmax.cc
index 47f59376..e9d3bb1d 100644
--- a/tensorflow/lite/micro/kernels/log_softmax.cc
+++ b/tensorflow/lite/micro/kernels/log_softmax.cc
@@ -25,7 +25,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 // used only with quantized data
@@ -74,7 +74,7 @@ TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node) {
 
     int input_left_shift;
     int reverse_scaling_right_shift;
-    tflite::PreprocessLogSoftmaxScalingExp(
+    tflite_micro::PreprocessLogSoftmaxScalingExp(
         kBeta, static_cast<double>(input->params.scale), kScaledDiffIntegerBits,
         &data->input_multiplier, &input_left_shift,
         &data->reverse_scaling_divisor, &reverse_scaling_right_shift);
@@ -84,7 +84,7 @@ TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node) {
     // diff_min has a negative value, and is used to limit the maximum magnitude
     // of the diffs, which are <= 0.
     data->diff_min =
-        -tflite::CalculateInputRadius(kScaledDiffIntegerBits, input_left_shift);
+        -tflite_micro::CalculateInputRadius(kScaledDiffIntegerBits, input_left_shift);
 
     RuntimeShape input_shape = GetTensorShape(input);
     const int trailing_dim = input_shape.DimensionsCount() - 1;
@@ -106,16 +106,16 @@ TfLiteStatus LogSoftmaxEval(TfLiteContext* context, TfLiteNode* node) {
   const LogSoftmaxOpData* data =
       static_cast<LogSoftmaxOpData*>(node->user_data);
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
   switch (input->type) {
     case kTfLiteFloat32: {
       SoftmaxParams op_params = {};
-      reference_ops::LogSoftmax(op_params, tflite::micro::GetTensorShape(input),
-                                tflite::micro::GetTensorData<float>(input),
-                                tflite::micro::GetTensorShape(output),
-                                tflite::micro::GetTensorData<float>(output));
+      reference_ops::LogSoftmax(op_params, tflite_micro::micro::GetTensorShape(input),
+                                tflite_micro::micro::GetTensorData<float>(input),
+                                tflite_micro::micro::GetTensorShape(output),
+                                tflite_micro::micro::GetTensorData<float>(output));
       return kTfLiteOk;
     }
     case kTfLiteInt8: {
@@ -126,10 +126,10 @@ TfLiteStatus LogSoftmaxEval(TfLiteContext* context, TfLiteNode* node) {
       op_params.reverse_scaling_right_shift = data->reverse_scaling_right_shift;
       op_params.diff_min = data->diff_min;
       reference_ops::LogSoftmax(op_params, data->outer_size, data->depth,
-                                tflite::micro::GetTensorShape(input),
-                                tflite::micro::GetTensorData<int8_t>(input),
-                                tflite::micro::GetTensorShape(output),
-                                tflite::micro::GetTensorData<int8_t>(output));
+                                tflite_micro::micro::GetTensorShape(input),
+                                tflite_micro::micro::GetTensorData<int8_t>(input),
+                                tflite_micro::micro::GetTensorShape(output),
+                                tflite_micro::micro::GetTensorData<int8_t>(output));
       return kTfLiteOk;
     }
     default:
@@ -142,7 +142,7 @@ TfLiteStatus LogSoftmaxEval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_LOG_SOFTMAX() {
-  return tflite::micro::RegisterOp(nullptr, LogSoftmaxPrepare, LogSoftmaxEval);
+  return tflite_micro::micro::RegisterOp(nullptr, LogSoftmaxPrepare, LogSoftmaxEval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/log_softmax_test.cc b/tensorflow/lite/micro/kernels/log_softmax_test.cc
index a34c5979..d0be885d 100644
--- a/tensorflow/lite/micro/kernels/log_softmax_test.cc
+++ b/tensorflow/lite/micro/kernels/log_softmax_test.cc
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -32,7 +32,7 @@ void ExecuteLogSoftmaxTest(int tensors_count, TfLiteTensor* tensors) {
   int kOutputArrayData[] = {1, 1};
   TfLiteIntArray* outputs_array = IntArrayFromInts(kOutputArrayData);
 
-  const TFLMRegistration registration = tflite::Register_LOG_SOFTMAX();
+  const TFLMRegistration registration = tflite_micro::Register_LOG_SOFTMAX();
   micro::KernelRunner runner(registration, tensors, tensors_count, inputs_array,
                              outputs_array, nullptr);
 
@@ -110,7 +110,7 @@ void TestLogSoftmaxQuantized(const TestLogSoftmaxParams<T>& params,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -137,7 +137,7 @@ TF_LITE_MICRO_TEST(FloatActivationsOpTestLogSoftmax) {
 
   constexpr float kTolerance = 1e-5;
 
-  tflite::testing::TestLogSoftmax(kTolerance, kDims1, kInput, kDims1, kExpect1,
+  tflite_micro::testing::TestLogSoftmax(kTolerance, kDims1, kInput, kDims1, kExpect1,
                                   output_data);
 
   // Same input, but a different shape.
@@ -147,7 +147,7 @@ TF_LITE_MICRO_TEST(FloatActivationsOpTestLogSoftmax) {
       -.00671534, -5.00671, -.000123374, -9.00012,
   };
 
-  tflite::testing::TestLogSoftmax(kTolerance, kDims2, kInput, kDims2, kExpect2,
+  tflite_micro::testing::TestLogSoftmax(kTolerance, kDims2, kInput, kDims2, kExpect2,
                                   output_data);
 }
 
@@ -166,7 +166,7 @@ TF_LITE_MICRO_TEST(LogSoftmaxOpTestSimpleTest) {
 
   constexpr float kTolerance = 1e-6;
 
-  tflite::testing::TestLogSoftmax(kTolerance, kDims, kInput, kDims, kExpect,
+  tflite_micro::testing::TestLogSoftmax(kTolerance, kDims, kInput, kDims, kExpect,
                                   output_data);
 }
 
@@ -191,14 +191,14 @@ TF_LITE_MICRO_TEST(QuantizedActivationsOpTestLogSoftmaxInt8) {
   constexpr float kMin = -10;
   constexpr float kMax = 10;
   constexpr float kLogSoftmaxQuantizedTolerance = 0.06355;
-  tflite::testing::TestLogSoftmaxParams<int8_t> params = {};
+  tflite_micro::testing::TestLogSoftmaxParams<int8_t> params = {};
   params.data_min = kMin;
   params.data_max = kMax;
   params.input_data = q_input_data;
   params.output_data = q_output_data;
   params.tolerance = kLogSoftmaxQuantizedTolerance;
 
-  tflite::testing::TestLogSoftmaxQuantized(
+  tflite_micro::testing::TestLogSoftmaxQuantized(
       params, kDims, kInput, kDims, kExpect, kExpectQuantized, output_data);
 }
 
@@ -216,14 +216,14 @@ TF_LITE_MICRO_TEST(ExtraTestLogSoftmaxInt8) {
   constexpr float kMin = -1;
   constexpr float kMax = 1;
   constexpr float kLogSoftmaxQuantizedTolerance = 0.06355;
-  tflite::testing::TestLogSoftmaxParams<int8_t> params = {};
+  tflite_micro::testing::TestLogSoftmaxParams<int8_t> params = {};
   params.data_min = kMin;
   params.data_max = kMax;
   params.input_data = q_input_data;
   params.output_data = q_output_data;
   params.tolerance = kLogSoftmaxQuantizedTolerance;
 
-  tflite::testing::TestLogSoftmaxQuantized(
+  tflite_micro::testing::TestLogSoftmaxQuantized(
       params, kDims, kInput, kDims, kExpect, kExpectQuantized, output_data);
 }
 
diff --git a/tensorflow/lite/micro/kernels/logical.cc b/tensorflow/lite/micro/kernels/logical.cc
index 53e282d6..87df063c 100644
--- a/tensorflow/lite/micro/kernels/logical.cc
+++ b/tensorflow/lite/micro/kernels/logical.cc
@@ -20,7 +20,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/op_macros.h"
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 TfLiteStatus LogicalOrEval(TfLiteContext* context, TfLiteNode* node) {
@@ -34,11 +34,11 @@ TfLiteStatus LogicalAndEval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_LOGICAL_OR() {
-  return tflite::micro::RegisterOp(nullptr, nullptr, LogicalOrEval);
+  return tflite_micro::micro::RegisterOp(nullptr, nullptr, LogicalOrEval);
 }
 
 TFLMRegistration Register_LOGICAL_AND() {
-  return tflite::micro::RegisterOp(nullptr, nullptr, LogicalAndEval);
+  return tflite_micro::micro::RegisterOp(nullptr, nullptr, LogicalAndEval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/logical.h b/tensorflow/lite/micro/kernels/logical.h
index e70e4576..a2b59ae3 100644
--- a/tensorflow/lite/micro/kernels/logical.h
+++ b/tensorflow/lite/micro/kernels/logical.h
@@ -18,7 +18,7 @@ limitations under the License.
 #include "tensorflow/lite/c/builtin_op_data.h"
 #include "tensorflow/lite/c/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 // Input/output tensor index.
 extern const int kLogicalInputTensor1;
 extern const int kLogicalInputTensor2;
@@ -30,6 +30,6 @@ TfLiteStatus LogicalImpl(TfLiteContext* context, TfLiteNode* node,
 bool LogicalOr(bool x, bool y);
 bool LogicalAnd(bool x, bool y);
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_LOGICAL_H_
diff --git a/tensorflow/lite/micro/kernels/logical_common.cc b/tensorflow/lite/micro/kernels/logical_common.cc
index 2612d3a4..da40ec14 100644
--- a/tensorflow/lite/micro/kernels/logical_common.cc
+++ b/tensorflow/lite/micro/kernels/logical_common.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/kernels/logical.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // Input/output tensor index.
 const int kLogicalInputTensor1 = 0;
@@ -29,28 +29,28 @@ const int kLogicalOutputTensor = 0;
 TfLiteStatus LogicalImpl(TfLiteContext* context, TfLiteNode* node,
                          bool (*func)(bool, bool)) {
   const TfLiteEvalTensor* input1 =
-      tflite::micro::GetEvalInput(context, node, kLogicalInputTensor1);
+      tflite_micro::micro::GetEvalInput(context, node, kLogicalInputTensor1);
   const TfLiteEvalTensor* input2 =
-      tflite::micro::GetEvalInput(context, node, kLogicalInputTensor2);
+      tflite_micro::micro::GetEvalInput(context, node, kLogicalInputTensor2);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kLogicalOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kLogicalOutputTensor);
 
-  if (tflite::micro::HaveSameShapes(input1, input2)) {
+  if (tflite_micro::micro::HaveSameShapes(input1, input2)) {
     reference_ops::BinaryFunction<bool, bool, bool>(
-        tflite::micro::GetTensorShape(input1),
-        tflite::micro::GetTensorData<bool>(input1),
-        tflite::micro::GetTensorShape(input2),
-        tflite::micro::GetTensorData<bool>(input2),
-        tflite::micro::GetTensorShape(output),
-        tflite::micro::GetTensorData<bool>(output), func);
+        tflite_micro::micro::GetTensorShape(input1),
+        tflite_micro::micro::GetTensorData<bool>(input1),
+        tflite_micro::micro::GetTensorShape(input2),
+        tflite_micro::micro::GetTensorData<bool>(input2),
+        tflite_micro::micro::GetTensorShape(output),
+        tflite_micro::micro::GetTensorData<bool>(output), func);
   } else {
     reference_ops::BroadcastBinaryFunction4DSlow<bool, bool, bool>(
-        tflite::micro::GetTensorShape(input1),
-        tflite::micro::GetTensorData<bool>(input1),
-        tflite::micro::GetTensorShape(input2),
-        tflite::micro::GetTensorData<bool>(input2),
-        tflite::micro::GetTensorShape(output),
-        tflite::micro::GetTensorData<bool>(output), func);
+        tflite_micro::micro::GetTensorShape(input1),
+        tflite_micro::micro::GetTensorData<bool>(input1),
+        tflite_micro::micro::GetTensorShape(input2),
+        tflite_micro::micro::GetTensorData<bool>(input2),
+        tflite_micro::micro::GetTensorShape(output),
+        tflite_micro::micro::GetTensorData<bool>(output), func);
   }
 
   return kTfLiteOk;
@@ -60,4 +60,4 @@ bool LogicalOr(bool x, bool y) { return x || y; }
 
 bool LogicalAnd(bool x, bool y) { return x && y; }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/logical_test.cc b/tensorflow/lite/micro/kernels/logical_test.cc
index eeab32ba..9cbe823e 100644
--- a/tensorflow/lite/micro/kernels/logical_test.cc
+++ b/tensorflow/lite/micro/kernels/logical_test.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -61,7 +61,7 @@ void TestLogicalOp(const TFLMRegistration& registration, int* input1_dims_data,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -71,7 +71,7 @@ TF_LITE_MICRO_TEST(LogicalOr) {
   const bool input2[] = {true, false, true, false};
   const bool golden[] = {true, false, true, true};
   bool output_data[4];
-  tflite::testing::TestLogicalOp(tflite::Register_LOGICAL_OR(), shape, input1,
+  tflite_micro::testing::TestLogicalOp(tflite_micro::Register_LOGICAL_OR(), shape, input1,
                                  shape, input2, shape, golden, output_data);
 }
 
@@ -82,7 +82,7 @@ TF_LITE_MICRO_TEST(BroadcastLogicalOr) {
   const bool input2[] = {false};
   const bool golden[] = {true, false, false, true};
   bool output_data[4];
-  tflite::testing::TestLogicalOp(tflite::Register_LOGICAL_OR(), input1_shape,
+  tflite_micro::testing::TestLogicalOp(tflite_micro::Register_LOGICAL_OR(), input1_shape,
                                  input1, input2_shape, input2, input1_shape,
                                  golden, output_data);
 }
@@ -93,7 +93,7 @@ TF_LITE_MICRO_TEST(LogicalAnd) {
   const bool input2[] = {true, false, true, false};
   const bool golden[] = {true, false, false, false};
   bool output_data[4];
-  tflite::testing::TestLogicalOp(tflite::Register_LOGICAL_AND(), shape, input1,
+  tflite_micro::testing::TestLogicalOp(tflite_micro::Register_LOGICAL_AND(), shape, input1,
                                  shape, input2, shape, golden, output_data);
 }
 
@@ -104,7 +104,7 @@ TF_LITE_MICRO_TEST(BroadcastLogicalAnd) {
   const bool input2[] = {true};
   const bool golden[] = {true, false, false, true};
   bool output_data[4];
-  tflite::testing::TestLogicalOp(tflite::Register_LOGICAL_AND(), input1_shape,
+  tflite_micro::testing::TestLogicalOp(tflite_micro::Register_LOGICAL_AND(), input1_shape,
                                  input1, input2_shape, input2, input1_shape,
                                  golden, output_data);
 }
diff --git a/tensorflow/lite/micro/kernels/logistic.cc b/tensorflow/lite/micro/kernels/logistic.cc
index da2b34f1..029814c4 100644
--- a/tensorflow/lite/micro/kernels/logistic.cc
+++ b/tensorflow/lite/micro/kernels/logistic.cc
@@ -27,7 +27,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/logistic.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 void* LogisticInit(TfLiteContext* context, const char* buffer, size_t length) {
@@ -37,9 +37,9 @@ void* LogisticInit(TfLiteContext* context, const char* buffer, size_t length) {
 
 TfLiteStatus LogisticEval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kLogisticInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kLogisticInputTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kLogisticOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kLogisticOutputTensor);
 
   TFLITE_DCHECK(node->user_data != nullptr);
   OpDataLogistic* data = static_cast<OpDataLogistic*>(node->user_data);
@@ -47,10 +47,10 @@ TfLiteStatus LogisticEval(TfLiteContext* context, TfLiteNode* node) {
   if (input->type == kTfLiteFloat32) {
     switch (output->type) {
       case kTfLiteFloat32: {
-        reference_ops::Logistic(tflite::micro::GetTensorShape(input),
-                                tflite::micro::GetTensorData<float>(input),
-                                tflite::micro::GetTensorShape(output),
-                                tflite::micro::GetTensorData<float>(output));
+        reference_ops::Logistic(tflite_micro::micro::GetTensorShape(input),
+                                tflite_micro::micro::GetTensorData<float>(input),
+                                tflite_micro::micro::GetTensorShape(output),
+                                tflite_micro::micro::GetTensorData<float>(output));
         return kTfLiteOk;
       }
       default:
@@ -65,8 +65,8 @@ TfLiteStatus LogisticEval(TfLiteContext* context, TfLiteNode* node) {
         reference_integer_ops::Logistic(
             data->input_multiplier, data->input_left_shift,
             NumElements(input->dims),
-            tflite::micro::GetTensorData<int16_t>(input),
-            tflite::micro::GetTensorData<int16_t>(output));
+            tflite_micro::micro::GetTensorData<int16_t>(input),
+            tflite_micro::micro::GetTensorData<int16_t>(output));
         return kTfLiteOk;
       }
       default:
@@ -82,8 +82,8 @@ TfLiteStatus LogisticEval(TfLiteContext* context, TfLiteNode* node) {
             data->input_zero_point, data->input_range_radius,
             data->input_multiplier, data->input_left_shift,
             NumElements(input->dims),
-            tflite::micro::GetTensorData<int8_t>(input),
-            tflite::micro::GetTensorData<int8_t>(output));
+            tflite_micro::micro::GetTensorData<int8_t>(input),
+            tflite_micro::micro::GetTensorData<int8_t>(output));
         return kTfLiteOk;
       }
       default:
@@ -106,6 +106,6 @@ TfLiteStatus LogisticEval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_LOGISTIC() {
-  return tflite::micro::RegisterOp(LogisticInit, LogisticPrepare, LogisticEval);
+  return tflite_micro::micro::RegisterOp(LogisticInit, LogisticPrepare, LogisticEval);
 }
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/logistic.h b/tensorflow/lite/micro/kernels/logistic.h
index 1de0cdab..8c7a8ce8 100644
--- a/tensorflow/lite/micro/kernels/logistic.h
+++ b/tensorflow/lite/micro/kernels/logistic.h
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/c/builtin_op_data.h"
 #include "tensorflow/lite/c/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 extern const int kLogisticInputTensor;
 extern const int kLogisticOutputTensor;
 
@@ -38,5 +38,5 @@ TfLiteStatus CalculateArithmeticOpDataLogistic(TfLiteContext* context,
 
 TfLiteStatus LogisticPrepare(TfLiteContext* context, TfLiteNode* node);
 
-}  // namespace tflite
+}  // namespace tflite_micro
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_LOGISTIC_H_
diff --git a/tensorflow/lite/micro/kernels/logistic_common.cc b/tensorflow/lite/micro/kernels/logistic_common.cc
index 67bb397b..71212308 100644
--- a/tensorflow/lite/micro/kernels/logistic_common.cc
+++ b/tensorflow/lite/micro/kernels/logistic_common.cc
@@ -25,7 +25,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/kernels/logistic.h"
 
-namespace tflite {
+namespace tflite_micro {
 const int kLogisticInputTensor = 0;
 const int kLogisticOutputTensor = 0;
 
@@ -116,4 +116,4 @@ TfLiteStatus LogisticPrepare(TfLiteContext* context, TfLiteNode* node) {
   return CalculateArithmeticOpDataLogistic(context, node, data);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/logistic_test.cc b/tensorflow/lite/micro/kernels/logistic_test.cc
index 224e4e49..ad224a90 100644
--- a/tensorflow/lite/micro/kernels/logistic_test.cc
+++ b/tensorflow/lite/micro/kernels/logistic_test.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -145,7 +145,7 @@ void ValidateLogisticGoldens(TfLiteTensor* tensors, const int tensor_count,
   int outputs_array_data[] = {1, 1};
   TfLiteIntArray* outputs_array = IntArrayFromInts(outputs_array_data);
 
-  const TFLMRegistration registration = tflite::Register_LOGISTIC();
+  const TFLMRegistration registration = tflite_micro::Register_LOGISTIC();
   micro::KernelRunner runner(registration, tensors, tensor_count, inputs_array,
                              outputs_array, nullptr);
 
@@ -198,7 +198,7 @@ void TestLogisticQuantized(int* input_dims_data, const float* input_data,
                             output_zero_point),
   };
 
-  tflite::Quantize(golden, golden_quantized, output_elements_count,
+  tflite_micro::Quantize(golden, golden_quantized, output_elements_count,
                    output_scale, output_zero_point);
   ValidateLogisticGoldens(tensors, tensors_size, output_data, golden_quantized,
                           output_elements_count, tolerance);
@@ -206,55 +206,55 @@ void TestLogisticQuantized(int* input_dims_data, const float* input_data,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(LogisticFloatBasicShouldMatchGolden) {
-  float output_data[tflite::testing::flat_size_basic];
-  tflite::testing::TestLogisticFloat(
-      tflite::testing::shape_basic, tflite::testing::input_data_basic,
-      tflite::testing::golden_basic, tflite::testing::shape_basic, output_data);
+  float output_data[tflite_micro::testing::flat_size_basic];
+  tflite_micro::testing::TestLogisticFloat(
+      tflite_micro::testing::shape_basic, tflite_micro::testing::input_data_basic,
+      tflite_micro::testing::golden_basic, tflite_micro::testing::shape_basic, output_data);
 }
 
 TF_LITE_MICRO_TEST(LogisticQuantizedInt8BasicShouldMatchGolden) {
   const float input_scale = 0.1;
   const int input_zero_point = 0;
-  int8_t input_quantized[tflite::testing::flat_size_basic];
-  int8_t golden_quantized[tflite::testing::flat_size_basic];
-  int8_t output_data[tflite::testing::flat_size_basic];
+  int8_t input_quantized[tflite_micro::testing::flat_size_basic];
+  int8_t golden_quantized[tflite_micro::testing::flat_size_basic];
+  int8_t output_data[tflite_micro::testing::flat_size_basic];
 
-  tflite::testing::TestLogisticQuantized<int8_t>(
-      tflite::testing::shape_basic, tflite::testing::input_data_basic,
+  tflite_micro::testing::TestLogisticQuantized<int8_t>(
+      tflite_micro::testing::shape_basic, tflite_micro::testing::input_data_basic,
       input_quantized, input_scale, input_zero_point,
-      tflite::testing::golden_basic, golden_quantized,
-      tflite::testing::shape_basic,
-      tflite::testing::quantized_output_scale_int8,
-      tflite::testing::quantized_output_zero_point_int8, output_data, 1.0f);
+      tflite_micro::testing::golden_basic, golden_quantized,
+      tflite_micro::testing::shape_basic,
+      tflite_micro::testing::quantized_output_scale_int8,
+      tflite_micro::testing::quantized_output_zero_point_int8, output_data, 1.0f);
 }
 
 TF_LITE_MICRO_TEST(LogisticFloatWideRangeShouldMatchGolden) {
-  float output_data[tflite::testing::flat_size_wide_range];
-  tflite::testing::TestLogisticFloat(
-      tflite::testing::shape_wide_range, tflite::testing::input_data_wide_range,
-      tflite::testing::golden_wide_range, tflite::testing::shape_wide_range,
+  float output_data[tflite_micro::testing::flat_size_wide_range];
+  tflite_micro::testing::TestLogisticFloat(
+      tflite_micro::testing::shape_wide_range, tflite_micro::testing::input_data_wide_range,
+      tflite_micro::testing::golden_wide_range, tflite_micro::testing::shape_wide_range,
       output_data);
 }
 
 TF_LITE_MICRO_TEST(LogisticQuantizedInt8WideRangeShouldMatchGolden) {
   const float input_scale = 1.0;
   const int input_zero_point = 0;
-  int8_t input_quantized[tflite::testing::flat_size_wide_range];
-  int8_t golden_quantized[tflite::testing::flat_size_wide_range];
-  int8_t output_data[tflite::testing::flat_size_wide_range];
+  int8_t input_quantized[tflite_micro::testing::flat_size_wide_range];
+  int8_t golden_quantized[tflite_micro::testing::flat_size_wide_range];
+  int8_t output_data[tflite_micro::testing::flat_size_wide_range];
 
-  tflite::testing::TestLogisticQuantized<int8_t>(
-      tflite::testing::shape_wide_range, tflite::testing::input_data_wide_range,
+  tflite_micro::testing::TestLogisticQuantized<int8_t>(
+      tflite_micro::testing::shape_wide_range, tflite_micro::testing::input_data_wide_range,
       input_quantized, input_scale, input_zero_point,
-      tflite::testing::golden_wide_range, golden_quantized,
-      tflite::testing::shape_wide_range,
-      tflite::testing::quantized_output_scale_int8,
-      tflite::testing::quantized_output_zero_point_int8, output_data, 1.0f);
+      tflite_micro::testing::golden_wide_range, golden_quantized,
+      tflite_micro::testing::shape_wide_range,
+      tflite_micro::testing::quantized_output_scale_int8,
+      tflite_micro::testing::quantized_output_zero_point_int8, output_data, 1.0f);
 }
 
 TF_LITE_MICRO_TEST(LogisticQuantizedInt16ShouldMatchGolden) {
@@ -262,15 +262,15 @@ TF_LITE_MICRO_TEST(LogisticQuantizedInt16ShouldMatchGolden) {
   const int input_zero_point = 0;
   const float output_scale = 2.f / 65536.f;
   const int output_zero_point = 0;
-  int16_t input_quantized[tflite::testing::int16_vec_size];
-  int16_t golden_quantized[tflite::testing::int16_vec_size];
-  int16_t output_data[tflite::testing::int16_vec_size];
+  int16_t input_quantized[tflite_micro::testing::int16_vec_size];
+  int16_t golden_quantized[tflite_micro::testing::int16_vec_size];
+  int16_t output_data[tflite_micro::testing::int16_vec_size];
 
-  tflite::testing::TestLogisticQuantized<int16_t>(
-      tflite::testing::shape_int16_vec, tflite::testing::int16_input_vec_fp,
+  tflite_micro::testing::TestLogisticQuantized<int16_t>(
+      tflite_micro::testing::shape_int16_vec, tflite_micro::testing::int16_input_vec_fp,
       input_quantized, input_scale, input_zero_point,
-      tflite::testing::int16_golden_vec_fp, golden_quantized,
-      tflite::testing::shape_int16_vec, output_scale, output_zero_point,
+      tflite_micro::testing::int16_golden_vec_fp, golden_quantized,
+      tflite_micro::testing::shape_int16_vec, output_scale, output_zero_point,
       output_data, 16.0f);
 }
 
diff --git a/tensorflow/lite/micro/kernels/lstm_eval.cc b/tensorflow/lite/micro/kernels/lstm_eval.cc
index 93d6bc7e..90941eca 100644
--- a/tensorflow/lite/micro/kernels/lstm_eval.cc
+++ b/tensorflow/lite/micro/kernels/lstm_eval.cc
@@ -26,7 +26,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/reference/tanh.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 LstmTensors::LstmTensors(TfLiteContext* context, TfLiteNode* node) {
   micro_context_ = GetMicroContext(context);
@@ -133,8 +133,8 @@ void Sigmoid(const RuntimeShape& data_shape, int16_t* data) {
   reference_integer_ops::Logistic(
       0 /*data->input_multiplier*/, 0 /*data->input_left_shift */,
       data_shape.FlatSize() /*NumElements(input->dims)*/,
-      data /* tflite::micro::GetTensorData<int16_t>(input) */,
-      data /*tflite::micro::GetTensorData<int16_t>(output) */);
+      data /* tflite_micro::micro::GetTensorData<int16_t>(input) */,
+      data /*tflite_micro::micro::GetTensorData<int16_t>(output) */);
 }
 
 void Sigmoid(const RuntimeShape& data_shape, float* data) {
@@ -192,7 +192,7 @@ void FullyConnected(const FullyConnectedParams& params,
                     const RuntimeShape& filter_shape, const int8_t* filter_data,
                     const RuntimeShape& bias_shape, const int32_t* bias_data,
                     const RuntimeShape& output_shape, int16_t* output_data) {
-  return tflite::reference_integer_ops::FullyConnected(
+  return tflite_micro::reference_integer_ops::FullyConnected(
       params, input_shape, input_data, filter_shape, filter_data, bias_shape,
       bias_data, output_shape, output_data);
 }
@@ -202,7 +202,7 @@ void FullyConnected(const FullyConnectedParams& params,
                     const RuntimeShape& filter_shape, const int8_t* filter_data,
                     const RuntimeShape& bias_shape, const int64_t* bias_data,
                     const RuntimeShape& output_shape, int16_t* output_data) {
-  return tflite::reference_integer_ops::FullyConnected(
+  return tflite_micro::reference_integer_ops::FullyConnected(
       params, input_shape, input_data, filter_shape, filter_data, bias_shape,
       bias_data, output_shape, output_data);
 }
@@ -212,7 +212,7 @@ void FullyConnected(const FullyConnectedParams& params,
                     const RuntimeShape& filter_shape, const float* filter_data,
                     const RuntimeShape& bias_shape, const float* bias_data,
                     const RuntimeShape& output_shape, float* output_data) {
-  return tflite::reference_ops::FullyConnected(
+  return tflite_micro::reference_ops::FullyConnected(
       params, input_shape, input_data, filter_shape, filter_data, bias_shape,
       bias_data, output_shape, output_data);
 }
@@ -292,4 +292,4 @@ RuntimeShape LstmStepManager::StateShape() const {
 }
 
 }  // namespace lstm_internal
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/lstm_eval.h b/tensorflow/lite/micro/kernels/lstm_eval.h
index 62bc6354..8db0971f 100644
--- a/tensorflow/lite/micro/kernels/lstm_eval.h
+++ b/tensorflow/lite/micro/kernels/lstm_eval.h
@@ -28,7 +28,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/lstm_shared.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // Interface to access all the TempTfLiteTensors of the LSTM kernel during the
 // preparation phase. Can only be constructed through the constructor to avoid
@@ -100,7 +100,7 @@ TfLiteStatus CreateGateParams(
     const TfLiteTensor* hidden_state_bias,
     /*Scale of the fc output (input to non-linear activation)*/
     const float nonlinear_activation_input_scale, const TfLiteType cell_type,
-    const tflite::GateParameters& gate_params);
+    const tflite_micro::GateParameters& gate_params);
 
 // Create parameters for element wise multiplication that happens in a) cell
 // state update ; b) hidden state update
@@ -108,7 +108,7 @@ TfLiteStatus CreateGateParams(
 // are required for input. However, during the hidden state update phase, the
 // output is the updated hidden state, which is asymmetrically quantized. Thus
 // output may require zero point
-tflite::ArithmeticParams CreateInterGateMulParams(const float input1_scale,
+tflite_micro::ArithmeticParams CreateInterGateMulParams(const float input1_scale,
                                                   const float input2_scale,
                                                   const float output_scale,
                                                   const TfLiteType output_type,
@@ -121,11 +121,11 @@ CellStateInfo CreateLstmCellStateInfo(const float cell_state_scale,
                                       const float cell_clip);
 
 CellStateInfo CreateLstmCellStateInfoFloat(const float cell_clip);
-tflite::FullyConnectedParams CreateFCParamsFloat();
+tflite_micro::FullyConnectedParams CreateFCParamsFloat();
 
-tflite::GateParameters CreateGateParamsFloat();
+tflite_micro::GateParameters CreateGateParamsFloat();
 
-tflite::ArithmeticParams CreateInterGateMulParamsFloat();
+tflite_micro::ArithmeticParams CreateInterGateMulParamsFloat();
 
 TfLiteStatus PrepareGateParametersFloat(TfLiteContext* context,
                                         const LstmTensors& lstm_tensors,
@@ -267,29 +267,29 @@ void CalculateLstmGate(
   const auto gate_output_shape = step_info.StateShape();
   // Check offset validity to avoid memory overflow
   TFLITE_DCHECK_LE(step_info.InputOffset() + step_info.InputShape().FlatSize(),
-                   tflite::micro::GetTensorShape(input).FlatSize());
+                   tflite_micro::micro::GetTensorShape(input).FlatSize());
   TFLITE_DCHECK_LE(
       step_info.HiddenStateOffset() + step_info.StateShape().FlatSize(),
-      tflite::micro::GetTensorShape(recurrent).FlatSize());
+      tflite_micro::micro::GetTensorShape(recurrent).FlatSize());
 
   // Input FC
   FullyConnected(gate_params.input_fc_params, step_info.InputShape(),
-                 tflite::micro::GetTensorData<ActivationType>(input) +
+                 tflite_micro::micro::GetTensorData<ActivationType>(input) +
                      step_info.InputOffset(),
                  micro::GetTensorShape(input_weight),
-                 tflite::micro::GetTensorData<WeightType>(input_weight),
-                 tflite::micro::GetTensorShape(input_bias),
-                 tflite::micro::GetOptionalTensorData<BiasType>(input_bias),
+                 tflite_micro::micro::GetTensorData<WeightType>(input_weight),
+                 tflite_micro::micro::GetTensorShape(input_bias),
+                 tflite_micro::micro::GetOptionalTensorData<BiasType>(input_bias),
                  gate_output_shape, gate_output);
 
   // Recurrent FC
   FullyConnected(gate_params.recurrent_fc_params, step_info.StateShape(),
-                 tflite::micro::GetTensorData<ActivationType>(recurrent) +
+                 tflite_micro::micro::GetTensorData<ActivationType>(recurrent) +
                      step_info.HiddenStateOffset(),
-                 tflite::micro::GetTensorShape(recurrent_weight),
-                 tflite::micro::GetTensorData<WeightType>(recurrent_weight),
-                 tflite::micro::GetTensorShape(recurrent_bias),
-                 tflite::micro::GetOptionalTensorData<BiasType>(recurrent_bias),
+                 tflite_micro::micro::GetTensorShape(recurrent_weight),
+                 tflite_micro::micro::GetTensorData<WeightType>(recurrent_weight),
+                 tflite_micro::micro::GetTensorShape(recurrent_bias),
+                 tflite_micro::micro::GetOptionalTensorData<BiasType>(recurrent_bias),
                  gate_output_shape, fc_output_buffer);
 
   AddElementWise(gate_output, fc_output_buffer,
@@ -329,31 +329,31 @@ void UpdateLstmCell(const LstmStepManager& step_info,
   // Check offset validity to avoid memory overflow
   TFLITE_DCHECK_LE(
       step_info.CellStateOffset() + step_info.StateShape().FlatSize(),
-      tflite::micro::GetTensorShape(cell_state).FlatSize());
+      tflite_micro::micro::GetTensorShape(cell_state).FlatSize());
 
   auto cell_state_shape = step_info.StateShape();
   // Forget Gate x Cell State
   Mul(cell_state_shape, forget_cell_mul_params, forget_gate_output,
-      tflite::micro::GetTensorData<CellType>(cell_state) +
+      tflite_micro::micro::GetTensorData<CellType>(cell_state) +
           step_info.CellStateOffset(),
-      tflite::micro::GetTensorData<CellType>(cell_state) +
+      tflite_micro::micro::GetTensorData<CellType>(cell_state) +
           step_info.CellStateOffset());
   // Input Gate x Cell Gate
   Mul(cell_state_shape, input_mul_params, input_gate_output, cell_gate_output,
       buffer);
 
   // Update the cell state
-  AddElementWise(tflite::micro::GetTensorData<CellType>(cell_state) +
+  AddElementWise(tflite_micro::micro::GetTensorData<CellType>(cell_state) +
                      step_info.CellStateOffset(),
                  buffer,
                  /*n_batch=*/cell_state_shape.DimsData()[0],
                  /*n_state=*/cell_state_shape.DimsData()[1],
-                 tflite::micro::GetTensorData<CellType>(cell_state) +
+                 tflite_micro::micro::GetTensorData<CellType>(cell_state) +
                      step_info.CellStateOffset());
 
   if (cell_state_info.cell_clip > 0) {
     Clipping(cell_state_shape.FlatSize(), cell_state_info,
-             tflite::micro::GetTensorData<CellType>(cell_state) +
+             tflite_micro::micro::GetTensorData<CellType>(cell_state) +
                  step_info.CellStateOffset());
   }
 }
@@ -371,21 +371,21 @@ void UpdateLstmHidden(const LstmStepManager& step_info,
   // Check offset validity to avoid memory overflow
   TFLITE_DCHECK_LE(
       step_info.CellStateOffset() + step_info.StateShape().FlatSize(),
-      tflite::micro::GetTensorShape(cell_state).FlatSize());
+      tflite_micro::micro::GetTensorShape(cell_state).FlatSize());
   TFLITE_DCHECK_LE(
       step_info.HiddenStateOffset() + step_info.StateShape().FlatSize(),
-      tflite::micro::GetTensorShape(hidden_state).FlatSize());
+      tflite_micro::micro::GetTensorShape(hidden_state).FlatSize());
 
   auto cell_state_shape = step_info.StateShape();
   CellType* cell_state_data =
-      tflite::micro::GetTensorData<CellType>(cell_state) +
+      tflite_micro::micro::GetTensorData<CellType>(cell_state) +
       step_info.CellStateOffset();
   // Tanh(cell_state)
   Tanh(cell_state_scale_power, cell_state_shape, cell_state_data,
        cell_state_shape, buffer);
   // Update the hidden state
   Mul(cell_state_shape, mul_params, buffer, output_gate_output,
-      tflite::micro::GetTensorData<ActivationType>(hidden_state) +
+      tflite_micro::micro::GetTensorData<ActivationType>(hidden_state) +
           step_info.HiddenStateOffset());
 }
 
@@ -400,13 +400,13 @@ void LstmStep(const LstmStepManager& step_info, const OpDataLSTM& op_data,
   CalculateLstmGate<ActivationType, WeightType, CellType, BiasType>(
       step_info, op_data.forget_gate_parameters,
       // Input FC
-      kernel_content.GetInternalTensor(tflite::kLstmInputTensor),
-      kernel_content.GetInternalTensor(tflite::kLstmInputToForgetWeightsTensor),
-      kernel_content.GetInternalTensor(tflite::kLstmForgetGateBiasTensor),
+      kernel_content.GetInternalTensor(tflite_micro::kLstmInputTensor),
+      kernel_content.GetInternalTensor(tflite_micro::kLstmInputToForgetWeightsTensor),
+      kernel_content.GetInternalTensor(tflite_micro::kLstmForgetGateBiasTensor),
       // Recurrent FC
       kernel_content.HiddenStateTensor(),
       kernel_content.GetInternalTensor(
-          tflite::kLstmRecurrentToForgetWeightsTensor),
+          tflite_micro::kLstmRecurrentToForgetWeightsTensor),
       /*recurrent_bias*/ nullptr,
       // Output
       forget_gate_output,
@@ -418,13 +418,13 @@ void LstmStep(const LstmStepManager& step_info, const OpDataLSTM& op_data,
   CalculateLstmGate<ActivationType, WeightType, CellType, BiasType>(
       step_info, op_data.input_gate_parameters,
       // Input FC
-      kernel_content.GetInternalTensor(tflite::kLstmInputTensor),
-      kernel_content.GetInternalTensor(tflite::kLstmInputToInputWeightsTensor),
-      kernel_content.GetInternalTensor(tflite::kLstmInputGateBiasTensor),
+      kernel_content.GetInternalTensor(tflite_micro::kLstmInputTensor),
+      kernel_content.GetInternalTensor(tflite_micro::kLstmInputToInputWeightsTensor),
+      kernel_content.GetInternalTensor(tflite_micro::kLstmInputGateBiasTensor),
       // Recurrent FC
       kernel_content.HiddenStateTensor(),
       kernel_content.GetInternalTensor(
-          tflite::kLstmRecurrentToInputWeightsTensor),
+          tflite_micro::kLstmRecurrentToInputWeightsTensor),
       /*recurrent_bias*/ nullptr,
       // Output
       input_gate_output,
@@ -436,13 +436,13 @@ void LstmStep(const LstmStepManager& step_info, const OpDataLSTM& op_data,
   CalculateLstmGate<ActivationType, WeightType, CellType, BiasType>(
       step_info, op_data.cell_gate_parameters,
       // Input FC
-      kernel_content.GetInternalTensor(tflite::kLstmInputTensor),
-      kernel_content.GetInternalTensor(tflite::kLstmInputToCellWeightsTensor),
-      kernel_content.GetInternalTensor(tflite::kLstmCellGateBiasTensor),
+      kernel_content.GetInternalTensor(tflite_micro::kLstmInputTensor),
+      kernel_content.GetInternalTensor(tflite_micro::kLstmInputToCellWeightsTensor),
+      kernel_content.GetInternalTensor(tflite_micro::kLstmCellGateBiasTensor),
       // Recurrent FC
       kernel_content.HiddenStateTensor(),
       kernel_content.GetInternalTensor(
-          tflite::kLstmRecurrentToCellWeightsTensor),
+          tflite_micro::kLstmRecurrentToCellWeightsTensor),
       /*recurrent_bias*/ nullptr,
       // Output
       cell_gate_output,
@@ -465,13 +465,13 @@ void LstmStep(const LstmStepManager& step_info, const OpDataLSTM& op_data,
   CalculateLstmGate<ActivationType, WeightType, CellType, BiasType>(
       step_info, op_data.output_gate_parameters,
       // Input FC
-      kernel_content.GetInternalTensor(tflite::kLstmInputTensor),
-      kernel_content.GetInternalTensor(tflite::kLstmInputToOutputWeightsTensor),
-      kernel_content.GetInternalTensor(tflite::kLstmOutputGateBiasTensor),
+      kernel_content.GetInternalTensor(tflite_micro::kLstmInputTensor),
+      kernel_content.GetInternalTensor(tflite_micro::kLstmInputToOutputWeightsTensor),
+      kernel_content.GetInternalTensor(tflite_micro::kLstmOutputGateBiasTensor),
       // Recurrent FC
       kernel_content.HiddenStateTensor(),
       kernel_content.GetInternalTensor(
-          tflite::kLstmRecurrentToOutputWeightsTensor),
+          tflite_micro::kLstmRecurrentToOutputWeightsTensor),
       /*recurrent_bias*/ nullptr,
       // Output
       output_gate_output,
@@ -479,7 +479,7 @@ void LstmStep(const LstmStepManager& step_info, const OpDataLSTM& op_data,
       gate_internal_buffer, kTfLiteActSigmoid);
 
   CellType* tanh_activated_cell_buffer = buffers.buffer0;  // reuse buffer
-  tflite::lstm_internal::UpdateLstmHidden<CellType, ActivationType>(
+  tflite_micro::lstm_internal::UpdateLstmHidden<CellType, ActivationType>(
       step_info, kernel_content.CellStateTensor(),
       kernel_content.HiddenStateTensor(), output_gate_output,
       inter_gate_params.output_mul_params,
@@ -490,13 +490,13 @@ void LstmStep(const LstmStepManager& step_info, const OpDataLSTM& op_data,
   // Check offset validity to avoid memory overflow
   TFLITE_DCHECK_LE(
       step_info.OutputOffset() + step_info.StateShape().FlatSize(),
-      tflite::micro::GetTensorShape(kernel_content.output_tensor).FlatSize());
+      tflite_micro::micro::GetTensorShape(kernel_content.output_tensor).FlatSize());
   // record the output (from the updated hidden state)
-  ActivationType* output_ptr = tflite::micro::GetTensorData<ActivationType>(
+  ActivationType* output_ptr = tflite_micro::micro::GetTensorData<ActivationType>(
       kernel_content.output_tensor);
   const auto* hidden_state = kernel_content.HiddenStateTensor();
   std::memcpy(output_ptr + step_info.OutputOffset(),
-              tflite::micro::GetTensorData<ActivationType>(hidden_state) +
+              tflite_micro::micro::GetTensorData<ActivationType>(hidden_state) +
                   step_info.HiddenStateOffset(),
               step_info.StateShape().FlatSize() * sizeof(ActivationType));
 }
@@ -536,6 +536,6 @@ TfLiteStatus EvalLstm(const OpDataLSTM& op_data,
   }
   return kTfLiteOk;
 }
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_LSTM_EVAL_16ACT_H_
diff --git a/tensorflow/lite/micro/kernels/lstm_eval_common.cc b/tensorflow/lite/micro/kernels/lstm_eval_common.cc
index 9631b4c1..a7040d4e 100644
--- a/tensorflow/lite/micro/kernels/lstm_eval_common.cc
+++ b/tensorflow/lite/micro/kernels/lstm_eval_common.cc
@@ -18,7 +18,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/fully_connected.h"
 #include "tensorflow/lite/micro/kernels/lstm_eval.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // Deduce the size information (Batch (B), Time Steps (T), Input dimension (I),
 // State dimension (S)) that defines the LSTM using the input and hidden state
@@ -111,7 +111,7 @@ TfLiteStatus CreateGateParams(
     const TfLiteTensor* hidden_state_bias,
     /*Scale of the fc output (input to non-linear activation)*/
     const float nonlinear_activation_input_scale, const TfLiteType cell_type,
-    tflite::GateParameters& gate_params) {
+    tflite_micro::GateParameters& gate_params) {
   // A temp tflite tensor to represent the output of fc operation. Only the data
   // type and quantization parameters are set since it is only used for
   // parameter calculations
@@ -121,7 +121,7 @@ TfLiteStatus CreateGateParams(
   fc_output_temp.params.zero_point = 0;  // symmetrical quantized
 
   // A temp fc opdata to reuse the helper function on creating fc parameters
-  tflite::OpDataFullyConnected fc_data_temp;
+  tflite_micro::OpDataFullyConnected fc_data_temp;
   // TODO(b/265853320): due to the lack of precision for the float scale,
   // scale_diff / output_scale <= 0.02 (potentially requires 1e-8 precision) can
   // not be satisified for the bias. Here we rely on the correctiveness of the
@@ -148,12 +148,12 @@ TfLiteStatus CreateGateParams(
 // are required for input. However, during the hidden state update phase, the
 // output is the updated hidden state, which is asymmetrically quantized. Thus
 // output may require zero point
-tflite::ArithmeticParams CreateInterGateMulParams(const float input1_scale,
+tflite_micro::ArithmeticParams CreateInterGateMulParams(const float input1_scale,
                                                   const float input2_scale,
                                                   const float output_scale,
                                                   const TfLiteType output_type,
                                                   const int output_zp) {
-  tflite::ArithmeticParams op_params = {};
+  tflite_micro::ArithmeticParams op_params = {};
   if (output_type == kTfLiteInt16) {
     op_params.quantized_activation_min = std::numeric_limits<int16_t>::min();
     op_params.quantized_activation_max = std::numeric_limits<int16_t>::max();
@@ -184,7 +184,7 @@ CellStateInfo CreateLstmCellStateInfo(const float cell_state_scale,
   CellStateInfo cell_state_info;
   // cell_state_scale_power: 2^-cell_state_scale_power = cell state scale
   int buffer;
-  tflite::CheckedLog2(cell_state_scale, &buffer);
+  tflite_micro::CheckedLog2(cell_state_scale, &buffer);
   cell_state_info.cell_state_scale_power = buffer;
   // Cell state specifics
   cell_state_info.cell_clip = cell_clip;
@@ -205,22 +205,22 @@ CellStateInfo CreateLstmCellStateInfoFloat(const float cell_clip) {
   return cell_state_info;
 }
 
-tflite::FullyConnectedParams CreateFCParamsFloat() {
+tflite_micro::FullyConnectedParams CreateFCParamsFloat() {
   FullyConnectedParams op_params;
   CalculateActivationRange(kTfLiteActNone, &op_params.float_activation_min,
                            &op_params.float_activation_max);
   return op_params;
 }
 
-tflite::GateParameters CreateGateParamsFloat() {
-  tflite::GateParameters gate_params = {};
+tflite_micro::GateParameters CreateGateParamsFloat() {
+  tflite_micro::GateParameters gate_params = {};
   gate_params.input_fc_params = CreateFCParamsFloat();
   gate_params.recurrent_fc_params = CreateFCParamsFloat();
   return gate_params;
 }
 
-tflite::ArithmeticParams CreateInterGateMulParamsFloat() {
-  tflite::ArithmeticParams op_params = {};
+tflite_micro::ArithmeticParams CreateInterGateMulParamsFloat() {
+  tflite_micro::ArithmeticParams op_params = {};
   CalculateActivationRange(kTfLiteActNone, &op_params.float_activation_min,
                            &op_params.float_activation_max);
   return op_params;
@@ -316,11 +316,11 @@ LSTMKernelContents CreateLSTMKernelContent(TfLiteContext* context,
   // Point to correct tensors
   for (size_t i = 0; i < 24; i++) {
     kernel_content.internal_tensors[i] =
-        tflite::micro::GetMutableEvalInput(context, node, i);
+        tflite_micro::micro::GetMutableEvalInput(context, node, i);
   }
   // Output tensor
-  kernel_content.output_tensor = tflite::micro::GetEvalOutput(context, node, 0);
+  kernel_content.output_tensor = tflite_micro::micro::GetEvalOutput(context, node, 0);
   return kernel_content;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/lstm_eval_test.cc b/tensorflow/lite/micro/kernels/lstm_eval_test.cc
index 53c0d7c4..5f60e865 100644
--- a/tensorflow/lite/micro/kernels/lstm_eval_test.cc
+++ b/tensorflow/lite/micro/kernels/lstm_eval_test.cc
@@ -41,66 +41,66 @@ TF_LITE_MICRO_TESTS_BEGIN
 // kernel is reconciled with reference kernel
 #if !defined(XTENSA)
 TF_LITE_MICRO_TEST(CheckGateOutputFloat) {
-  const tflite::testing::GateOutputCheckData<4, 4> gate_output_data =
-      tflite::testing::Get2X2GateOutputCheckData();
-  tflite::testing::LstmNodeContent<float, float, float, float, 2, 3, 2, 2>
-      float_node_contents = tflite::testing::Create2x3x2X2FloatNodeContents(
+  const tflite_micro::testing::GateOutputCheckData<4, 4> gate_output_data =
+      tflite_micro::testing::Get2X2GateOutputCheckData();
+  tflite_micro::testing::LstmNodeContent<float, float, float, float, 2, 3, 2, 2>
+      float_node_contents = tflite_micro::testing::Create2x3x2X2FloatNodeContents(
           gate_output_data.input_data, gate_output_data.hidden_state,
           gate_output_data.cell_state);
 
   // Forget gate
-  tflite::testing::TestCalculateLstmGateFloat<2, 2>(
-      float_node_contents.GetEvalTensor(tflite::kLstmInputTensor),
+  tflite_micro::testing::TestCalculateLstmGateFloat<2, 2>(
+      float_node_contents.GetEvalTensor(tflite_micro::kLstmInputTensor),
       float_node_contents.GetEvalTensor(
-          tflite::kLstmInputToForgetWeightsTensor),
-      float_node_contents.GetEvalTensor(tflite::kLstmForgetGateBiasTensor),
+          tflite_micro::kLstmInputToForgetWeightsTensor),
+      float_node_contents.GetEvalTensor(tflite_micro::kLstmForgetGateBiasTensor),
       // Recurrent FC
       float_node_contents.HiddenStateEvalTensor(),
       float_node_contents.GetEvalTensor(
-          tflite::kLstmRecurrentToForgetWeightsTensor),
+          tflite_micro::kLstmRecurrentToForgetWeightsTensor),
       nullptr,  // bias fused to activation FC,
       // Result comparison
       kTfLiteActSigmoid, gate_output_data.expected_forget_gate_output,
       kTestFloatTolerance);
 
   // Input gate
-  tflite::testing::TestCalculateLstmGateFloat<2, 2>(
-      float_node_contents.GetEvalTensor(tflite::kLstmInputTensor),
-      float_node_contents.GetEvalTensor(tflite::kLstmInputToInputWeightsTensor),
-      float_node_contents.GetEvalTensor(tflite::kLstmInputGateBiasTensor),
+  tflite_micro::testing::TestCalculateLstmGateFloat<2, 2>(
+      float_node_contents.GetEvalTensor(tflite_micro::kLstmInputTensor),
+      float_node_contents.GetEvalTensor(tflite_micro::kLstmInputToInputWeightsTensor),
+      float_node_contents.GetEvalTensor(tflite_micro::kLstmInputGateBiasTensor),
       // Recurrent FC
       float_node_contents.HiddenStateEvalTensor(),
       float_node_contents.GetEvalTensor(
-          tflite::kLstmRecurrentToInputWeightsTensor),
+          tflite_micro::kLstmRecurrentToInputWeightsTensor),
       nullptr,  // bias fused to activation FC,
       // Result comparison
       kTfLiteActSigmoid, gate_output_data.expected_input_gate_output,
       kTestFloatTolerance);
 
   // Output gate
-  tflite::testing::TestCalculateLstmGateFloat<2, 2>(
-      float_node_contents.GetEvalTensor(tflite::kLstmInputTensor),
+  tflite_micro::testing::TestCalculateLstmGateFloat<2, 2>(
+      float_node_contents.GetEvalTensor(tflite_micro::kLstmInputTensor),
       float_node_contents.GetEvalTensor(
-          tflite::kLstmInputToOutputWeightsTensor),
-      float_node_contents.GetEvalTensor(tflite::kLstmOutputGateBiasTensor),
+          tflite_micro::kLstmInputToOutputWeightsTensor),
+      float_node_contents.GetEvalTensor(tflite_micro::kLstmOutputGateBiasTensor),
       // Recurrent FC
       float_node_contents.HiddenStateEvalTensor(),
       float_node_contents.GetEvalTensor(
-          tflite::kLstmRecurrentToOutputWeightsTensor),
+          tflite_micro::kLstmRecurrentToOutputWeightsTensor),
       nullptr,  // bias fused to activation FC,
       // Result comparison
       kTfLiteActSigmoid, gate_output_data.expected_output_gate_output,
       kTestFloatTolerance);
 
   // Cell gate
-  tflite::testing::TestCalculateLstmGateFloat<2, 2>(
-      float_node_contents.GetEvalTensor(tflite::kLstmInputTensor),
-      float_node_contents.GetEvalTensor(tflite::kLstmInputToCellWeightsTensor),
-      float_node_contents.GetEvalTensor(tflite::kLstmCellGateBiasTensor),
+  tflite_micro::testing::TestCalculateLstmGateFloat<2, 2>(
+      float_node_contents.GetEvalTensor(tflite_micro::kLstmInputTensor),
+      float_node_contents.GetEvalTensor(tflite_micro::kLstmInputToCellWeightsTensor),
+      float_node_contents.GetEvalTensor(tflite_micro::kLstmCellGateBiasTensor),
       // Recurrent FC
       float_node_contents.HiddenStateEvalTensor(),
       float_node_contents.GetEvalTensor(
-          tflite::kLstmRecurrentToCellWeightsTensor),
+          tflite_micro::kLstmRecurrentToCellWeightsTensor),
       nullptr,  // bias fused to activation FC,
       // Result comparison
       float_node_contents.BuiltinData().activation,
@@ -108,25 +108,25 @@ TF_LITE_MICRO_TEST(CheckGateOutputFloat) {
 }
 
 TF_LITE_MICRO_TEST(CheckGateOutputInt8) {
-  const tflite::testing::GateOutputCheckData<4, 4> gate_output_data =
-      tflite::testing::Get2X2GateOutputCheckData();
-  tflite::testing::LstmNodeContent<int8_t, int8_t, int32_t, int16_t, 2, 3, 2, 2>
-      int8_node_contents = tflite::testing::Create2x3x2X2Int8NodeContents(
+  const tflite_micro::testing::GateOutputCheckData<4, 4> gate_output_data =
+      tflite_micro::testing::Get2X2GateOutputCheckData();
+  tflite_micro::testing::LstmNodeContent<int8_t, int8_t, int32_t, int16_t, 2, 3, 2, 2>
+      int8_node_contents = tflite_micro::testing::Create2x3x2X2Int8NodeContents(
           gate_output_data.input_data, gate_output_data.hidden_state,
           gate_output_data.cell_state);
 
   // Forget gate
   // Quantization performs badly here due to integer overflow!!!
   float tolerance = 1e-1f;
-  tflite::testing::TestCalculateLstmGateInteger<int8_t, int8_t, int32_t,
+  tflite_micro::testing::TestCalculateLstmGateInteger<int8_t, int8_t, int32_t,
                                                 int16_t, 2, 2>(
-      int8_node_contents.GetEvalTensor(tflite::kLstmInputTensor),
-      int8_node_contents.GetEvalTensor(tflite::kLstmInputToForgetWeightsTensor),
-      int8_node_contents.GetEvalTensor(tflite::kLstmForgetGateBiasTensor),
+      int8_node_contents.GetEvalTensor(tflite_micro::kLstmInputTensor),
+      int8_node_contents.GetEvalTensor(tflite_micro::kLstmInputToForgetWeightsTensor),
+      int8_node_contents.GetEvalTensor(tflite_micro::kLstmForgetGateBiasTensor),
       // Recurrent FC
       int8_node_contents.HiddenStateEvalTensor(),
       int8_node_contents.GetEvalTensor(
-          tflite::kLstmRecurrentToForgetWeightsTensor),
+          tflite_micro::kLstmRecurrentToForgetWeightsTensor),
       nullptr,  // bias fused to activation FC,
       // Quantization settings
       int8_node_contents.QuantizationSettings(),
@@ -138,15 +138,15 @@ TF_LITE_MICRO_TEST(CheckGateOutputInt8) {
   // Input gate
   // Quantization performs badly here due to integer overflow!!!
   tolerance = 1e-1f;
-  tflite::testing::TestCalculateLstmGateInteger<int8_t, int8_t, int32_t,
+  tflite_micro::testing::TestCalculateLstmGateInteger<int8_t, int8_t, int32_t,
                                                 int16_t, 2, 2>(
-      int8_node_contents.GetEvalTensor(tflite::kLstmInputTensor),
-      int8_node_contents.GetEvalTensor(tflite::kLstmInputToInputWeightsTensor),
-      int8_node_contents.GetEvalTensor(tflite::kLstmInputGateBiasTensor),
+      int8_node_contents.GetEvalTensor(tflite_micro::kLstmInputTensor),
+      int8_node_contents.GetEvalTensor(tflite_micro::kLstmInputToInputWeightsTensor),
+      int8_node_contents.GetEvalTensor(tflite_micro::kLstmInputGateBiasTensor),
       // Recurrent FC
       int8_node_contents.HiddenStateEvalTensor(),
       int8_node_contents.GetEvalTensor(
-          tflite::kLstmRecurrentToInputWeightsTensor),
+          tflite_micro::kLstmRecurrentToInputWeightsTensor),
       nullptr,  // bias fused to activation FC,
       // Quantization settings
       int8_node_contents.QuantizationSettings(),
@@ -157,15 +157,15 @@ TF_LITE_MICRO_TEST(CheckGateOutputInt8) {
 
   // Output gate
   tolerance = 1e-2f;
-  tflite::testing::TestCalculateLstmGateInteger<int8_t, int8_t, int32_t,
+  tflite_micro::testing::TestCalculateLstmGateInteger<int8_t, int8_t, int32_t,
                                                 int16_t, 2, 2>(
-      int8_node_contents.GetEvalTensor(tflite::kLstmInputTensor),
-      int8_node_contents.GetEvalTensor(tflite::kLstmInputToOutputWeightsTensor),
-      int8_node_contents.GetEvalTensor(tflite::kLstmOutputGateBiasTensor),
+      int8_node_contents.GetEvalTensor(tflite_micro::kLstmInputTensor),
+      int8_node_contents.GetEvalTensor(tflite_micro::kLstmInputToOutputWeightsTensor),
+      int8_node_contents.GetEvalTensor(tflite_micro::kLstmOutputGateBiasTensor),
       // Recurrent FC
       int8_node_contents.HiddenStateEvalTensor(),
       int8_node_contents.GetEvalTensor(
-          tflite::kLstmRecurrentToOutputWeightsTensor),
+          tflite_micro::kLstmRecurrentToOutputWeightsTensor),
       nullptr,  // bias fused to activation FC,
       // Quantization settings
       int8_node_contents.QuantizationSettings(),
@@ -176,15 +176,15 @@ TF_LITE_MICRO_TEST(CheckGateOutputInt8) {
 
   // Cell gate
   tolerance = 1e-2f;
-  tflite::testing::TestCalculateLstmGateInteger<int8_t, int8_t, int32_t,
+  tflite_micro::testing::TestCalculateLstmGateInteger<int8_t, int8_t, int32_t,
                                                 int16_t, 2, 2>(
-      int8_node_contents.GetEvalTensor(tflite::kLstmInputTensor),
-      int8_node_contents.GetEvalTensor(tflite::kLstmInputToCellWeightsTensor),
-      int8_node_contents.GetEvalTensor(tflite::kLstmCellGateBiasTensor),
+      int8_node_contents.GetEvalTensor(tflite_micro::kLstmInputTensor),
+      int8_node_contents.GetEvalTensor(tflite_micro::kLstmInputToCellWeightsTensor),
+      int8_node_contents.GetEvalTensor(tflite_micro::kLstmCellGateBiasTensor),
       // Recurrent FC
       int8_node_contents.HiddenStateEvalTensor(),
       int8_node_contents.GetEvalTensor(
-          tflite::kLstmRecurrentToCellWeightsTensor),
+          tflite_micro::kLstmRecurrentToCellWeightsTensor),
       nullptr,  // bias fused to activation FC,
       // Quantization settings
       int8_node_contents.QuantizationSettings(),
@@ -195,27 +195,27 @@ TF_LITE_MICRO_TEST(CheckGateOutputInt8) {
 }
 
 TF_LITE_MICRO_TEST(CheckGateOutputInt16) {
-  const tflite::testing::GateOutputCheckData<4, 4> gate_output_data =
-      tflite::testing::Get2X2GateOutputCheckData();
-  tflite::testing::LstmNodeContent<int16_t, int8_t, int64_t, int16_t, 2, 3, 2,
+  const tflite_micro::testing::GateOutputCheckData<4, 4> gate_output_data =
+      tflite_micro::testing::Get2X2GateOutputCheckData();
+  tflite_micro::testing::LstmNodeContent<int16_t, int8_t, int64_t, int16_t, 2, 3, 2,
                                    2>
-      int16_node_contents = tflite::testing::Create2x3x2X2Int16NodeContents(
+      int16_node_contents = tflite_micro::testing::Create2x3x2X2Int16NodeContents(
           gate_output_data.input_data, gate_output_data.hidden_state,
           gate_output_data.cell_state);
 
   // Forget gate
   // Quantization performs badly here due to integer overflow (from batch2)!!!
   float tolerance = 1e-1f;
-  tflite::testing::TestCalculateLstmGateInteger<int16_t, int8_t, int64_t,
+  tflite_micro::testing::TestCalculateLstmGateInteger<int16_t, int8_t, int64_t,
                                                 int16_t, 2, 2>(
-      int16_node_contents.GetEvalTensor(tflite::kLstmInputTensor),
+      int16_node_contents.GetEvalTensor(tflite_micro::kLstmInputTensor),
       int16_node_contents.GetEvalTensor(
-          tflite::kLstmInputToForgetWeightsTensor),
-      int16_node_contents.GetEvalTensor(tflite::kLstmForgetGateBiasTensor),
+          tflite_micro::kLstmInputToForgetWeightsTensor),
+      int16_node_contents.GetEvalTensor(tflite_micro::kLstmForgetGateBiasTensor),
       // Recurrent FC
       int16_node_contents.HiddenStateEvalTensor(),
       int16_node_contents.GetEvalTensor(
-          tflite::kLstmRecurrentToForgetWeightsTensor),
+          tflite_micro::kLstmRecurrentToForgetWeightsTensor),
       nullptr,  // bias fused to activation FC,
       // Quantization settings
       int16_node_contents.QuantizationSettings(),
@@ -227,15 +227,15 @@ TF_LITE_MICRO_TEST(CheckGateOutputInt16) {
   // Input gate
   // Quantization performs badly here due to integer overflow (from batch2)!!!
   tolerance = 1e-1f;
-  tflite::testing::TestCalculateLstmGateInteger<int16_t, int8_t, int64_t,
+  tflite_micro::testing::TestCalculateLstmGateInteger<int16_t, int8_t, int64_t,
                                                 int16_t, 2, 2>(
-      int16_node_contents.GetEvalTensor(tflite::kLstmInputTensor),
-      int16_node_contents.GetEvalTensor(tflite::kLstmInputToInputWeightsTensor),
-      int16_node_contents.GetEvalTensor(tflite::kLstmInputGateBiasTensor),
+      int16_node_contents.GetEvalTensor(tflite_micro::kLstmInputTensor),
+      int16_node_contents.GetEvalTensor(tflite_micro::kLstmInputToInputWeightsTensor),
+      int16_node_contents.GetEvalTensor(tflite_micro::kLstmInputGateBiasTensor),
       // Recurrent FC
       int16_node_contents.HiddenStateEvalTensor(),
       int16_node_contents.GetEvalTensor(
-          tflite::kLstmRecurrentToInputWeightsTensor),
+          tflite_micro::kLstmRecurrentToInputWeightsTensor),
       nullptr,  // bias fused to activation FC,
       // Quantization settings
       int16_node_contents.QuantizationSettings(),
@@ -247,16 +247,16 @@ TF_LITE_MICRO_TEST(CheckGateOutputInt16) {
   // Output gate
   // Quantization scale (theoritical lowest range) is at range 1e-5
   tolerance = 1e-4f;
-  tflite::testing::TestCalculateLstmGateInteger<int16_t, int8_t, int64_t,
+  tflite_micro::testing::TestCalculateLstmGateInteger<int16_t, int8_t, int64_t,
                                                 int16_t, 2, 2>(
-      int16_node_contents.GetEvalTensor(tflite::kLstmInputTensor),
+      int16_node_contents.GetEvalTensor(tflite_micro::kLstmInputTensor),
       int16_node_contents.GetEvalTensor(
-          tflite::kLstmInputToOutputWeightsTensor),
-      int16_node_contents.GetEvalTensor(tflite::kLstmOutputGateBiasTensor),
+          tflite_micro::kLstmInputToOutputWeightsTensor),
+      int16_node_contents.GetEvalTensor(tflite_micro::kLstmOutputGateBiasTensor),
       // Recurrent FC
       int16_node_contents.HiddenStateEvalTensor(),
       int16_node_contents.GetEvalTensor(
-          tflite::kLstmRecurrentToOutputWeightsTensor),
+          tflite_micro::kLstmRecurrentToOutputWeightsTensor),
       nullptr,  // bias fused to activation FC,
       // Quantization settings
       int16_node_contents.QuantizationSettings(),
@@ -267,15 +267,15 @@ TF_LITE_MICRO_TEST(CheckGateOutputInt16) {
 
   // Cell gate
   tolerance = 1e-4f;
-  tflite::testing::TestCalculateLstmGateInteger<int16_t, int8_t, int64_t,
+  tflite_micro::testing::TestCalculateLstmGateInteger<int16_t, int8_t, int64_t,
                                                 int16_t, 2, 2>(
-      int16_node_contents.GetEvalTensor(tflite::kLstmInputTensor),
-      int16_node_contents.GetEvalTensor(tflite::kLstmInputToCellWeightsTensor),
-      int16_node_contents.GetEvalTensor(tflite::kLstmCellGateBiasTensor),
+      int16_node_contents.GetEvalTensor(tflite_micro::kLstmInputTensor),
+      int16_node_contents.GetEvalTensor(tflite_micro::kLstmInputToCellWeightsTensor),
+      int16_node_contents.GetEvalTensor(tflite_micro::kLstmCellGateBiasTensor),
       // Recurrent FC
       int16_node_contents.HiddenStateEvalTensor(),
       int16_node_contents.GetEvalTensor(
-          tflite::kLstmRecurrentToCellWeightsTensor),
+          tflite_micro::kLstmRecurrentToCellWeightsTensor),
       nullptr,  // bias fused to activation FC,
       // Quantization settings
       int16_node_contents.QuantizationSettings(),
@@ -286,22 +286,22 @@ TF_LITE_MICRO_TEST(CheckGateOutputInt16) {
 }
 
 TF_LITE_MICRO_TEST(CheckCellStateUpdateFloat) {
-  const tflite::testing::GateOutputCheckData<4, 4> gate_output_data =
-      tflite::testing::Get2X2GateOutputCheckData();
-  tflite::testing::LstmNodeContent<float, float, float, float, 2, 3, 2, 2>
-      float_node_contents = tflite::testing::Create2x3x2X2FloatNodeContents(
+  const tflite_micro::testing::GateOutputCheckData<4, 4> gate_output_data =
+      tflite_micro::testing::Get2X2GateOutputCheckData();
+  tflite_micro::testing::LstmNodeContent<float, float, float, float, 2, 3, 2, 2>
+      float_node_contents = tflite_micro::testing::Create2x3x2X2FloatNodeContents(
           gate_output_data.input_data, gate_output_data.hidden_state,
           gate_output_data.cell_state);
 
-  tflite::testing::TestUpdateLstmCellFloat(
+  tflite_micro::testing::TestUpdateLstmCellFloat(
       gate_output_data, float_node_contents, kTestFloatTolerance);
 }
 
 TF_LITE_MICRO_TEST(CheckCellStateUpdateInt8) {
-  const tflite::testing::GateOutputCheckData<4, 4> gate_output_data =
-      tflite::testing::Get2X2GateOutputCheckData();
-  tflite::testing::LstmNodeContent<int8_t, int8_t, int32_t, int16_t, 2, 3, 2, 2>
-      int8_node_contents = tflite::testing::Create2x3x2X2Int8NodeContents(
+  const tflite_micro::testing::GateOutputCheckData<4, 4> gate_output_data =
+      tflite_micro::testing::Get2X2GateOutputCheckData();
+  tflite_micro::testing::LstmNodeContent<int8_t, int8_t, int32_t, int16_t, 2, 3, 2, 2>
+      int8_node_contents = tflite_micro::testing::Create2x3x2X2Int8NodeContents(
           gate_output_data.input_data, gate_output_data.hidden_state,
           gate_output_data.cell_state);
 
@@ -309,148 +309,148 @@ TF_LITE_MICRO_TEST(CheckCellStateUpdateInt8) {
   // quantization error of the clip value (~1e-5), but cannot actually reach
   // the precision due to integer overflow of the elements
   const float tolerance = 1e-3f;
-  tflite::testing::TestUpdateLstmCellInteger(gate_output_data,
+  tflite_micro::testing::TestUpdateLstmCellInteger(gate_output_data,
                                              int8_node_contents, tolerance);
 }
 
 TF_LITE_MICRO_TEST(CheckCellStateUpdateInt16) {
-  const tflite::testing::GateOutputCheckData<4, 4> gate_output_data =
-      tflite::testing::Get2X2GateOutputCheckData();
-  tflite::testing::LstmNodeContent<int16_t, int8_t, int64_t, int16_t, 2, 3, 2,
+  const tflite_micro::testing::GateOutputCheckData<4, 4> gate_output_data =
+      tflite_micro::testing::Get2X2GateOutputCheckData();
+  tflite_micro::testing::LstmNodeContent<int16_t, int8_t, int64_t, int16_t, 2, 3, 2,
                                    2>
-      int16_node_contents = tflite::testing::Create2x3x2X2Int16NodeContents(
+      int16_node_contents = tflite_micro::testing::Create2x3x2X2Int16NodeContents(
           gate_output_data.input_data, gate_output_data.hidden_state,
           gate_output_data.cell_state);
   // Very high precision. The error is introduced by the
   // quantization error of the clip value (~1e-5), but cannot actually reach
   // the precision due to integer overflow of the elements
   const float tolerance = 1e-3f;
-  tflite::testing::TestUpdateLstmCellInteger(gate_output_data,
+  tflite_micro::testing::TestUpdateLstmCellInteger(gate_output_data,
                                              int16_node_contents, tolerance);
 }
 
 TF_LITE_MICRO_TEST(CheckHiddenStateUpdateFloat) {
-  const tflite::testing::GateOutputCheckData<4, 4> gate_output_data =
-      tflite::testing::Get2X2GateOutputCheckData();
-  tflite::testing::LstmNodeContent<float, float, float, float, 2, 3, 2, 2>
-      float_node_contents = tflite::testing::Create2x3x2X2FloatNodeContents(
+  const tflite_micro::testing::GateOutputCheckData<4, 4> gate_output_data =
+      tflite_micro::testing::Get2X2GateOutputCheckData();
+  tflite_micro::testing::LstmNodeContent<float, float, float, float, 2, 3, 2, 2>
+      float_node_contents = tflite_micro::testing::Create2x3x2X2FloatNodeContents(
           gate_output_data.input_data, gate_output_data.hidden_state,
           gate_output_data.expected_updated_cell);
 
-  tflite::testing::TestUpdateLstmHiddenFloat(
+  tflite_micro::testing::TestUpdateLstmHiddenFloat(
       gate_output_data, float_node_contents, kTestFloatTolerance);
 }
 
 TF_LITE_MICRO_TEST(CheckHiddenStateUpdateInt8) {
-  const tflite::testing::GateOutputCheckData<4, 4> gate_output_data =
-      tflite::testing::Get2X2GateOutputCheckData();
-  tflite::testing::LstmNodeContent<int8_t, int8_t, int32_t, int16_t, 2, 3, 2, 2>
-      int8_node_contents = tflite::testing::Create2x3x2X2Int8NodeContents(
+  const tflite_micro::testing::GateOutputCheckData<4, 4> gate_output_data =
+      tflite_micro::testing::Get2X2GateOutputCheckData();
+  tflite_micro::testing::LstmNodeContent<int8_t, int8_t, int32_t, int16_t, 2, 3, 2, 2>
+      int8_node_contents = tflite_micro::testing::Create2x3x2X2Int8NodeContents(
           gate_output_data.input_data, gate_output_data.hidden_state,
           gate_output_data.expected_updated_cell);
 
   // Theoritical error floor = quantization scale = 0.004705882165580988
   const float tolerance = 1e-2;
-  tflite::testing::TestUpdateLstmHiddenInteger(gate_output_data,
+  tflite_micro::testing::TestUpdateLstmHiddenInteger(gate_output_data,
                                                int8_node_contents, tolerance);
 }
 
 TF_LITE_MICRO_TEST(CheckHiddenStateUpdateInt16) {
-  const tflite::testing::GateOutputCheckData<4, 4> gate_output_data =
-      tflite::testing::Get2X2GateOutputCheckData();
-  tflite::testing::LstmNodeContent<int16_t, int8_t, int64_t, int16_t, 2, 3, 2,
+  const tflite_micro::testing::GateOutputCheckData<4, 4> gate_output_data =
+      tflite_micro::testing::Get2X2GateOutputCheckData();
+  tflite_micro::testing::LstmNodeContent<int16_t, int8_t, int64_t, int16_t, 2, 3, 2,
                                    2>
-      int16_node_contents = tflite::testing::Create2x3x2X2Int16NodeContents(
+      int16_node_contents = tflite_micro::testing::Create2x3x2X2Int16NodeContents(
           gate_output_data.input_data, gate_output_data.hidden_state,
           gate_output_data.expected_updated_cell);
 
   const float tolerance = 1e-4;
-  tflite::testing::TestUpdateLstmHiddenInteger(gate_output_data,
+  tflite_micro::testing::TestUpdateLstmHiddenInteger(gate_output_data,
                                                int16_node_contents, tolerance);
 }
 
 TF_LITE_MICRO_TEST(CheckOneStepLSTMFloat) {
-  const tflite::testing::GateOutputCheckData<4, 4> gate_output_data =
-      tflite::testing::Get2X2GateOutputCheckData();
-  tflite::testing::LstmNodeContent<float, float, float, float, 2, 3, 2, 2>
-      float_node_contents = tflite::testing::Create2x3x2X2FloatNodeContents(
+  const tflite_micro::testing::GateOutputCheckData<4, 4> gate_output_data =
+      tflite_micro::testing::Get2X2GateOutputCheckData();
+  tflite_micro::testing::LstmNodeContent<float, float, float, float, 2, 3, 2, 2>
+      float_node_contents = tflite_micro::testing::Create2x3x2X2FloatNodeContents(
           gate_output_data.input_data, gate_output_data.hidden_state,
           gate_output_data.cell_state);
-  tflite::testing::TestLstmStepFloat(gate_output_data, kTestFloatTolerance,
+  tflite_micro::testing::TestLstmStepFloat(gate_output_data, kTestFloatTolerance,
                                      kTestFloatTolerance, float_node_contents);
 }
 
 TF_LITE_MICRO_TEST(CheckOneStepLSTMInt8) {
-  const tflite::testing::GateOutputCheckData<4, 4> gate_output_data =
-      tflite::testing::Get2X2GateOutputCheckData();
-  tflite::testing::LstmNodeContent<int8_t, int8_t, int32_t, int16_t, 2, 3, 2, 2>
-      int8_node_contents = tflite::testing::Create2x3x2X2Int8NodeContents(
+  const tflite_micro::testing::GateOutputCheckData<4, 4> gate_output_data =
+      tflite_micro::testing::Get2X2GateOutputCheckData();
+  tflite_micro::testing::LstmNodeContent<int8_t, int8_t, int32_t, int16_t, 2, 3, 2, 2>
+      int8_node_contents = tflite_micro::testing::Create2x3x2X2Int8NodeContents(
           gate_output_data.input_data, gate_output_data.hidden_state,
           gate_output_data.cell_state);
 
   const float hidden_state_tolerance = 1e-2;
   // cell state degrade due to integer overflow
   const float cell_state_tolerance = 1e-1;
-  tflite::testing::TestLstmStepInteger(gate_output_data, hidden_state_tolerance,
+  tflite_micro::testing::TestLstmStepInteger(gate_output_data, hidden_state_tolerance,
                                        cell_state_tolerance,
                                        int8_node_contents);
 }
 
 TF_LITE_MICRO_TEST(CheckOneStepLSTMInt16) {
-  const tflite::testing::GateOutputCheckData<4, 4> gate_output_data =
-      tflite::testing::Get2X2GateOutputCheckData();
-  tflite::testing::LstmNodeContent<int16_t, int8_t, int64_t, int16_t, 2, 3, 2,
+  const tflite_micro::testing::GateOutputCheckData<4, 4> gate_output_data =
+      tflite_micro::testing::Get2X2GateOutputCheckData();
+  tflite_micro::testing::LstmNodeContent<int16_t, int8_t, int64_t, int16_t, 2, 3, 2,
                                    2>
-      int16_node_contents = tflite::testing::Create2x3x2X2Int16NodeContents(
+      int16_node_contents = tflite_micro::testing::Create2x3x2X2Int16NodeContents(
           gate_output_data.input_data, gate_output_data.hidden_state,
           gate_output_data.cell_state);
   const float hidden_state_tolerance = 1e-3;  // actually very close to 1e-4
   // cell state degrade due to integer overflow
   const float cell_state_tolerance = 1e-1;
-  tflite::testing::TestLstmStepInteger<int16_t, int8_t, int64_t, int16_t, 2, 3,
+  tflite_micro::testing::TestLstmStepInteger<int16_t, int8_t, int64_t, int16_t, 2, 3,
                                        2, 2>(
       gate_output_data, hidden_state_tolerance, cell_state_tolerance,
       int16_node_contents);
 }
 
 TF_LITE_MICRO_TEST(TestLSTMEvalFloat) {
-  const tflite::testing::LstmEvalCheckData<12, 4, 12> kernel_eval_data =
-      tflite::testing::Get2X2LstmEvalCheckData();
-  tflite::testing::LstmNodeContent<float, float, float, float, 2, 3, 2, 2>
-      float_node_contents = tflite::testing::Create2x3x2X2FloatNodeContents(
+  const tflite_micro::testing::LstmEvalCheckData<12, 4, 12> kernel_eval_data =
+      tflite_micro::testing::Get2X2LstmEvalCheckData();
+  tflite_micro::testing::LstmNodeContent<float, float, float, float, 2, 3, 2, 2>
+      float_node_contents = tflite_micro::testing::Create2x3x2X2FloatNodeContents(
           kernel_eval_data.input_data, kernel_eval_data.hidden_state);
 
-  tflite::testing::TestEvalLstmFloat(kernel_eval_data, kTestFloatTolerance,
+  tflite_micro::testing::TestEvalLstmFloat(kernel_eval_data, kTestFloatTolerance,
                                      kTestFloatTolerance, float_node_contents);
 }
 
 TF_LITE_MICRO_TEST(TestLSTMEvalInt8) {
-  const tflite::testing::LstmEvalCheckData<12, 4, 12> kernel_eval_data =
-      tflite::testing::Get2X2LstmEvalCheckData();
-  tflite::testing::LstmNodeContent<int8_t, int8_t, int32_t, int16_t, 2, 3, 2, 2>
-      int8_node_contents = tflite::testing::Create2x3x2X2Int8NodeContents(
+  const tflite_micro::testing::LstmEvalCheckData<12, 4, 12> kernel_eval_data =
+      tflite_micro::testing::Get2X2LstmEvalCheckData();
+  tflite_micro::testing::LstmNodeContent<int8_t, int8_t, int32_t, int16_t, 2, 3, 2, 2>
+      int8_node_contents = tflite_micro::testing::Create2x3x2X2Int8NodeContents(
           kernel_eval_data.input_data, kernel_eval_data.hidden_state);
 
   const float hidden_state_tolerance = 1e-2;
   // cell state degrade due to integer overflow
   const float cell_state_tolerance = 1e-2;
-  tflite::testing::TestEvalLstmInteger(kernel_eval_data, hidden_state_tolerance,
+  tflite_micro::testing::TestEvalLstmInteger(kernel_eval_data, hidden_state_tolerance,
                                        cell_state_tolerance,
                                        int8_node_contents);
 }
 
 TF_LITE_MICRO_TEST(TestLSTMEvalInt16) {
-  const tflite::testing::LstmEvalCheckData<12, 4, 12> kernel_eval_data =
-      tflite::testing::Get2X2LstmEvalCheckData();
-  tflite::testing::LstmNodeContent<int16_t, int8_t, int64_t, int16_t, 2, 3, 2,
+  const tflite_micro::testing::LstmEvalCheckData<12, 4, 12> kernel_eval_data =
+      tflite_micro::testing::Get2X2LstmEvalCheckData();
+  tflite_micro::testing::LstmNodeContent<int16_t, int8_t, int64_t, int16_t, 2, 3, 2,
                                    2>
-      int16_node_contents = tflite::testing::Create2x3x2X2Int16NodeContents(
+      int16_node_contents = tflite_micro::testing::Create2x3x2X2Int16NodeContents(
           kernel_eval_data.input_data, kernel_eval_data.hidden_state);
 
   const float hidden_state_tolerance = 1e-3;  // actually very close to 1e-4
   // cell state degrade due to integer overflow
   const float cell_state_tolerance = 1e-2;
-  tflite::testing::TestEvalLstmInteger(kernel_eval_data, hidden_state_tolerance,
+  tflite_micro::testing::TestEvalLstmInteger(kernel_eval_data, hidden_state_tolerance,
                                        cell_state_tolerance,
                                        int16_node_contents);
 }
diff --git a/tensorflow/lite/micro/kernels/lstm_eval_test.h b/tensorflow/lite/micro/kernels/lstm_eval_test.h
index aee12cf3..10e443de 100644
--- a/tensorflow/lite/micro/kernels/lstm_eval_test.h
+++ b/tensorflow/lite/micro/kernels/lstm_eval_test.h
@@ -24,7 +24,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 
 /*Helper Functions (mainly about mimicking the kernel preparation)*/
@@ -35,7 +35,7 @@ namespace testing {
 // (put into stack memory) CalculateOpDataFullyConnected in
 // tensorflow/lite/micro/kernels/fully_connected_common.cc
 template <typename CellType>
-tflite::FullyConnectedParams CreateFCParams(
+tflite_micro::FullyConnectedParams CreateFCParams(
     const TensorQuantizationParameters& input_quant_params,
     const TensorQuantizationParameters& weight_quant_params,
     const float nonlinear_activation_input_scale) {
@@ -57,10 +57,10 @@ tflite::FullyConnectedParams CreateFCParams(
   data.output_activation_min = std::numeric_limits<CellType>::min();
   data.output_activation_max = std::numeric_limits<CellType>::max();
 
-  return tflite::FullyConnectedParamsQuantized(data);
+  return tflite_micro::FullyConnectedParamsQuantized(data);
 }
 
-inline tflite::FullyConnectedParams CreateFCParamsFloat() {
+inline tflite_micro::FullyConnectedParams CreateFCParamsFloat() {
   FullyConnectedParams op_params;
   CalculateActivationRange(kTfLiteActNone, &op_params.float_activation_min,
                            &op_params.float_activation_max);
@@ -69,12 +69,12 @@ inline tflite::FullyConnectedParams CreateFCParamsFloat() {
 
 // Wrapper function to create gate parameters for the four internal LSTM gates
 template <typename CellType>
-tflite::GateParameters CreateGateParams(
+tflite_micro::GateParameters CreateGateParams(
     const TensorQuantizationParameters& input_quant_params,
     const TensorQuantizationParameters& hidden_state_quant_params,
     const GateQuantizationParameters& gate_quantization_settings,
     const float nonlinear_activation_input_scale) {
-  tflite::GateParameters gate_params = {};
+  tflite_micro::GateParameters gate_params = {};
   gate_params.input_fc_params = CreateFCParams<CellType>(
       input_quant_params, gate_quantization_settings.activation_weight,
       nonlinear_activation_input_scale);
@@ -84,8 +84,8 @@ tflite::GateParameters CreateGateParams(
   return gate_params;
 }
 
-inline tflite::GateParameters CreateGateParamsFloat() {
-  tflite::GateParameters gate_params = {};
+inline tflite_micro::GateParameters CreateGateParamsFloat() {
+  tflite_micro::GateParameters gate_params = {};
   gate_params.input_fc_params = CreateFCParamsFloat();
   gate_params.recurrent_fc_params = CreateFCParamsFloat();
   return gate_params;
@@ -97,11 +97,11 @@ inline tflite::GateParameters CreateGateParamsFloat() {
 // output is the updated hidden state, which is asymmetrically quantized. Thus
 // output may require zero point
 template <typename OutputType>
-tflite::ArithmeticParams CreateInterGateMulParams(const float input1_scale,
+tflite_micro::ArithmeticParams CreateInterGateMulParams(const float input1_scale,
                                                   const float input2_scale,
                                                   const float output_scale,
                                                   const int output_zp = 0) {
-  tflite::ArithmeticParams op_params = {};
+  tflite_micro::ArithmeticParams op_params = {};
   op_params.quantized_activation_min = std::numeric_limits<OutputType>::min();
   op_params.quantized_activation_max = std::numeric_limits<OutputType>::max();
   op_params.input1_offset = 0;
@@ -118,8 +118,8 @@ tflite::ArithmeticParams CreateInterGateMulParams(const float input1_scale,
   return op_params;
 }
 
-inline tflite::ArithmeticParams CreateInterGateMulParamsFloat() {
-  tflite::ArithmeticParams op_params = {};
+inline tflite_micro::ArithmeticParams CreateInterGateMulParamsFloat() {
+  tflite_micro::ArithmeticParams op_params = {};
   CalculateActivationRange(kTfLiteActNone, &op_params.float_activation_min,
                            &op_params.float_activation_max);
   return op_params;
@@ -133,7 +133,7 @@ CellStateInfo CreateLstmCellStateInfo(const float cell_state_scale,
   CellStateInfo cell_state_info;
   // cell_state_scale_power: 2^-cell_state_scale_power = cell state scale
   int buffer;
-  tflite::CheckedLog2(cell_state_scale, &buffer);
+  tflite_micro::CheckedLog2(cell_state_scale, &buffer);
   cell_state_info.cell_state_scale_power = buffer;
   // Cell state specifics
   cell_state_info.cell_clip = cell_clip;
@@ -344,16 +344,16 @@ void TestCalculateLstmGateFloat(const TfLiteEvalTensor* input,
   float gate_output[batch_size * state_dimension] = {};
   float fc_output_buffer[batch_size * state_dimension] = {};
 
-  tflite::GateParameters gate_params = CreateGateParamsFloat();
+  tflite_micro::GateParameters gate_params = CreateGateParamsFloat();
 
   // Create step information: only one time step, no need to update
-  auto size_info = tflite::testing::CreateLstmSizeInfo(
+  auto size_info = tflite_micro::testing::CreateLstmSizeInfo(
       /*time_major*/ false, input->dims, recurrent->dims);
   // revise time_major = true to enable batch inference
   size_info.time_major = true;
-  tflite::lstm_internal::LstmStepManager step_info(&size_info);
+  tflite_micro::lstm_internal::LstmStepManager step_info(&size_info);
 
-  tflite::lstm_internal::CalculateLstmGate<float, float, float, float>(
+  tflite_micro::lstm_internal::CalculateLstmGate<float, float, float, float>(
       step_info, gate_params,
       // Input FC
       input, input_weight, input_bias,
@@ -385,20 +385,20 @@ void TestCalculateLstmGateInteger(
   CellType gate_output[batch_size * state_dimension] = {};
   CellType fc_output_buffer[batch_size * state_dimension] = {};
 
-  tflite::GateParameters gate_params = CreateGateParams<CellType>(
+  tflite_micro::GateParameters gate_params = CreateGateParams<CellType>(
       node_quantization_settings.input, node_quantization_settings.hidden_state,
       gate_quantization_settings,
       node_quantization_settings.nonlinear_activation_input_scale);
 
   // Create step information: only one time step, no need to update
-  auto size_info = tflite::testing::CreateLstmSizeInfo(
+  auto size_info = tflite_micro::testing::CreateLstmSizeInfo(
       /*time_major*/ false, input->dims, recurrent->dims);
   // revise time_major = true to enable batch inference
   size_info.time_major = true;
-  tflite::lstm_internal::LstmStepManager step_info(&size_info);
+  tflite_micro::lstm_internal::LstmStepManager step_info(&size_info);
 
   // only int8 weight is supported now
-  tflite::lstm_internal::CalculateLstmGate<ActivationType, WeightType, CellType,
+  tflite_micro::lstm_internal::CalculateLstmGate<ActivationType, WeightType, CellType,
                                            BiasType>(
       step_info, gate_params,
       // Input FC
@@ -434,13 +434,13 @@ void TestUpdateLstmCellFloat(
 
   auto cell_state = node_content.CellStateEvalTensor();
   // Create step information: only one time step, no need to update
-  auto size_info = tflite::testing::CreateLstmSizeInfo(
+  auto size_info = tflite_micro::testing::CreateLstmSizeInfo(
       /*time_major*/ false,
-      node_content.GetEvalTensor(tflite::kLstmInputTensor)->dims,
+      node_content.GetEvalTensor(tflite_micro::kLstmInputTensor)->dims,
       node_content.HiddenStateEvalTensor()->dims);
   // revise time_major = true to enable batch inference
   size_info.time_major = true;
-  tflite::lstm_internal::LstmStepManager step_info(&size_info);
+  tflite_micro::lstm_internal::LstmStepManager step_info(&size_info);
 
   // copy the data since it will be updated
   float forget_gate[batch_size * state_dimension] = {};
@@ -450,14 +450,14 @@ void TestUpdateLstmCellFloat(
   CellStateInfo cell_state_info;
   cell_state_info.cell_clip = node_content.BuiltinData().cell_clip;
   // Call the function to be tested
-  tflite::lstm_internal::UpdateLstmCell<float>(
+  tflite_micro::lstm_internal::UpdateLstmCell<float>(
       step_info, cell_state, forget_gate,
       gate_output_data.expected_input_gate_output,
       gate_output_data.expected_cell_gate_output, forget_cell_mul_params,
       input_mul_params, cell_state_info, buffer);
 
   ValidateResultGoldens(gate_output_data.expected_updated_cell,
-                        tflite::micro::GetTensorData<float>(cell_state),
+                        tflite_micro::micro::GetTensorData<float>(cell_state),
                         batch_size * state_dimension, tolerance);
 }
 
@@ -472,17 +472,17 @@ void TestUpdateLstmCellInteger(
     const float tolerance) {
   const auto& quantization_settings = node_content.QuantizationSettings();
   CellType quantized_forget_gate[batch_size * state_dimension] = {};
-  tflite::Quantize(gate_output_data.expected_forget_gate_output,
+  tflite_micro::Quantize(gate_output_data.expected_forget_gate_output,
                    quantized_forget_gate, batch_size * state_dimension,
                    quantization_settings.nonlinear_activation_output_scale, 0);
 
   CellType quantized_input_gate[batch_size * state_dimension] = {};
-  tflite::Quantize(gate_output_data.expected_input_gate_output,
+  tflite_micro::Quantize(gate_output_data.expected_input_gate_output,
                    quantized_input_gate, batch_size * state_dimension,
                    quantization_settings.nonlinear_activation_output_scale, 0);
 
   CellType quantized_cell_gate[batch_size * state_dimension] = {};
-  tflite::Quantize(gate_output_data.expected_cell_gate_output,
+  tflite_micro::Quantize(gate_output_data.expected_cell_gate_output,
                    quantized_cell_gate, batch_size * state_dimension,
                    quantization_settings.nonlinear_activation_output_scale, 0);
 
@@ -503,22 +503,22 @@ void TestUpdateLstmCellInteger(
 
   auto cell_state = node_content.CellStateEvalTensor();
   // Create step information: only one time step, no need to update
-  auto size_info = tflite::testing::CreateLstmSizeInfo(
+  auto size_info = tflite_micro::testing::CreateLstmSizeInfo(
       /*time_major*/ false,
-      node_content.GetEvalTensor(tflite::kLstmInputTensor)->dims,
+      node_content.GetEvalTensor(tflite_micro::kLstmInputTensor)->dims,
       node_content.HiddenStateEvalTensor()->dims);
   // revise time_major = true to enable batch inference
   size_info.time_major = true;
-  tflite::lstm_internal::LstmStepManager step_info(&size_info);
+  tflite_micro::lstm_internal::LstmStepManager step_info(&size_info);
 
   // Call the function to be tested
-  tflite::lstm_internal::UpdateLstmCell<CellType>(
+  tflite_micro::lstm_internal::UpdateLstmCell<CellType>(
       step_info, cell_state, quantized_forget_gate, quantized_input_gate,
       quantized_cell_gate, forget_cell_mul_params, input_mul_params,
       cell_state_info, buffer);
 
   float cell_state_float[batch_size * state_dimension] = {};
-  Dequantize(tflite::micro::GetTensorData<CellType>(cell_state),
+  Dequantize(tflite_micro::micro::GetTensorData<CellType>(cell_state),
              batch_size * state_dimension,
              quantization_settings.cell_state.scale,
              quantization_settings.cell_state.zero_point, cell_state_float);
@@ -543,24 +543,24 @@ void TestUpdateLstmHiddenFloat(
   int32_t cell_state_scale_power = 0;
 
   // Create step information: only one time step, no need to update
-  auto size_info = tflite::testing::CreateLstmSizeInfo(
+  auto size_info = tflite_micro::testing::CreateLstmSizeInfo(
       /*time_major*/ false,
-      node_content.GetEvalTensor(tflite::kLstmInputTensor)->dims,
+      node_content.GetEvalTensor(tflite_micro::kLstmInputTensor)->dims,
       node_content.HiddenStateEvalTensor()->dims);
   // revise time_major = true to enable batch inference
   size_info.time_major = true;
-  tflite::lstm_internal::LstmStepManager step_info(&size_info);
+  tflite_micro::lstm_internal::LstmStepManager step_info(&size_info);
 
   auto cell_state = node_content.CellStateEvalTensor();
   auto hidden_state = node_content.HiddenStateEvalTensor();
 
-  tflite::lstm_internal::UpdateLstmHidden<float, float>(
+  tflite_micro::lstm_internal::UpdateLstmHidden<float, float>(
       step_info, cell_state, hidden_state,
       gate_output_data.expected_output_gate_output, mul_params,
       cell_state_scale_power, buffer);
 
   ValidateResultGoldens(gate_output_data.expected_updated_hidden,
-                        tflite::micro::GetTensorData<float>(hidden_state),
+                        tflite_micro::micro::GetTensorData<float>(hidden_state),
                         batch_size * state_dimension, tolerance);
 }
 
@@ -575,7 +575,7 @@ void TestUpdateLstmHiddenInteger(
     const float tolerance) {
   const auto& quantization_settings = node_content.QuantizationSettings();
   CellType quantized_output_gate[batch_size * state_dimension] = {};
-  tflite::Quantize(gate_output_data.expected_output_gate_output,
+  tflite_micro::Quantize(gate_output_data.expected_output_gate_output,
                    quantized_output_gate, batch_size * state_dimension,
                    quantization_settings.nonlinear_activation_output_scale, 0);
 
@@ -588,28 +588,28 @@ void TestUpdateLstmHiddenInteger(
       quantization_settings.hidden_state.zero_point);
 
   int cell_state_scale_power_buffer;
-  tflite::CheckedLog2(quantization_settings.cell_state.scale,
+  tflite_micro::CheckedLog2(quantization_settings.cell_state.scale,
                       &cell_state_scale_power_buffer);
   int32_t cell_state_scale_power = cell_state_scale_power_buffer;
 
   // Create step information: only one time step, no need to update
-  auto size_info = tflite::testing::CreateLstmSizeInfo(
+  auto size_info = tflite_micro::testing::CreateLstmSizeInfo(
       /*time_major*/ false,
-      node_content.GetEvalTensor(tflite::kLstmInputTensor)->dims,
+      node_content.GetEvalTensor(tflite_micro::kLstmInputTensor)->dims,
       node_content.HiddenStateEvalTensor()->dims);
   // revise time_major = true to enable batch inference
   size_info.time_major = true;
-  tflite::lstm_internal::LstmStepManager step_info(&size_info);
+  tflite_micro::lstm_internal::LstmStepManager step_info(&size_info);
 
   auto cell_state = node_content.CellStateEvalTensor();
   auto hidden_state = node_content.HiddenStateEvalTensor();
 
-  tflite::lstm_internal::UpdateLstmHidden<CellType, ActivationType>(
+  tflite_micro::lstm_internal::UpdateLstmHidden<CellType, ActivationType>(
       step_info, cell_state, hidden_state, quantized_output_gate, mul_params,
       cell_state_scale_power, buffer);
 
   float hidden_state_float[batch_size * state_dimension] = {};
-  Dequantize(tflite::micro::GetTensorData<ActivationType>(hidden_state),
+  Dequantize(tflite_micro::micro::GetTensorData<ActivationType>(hidden_state),
              batch_size * state_dimension,
              quantization_settings.hidden_state.scale,
              quantization_settings.hidden_state.zero_point, hidden_state_float);
@@ -644,17 +644,17 @@ void TestLstmStepFloat(
   OpDataLSTM op_data = CreateLstmOpDataFloat(node_contents);
   // set time_major to true to test batch inference
   op_data.size_info.time_major = true;
-  tflite::lstm_internal::LstmStepManager step_info(&op_data.size_info);
-  tflite::lstm_internal::LstmStep<float, float, float, float>(
+  tflite_micro::lstm_internal::LstmStepManager step_info(&op_data.size_info);
+  tflite_micro::lstm_internal::LstmStep<float, float, float, float>(
       step_info, op_data, kernel_content, buffers);
 
   ValidateResultGoldens(
       gate_output_data.expected_updated_hidden,
-      tflite::micro::GetTensorData<float>(kernel_content.HiddenStateTensor()),
+      tflite_micro::micro::GetTensorData<float>(kernel_content.HiddenStateTensor()),
       batch_size * state_dimension, hidden_state_tolerance);
   ValidateResultGoldens(
       gate_output_data.expected_updated_cell,
-      tflite::micro::GetTensorData<float>(kernel_content.CellStateTensor()),
+      tflite_micro::micro::GetTensorData<float>(kernel_content.CellStateTensor()),
       batch_size * state_dimension, cell_state_tolerance);
 }
 
@@ -686,22 +686,22 @@ void TestLstmStepInteger(
   OpDataLSTM op_data = CreateLstmOpData(node_contents);
   // set time_major to true to test batch inference
   op_data.size_info.time_major = true;
-  tflite::lstm_internal::LstmStepManager step_info(&op_data.size_info);
-  tflite::lstm_internal::LstmStep<ActivationType, WeightType, CellType,
+  tflite_micro::lstm_internal::LstmStepManager step_info(&op_data.size_info);
+  tflite_micro::lstm_internal::LstmStep<ActivationType, WeightType, CellType,
                                   BiasType>(step_info, op_data, kernel_content,
                                             buffers);
 
   const auto& quantization_settings = node_contents.QuantizationSettings();
   float dequantized_hidden_state[batch_size * state_dimension] = {};
   Dequantize(
-      tflite::micro::GetTensorData<ActivationType>(
+      tflite_micro::micro::GetTensorData<ActivationType>(
           kernel_content.HiddenStateTensor()),
       batch_size * state_dimension, quantization_settings.hidden_state.scale,
       quantization_settings.hidden_state.zero_point, dequantized_hidden_state);
 
   float dequantized_cell_state[batch_size * state_dimension] = {};
   Dequantize(
-      tflite::micro::GetTensorData<CellType>(kernel_content.CellStateTensor()),
+      tflite_micro::micro::GetTensorData<CellType>(kernel_content.CellStateTensor()),
       batch_size * state_dimension, quantization_settings.cell_state.scale,
       quantization_settings.cell_state.zero_point, dequantized_cell_state);
 
@@ -737,7 +737,7 @@ void TestEvalLstmFloat(
 
   OpDataLSTM op_data = CreateLstmOpDataFloat(node_contents);
 
-  tflite::EvalLstm<float, float, float, float>(op_data, kernel_content,
+  tflite_micro::EvalLstm<float, float, float, float>(op_data, kernel_content,
                                                buffers);
 
   ValidateResultGoldens(eval_check_data.expected_hidden_state,
@@ -779,7 +779,7 @@ void TestEvalLstmInteger(
 
   OpDataLSTM op_data = CreateLstmOpData(node_contents);
 
-  tflite::EvalLstm<ActivationType, WeightType, CellType, BiasType>(
+  tflite_micro::EvalLstm<ActivationType, WeightType, CellType, BiasType>(
       op_data, kernel_content, buffers);
 
   const auto& quantization_settings = node_contents.QuantizationSettings();
@@ -812,6 +812,6 @@ void TestEvalLstmInteger(
 }
 
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_LSTM_EVAL_TEST_H_
diff --git a/tensorflow/lite/micro/kernels/lstm_shared.h b/tensorflow/lite/micro/kernels/lstm_shared.h
index dbdc3c55..d30728b9 100644
--- a/tensorflow/lite/micro/kernels/lstm_shared.h
+++ b/tensorflow/lite/micro/kernels/lstm_shared.h
@@ -18,7 +18,7 @@ limitations under the License.
 #include "tensorflow/lite/c/builtin_op_data.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // Input Tensors of size {n_batch, n_input}
 constexpr int kLstmInputTensor = 0;
@@ -146,5 +146,5 @@ struct LSTMBuffers {
   CellType* buffer3;
 };
 
-}  // namespace tflite
+}  // namespace tflite_micro
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_LSTM_SHARED_H_
diff --git a/tensorflow/lite/micro/kernels/maximum_minimum.cc b/tensorflow/lite/micro/kernels/maximum_minimum.cc
index 48717074..fbb9caa6 100644
--- a/tensorflow/lite/micro/kernels/maximum_minimum.cc
+++ b/tensorflow/lite/micro/kernels/maximum_minimum.cc
@@ -25,7 +25,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -40,9 +40,9 @@ constexpr int kOutputTensor = 0;
 
 struct OpContext {
   OpContext(TfLiteContext* context, TfLiteNode* node) {
-    input1 = tflite::micro::GetEvalInput(context, node, kInputTensor1);
-    input2 = tflite::micro::GetEvalInput(context, node, kInputTensor2);
-    output = tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+    input1 = tflite_micro::micro::GetEvalInput(context, node, kInputTensor1);
+    input2 = tflite_micro::micro::GetEvalInput(context, node, kInputTensor2);
+    output = tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
   }
   const TfLiteEvalTensor* input1;
   const TfLiteEvalTensor* input2;
@@ -67,12 +67,12 @@ template <typename data_type, typename op_type>
 void TFLiteOperation(TfLiteContext* context, TfLiteNode* node,
                      const OpContext& op_context) {
   reference_ops::MaximumMinimumBroadcastSlow(
-      tflite::micro::GetTensorShape(op_context.input1),
-      tflite::micro::GetTensorData<data_type>(op_context.input1),
-      tflite::micro::GetTensorShape(op_context.input2),
-      tflite::micro::GetTensorData<data_type>(op_context.input2),
-      tflite::micro::GetTensorShape(op_context.output),
-      tflite::micro::GetTensorData<data_type>(op_context.output),
+      tflite_micro::micro::GetTensorShape(op_context.input1),
+      tflite_micro::micro::GetTensorData<data_type>(op_context.input1),
+      tflite_micro::micro::GetTensorShape(op_context.input2),
+      tflite_micro::micro::GetTensorData<data_type>(op_context.input2),
+      tflite_micro::micro::GetTensorShape(op_context.output),
+      tflite_micro::micro::GetTensorData<data_type>(op_context.output),
       op_type::template op<data_type>);
 }
 
@@ -110,13 +110,13 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_MAXIMUM() {
-  return tflite::micro::RegisterOp(nullptr, nullptr,
+  return tflite_micro::micro::RegisterOp(nullptr, nullptr,
                                    Eval<kReference, MaximumOp>);
 }
 
 TFLMRegistration Register_MINIMUM() {
-  return tflite::micro::RegisterOp(nullptr, nullptr,
+  return tflite_micro::micro::RegisterOp(nullptr, nullptr,
                                    Eval<kReference, MinimumOp>);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/maximum_minimum_test.cc b/tensorflow/lite/micro/kernels/maximum_minimum_test.cc
index f8165b68..42301da4 100644
--- a/tensorflow/lite/micro/kernels/maximum_minimum_test.cc
+++ b/tensorflow/lite/micro/kernels/maximum_minimum_test.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -139,7 +139,7 @@ void TestMaxMinQuantizedInt32(const TFLMRegistration& registration,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -151,10 +151,10 @@ TF_LITE_MICRO_TEST(FloatTest) {
   const float golden_min[] = {-1.0, 0.0, -1.0, 11.0, -3.0, -1.44};
   float output_data[6];
 
-  tflite::testing::TestMaxMinFloat(tflite::Register_MAXIMUM(), dims, data1,
+  tflite_micro::testing::TestMaxMinFloat(tflite_micro::Register_MAXIMUM(), dims, data1,
                                    dims, data2, golden_max, dims, output_data);
 
-  tflite::testing::TestMaxMinFloat(tflite::Register_MINIMUM(), dims, data1,
+  tflite_micro::testing::TestMaxMinFloat(tflite_micro::Register_MINIMUM(), dims, data1,
                                    dims, data2, golden_min, dims, output_data);
 }
 
@@ -172,13 +172,13 @@ TF_LITE_MICRO_TEST(Int8Test) {
 
   int8_t output_data[6];
 
-  tflite::testing::TestMaxMinQuantized(
-      tflite::Register_MAXIMUM(), dims, data1, input_scale, input_zero_point,
+  tflite_micro::testing::TestMaxMinQuantized(
+      tflite_micro::Register_MAXIMUM(), dims, data1, input_scale, input_zero_point,
       dims, data2, input_scale, input_zero_point, golden_max, output_scale,
       output_zero_point, dims, output_data);
 
-  tflite::testing::TestMaxMinQuantized(
-      tflite::Register_MINIMUM(), dims, data1, input_scale, input_zero_point,
+  tflite_micro::testing::TestMaxMinQuantized(
+      tflite_micro::Register_MINIMUM(), dims, data1, input_scale, input_zero_point,
       dims, data2, input_scale, input_zero_point, golden_min, output_scale,
       output_zero_point, dims, output_data);
 }
@@ -192,11 +192,11 @@ TF_LITE_MICRO_TEST(FloatWithBroadcastTest) {
   const float golden_min[] = {0.5, 0.0, -1.0, -2.0, -1.44, 2.0};
   float output_data[6];
 
-  tflite::testing::TestMaxMinFloat(tflite::Register_MAXIMUM(), dims, data1,
+  tflite_micro::testing::TestMaxMinFloat(tflite_micro::Register_MAXIMUM(), dims, data1,
                                    dims_scalar, data2, golden_max, dims,
                                    output_data);
 
-  tflite::testing::TestMaxMinFloat(tflite::Register_MINIMUM(), dims, data1,
+  tflite_micro::testing::TestMaxMinFloat(tflite_micro::Register_MINIMUM(), dims, data1,
                                    dims_scalar, data2, golden_min, dims,
                                    output_data);
 }
@@ -210,11 +210,11 @@ TF_LITE_MICRO_TEST(Int32WithBroadcastTest) {
   const int32_t golden_min[] = {1, 0, -1, -2, 2, 2};
   int32_t output_data[6];
 
-  tflite::testing::TestMaxMinQuantizedInt32(tflite::Register_MAXIMUM(), dims,
+  tflite_micro::testing::TestMaxMinQuantizedInt32(tflite_micro::Register_MAXIMUM(), dims,
                                             data1, dims_scalar, data2,
                                             golden_max, dims, output_data);
 
-  tflite::testing::TestMaxMinQuantizedInt32(tflite::Register_MINIMUM(), dims,
+  tflite_micro::testing::TestMaxMinQuantizedInt32(tflite_micro::Register_MINIMUM(), dims,
                                             data1, dims_scalar, data2,
                                             golden_min, dims, output_data);
 }
diff --git a/tensorflow/lite/micro/kernels/micro_ops.h b/tensorflow/lite/micro/kernels/micro_ops.h
index 2e33a673..52798128 100644
--- a/tensorflow/lite/micro/kernels/micro_ops.h
+++ b/tensorflow/lite/micro/kernels/micro_ops.h
@@ -27,7 +27,7 @@ limitations under the License.
 // their model requires, using a custom `(Micro)MutableOpResolver`. Selective
 // registration in turn allows the linker to strip unused kernels.
 
-namespace tflite {
+namespace tflite_micro {
 
 // TFLM is incrementally moving towards a flat tflite namespace
 // (https://abseil.io/tips/130). Any new ops (or cleanup of existing ops should
@@ -153,6 +153,6 @@ TFLMRegistration* Register_STACKER();
 TFLMRegistration* Register_WINDOW();
 }  // namespace tflm_signal
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_MICRO_OPS_H_
diff --git a/tensorflow/lite/micro/kernels/micro_tensor_utils.cc b/tensorflow/lite/micro/kernels/micro_tensor_utils.cc
index 87cfe0c1..08269f2d 100644
--- a/tensorflow/lite/micro/kernels/micro_tensor_utils.cc
+++ b/tensorflow/lite/micro/kernels/micro_tensor_utils.cc
@@ -27,7 +27,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/cppmath.h"
 #include "tensorflow/lite/kernels/op_macros.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // Apply sigmoid to elements of a vector.
 void PortableApplySigmoidToVector(const float* vector, int v_size,
@@ -50,18 +50,18 @@ void PortableApplyActivationToVector(const float* vector, int v_size,
     case kTfLiteActNone:
       return;
     case kTfLiteActRelu:
-      return tflite::tensor_utils::ApplyReluToVector(vector, v_size, result);
+      return tflite_micro::tensor_utils::ApplyReluToVector(vector, v_size, result);
     case kTfLiteActReluN1To1:
-      return tflite::tensor_utils::ApplyRelu1ToVector(vector, v_size, result);
+      return tflite_micro::tensor_utils::ApplyRelu1ToVector(vector, v_size, result);
     case kTfLiteActRelu6:
-      return tflite::tensor_utils::ApplyRelu6ToVector(vector, v_size, result);
+      return tflite_micro::tensor_utils::ApplyRelu6ToVector(vector, v_size, result);
     case kTfLiteActTanh:
       return PortableApplyTanhToVector(vector, v_size, result);
     case kTfLiteActSignBit:
-      return tflite::tensor_utils::ApplySignbitToVector(vector, v_size, result);
+      return tflite_micro::tensor_utils::ApplySignbitToVector(vector, v_size, result);
     case kTfLiteActSigmoid:
       return PortableApplySigmoidToVector(vector, v_size, result);
   }
 }
 
-}  // namespace tflite
\ No newline at end of file
+}  // namespace tflite_micro
\ No newline at end of file
diff --git a/tensorflow/lite/micro/kernels/micro_tensor_utils.h b/tensorflow/lite/micro/kernels/micro_tensor_utils.h
index 0b87f0ae..6e2d328f 100644
--- a/tensorflow/lite/micro/kernels/micro_tensor_utils.h
+++ b/tensorflow/lite/micro/kernels/micro_tensor_utils.h
@@ -34,7 +34,7 @@ limitations under the License.
 #define __restrict__ __restrict
 #endif
 
-namespace tflite {
+namespace tflite_micro {
 
 // Not all backends support CpuBackendContext usage, so forward declare to avoid
 // pulling in its implementation.
@@ -51,6 +51,6 @@ void PortableApplyActivationToVector(const float* vector, int v_size,
                                      TfLiteFusedActivation activation,
                                      float* result);
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_MICRO_TENSOR_UTILS_H_
\ No newline at end of file
diff --git a/tensorflow/lite/micro/kernels/mirror_pad.cc b/tensorflow/lite/micro/kernels/mirror_pad.cc
index 4cbaf52f..11c3c7a0 100644
--- a/tensorflow/lite/micro/kernels/mirror_pad.cc
+++ b/tensorflow/lite/micro/kernels/mirror_pad.cc
@@ -18,7 +18,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 struct OpDataMirrorPad {
@@ -107,12 +107,12 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
       static_cast<const OpDataMirrorPad*>(node->user_data);
 
   const TfLiteEvalTensor* input_tensor =
-      tflite::micro::GetEvalInput(context, node, 0);
+      tflite_micro::micro::GetEvalInput(context, node, 0);
   const TfLiteEvalTensor* padding_matrix =
-      tflite::micro::GetEvalInput(context, node, 1);
+      tflite_micro::micro::GetEvalInput(context, node, 1);
 
   TfLiteEvalTensor* output_tensor =
-      tflite::micro::GetEvalOutput(context, node, 0);
+      tflite_micro::micro::GetEvalOutput(context, node, 0);
   const int input_dims = data->input_dims;
   const int output_size = data->output_size;
 
@@ -138,16 +138,16 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
     case kTfLiteFloat32: {
       MirrorPad(padding_matrix, input_tensor->dims, output_dims_num_elements,
                 input_dims_num_elements,
-                tflite::micro::GetTensorData<float>(input_tensor),
-                tflite::micro::GetTensorData<float>(output_tensor),
+                tflite_micro::micro::GetTensorData<float>(input_tensor),
+                tflite_micro::micro::GetTensorData<float>(output_tensor),
                 data->offset, input_dims, output_size);
       break;
     }
     case kTfLiteInt8: {
       MirrorPad(padding_matrix, input_tensor->dims, output_dims_num_elements,
                 input_dims_num_elements,
-                tflite::micro::GetTensorData<int8_t>(input_tensor),
-                tflite::micro::GetTensorData<int8_t>(output_tensor),
+                tflite_micro::micro::GetTensorData<int8_t>(input_tensor),
+                tflite_micro::micro::GetTensorData<int8_t>(output_tensor),
                 data->offset, input_dims, output_size);
       break;
     }
@@ -209,7 +209,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_MIRROR_PAD() {
-  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/mirror_pad_test.cc b/tensorflow/lite/micro/kernels/mirror_pad_test.cc
index 06702263..bed46759 100644
--- a/tensorflow/lite/micro/kernels/mirror_pad_test.cc
+++ b/tensorflow/lite/micro/kernels/mirror_pad_test.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -52,26 +52,26 @@ void TestMirrorPad(int* input_shape, const T* input_data, int* pad_shape,
                    const int32_t* pad_data, int* output_shape,
                    const T* golden_data, TfLiteMirrorPaddingMode mode,
                    T* output_data) {
-  TfLiteIntArray* input_dims = tflite::testing::IntArrayFromInts(input_shape);
-  TfLiteIntArray* pad_dims = tflite::testing::IntArrayFromInts(pad_shape);
-  TfLiteIntArray* output_dims = tflite::testing::IntArrayFromInts(output_shape);
+  TfLiteIntArray* input_dims = tflite_micro::testing::IntArrayFromInts(input_shape);
+  TfLiteIntArray* pad_dims = tflite_micro::testing::IntArrayFromInts(pad_shape);
+  TfLiteIntArray* output_dims = tflite_micro::testing::IntArrayFromInts(output_shape);
 
   constexpr int inputs_size = 2;
   constexpr int outputs_size = 1;
   constexpr int tensors_size = inputs_size + outputs_size;
   TfLiteTensor tensors[tensors_size] = {
-      tflite::testing::CreateTensor(input_data, input_dims),
-      tflite::testing::CreateTensor(pad_data, pad_dims),
-      tflite::testing::CreateTensor(output_data, output_dims),
+      tflite_micro::testing::CreateTensor(input_data, input_dims),
+      tflite_micro::testing::CreateTensor(pad_data, pad_dims),
+      tflite_micro::testing::CreateTensor(output_data, output_dims),
   };
 
   ValidateMirrorPadGoldens(tensors, tensors_size, golden_data, output_data,
-                           tflite::ElementCount(*output_dims), mode);
+                           tflite_micro::ElementCount(*output_dims), mode);
 }
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -85,7 +85,7 @@ TF_LITE_MICRO_TEST(EmptyPad) {
   int8_t output_data[6];
   const int8_t golden_data[] = {1, 2, 3, 4, 5, 6};
 
-  tflite::testing::TestMirrorPad(input_shape, input_data, pad_shape, pad_data,
+  tflite_micro::testing::TestMirrorPad(input_shape, input_data, pad_shape, pad_data,
                                  output_shape, golden_data,
                                  kTfLiteMirrorPaddingReflect, output_data);
 }
@@ -100,7 +100,7 @@ TF_LITE_MICRO_TEST(PadOneSide_right_Reflect) {
   int8_t output_data[12];
   const int8_t golden_data[] = {1, 2, 3, 2, 4, 5, 6, 5, 1, 2, 3, 2};
 
-  tflite::testing::TestMirrorPad(input_shape, input_data, pad_shape, pad_data,
+  tflite_micro::testing::TestMirrorPad(input_shape, input_data, pad_shape, pad_data,
                                  output_shape, golden_data,
                                  kTfLiteMirrorPaddingReflect, output_data);
 }
@@ -115,7 +115,7 @@ TF_LITE_MICRO_TEST(PadOneSide_left_Reflect) {
   int8_t output_data[12];
   const int8_t golden_data[] = {5, 4, 5, 6, 2, 1, 2, 3, 5, 4, 5, 6};
 
-  tflite::testing::TestMirrorPad(input_shape, input_data, pad_shape, pad_data,
+  tflite_micro::testing::TestMirrorPad(input_shape, input_data, pad_shape, pad_data,
                                  output_shape, golden_data,
                                  kTfLiteMirrorPaddingReflect, output_data);
 }
@@ -130,7 +130,7 @@ TF_LITE_MICRO_TEST(PadOneSide_right_Symmetric) {
   int8_t output_data[12];
   const int8_t golden_data[] = {1, 2, 3, 3, 4, 5, 6, 6, 4, 5, 6, 6};
 
-  tflite::testing::TestMirrorPad(input_shape, input_data, pad_shape, pad_data,
+  tflite_micro::testing::TestMirrorPad(input_shape, input_data, pad_shape, pad_data,
                                  output_shape, golden_data,
                                  kTfLiteMirrorPaddingSymmetric, output_data);
 }
@@ -145,7 +145,7 @@ TF_LITE_MICRO_TEST(PadOneSide_left_Symmetric) {
   int8_t output_data[12];
   const int8_t golden_data[] = {1, 1, 2, 3, 1, 1, 2, 3, 4, 4, 5, 6};
 
-  tflite::testing::TestMirrorPad(input_shape, input_data, pad_shape, pad_data,
+  tflite_micro::testing::TestMirrorPad(input_shape, input_data, pad_shape, pad_data,
                                  output_shape, golden_data,
                                  kTfLiteMirrorPaddingSymmetric, output_data);
 }
@@ -160,7 +160,7 @@ TF_LITE_MICRO_TEST(PadBothSides_Symmetric) {
   const int8_t golden_data[] = {1, 1, 2, 3, 3, 1, 1, 2, 3, 3,
                                 4, 4, 5, 6, 6, 4, 4, 5, 6, 6};
 
-  tflite::testing::TestMirrorPad(input_shape, input_data, pad_shape, pad_data,
+  tflite_micro::testing::TestMirrorPad(input_shape, input_data, pad_shape, pad_data,
                                  output_shape, golden_data,
                                  kTfLiteMirrorPaddingSymmetric, output_data);
 }
@@ -176,7 +176,7 @@ TF_LITE_MICRO_TEST(PadBothSides_Reflect) {
   const int8_t golden_data[] = {5, 4, 5, 6, 5, 2, 1, 2, 3, 2,
                                 5, 4, 5, 6, 5, 2, 1, 2, 3, 2};
 
-  tflite::testing::TestMirrorPad(input_shape, input_data, pad_shape, pad_data,
+  tflite_micro::testing::TestMirrorPad(input_shape, input_data, pad_shape, pad_data,
                                  output_shape, golden_data,
                                  kTfLiteMirrorPaddingReflect, output_data);
 }
@@ -194,7 +194,7 @@ TF_LITE_MICRO_TEST(PadBothSides_Symmetric_Whole) {
                                 5, 4, 4, 5, 6, 6, 5, 4, 6, 5, 4, 4, 5, 6,
                                 6, 5, 4, 3, 2, 1, 1, 2, 3, 3, 2, 1};
 
-  tflite::testing::TestMirrorPad(input_shape, input_data, pad_shape, pad_data,
+  tflite_micro::testing::TestMirrorPad(input_shape, input_data, pad_shape, pad_data,
                                  output_shape, golden_data,
                                  kTfLiteMirrorPaddingSymmetric, output_data);
 }
@@ -210,7 +210,7 @@ TF_LITE_MICRO_TEST(PadBothSides_Reflect_Whole) {
   const int8_t golden_data[] = {6, 5, 4, 5, 6, 5, 4, 3, 2, 1, 2, 3, 2, 1,
                                 6, 5, 4, 5, 6, 5, 4, 3, 2, 1, 2, 3, 2, 1};
 
-  tflite::testing::TestMirrorPad(input_shape, input_data, pad_shape, pad_data,
+  tflite_micro::testing::TestMirrorPad(input_shape, input_data, pad_shape, pad_data,
                                  output_shape, golden_data,
                                  kTfLiteMirrorPaddingReflect, output_data);
 }
@@ -226,7 +226,7 @@ TF_LITE_MICRO_TEST(Pad_Symmetric) {
   const int8_t golden_data[] = {2, 1, 1, 2, 3, 3, 2, 2, 1, 1, 2, 3, 3, 2,
                                 5, 4, 4, 5, 6, 6, 5, 5, 4, 4, 5, 6, 6, 5};
 
-  tflite::testing::TestMirrorPad(input_shape, input_data, pad_shape, pad_data,
+  tflite_micro::testing::TestMirrorPad(input_shape, input_data, pad_shape, pad_data,
                                  output_shape, golden_data,
                                  kTfLiteMirrorPaddingSymmetric, output_data);
 }
@@ -241,7 +241,7 @@ TF_LITE_MICRO_TEST(Pad_1D_Reflect) {
   int8_t output_data[5];
   const int8_t golden_data[] = {1, 2, 3, 2, 1};
 
-  tflite::testing::TestMirrorPad(input_shape, input_data, pad_shape, pad_data,
+  tflite_micro::testing::TestMirrorPad(input_shape, input_data, pad_shape, pad_data,
                                  output_shape, golden_data,
                                  kTfLiteMirrorPaddingReflect, output_data);
 }
@@ -256,7 +256,7 @@ TF_LITE_MICRO_TEST(Pad_1D_Symmetric) {
   int8_t output_data[5];
   const int8_t golden_data[] = {1, 2, 3, 3, 2};
 
-  tflite::testing::TestMirrorPad(input_shape, input_data, pad_shape, pad_data,
+  tflite_micro::testing::TestMirrorPad(input_shape, input_data, pad_shape, pad_data,
                                  output_shape, golden_data,
                                  kTfLiteMirrorPaddingSymmetric, output_data);
 }
diff --git a/tensorflow/lite/micro/kernels/mul.cc b/tensorflow/lite/micro/kernels/mul.cc
index d9473d4e..937a7054 100644
--- a/tensorflow/lite/micro/kernels/mul.cc
+++ b/tensorflow/lite/micro/kernels/mul.cc
@@ -26,7 +26,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/memory_helpers.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 TfLiteStatus MulEval(TfLiteContext* context, TfLiteNode* node) {
   TFLITE_DCHECK(node->builtin_data != nullptr);
@@ -36,11 +36,11 @@ TfLiteStatus MulEval(TfLiteContext* context, TfLiteNode* node) {
   const OpDataMul* data = static_cast<const OpDataMul*>(node->user_data);
 
   const TfLiteEvalTensor* input1 =
-      tflite::micro::GetEvalInput(context, node, kMulInput1Tensor);
+      tflite_micro::micro::GetEvalInput(context, node, kMulInput1Tensor);
   const TfLiteEvalTensor* input2 =
-      tflite::micro::GetEvalInput(context, node, kMulInput2Tensor);
+      tflite_micro::micro::GetEvalInput(context, node, kMulInput2Tensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kMulOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kMulOutputTensor);
 
   switch (input1->type) {
     case kTfLiteInt8:
@@ -62,7 +62,7 @@ TfLiteStatus MulEval(TfLiteContext* context, TfLiteNode* node) {
 }
 
 TFLMRegistration Register_MUL() {
-  return tflite::micro::RegisterOp(MulInit, MulPrepare, MulEval);
+  return tflite_micro::micro::RegisterOp(MulInit, MulPrepare, MulEval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/mul.h b/tensorflow/lite/micro/kernels/mul.h
index 32407ed5..a97049e0 100644
--- a/tensorflow/lite/micro/kernels/mul.h
+++ b/tensorflow/lite/micro/kernels/mul.h
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/c/builtin_op_data.h"
 #include "tensorflow/lite/micro/micro_common.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 extern const int kMulInput1Tensor;
 extern const int kMulInput2Tensor;
@@ -69,6 +69,6 @@ TFLMRegistration Register_MUL_INT8();
 // Fallback registration
 inline TFLMRegistration Register_MUL_INT8() { return Register_MUL(); }
 #endif
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_MUL_H_
diff --git a/tensorflow/lite/micro/kernels/mul_common.cc b/tensorflow/lite/micro/kernels/mul_common.cc
index 45e7c1e4..6c186efb 100644
--- a/tensorflow/lite/micro/kernels/mul_common.cc
+++ b/tensorflow/lite/micro/kernels/mul_common.cc
@@ -24,7 +24,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/mul.h"
 #include "tensorflow/lite/micro/memory_helpers.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 const int kMulInput1Tensor = 0;
 const int kMulInput2Tensor = 1;
@@ -104,7 +104,7 @@ TfLiteStatus EvalMulQuantizedReference(TfLiteContext* context, TfLiteNode* node,
                                        const TfLiteEvalTensor* input1,
                                        const TfLiteEvalTensor* input2,
                                        TfLiteEvalTensor* output) {
-  tflite::ArithmeticParams op_params = {};
+  tflite_micro::ArithmeticParams op_params = {};
   op_params.quantized_activation_min = data->output_activation_min;
   op_params.quantized_activation_max = data->output_activation_max;
   op_params.float_activation_max = data->output_activation_max_f32;
@@ -115,43 +115,43 @@ TfLiteStatus EvalMulQuantizedReference(TfLiteContext* context, TfLiteNode* node,
   op_params.output_shift = data->output_shift;
 
   bool need_broadcast = reference_ops::ProcessBroadcastShapes(
-      tflite::micro::GetTensorShape(input1),
-      tflite::micro::GetTensorShape(input2), &op_params);
+      tflite_micro::micro::GetTensorShape(input1),
+      tflite_micro::micro::GetTensorShape(input2), &op_params);
 
   if (input1->type == kTfLiteInt8) {
     if (need_broadcast) {
       reference_integer_ops::BroadcastMul4DSlow(
-          op_params, tflite::micro::GetTensorShape(input1),
-          tflite::micro::GetTensorData<int8_t>(input1),
-          tflite::micro::GetTensorShape(input2),
-          tflite::micro::GetTensorData<int8_t>(input2),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<int8_t>(output));
+          op_params, tflite_micro::micro::GetTensorShape(input1),
+          tflite_micro::micro::GetTensorData<int8_t>(input1),
+          tflite_micro::micro::GetTensorShape(input2),
+          tflite_micro::micro::GetTensorData<int8_t>(input2),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<int8_t>(output));
     } else {
       reference_integer_ops::Mul(op_params,
-                                 tflite::micro::GetTensorShape(input1),
-                                 tflite::micro::GetTensorData<int8_t>(input1),
-                                 tflite::micro::GetTensorShape(input2),
-                                 tflite::micro::GetTensorData<int8_t>(input2),
-                                 tflite::micro::GetTensorShape(output),
-                                 tflite::micro::GetTensorData<int8_t>(output));
+                                 tflite_micro::micro::GetTensorShape(input1),
+                                 tflite_micro::micro::GetTensorData<int8_t>(input1),
+                                 tflite_micro::micro::GetTensorShape(input2),
+                                 tflite_micro::micro::GetTensorData<int8_t>(input2),
+                                 tflite_micro::micro::GetTensorShape(output),
+                                 tflite_micro::micro::GetTensorData<int8_t>(output));
     }
   } else if (input1->type == kTfLiteInt32) {
     if (need_broadcast) {
       reference_ops::BroadcastMul4DSlow(
-          op_params, tflite::micro::GetTensorShape(input1),
-          tflite::micro::GetTensorData<int32_t>(input1),
-          tflite::micro::GetTensorShape(input2),
-          tflite::micro::GetTensorData<int32_t>(input2),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<int32_t>(output));
+          op_params, tflite_micro::micro::GetTensorShape(input1),
+          tflite_micro::micro::GetTensorData<int32_t>(input1),
+          tflite_micro::micro::GetTensorShape(input2),
+          tflite_micro::micro::GetTensorData<int32_t>(input2),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<int32_t>(output));
     } else {
-      reference_ops::Mul(op_params, tflite::micro::GetTensorShape(input1),
-                         tflite::micro::GetTensorData<int32_t>(input1),
-                         tflite::micro::GetTensorShape(input2),
-                         tflite::micro::GetTensorData<int32_t>(input2),
-                         tflite::micro::GetTensorShape(output),
-                         tflite::micro::GetTensorData<int32_t>(output));
+      reference_ops::Mul(op_params, tflite_micro::micro::GetTensorShape(input1),
+                         tflite_micro::micro::GetTensorData<int32_t>(input1),
+                         tflite_micro::micro::GetTensorShape(input2),
+                         tflite_micro::micro::GetTensorData<int32_t>(input2),
+                         tflite_micro::micro::GetTensorShape(output),
+                         tflite_micro::micro::GetTensorData<int32_t>(output));
     }
   } else if (input1->type == kTfLiteInt16) {
     TF_LITE_ENSURE_EQ(context, op_params.input1_offset, 0);
@@ -160,20 +160,20 @@ TfLiteStatus EvalMulQuantizedReference(TfLiteContext* context, TfLiteNode* node,
 
     if (need_broadcast) {
       reference_integer_ops::BroadcastMul4DSlow(
-          op_params, tflite::micro::GetTensorShape(input1),
-          tflite::micro::GetTensorData<int16_t>(input1),
-          tflite::micro::GetTensorShape(input2),
-          tflite::micro::GetTensorData<int16_t>(input2),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<int16_t>(output));
+          op_params, tflite_micro::micro::GetTensorShape(input1),
+          tflite_micro::micro::GetTensorData<int16_t>(input1),
+          tflite_micro::micro::GetTensorShape(input2),
+          tflite_micro::micro::GetTensorData<int16_t>(input2),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<int16_t>(output));
     } else {
       reference_integer_ops::Mul(op_params,
-                                 tflite::micro::GetTensorShape(input1),
-                                 tflite::micro::GetTensorData<int16_t>(input1),
-                                 tflite::micro::GetTensorShape(input2),
-                                 tflite::micro::GetTensorData<int16_t>(input2),
-                                 tflite::micro::GetTensorShape(output),
-                                 tflite::micro::GetTensorData<int16_t>(output));
+                                 tflite_micro::micro::GetTensorShape(input1),
+                                 tflite_micro::micro::GetTensorData<int16_t>(input1),
+                                 tflite_micro::micro::GetTensorShape(input2),
+                                 tflite_micro::micro::GetTensorData<int16_t>(input2),
+                                 tflite_micro::micro::GetTensorShape(output),
+                                 tflite_micro::micro::GetTensorData<int16_t>(output));
     }
   }
   return kTfLiteOk;
@@ -184,30 +184,30 @@ void EvalMulFloatReference(TfLiteContext* context, TfLiteNode* node,
                            const TfLiteEvalTensor* input1,
                            const TfLiteEvalTensor* input2,
                            TfLiteEvalTensor* output) {
-  tflite::ArithmeticParams op_params = {};
+  tflite_micro::ArithmeticParams op_params = {};
   op_params.float_activation_min = data->output_activation_min_f32;
   op_params.float_activation_max = data->output_activation_max_f32;
 
   bool need_broadcast = reference_ops::ProcessBroadcastShapes(
-      tflite::micro::GetTensorShape(input1),
-      tflite::micro::GetTensorShape(input2), &op_params);
+      tflite_micro::micro::GetTensorShape(input1),
+      tflite_micro::micro::GetTensorShape(input2), &op_params);
 
   if (need_broadcast) {
     reference_ops::BroadcastMul4DSlow(
-        op_params, tflite::micro::GetTensorShape(input1),
-        tflite::micro::GetTensorData<float>(input1),
-        tflite::micro::GetTensorShape(input2),
-        tflite::micro::GetTensorData<float>(input2),
-        tflite::micro::GetTensorShape(output),
-        tflite::micro::GetTensorData<float>(output));
+        op_params, tflite_micro::micro::GetTensorShape(input1),
+        tflite_micro::micro::GetTensorData<float>(input1),
+        tflite_micro::micro::GetTensorShape(input2),
+        tflite_micro::micro::GetTensorData<float>(input2),
+        tflite_micro::micro::GetTensorShape(output),
+        tflite_micro::micro::GetTensorData<float>(output));
   } else {
-    reference_ops::Mul(op_params, tflite::micro::GetTensorShape(input1),
-                       tflite::micro::GetTensorData<float>(input1),
-                       tflite::micro::GetTensorShape(input2),
-                       tflite::micro::GetTensorData<float>(input2),
-                       tflite::micro::GetTensorShape(output),
-                       tflite::micro::GetTensorData<float>(output));
+    reference_ops::Mul(op_params, tflite_micro::micro::GetTensorShape(input1),
+                       tflite_micro::micro::GetTensorData<float>(input1),
+                       tflite_micro::micro::GetTensorShape(input2),
+                       tflite_micro::micro::GetTensorData<float>(input2),
+                       tflite_micro::micro::GetTensorShape(output),
+                       tflite_micro::micro::GetTensorData<float>(output));
   }
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/mul_test.cc b/tensorflow/lite/micro/kernels/mul_test.cc
index 2234a1f7..5ffcc935 100644
--- a/tensorflow/lite/micro/kernels/mul_test.cc
+++ b/tensorflow/lite/micro/kernels/mul_test.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -54,7 +54,7 @@ void ValidateMulGoldens(TfLiteTensor* tensors, int tensors_size,
   int outputs_array_data[] = {1, 2};
   TfLiteIntArray* outputs_array = IntArrayFromInts(outputs_array_data);
 
-  const TFLMRegistration registration = tflite::Register_MUL();
+  const TFLMRegistration registration = tflite_micro::Register_MUL();
   micro::KernelRunner runner(registration, tensors, tensors_size, inputs_array,
                              outputs_array,
                              reinterpret_cast<void*>(&builtin_data));
@@ -124,128 +124,128 @@ void TestMulQuantized(int* input1_dims_data, const float* input1_data,
 }  // namespace
 
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(SimpleFloatNoActivationShouldMatchGolden) {
-  float output_data[tflite::testing::flat_size_simple];
+  float output_data[tflite_micro::testing::flat_size_simple];
 
-  tflite::testing::TestMulFloat(
-      tflite::testing::dims_simple, tflite::testing::input1_simple,
-      tflite::testing::dims_simple, tflite::testing::input2_simple,
-      tflite::testing::dims_simple, tflite::testing::golden_simple, output_data,
+  tflite_micro::testing::TestMulFloat(
+      tflite_micro::testing::dims_simple, tflite_micro::testing::input1_simple,
+      tflite_micro::testing::dims_simple, tflite_micro::testing::input2_simple,
+      tflite_micro::testing::dims_simple, tflite_micro::testing::golden_simple, output_data,
       kTfLiteActNone);
 }
 
 TF_LITE_MICRO_TEST(SimpleFloatReluShouldMatchGolden) {
-  float output_data[tflite::testing::flat_size_simple];
+  float output_data[tflite_micro::testing::flat_size_simple];
 
-  tflite::testing::TestMulFloat(
-      tflite::testing::dims_simple, tflite::testing::input1_simple,
-      tflite::testing::dims_simple, tflite::testing::input2_simple,
-      tflite::testing::dims_simple, tflite::testing::golden_simple_relu,
+  tflite_micro::testing::TestMulFloat(
+      tflite_micro::testing::dims_simple, tflite_micro::testing::input1_simple,
+      tflite_micro::testing::dims_simple, tflite_micro::testing::input2_simple,
+      tflite_micro::testing::dims_simple, tflite_micro::testing::golden_simple_relu,
       output_data, kTfLiteActRelu);
 }
 
 TF_LITE_MICRO_TEST(SimpleInt8NoActivationShouldMatchGolden) {
-  int8_t input1_quantized[tflite::testing::flat_size_simple];
-  int8_t input2_quantized[tflite::testing::flat_size_simple];
-  int8_t golden_quantized[tflite::testing::flat_size_simple];
-  int8_t output_data[tflite::testing::flat_size_simple];
-
-  tflite::testing::TestMulQuantized(
-      tflite::testing::dims_simple, tflite::testing::input1_simple,
-      input1_quantized, tflite::testing::dims_simple,
-      tflite::testing::input2_simple, input2_quantized,
-      tflite::testing::scale_simple, 0, tflite::testing::dims_simple,
-      tflite::testing::golden_simple, golden_quantized,
-      tflite::testing::scale_simple, 0, output_data, kTfLiteActNone);
+  int8_t input1_quantized[tflite_micro::testing::flat_size_simple];
+  int8_t input2_quantized[tflite_micro::testing::flat_size_simple];
+  int8_t golden_quantized[tflite_micro::testing::flat_size_simple];
+  int8_t output_data[tflite_micro::testing::flat_size_simple];
+
+  tflite_micro::testing::TestMulQuantized(
+      tflite_micro::testing::dims_simple, tflite_micro::testing::input1_simple,
+      input1_quantized, tflite_micro::testing::dims_simple,
+      tflite_micro::testing::input2_simple, input2_quantized,
+      tflite_micro::testing::scale_simple, 0, tflite_micro::testing::dims_simple,
+      tflite_micro::testing::golden_simple, golden_quantized,
+      tflite_micro::testing::scale_simple, 0, output_data, kTfLiteActNone);
 }
 
 TF_LITE_MICRO_TEST(SimpleInt16NoActivationShouldMatchGolden) {
-  int16_t input1_quantized[tflite::testing::flat_size_simple];
-  int16_t input2_quantized[tflite::testing::flat_size_simple];
-  int16_t golden_quantized[tflite::testing::flat_size_simple];
-  int16_t output_data[tflite::testing::flat_size_simple];
-
-  tflite::testing::TestMulQuantized(
-      tflite::testing::dims_simple, tflite::testing::input1_simple,
-      input1_quantized, tflite::testing::dims_simple,
-      tflite::testing::input2_simple, input2_quantized,
-      tflite::testing::scale_simple, 0, tflite::testing::dims_simple,
-      tflite::testing::golden_simple, golden_quantized,
-      tflite::testing::scale_simple, 0, output_data, kTfLiteActNone);
+  int16_t input1_quantized[tflite_micro::testing::flat_size_simple];
+  int16_t input2_quantized[tflite_micro::testing::flat_size_simple];
+  int16_t golden_quantized[tflite_micro::testing::flat_size_simple];
+  int16_t output_data[tflite_micro::testing::flat_size_simple];
+
+  tflite_micro::testing::TestMulQuantized(
+      tflite_micro::testing::dims_simple, tflite_micro::testing::input1_simple,
+      input1_quantized, tflite_micro::testing::dims_simple,
+      tflite_micro::testing::input2_simple, input2_quantized,
+      tflite_micro::testing::scale_simple, 0, tflite_micro::testing::dims_simple,
+      tflite_micro::testing::golden_simple, golden_quantized,
+      tflite_micro::testing::scale_simple, 0, output_data, kTfLiteActNone);
 }
 
 TF_LITE_MICRO_TEST(BroadcastFloatNoActivationShouldMatchGolden) {
-  float output_data[tflite::testing::flat_size_broadcast];
+  float output_data[tflite_micro::testing::flat_size_broadcast];
 
-  tflite::testing::TestMulFloat(
-      tflite::testing::dims_broadcast, tflite::testing::input1_broadcast,
-      tflite::testing::dims_scalar_broadcast, tflite::testing::input2_broadcast,
-      tflite::testing::dims_broadcast, tflite::testing::golden_broadcast,
+  tflite_micro::testing::TestMulFloat(
+      tflite_micro::testing::dims_broadcast, tflite_micro::testing::input1_broadcast,
+      tflite_micro::testing::dims_scalar_broadcast, tflite_micro::testing::input2_broadcast,
+      tflite_micro::testing::dims_broadcast, tflite_micro::testing::golden_broadcast,
       output_data, kTfLiteActNone);
 }
 
 TF_LITE_MICRO_TEST(BroadcastFloatReluShouldMatchGolden) {
-  float output_data[tflite::testing::flat_size_broadcast];
+  float output_data[tflite_micro::testing::flat_size_broadcast];
 
-  tflite::testing::TestMulFloat(
-      tflite::testing::dims_broadcast, tflite::testing::input1_broadcast,
-      tflite::testing::dims_scalar_broadcast, tflite::testing::input2_broadcast,
-      tflite::testing::dims_broadcast, tflite::testing::golden_broadcast_relu,
+  tflite_micro::testing::TestMulFloat(
+      tflite_micro::testing::dims_broadcast, tflite_micro::testing::input1_broadcast,
+      tflite_micro::testing::dims_scalar_broadcast, tflite_micro::testing::input2_broadcast,
+      tflite_micro::testing::dims_broadcast, tflite_micro::testing::golden_broadcast_relu,
       output_data, kTfLiteActRelu);
 }
 
 TF_LITE_MICRO_TEST(BroadcastInt8NoActivationShouldMatchGolden) {
-  int8_t input1_quantized[tflite::testing::flat_size_broadcast];
-  int8_t input2_quantized[tflite::testing::flat_size_broadcast];
-  int8_t golden_quantized[tflite::testing::flat_size_broadcast];
-  int8_t output_data[tflite::testing::flat_size_broadcast];
-
-  tflite::testing::TestMulQuantized(
-      tflite::testing::dims_broadcast, tflite::testing::input1_broadcast,
-      input1_quantized, tflite::testing::dims_scalar_broadcast,
-      tflite::testing::input2_broadcast, input2_quantized,
-      tflite::testing::input_scale_broadcast, 0,
-      tflite::testing::dims_broadcast, tflite::testing::golden_broadcast,
-      golden_quantized, tflite::testing::output_scale_broadcast, 0, output_data,
+  int8_t input1_quantized[tflite_micro::testing::flat_size_broadcast];
+  int8_t input2_quantized[tflite_micro::testing::flat_size_broadcast];
+  int8_t golden_quantized[tflite_micro::testing::flat_size_broadcast];
+  int8_t output_data[tflite_micro::testing::flat_size_broadcast];
+
+  tflite_micro::testing::TestMulQuantized(
+      tflite_micro::testing::dims_broadcast, tflite_micro::testing::input1_broadcast,
+      input1_quantized, tflite_micro::testing::dims_scalar_broadcast,
+      tflite_micro::testing::input2_broadcast, input2_quantized,
+      tflite_micro::testing::input_scale_broadcast, 0,
+      tflite_micro::testing::dims_broadcast, tflite_micro::testing::golden_broadcast,
+      golden_quantized, tflite_micro::testing::output_scale_broadcast, 0, output_data,
       kTfLiteActNone);
 }
 
 TF_LITE_MICRO_TEST(BroadcastInt16NoActivationShouldMatchGolden) {
-  int16_t input1_quantized[tflite::testing::flat_size_broadcast];
-  int16_t input2_quantized[tflite::testing::flat_size_broadcast];
-  int16_t golden_quantized[tflite::testing::flat_size_broadcast];
-  int16_t output_data[tflite::testing::flat_size_broadcast];
-
-  tflite::testing::TestMulQuantized(
-      tflite::testing::dims_broadcast, tflite::testing::input1_broadcast,
-      input1_quantized, tflite::testing::dims_scalar_broadcast,
-      tflite::testing::input2_broadcast, input2_quantized,
-      tflite::testing::input_scale_broadcast, 0,
-      tflite::testing::dims_broadcast, tflite::testing::golden_broadcast,
-      golden_quantized, tflite::testing::output_scale_broadcast, 0, output_data,
+  int16_t input1_quantized[tflite_micro::testing::flat_size_broadcast];
+  int16_t input2_quantized[tflite_micro::testing::flat_size_broadcast];
+  int16_t golden_quantized[tflite_micro::testing::flat_size_broadcast];
+  int16_t output_data[tflite_micro::testing::flat_size_broadcast];
+
+  tflite_micro::testing::TestMulQuantized(
+      tflite_micro::testing::dims_broadcast, tflite_micro::testing::input1_broadcast,
+      input1_quantized, tflite_micro::testing::dims_scalar_broadcast,
+      tflite_micro::testing::input2_broadcast, input2_quantized,
+      tflite_micro::testing::input_scale_broadcast, 0,
+      tflite_micro::testing::dims_broadcast, tflite_micro::testing::golden_broadcast,
+      golden_quantized, tflite_micro::testing::output_scale_broadcast, 0, output_data,
       kTfLiteActNone);
 }
 
 TF_LITE_MICRO_TEST(SimpleInt32NoActivationShouldMatchGolden) {
-  int32_t input1_quantized[tflite::testing::flat_size_simple];
-  int32_t input2_quantized[tflite::testing::flat_size_simple];
-  int32_t golden_quantized[tflite::testing::flat_size_simple];
-  int32_t output_data[tflite::testing::flat_size_simple];
+  int32_t input1_quantized[tflite_micro::testing::flat_size_simple];
+  int32_t input2_quantized[tflite_micro::testing::flat_size_simple];
+  int32_t golden_quantized[tflite_micro::testing::flat_size_simple];
+  int32_t output_data[tflite_micro::testing::flat_size_simple];
 
   // Int32 mul ignores quantization parameters with TFLite and TFLM. Use
   // TestMulQuantized method to convert float arrays to int32 arrays, but use
   // quantization parameters of 0.01 for both inputs and 0.0001 for output,
   // since input scales are multiplied together to get output scale when there
   // is no rescaling inside the op.
-  tflite::testing::TestMulQuantized(
-      tflite::testing::dims_simple, tflite::testing::input1_simple,
-      input1_quantized, tflite::testing::dims_simple,
-      tflite::testing::input2_simple, input2_quantized, 0.01, 0,
-      tflite::testing::dims_simple, tflite::testing::golden_simple,
+  tflite_micro::testing::TestMulQuantized(
+      tflite_micro::testing::dims_simple, tflite_micro::testing::input1_simple,
+      input1_quantized, tflite_micro::testing::dims_simple,
+      tflite_micro::testing::input2_simple, input2_quantized, 0.01, 0,
+      tflite_micro::testing::dims_simple, tflite_micro::testing::golden_simple,
       golden_quantized, 0.0001, 0, output_data, kTfLiteActNone);
 }
 
diff --git a/tensorflow/lite/micro/kernels/neg.cc b/tensorflow/lite/micro/kernels/neg.cc
index c80a8093..97fad81f 100644
--- a/tensorflow/lite/micro/kernels/neg.cc
+++ b/tensorflow/lite/micro/kernels/neg.cc
@@ -20,7 +20,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -29,16 +29,16 @@ constexpr int kOutputTensor = 0;
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
   switch (input->type) {
     // TODO(wangtz): handle for kTfLiteInt8
     case kTfLiteFloat32:
-      reference_ops::Negate(tflite::micro::GetTensorShape(input),
-                            tflite::micro::GetTensorData<float>(input),
-                            tflite::micro::GetTensorShape(output),
-                            tflite::micro::GetTensorData<float>(output));
+      reference_ops::Negate(tflite_micro::micro::GetTensorShape(input),
+                            tflite_micro::micro::GetTensorData<float>(input),
+                            tflite_micro::micro::GetTensorShape(output),
+                            tflite_micro::micro::GetTensorData<float>(output));
       break;
     default:
       MicroPrintf("Type %s (%d) not supported.", TfLiteTypeGetName(input->type),
@@ -51,7 +51,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_NEG() {
-  return tflite::micro::RegisterOp(nullptr, nullptr, Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, nullptr, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/neg_test.cc b/tensorflow/lite/micro/kernels/neg_test.cc
index a1d16858..b1fddd31 100644
--- a/tensorflow/lite/micro/kernels/neg_test.cc
+++ b/tensorflow/lite/micro/kernels/neg_test.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -58,7 +58,7 @@ void TestNegFloat(int* input_dims_data, const float* input_data,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -68,7 +68,7 @@ TF_LITE_MICRO_TEST(NegOpSingleFloat) {
   const float golden[] = {-8.5, 0.0};
   float output_data[2];
 
-  tflite::testing::TestNegFloat(dims, input_data, golden, dims, output_data);
+  tflite_micro::testing::TestNegFloat(dims, input_data, golden, dims, output_data);
 }
 
 TF_LITE_MICRO_TEST(NegOpFloat) {
@@ -77,7 +77,7 @@ TF_LITE_MICRO_TEST(NegOpFloat) {
   const float golden[] = {2.0f, 1.0f, -0.f, -1.0f, -2.0f, -3.0f};
   float output_data[6];
 
-  tflite::testing::TestNegFloat(dims, input_data, golden, dims, output_data);
+  tflite_micro::testing::TestNegFloat(dims, input_data, golden, dims, output_data);
 }
 
 TF_LITE_MICRO_TESTS_END
diff --git a/tensorflow/lite/micro/kernels/pack.cc b/tensorflow/lite/micro/kernels/pack.cc
index 7b4aeef2..62d0f47e 100644
--- a/tensorflow/lite/micro/kernels/pack.cc
+++ b/tensorflow/lite/micro/kernels/pack.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -29,7 +29,7 @@ template <typename T>
 TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
                       TfLiteEvalTensor* output, int values_count, int axis) {
   const TfLiteEvalTensor* input0 =
-      tflite::micro::GetEvalInput(context, node, 0);
+      tflite_micro::micro::GetEvalInput(context, node, 0);
 
   const int dimensions = output->dims->size;
   const TfLiteIntArray* input_dims = input0->dims;
@@ -53,11 +53,11 @@ TfLiteStatus PackImpl(TfLiteContext* context, TfLiteNode* node,
   }
   TFLITE_DCHECK_EQ(input_size, copy_size * outer_size);
 
-  T* output_data = tflite::micro::GetTensorData<T>(output);
+  T* output_data = tflite_micro::micro::GetTensorData<T>(output);
 
   for (int i = 0; i < values_count; ++i) {
-    const TfLiteEvalTensor* t = tflite::micro::GetEvalInput(context, node, i);
-    const T* input_data = tflite::micro::GetTensorData<T>(t);
+    const TfLiteEvalTensor* t = tflite_micro::micro::GetEvalInput(context, node, i);
+    const T* input_data = tflite_micro::micro::GetTensorData<T>(t);
     for (int k = 0; k < outer_size; ++k) {
       const T* input_ptr = input_data + copy_size * k;
       int loc = k * values_count * copy_size + i * copy_size;
@@ -74,7 +74,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
       reinterpret_cast<TfLitePackParams*>(node->builtin_data);
 
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
   switch (output->type) {
     case kTfLiteFloat32: {
@@ -85,6 +85,10 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
       return PackImpl<int8_t>(context, node, output, data->values_count,
                               data->axis);
     }
+    case kTfLiteInt16: {
+      return PackImpl<int16_t>(context, node, output, data->values_count,
+                              data->axis);
+    }
     case kTfLiteInt32: {
       return PackImpl<int32_t>(context, node, output, data->values_count,
                                data->axis);
@@ -106,7 +110,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_PACK() {
-  return tflite::micro::RegisterOp(nullptr, nullptr, Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, nullptr, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/pack_test.cc b/tensorflow/lite/micro/kernels/pack_test.cc
index 09422176..99a680d5 100644
--- a/tensorflow/lite/micro/kernels/pack_test.cc
+++ b/tensorflow/lite/micro/kernels/pack_test.cc
@@ -20,7 +20,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 
 template <typename T>
@@ -175,7 +175,7 @@ void TestPackTwoInputsQuantized32(
 }
 
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -190,7 +190,7 @@ TF_LITE_MICRO_TEST(PackFloatThreeInputs) {
   constexpr int output_dims_count = 6;
   float output_data[output_dims_count];
 
-  tflite::testing::TestPackThreeInputsFloat(
+  tflite_micro::testing::TestPackThreeInputsFloat(
       input_shape, input1_values, input_shape, input2_values, input_shape,
       input3_values, axis, output_shape, golden, output_data);
 }
@@ -206,7 +206,7 @@ TF_LITE_MICRO_TEST(PackFloatThreeInputsDifferentAxis) {
   constexpr int output_dims_count = 6;
   float output_data[output_dims_count];
 
-  tflite::testing::TestPackThreeInputsFloat(
+  tflite_micro::testing::TestPackThreeInputsFloat(
       input_shape, input1_values, input_shape, input2_values, input_shape,
       input3_values, axis, output_shape, golden, output_data);
 }
@@ -222,7 +222,7 @@ TF_LITE_MICRO_TEST(PackFloatThreeInputsNegativeAxis) {
   constexpr int output_dims_count = 6;
   float output_data[output_dims_count];
 
-  tflite::testing::TestPackThreeInputsFloat(
+  tflite_micro::testing::TestPackThreeInputsFloat(
       input_shape, input1_values, input_shape, input2_values, input_shape,
       input3_values, axis, output_shape, golden, output_data);
 }
@@ -237,7 +237,7 @@ TF_LITE_MICRO_TEST(PackFloatMultilDimensions) {
   constexpr int output_dims_count = 12;
   float output_data[output_dims_count];
 
-  tflite::testing::TestPackTwoInputsFloat(input_shape, input1_values,
+  tflite_micro::testing::TestPackTwoInputsFloat(input_shape, input1_values,
                                           input_shape, input2_values, axis,
                                           output_shape, golden, output_data);
 }
@@ -252,7 +252,7 @@ TF_LITE_MICRO_TEST(PackQuantizedMultilDimensions) {
   constexpr int output_dims_count = 12;
   int8_t output_data[output_dims_count];
 
-  tflite::testing::TestPackTwoInputsQuantized(
+  tflite_micro::testing::TestPackTwoInputsQuantized(
       input_shape, input1_values, input_shape, input2_values, axis,
       output_shape, golden, output_data);
 }
@@ -267,7 +267,7 @@ TF_LITE_MICRO_TEST(PackQuantized32MultilDimensions) {
   constexpr int output_dims_count = 12;
   int32_t output_data[output_dims_count];
 
-  tflite::testing::TestPackTwoInputsQuantized32(
+  tflite_micro::testing::TestPackTwoInputsQuantized32(
       input_shape, input1_values, input_shape, input2_values, axis,
       output_shape, golden, output_data);
 }
diff --git a/tensorflow/lite/micro/kernels/pad.cc b/tensorflow/lite/micro/kernels/pad.cc
index f8d40ade..68170de0 100644
--- a/tensorflow/lite/micro/kernels/pad.cc
+++ b/tensorflow/lite/micro/kernels/pad.cc
@@ -24,7 +24,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 struct OpData {
@@ -42,31 +42,31 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const OpData* data = static_cast<const OpData*>(node->user_data);
 
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, /*index=*/0);
+      tflite_micro::micro::GetEvalInput(context, node, /*index=*/0);
   const TfLiteEvalTensor* constant_values =
       NumInputs(node) == 3
-          ? tflite::micro::GetEvalInput(context, node, /*index=*/2)
+          ? tflite_micro::micro::GetEvalInput(context, node, /*index=*/2)
           : nullptr;
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, /*index=*/0);
+      tflite_micro::micro::GetEvalOutput(context, node, /*index=*/0);
 
   switch (input->type) {
     case kTfLiteFloat32: {
       float pad_value =
           constant_values == nullptr
               ? 0.f
-              : *tflite::micro::GetTensorData<float>(constant_values);
+              : *tflite_micro::micro::GetTensorData<float>(constant_values);
       if (data->params.resizing_category == ResizingCategory::kImageStyle) {
         reference_ops::PadImageStyle(
-            data->params, tflite::micro::GetTensorShape(input),
-            tflite::micro::GetTensorData<float>(input), &pad_value,
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<float>(output));
+            data->params, tflite_micro::micro::GetTensorShape(input),
+            tflite_micro::micro::GetTensorData<float>(input), &pad_value,
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<float>(output));
       } else {
-        reference_ops::Pad(data->params, tflite::micro::GetTensorShape(input),
-                           tflite::micro::GetTensorData<float>(input),
-                           &pad_value, tflite::micro::GetTensorShape(output),
-                           tflite::micro::GetTensorData<float>(output));
+        reference_ops::Pad(data->params, tflite_micro::micro::GetTensorShape(input),
+                           tflite_micro::micro::GetTensorData<float>(input),
+                           &pad_value, tflite_micro::micro::GetTensorShape(output),
+                           tflite_micro::micro::GetTensorData<float>(output));
       }
     } break;
     case kTfLiteInt8: {
@@ -74,40 +74,40 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
       if (constant_values == nullptr) {
         pad_value = static_cast<uint8_t>(data->output_zero_point);
       } else {
-        pad_value = *tflite::micro::GetTensorData<int8_t>(constant_values);
+        pad_value = *tflite_micro::micro::GetTensorData<int8_t>(constant_values);
       }
       if (data->params.resizing_category == ResizingCategory::kImageStyle) {
         reference_ops::PadImageStyle(
-            data->params, tflite::micro::GetTensorShape(input),
-            tflite::micro::GetTensorData<int8_t>(input), &pad_value,
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<int8_t>(output));
+            data->params, tflite_micro::micro::GetTensorShape(input),
+            tflite_micro::micro::GetTensorData<int8_t>(input), &pad_value,
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int8_t>(output));
       } else {
-        reference_ops::Pad(data->params, tflite::micro::GetTensorShape(input),
-                           tflite::micro::GetTensorData<int8_t>(input),
-                           &pad_value, tflite::micro::GetTensorShape(output),
-                           tflite::micro::GetTensorData<int8_t>(output));
+        reference_ops::Pad(data->params, tflite_micro::micro::GetTensorShape(input),
+                           tflite_micro::micro::GetTensorData<int8_t>(input),
+                           &pad_value, tflite_micro::micro::GetTensorShape(output),
+                           tflite_micro::micro::GetTensorData<int8_t>(output));
       }
     } break;
     case kTfLiteInt16: {
       int16_t pad_value =
           constant_values == nullptr
               ? 0
-              : *tflite::micro::GetTensorData<int16_t>(constant_values);
-      reference_ops::Pad(data->params, tflite::micro::GetTensorShape(input),
-                         tflite::micro::GetTensorData<int16_t>(input),
-                         &pad_value, tflite::micro::GetTensorShape(output),
-                         tflite::micro::GetTensorData<int16_t>(output));
+              : *tflite_micro::micro::GetTensorData<int16_t>(constant_values);
+      reference_ops::Pad(data->params, tflite_micro::micro::GetTensorShape(input),
+                         tflite_micro::micro::GetTensorData<int16_t>(input),
+                         &pad_value, tflite_micro::micro::GetTensorShape(output),
+                         tflite_micro::micro::GetTensorData<int16_t>(output));
     } break;
     case kTfLiteInt32: {
       int32_t pad_value =
           constant_values == nullptr
               ? 0
-              : *tflite::micro::GetTensorData<int32_t>(constant_values);
-      reference_ops::Pad(data->params, tflite::micro::GetTensorShape(input),
-                         tflite::micro::GetTensorData<int32_t>(input),
-                         &pad_value, tflite::micro::GetTensorShape(output),
-                         tflite::micro::GetTensorData<int32_t>(output));
+              : *tflite_micro::micro::GetTensorData<int32_t>(constant_values);
+      reference_ops::Pad(data->params, tflite_micro::micro::GetTensorShape(input),
+                         tflite_micro::micro::GetTensorData<int32_t>(input),
+                         &pad_value, tflite_micro::micro::GetTensorShape(output),
+                         tflite_micro::micro::GetTensorData<int32_t>(output));
     } break;
     default:
 
@@ -218,12 +218,12 @@ TfLiteStatus PadPrepare(TfLiteContext* context, TfLiteNode* node) {
 }
 
 TFLMRegistration Register_PAD() {
-  return tflite::micro::RegisterOp(Init, PadPrepare, Eval);
+  return tflite_micro::micro::RegisterOp(Init, PadPrepare, Eval);
 }
 
 // Also register Pad as PadV2.
 TFLMRegistration Register_PADV2() {
-  return tflite::micro::RegisterOp(Init, PadPrepare, Eval);
+  return tflite_micro::micro::RegisterOp(Init, PadPrepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/pad.h b/tensorflow/lite/micro/kernels/pad.h
index ad90890b..265700a0 100644
--- a/tensorflow/lite/micro/kernels/pad.h
+++ b/tensorflow/lite/micro/kernels/pad.h
@@ -18,10 +18,10 @@ limitations under the License.
 
 #include "tensorflow/lite/c/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 TfLiteStatus PadPrepare(TfLiteContext* context, TfLiteNode* node);
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_PAD_H_
diff --git a/tensorflow/lite/micro/kernels/pad_test.cc b/tensorflow/lite/micro/kernels/pad_test.cc
index 21939c8f..87f76776 100644
--- a/tensorflow/lite/micro/kernels/pad_test.cc
+++ b/tensorflow/lite/micro/kernels/pad_test.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -165,7 +165,7 @@ void TestPadQuantized(int* input_dims_data, const float* input_data,
   // Pad tensor must be constant.
   tensors[1].allocation_type = kTfLiteMmapRo;
 
-  tflite::Quantize(golden, golden_quantized, output_dims_count, output_scale,
+  tflite_micro::Quantize(golden, golden_quantized, output_dims_count, output_scale,
                    output_zero_point);
   TF_LITE_MICRO_EXPECT_EQ(
       expected_status,
@@ -205,7 +205,7 @@ void TestPadV2Quantized(
   tensors[2].params.scale = pad_value_scale;
   tensors[3].params.scale = output_scale;
 
-  tflite::Quantize(golden, golden_quantized, output_dims_count, output_scale,
+  tflite_micro::Quantize(golden, golden_quantized, output_dims_count, output_scale,
                    output_zero_point);
   TF_LITE_MICRO_EXPECT_EQ(
       expected_status,
@@ -215,7 +215,7 @@ void TestPadV2Quantized(
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -229,7 +229,7 @@ TF_LITE_MICRO_TEST(Test2DFloat) {
                           0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0};
   float output_data[24];
 
-  tflite::testing::TestPadFloat(input_dims, input_values, pad_dims, pad_values,
+  tflite_micro::testing::TestPadFloat(input_dims, input_values, pad_dims, pad_values,
                                 output_dims, golden, output_data);
 }
 
@@ -247,7 +247,7 @@ TF_LITE_MICRO_TEST(Test4DFloat) {
   golden[40] = 42;
   float output_data[kOutputLen];
 
-  tflite::testing::TestPadFloat(input_dims, input_values, pad_dims, pad_values,
+  tflite_micro::testing::TestPadFloat(input_dims, input_values, pad_dims, pad_values,
                                 output_dims, const_cast<const float*>(golden),
                                 output_data);
 }
@@ -263,7 +263,7 @@ TF_LITE_MICRO_TEST(Test2DFloatV2) {
                           42, 3,  4,  42, 42, 42, 42, 42, 42, 42, 42, 42};
   float output_data[24];
 
-  tflite::testing::TestPadV2Float(input_dims, input_values, pad_dims,
+  tflite_micro::testing::TestPadV2Float(input_dims, input_values, pad_dims,
                                   pad_values, pad_value, output_dims, golden,
                                   output_data);
 }
@@ -284,7 +284,7 @@ TF_LITE_MICRO_TEST(Test2DInt8) {
   int8_t input_quantized[4];
   int8_t golden_quantized[24];
 
-  tflite::testing::TestPadQuantized(
+  tflite_micro::testing::TestPadQuantized(
       input_dims, input_values, input_quantized, input_scale, input_zero_point,
       pad_dims, pad_values, output_dims, golden, golden_quantized, output_scale,
       output_zero_point, output_data);
@@ -306,7 +306,7 @@ TF_LITE_MICRO_TEST(Test2DInt16) {
   int16_t input_quantized[4];
   int16_t golden_quantized[24];
 
-  tflite::testing::TestPadQuantized(
+  tflite_micro::testing::TestPadQuantized(
       input_dims, input_values, input_quantized, input_scale, input_zero_point,
       pad_dims, pad_values, output_dims, golden, golden_quantized, output_scale,
       output_zero_point, output_data);
@@ -328,7 +328,7 @@ TF_LITE_MICRO_TEST(Test2DInt32) {
   int32_t input_quantized[4];
   int32_t golden_quantized[24];
 
-  tflite::testing::TestPadQuantized(
+  tflite_micro::testing::TestPadQuantized(
       input_dims, input_values, input_quantized, input_scale, input_zero_point,
       pad_dims, pad_values, output_dims, golden, golden_quantized, output_scale,
       output_zero_point, output_data);
@@ -353,7 +353,7 @@ TF_LITE_MICRO_TEST(Test2DInt8V2) {
   int8_t input_quantized[4];
   int8_t golden_quantized[24];
 
-  tflite::testing::TestPadV2Quantized(
+  tflite_micro::testing::TestPadV2Quantized(
       input_dims, input_values, input_quantized, input_scale, input_zero_point,
       pad_dims, pad_values, pad_value, pad_value_scale, pad_value_zero_point,
       output_dims, golden, golden_quantized, output_scale, output_zero_point,
@@ -379,7 +379,7 @@ TF_LITE_MICRO_TEST(Test2DInt16V2) {
   int16_t input_quantized[4];
   int16_t golden_quantized[24];
 
-  tflite::testing::TestPadV2Quantized(
+  tflite_micro::testing::TestPadV2Quantized(
       input_dims, input_values, input_quantized, input_scale, input_zero_point,
       pad_dims, pad_values, pad_value, pad_value_scale, pad_value_zero_point,
       output_dims, golden, golden_quantized, output_scale, output_zero_point,
@@ -405,7 +405,7 @@ TF_LITE_MICRO_TEST(Test2DInt32V2) {
   int32_t input_quantized[4];
   int32_t golden_quantized[24];
 
-  tflite::testing::TestPadV2Quantized(
+  tflite_micro::testing::TestPadV2Quantized(
       input_dims, input_values, input_quantized, input_scale, input_zero_point,
       pad_dims, pad_values, pad_value, pad_value_scale, pad_value_zero_point,
       output_dims, golden, golden_quantized, output_scale, output_zero_point,
@@ -433,7 +433,7 @@ TF_LITE_MICRO_TEST(Test2DInt8V2ExpectFailurePadValueQuantizationMismatch) {
   int8_t input_quantized[4];
   int8_t golden_quantized[24];
 
-  tflite::testing::TestPadV2Quantized(
+  tflite_micro::testing::TestPadV2Quantized(
       input_dims, input_values, input_quantized, input_scale, input_zero_point,
       pad_dims, pad_values, pad_value, pad_value_scale, pad_value_zero_point,
       output_dims, golden, golden_quantized, output_scale, output_zero_point,
@@ -461,7 +461,7 @@ TF_LITE_MICRO_TEST(Test2DInt8V2ExpectFailurePadValueQuantizationMismatch) {
   int8_t input_quantized[4];
   int8_t golden_quantized[24];
 
-  tflite::testing::TestPadV2Quantized(
+  tflite_micro::testing::TestPadV2Quantized(
       input_dims, input_values, input_quantized, input_scale, input_zero_point,
       pad_dims, pad_values, pad_value, pad_value_scale, pad_value_zero_point,
       output_dims, golden, golden_quantized, output_scale, output_zero_point,
@@ -485,7 +485,7 @@ TF_LITE_MICRO_TEST(Test2DInt8ExpectFailureQuantizationRangeExcludesZero) {
   int8_t input_quantized[4];
   int8_t golden_quantized[24];
 
-  tflite::testing::TestPadQuantized(
+  tflite_micro::testing::TestPadQuantized(
       input_dims, input_values, input_quantized, input_scale, input_zero_point,
       pad_dims, pad_values, output_dims, golden, golden_quantized, output_scale,
       output_zero_point, output_data, kTfLiteError);
diff --git a/tensorflow/lite/micro/kernels/pooling.cc b/tensorflow/lite/micro/kernels/pooling.cc
index e03f72ec..1f71fd22 100644
--- a/tensorflow/lite/micro/kernels/pooling.cc
+++ b/tensorflow/lite/micro/kernels/pooling.cc
@@ -20,7 +20,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/pooling.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -99,11 +99,11 @@ void* Init(TfLiteContext* context, const char* buffer, size_t length) {
 }  // namespace
 
 TFLMRegistration Register_AVERAGE_POOL_2D() {
-  return tflite::micro::RegisterOp(Init, PoolingPrepare, AverageEval);
+  return tflite_micro::micro::RegisterOp(Init, PoolingPrepare, AverageEval);
 }
 
 TFLMRegistration Register_MAX_POOL_2D() {
-  return tflite::micro::RegisterOp(Init, PoolingPrepare, MaxEval);
+  return tflite_micro::micro::RegisterOp(Init, PoolingPrepare, MaxEval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/pooling.h b/tensorflow/lite/micro/kernels/pooling.h
index a87e22c3..2c0aa0f0 100644
--- a/tensorflow/lite/micro/kernels/pooling.h
+++ b/tensorflow/lite/micro/kernels/pooling.h
@@ -29,7 +29,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/micro_ops.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 extern const int kPoolingInputTensor;
 extern const int kPoolingOutputTensor;
@@ -76,10 +76,10 @@ void AveragePoolingEvalQuantized(TfLiteContext* context, const TfLiteNode* node,
   op_params.quantized_activation_max = data->activation_max;
 
   reference_integer_ops::AveragePool(op_params,
-                                     tflite::micro::GetTensorShape(input),
-                                     tflite::micro::GetTensorData<T>(input),
-                                     tflite::micro::GetTensorShape(output),
-                                     tflite::micro::GetTensorData<T>(output));
+                                     tflite_micro::micro::GetTensorShape(input),
+                                     tflite_micro::micro::GetTensorData<T>(input),
+                                     tflite_micro::micro::GetTensorShape(output),
+                                     tflite_micro::micro::GetTensorData<T>(output));
 }
 
 void MaxPoolingEvalFloat(TfLiteContext* context, TfLiteNode* node,
@@ -95,7 +95,7 @@ void MaxPoolingEvalQuantized(TfLiteContext* context, TfLiteNode* node,
                              TfLiteEvalTensor* output) {
   TFLITE_DCHECK(input->type == kTfLiteInt8 || input->type == kTfLiteInt16);
 
-  tflite::PoolParams op_params;
+  tflite_micro::PoolParams op_params;
   op_params.stride_height = params->stride_height;
   op_params.stride_width = params->stride_width;
   op_params.filter_height = params->filter_height;
@@ -106,10 +106,10 @@ void MaxPoolingEvalQuantized(TfLiteContext* context, TfLiteNode* node,
   op_params.quantized_activation_max = data->activation_max;
 
   reference_integer_ops::MaxPool(op_params,
-                                 tflite::micro::GetTensorShape(input),
-                                 tflite::micro::GetTensorData<T>(input),
-                                 tflite::micro::GetTensorShape(output),
-                                 tflite::micro::GetTensorData<T>(output));
+                                 tflite_micro::micro::GetTensorShape(input),
+                                 tflite_micro::micro::GetTensorData<T>(input),
+                                 tflite_micro::micro::GetTensorShape(output),
+                                 tflite_micro::micro::GetTensorData<T>(output));
 }
 
 #if defined(CMSIS_NN) || defined(XTENSA)
@@ -122,21 +122,21 @@ TFLMRegistration Register_AVERAGE_POOL_2D_INT16();
 TFLMRegistration Register_MAX_POOL_2D_INT16();
 #else
 inline TFLMRegistration Register_AVERAGE_POOL_2D_INT8() {
-  return tflite::Register_AVERAGE_POOL_2D();
+  return tflite_micro::Register_AVERAGE_POOL_2D();
 }
 
 inline TFLMRegistration Register_MAX_POOL_2D_INT8() {
-  return tflite::Register_MAX_POOL_2D();
+  return tflite_micro::Register_MAX_POOL_2D();
 }
 
 inline TFLMRegistration Register_AVERAGE_POOL_2D_INT16() {
-  return tflite::Register_AVERAGE_POOL_2D();
+  return tflite_micro::Register_AVERAGE_POOL_2D();
 }
 
 inline TFLMRegistration Register_MAX_POOL_2D_INT16() {
-  return tflite::Register_MAX_POOL_2D();
+  return tflite_micro::Register_MAX_POOL_2D();
 }
 #endif
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_POOLING_H_
diff --git a/tensorflow/lite/micro/kernels/pooling_common.cc b/tensorflow/lite/micro/kernels/pooling_common.cc
index b39e9d84..d53e9d11 100644
--- a/tensorflow/lite/micro/kernels/pooling_common.cc
+++ b/tensorflow/lite/micro/kernels/pooling_common.cc
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/kernels/pooling.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 const int kPoolingInputTensor = 0;
 const int kPoolingOutputTensor = 0;
@@ -100,17 +100,17 @@ void AveragePoolingEvalFloat(const TfLiteContext* context,
   op_params.padding_values.width = data->padding.width;
   op_params.float_activation_min = data->activation_min_f32;
   op_params.float_activation_max = data->activation_max_f32;
-  reference_ops::AveragePool(op_params, tflite::micro::GetTensorShape(input),
-                             tflite::micro::GetTensorData<float>(input),
-                             tflite::micro::GetTensorShape(output),
-                             tflite::micro::GetTensorData<float>(output));
+  reference_ops::AveragePool(op_params, tflite_micro::micro::GetTensorShape(input),
+                             tflite_micro::micro::GetTensorData<float>(input),
+                             tflite_micro::micro::GetTensorShape(output),
+                             tflite_micro::micro::GetTensorData<float>(output));
 }
 
 void MaxPoolingEvalFloat(TfLiteContext* context, TfLiteNode* node,
                          TfLitePoolParams* params, const OpDataPooling* data,
                          const TfLiteEvalTensor* input,
                          TfLiteEvalTensor* output) {
-  tflite::PoolParams op_params;
+  tflite_micro::PoolParams op_params;
   op_params.stride_height = params->stride_height;
   op_params.stride_width = params->stride_width;
   op_params.filter_height = params->filter_height;
@@ -119,10 +119,10 @@ void MaxPoolingEvalFloat(TfLiteContext* context, TfLiteNode* node,
   op_params.padding_values.width = data->padding.width;
   op_params.float_activation_min = data->activation_min_f32;
   op_params.float_activation_max = data->activation_max_f32;
-  reference_ops::MaxPool(op_params, tflite::micro::GetTensorShape(input),
-                         tflite::micro::GetTensorData<float>(input),
-                         tflite::micro::GetTensorShape(output),
-                         tflite::micro::GetTensorData<float>(output));
+  reference_ops::MaxPool(op_params, tflite_micro::micro::GetTensorShape(input),
+                         tflite_micro::micro::GetTensorData<float>(input),
+                         tflite_micro::micro::GetTensorShape(output),
+                         tflite_micro::micro::GetTensorData<float>(output));
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/pooling_test.cc b/tensorflow/lite/micro/kernels/pooling_test.cc
index b0ee3a3d..6cd989d2 100644
--- a/tensorflow/lite/micro/kernels/pooling_test.cc
+++ b/tensorflow/lite/micro/kernels/pooling_test.cc
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -174,7 +174,7 @@ void TestMaxPoolQuantized(int* input_dims_data, const T* input_data,
 }  // namespace
 
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -188,7 +188,7 @@ TF_LITE_MICRO_TEST(SimpleAveragePoolTestFloat) {
   const float golden[] = {2.75, 5.75};
   int output_shape[] = {4, 1, 1, 2, 1};
   float output_data[2];
-  tflite::testing::TestAveragePoolFloat(
+  tflite_micro::testing::TestAveragePoolFloat(
       input_shape, input_values, filter_height, filter_width, stride_height,
       stride_width, golden, output_shape, kTfLitePaddingValid, kTfLiteActNone,
       output_data);
@@ -209,7 +209,7 @@ TF_LITE_MICRO_TEST(SimpleAveragePoolTestInt8PaddingValidStride2ActNone) {
   const int input_zero_point = 0;
   const float output_scale = .25;
   const int output_zero_point = 0;
-  tflite::testing::TestAveragePoolQuantized(
+  tflite_micro::testing::TestAveragePoolQuantized(
       input_shape, input_values, input_scale, input_zero_point, filter_height,
       filter_width, stride_height, stride_width, golden, output_shape,
       output_scale, output_zero_point, kTfLitePaddingValid, kTfLiteActNone,
@@ -231,7 +231,7 @@ TF_LITE_MICRO_TEST(SimpleAveragePoolTestInt8PaddingValidStride1Stride2Relu) {
   const int input_zero_point = 0;
   const float output_scale = .25;
   const int output_zero_point = 0;
-  tflite::testing::TestAveragePoolQuantized(
+  tflite_micro::testing::TestAveragePoolQuantized(
       input_shape, input_values, input_scale, input_zero_point, filter_height,
       filter_width, stride_height, stride_width, golden, output_shape,
       output_scale, output_zero_point, kTfLitePaddingValid, kTfLiteActRelu,
@@ -254,7 +254,7 @@ TF_LITE_MICRO_TEST(
   const int input_zero_point = 0;
   const float output_scale = .25;
   const int output_zero_point = 0;
-  tflite::testing::TestAveragePoolQuantized(
+  tflite_micro::testing::TestAveragePoolQuantized(
       input_shape, input_values, input_scale, input_zero_point, filter_height,
       filter_width, stride_height, stride_width, golden, output_shape,
       output_scale, output_zero_point, kTfLitePaddingValid, kTfLiteActReluN1To1,
@@ -276,7 +276,7 @@ TF_LITE_MICRO_TEST(SimpleAveragePoolTestInt8PaddingValidStride2Relu6) {
   const int input_zero_point = 0;
   const float output_scale = .25;
   const int output_zero_point = 0;
-  tflite::testing::TestAveragePoolQuantized(
+  tflite_micro::testing::TestAveragePoolQuantized(
       input_shape, input_values, input_scale, input_zero_point, filter_height,
       filter_width, stride_height, stride_width, golden, output_shape,
       output_scale, output_zero_point, kTfLitePaddingValid, kTfLiteActRelu6,
@@ -298,7 +298,7 @@ TF_LITE_MICRO_TEST(SimpleAveragePoolTestInt8PaddingSameStride1ActNone) {
   const int input_zero_point = 0;
   const float output_scale = .25;
   const int output_zero_point = 0;
-  tflite::testing::TestAveragePoolQuantized(
+  tflite_micro::testing::TestAveragePoolQuantized(
       input_shape, input_values, input_scale, input_zero_point, filter_height,
       filter_width, stride_height, stride_width, golden, output_shape,
       output_scale, output_zero_point, kTfLitePaddingValid, kTfLiteActNone,
@@ -320,7 +320,7 @@ TF_LITE_MICRO_TEST(SimpleAveragePoolTestInt16PaddingValidStride2ActNone) {
   const int input_zero_point = 0;
   const float output_scale = .25;
   const int output_zero_point = 0;
-  tflite::testing::TestAveragePoolQuantized(
+  tflite_micro::testing::TestAveragePoolQuantized(
       input_shape, input_values, input_scale, input_zero_point, filter_height,
       filter_width, stride_height, stride_width, golden, output_shape,
       output_scale, output_zero_point, kTfLitePaddingValid, kTfLiteActNone,
@@ -342,7 +342,7 @@ TF_LITE_MICRO_TEST(SimpleAveragePoolTestInt16PaddingValidStride1Stride2Relu) {
   const int input_zero_point = 0;
   const float output_scale = .25;
   const int output_zero_point = 0;
-  tflite::testing::TestAveragePoolQuantized(
+  tflite_micro::testing::TestAveragePoolQuantized(
       input_shape, input_values, input_scale, input_zero_point, filter_height,
       filter_width, stride_height, stride_width, golden, output_shape,
       output_scale, output_zero_point, kTfLitePaddingValid, kTfLiteActRelu,
@@ -365,7 +365,7 @@ TF_LITE_MICRO_TEST(
   const int input_zero_point = 0;
   const float output_scale = .25;
   const int output_zero_point = 0;
-  tflite::testing::TestAveragePoolQuantized(
+  tflite_micro::testing::TestAveragePoolQuantized(
       input_shape, input_values, input_scale, input_zero_point, filter_height,
       filter_width, stride_height, stride_width, golden, output_shape,
       output_scale, output_zero_point, kTfLitePaddingValid, kTfLiteActReluN1To1,
@@ -387,7 +387,7 @@ TF_LITE_MICRO_TEST(SimpleAveragePoolTestInt16PaddingValidStride2Relu6) {
   const int input_zero_point = 0;
   const float output_scale = .25;
   const int output_zero_point = 0;
-  tflite::testing::TestAveragePoolQuantized(
+  tflite_micro::testing::TestAveragePoolQuantized(
       input_shape, input_values, input_scale, input_zero_point, filter_height,
       filter_width, stride_height, stride_width, golden, output_shape,
       output_scale, output_zero_point, kTfLitePaddingValid, kTfLiteActRelu6,
@@ -409,7 +409,7 @@ TF_LITE_MICRO_TEST(SimpleAveragePoolTestInt16PaddingSameStride1ActNone) {
   const int input_zero_point = 0;
   const float output_scale = .25;
   const int output_zero_point = 0;
-  tflite::testing::TestAveragePoolQuantized(
+  tflite_micro::testing::TestAveragePoolQuantized(
       input_shape, input_values, input_scale, input_zero_point, filter_height,
       filter_width, stride_height, stride_width, golden, output_shape,
       output_scale, output_zero_point, kTfLitePaddingValid, kTfLiteActNone,
@@ -426,7 +426,7 @@ TF_LITE_MICRO_TEST(SimpleMaxPoolTestFloat) {
   const float golden[] = {6, 10};
   int output_shape[] = {4, 1, 1, 2, 1};
   float output_data[2];
-  tflite::testing::TestMaxPoolFloat(input_shape, input_values, filter_height,
+  tflite_micro::testing::TestMaxPoolFloat(input_shape, input_values, filter_height,
                                     filter_width, stride_height, stride_width,
                                     golden, output_shape, kTfLitePaddingValid,
                                     kTfLiteActNone, output_data);
@@ -442,7 +442,7 @@ TF_LITE_MICRO_TEST(SimpleMaxPoolTestFloatRelu) {
   const float golden[] = {0, 10.5};
   int output_shape[] = {4, 1, 1, 2, 1};
   float output_data[2];
-  tflite::testing::TestMaxPoolFloat(input_shape, input_values, filter_height,
+  tflite_micro::testing::TestMaxPoolFloat(input_shape, input_values, filter_height,
                                     filter_width, stride_height, stride_width,
                                     golden, output_shape, kTfLitePaddingValid,
                                     kTfLiteActRelu, output_data);
@@ -458,14 +458,14 @@ TF_LITE_MICRO_TEST(SimpleMaxPoolTestFloatReluN1To1) {
   const float golden1[] = {-1.0, 0.7};
   int output_shape[] = {4, 1, 1, 2, 1};
   float output_data[2];
-  tflite::testing::TestMaxPoolFloat(input_shape, input_values1, filter_height,
+  tflite_micro::testing::TestMaxPoolFloat(input_shape, input_values1, filter_height,
                                     filter_width, stride_height, stride_width,
                                     golden1, output_shape, kTfLitePaddingValid,
                                     kTfLiteActReluN1To1, output_data);
 
   const float input_values2[] = {-2.75, -6, -2, -4, -3, -2, 10, -7};
   const float golden2[] = {-1.0, 1.0};
-  tflite::testing::TestMaxPoolFloat(input_shape, input_values2, filter_height,
+  tflite_micro::testing::TestMaxPoolFloat(input_shape, input_values2, filter_height,
                                     filter_width, stride_height, stride_width,
                                     golden2, output_shape, kTfLitePaddingValid,
                                     kTfLiteActReluN1To1, output_data);
@@ -481,14 +481,14 @@ TF_LITE_MICRO_TEST(SimpleMaxPoolTestFloatRelu6) {
   const float golden1[] = {0, 6};
   int output_shape[] = {4, 1, 1, 2, 1};
   float output_data[2];
-  tflite::testing::TestMaxPoolFloat(input_shape, input_values1, filter_height,
+  tflite_micro::testing::TestMaxPoolFloat(input_shape, input_values1, filter_height,
                                     filter_width, stride_height, stride_width,
                                     golden1, output_shape, kTfLitePaddingValid,
                                     kTfLiteActRelu6, output_data);
 
   const float input_values2[] = {0, 4.5, 12, 4, 3, 2, 10, 7};
   const float golden2[] = {4.5, 6};
-  tflite::testing::TestMaxPoolFloat(input_shape, input_values2, filter_height,
+  tflite_micro::testing::TestMaxPoolFloat(input_shape, input_values2, filter_height,
                                     filter_width, stride_height, stride_width,
                                     golden2, output_shape, kTfLitePaddingValid,
                                     kTfLiteActRelu6, output_data);
@@ -504,7 +504,7 @@ TF_LITE_MICRO_TEST(SimpleMaxPoolTestPaddingSameStride1) {
   const float golden[] = {6, 10, 10, 7, 3, 10, 10, 7};
   int output_shape[] = {4, 1, 2, 4, 1};
   float output_data[8];
-  tflite::testing::TestMaxPoolFloat(input_shape, input_values, filter_height,
+  tflite_micro::testing::TestMaxPoolFloat(input_shape, input_values, filter_height,
                                     filter_width, stride_height, stride_width,
                                     golden, output_shape, kTfLitePaddingSame,
                                     kTfLiteActNone, output_data);
@@ -520,7 +520,7 @@ TF_LITE_MICRO_TEST(SimpleMaxPoolTestPaddingValidStride1) {
   const float golden[] = {6, 10, 10};
   int output_shape[] = {4, 1, 1, 3, 1};
   float output_data[8];
-  tflite::testing::TestMaxPoolFloat(input_shape, input_values, filter_height,
+  tflite_micro::testing::TestMaxPoolFloat(input_shape, input_values, filter_height,
                                     filter_width, stride_height, stride_width,
                                     golden, output_shape, kTfLitePaddingValid,
                                     kTfLiteActNone, output_data);
@@ -541,7 +541,7 @@ TF_LITE_MICRO_TEST(SimpleMaxPoolTestInt8ActNone) {
   const int input_zero_point = 0;
   const float output_scale = 1.0;
   const int output_zero_point = 0;
-  tflite::testing::TestMaxPoolQuantized(
+  tflite_micro::testing::TestMaxPoolQuantized(
       input_shape, input_values1, input_scale, input_zero_point, filter_height,
       filter_width, stride_height, stride_width, golden1, output_shape,
       output_scale, output_zero_point, kTfLitePaddingValid, kTfLiteActNone,
@@ -563,7 +563,7 @@ TF_LITE_MICRO_TEST(MaxPoolTestInt8ActRelu) {
   const int input_zero_point = 0;
   const float output_scale = 0.5;
   const int output_zero_point = 0;
-  tflite::testing::TestMaxPoolQuantized(
+  tflite_micro::testing::TestMaxPoolQuantized(
       input_shape, input_values1, input_scale, input_zero_point, filter_height,
       filter_width, stride_height, stride_width, golden1, output_shape,
       output_scale, output_zero_point, kTfLitePaddingValid, kTfLiteActRelu,
@@ -585,7 +585,7 @@ TF_LITE_MICRO_TEST(MaxPoolTestInt8ActReluN1To1) {
   const int input_zero_point = 0;
   const float output_scale = 1.0;
   const int output_zero_point = 0;
-  tflite::testing::TestMaxPoolQuantized(
+  tflite_micro::testing::TestMaxPoolQuantized(
       input_shape, input_values1, input_scale, input_zero_point, filter_height,
       filter_width, stride_height, stride_width, golden1, output_shape,
       output_scale, output_zero_point, kTfLitePaddingValid, kTfLiteActReluN1To1,
@@ -607,7 +607,7 @@ TF_LITE_MICRO_TEST(MaxPoolTestInt8ActRelu6) {
   const int input_zero_point = 0;
   const float output_scale = 1.0;
   const int output_zero_point = 0;
-  tflite::testing::TestMaxPoolQuantized(
+  tflite_micro::testing::TestMaxPoolQuantized(
       input_shape, input_values1, input_scale, input_zero_point, filter_height,
       filter_width, stride_height, stride_width, golden1, output_shape,
       output_scale, output_zero_point, kTfLitePaddingValid, kTfLiteActRelu6,
@@ -629,7 +629,7 @@ TF_LITE_MICRO_TEST(SimpleMaxPoolTestInt16ActNone) {
   const int input_zero_point = 0;
   const float output_scale = 1.0;
   const int output_zero_point = 0;
-  tflite::testing::TestMaxPoolQuantized(
+  tflite_micro::testing::TestMaxPoolQuantized(
       input_shape, input_values1, input_scale, input_zero_point, filter_height,
       filter_width, stride_height, stride_width, golden1, output_shape,
       output_scale, output_zero_point, kTfLitePaddingValid, kTfLiteActNone,
@@ -651,7 +651,7 @@ TF_LITE_MICRO_TEST(MaxPoolTestInt16ActRelu) {
   const int input_zero_point = 0;
   const float output_scale = 0.5;
   const int output_zero_point = 0;
-  tflite::testing::TestMaxPoolQuantized(
+  tflite_micro::testing::TestMaxPoolQuantized(
       input_shape, input_values1, input_scale, input_zero_point, filter_height,
       filter_width, stride_height, stride_width, golden1, output_shape,
       output_scale, output_zero_point, kTfLitePaddingValid, kTfLiteActRelu,
@@ -673,7 +673,7 @@ TF_LITE_MICRO_TEST(MaxPoolTestInt16ActReluN1To1) {
   const int input_zero_point = 0;
   const float output_scale = 1.0;
   const int output_zero_point = 0;
-  tflite::testing::TestMaxPoolQuantized(
+  tflite_micro::testing::TestMaxPoolQuantized(
       input_shape, input_values1, input_scale, input_zero_point, filter_height,
       filter_width, stride_height, stride_width, golden1, output_shape,
       output_scale, output_zero_point, kTfLitePaddingValid, kTfLiteActReluN1To1,
@@ -695,7 +695,7 @@ TF_LITE_MICRO_TEST(MaxPoolTestInt16ActRelu6) {
   const int input_zero_point = 0;
   const float output_scale = 1.0;
   const int output_zero_point = 0;
-  tflite::testing::TestMaxPoolQuantized(
+  tflite_micro::testing::TestMaxPoolQuantized(
       input_shape, input_values1, input_scale, input_zero_point, filter_height,
       filter_width, stride_height, stride_width, golden1, output_shape,
       output_scale, output_zero_point, kTfLitePaddingValid, kTfLiteActRelu6,
diff --git a/tensorflow/lite/micro/kernels/prelu.cc b/tensorflow/lite/micro/kernels/prelu.cc
index 66a017b2..5629cd70 100644
--- a/tensorflow/lite/micro/kernels/prelu.cc
+++ b/tensorflow/lite/micro/kernels/prelu.cc
@@ -25,7 +25,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/prelu.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 void* PreluInit(TfLiteContext* context, const char* buffer, size_t length) {
   TFLITE_DCHECK(context->AllocatePersistentBuffer != nullptr);
@@ -37,28 +37,28 @@ TfLiteStatus PreluEval(TfLiteContext* context, TfLiteNode* node) {
   const PreluParams& params =
       *(static_cast<const PreluParams*>(node->user_data));
 
-  const TfLiteEvalTensor* input = tflite::micro::GetEvalInput(context, node, 0);
-  const TfLiteEvalTensor* alpha = tflite::micro::GetEvalInput(context, node, 1);
-  TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, 0);
+  const TfLiteEvalTensor* input = tflite_micro::micro::GetEvalInput(context, node, 0);
+  const TfLiteEvalTensor* alpha = tflite_micro::micro::GetEvalInput(context, node, 1);
+  TfLiteEvalTensor* output = tflite_micro::micro::GetEvalOutput(context, node, 0);
 
   switch (input->type) {
     case kTfLiteFloat32: {
-      BroadcastPrelu4DSlowFloat(tflite::micro::GetTensorShape(input),
-                                tflite::micro::GetTensorData<float>(input),
-                                tflite::micro::GetTensorShape(alpha),
-                                tflite::micro::GetTensorData<float>(alpha),
-                                tflite::micro::GetTensorShape(output),
-                                tflite::micro::GetTensorData<float>(output));
+      BroadcastPrelu4DSlowFloat(tflite_micro::micro::GetTensorShape(input),
+                                tflite_micro::micro::GetTensorData<float>(input),
+                                tflite_micro::micro::GetTensorShape(alpha),
+                                tflite_micro::micro::GetTensorData<float>(alpha),
+                                tflite_micro::micro::GetTensorShape(output),
+                                tflite_micro::micro::GetTensorData<float>(output));
       return kTfLiteOk;
     } break;
     case kTfLiteInt8: {
       reference_ops::BroadcastPrelu4DSlow(
-          params, tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<int8_t>(input),
-          tflite::micro::GetTensorShape(alpha),
-          tflite::micro::GetTensorData<int8_t>(alpha),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<int8_t>(output));
+          params, tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<int8_t>(input),
+          tflite_micro::micro::GetTensorShape(alpha),
+          tflite_micro::micro::GetTensorData<int8_t>(alpha),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<int8_t>(output));
       return kTfLiteOk;
     } break;
     default:
@@ -69,7 +69,7 @@ TfLiteStatus PreluEval(TfLiteContext* context, TfLiteNode* node) {
 }
 
 TFLMRegistration Register_PRELU() {
-  return tflite::micro::RegisterOp(PreluInit, PreluPrepare, PreluEval);
+  return tflite_micro::micro::RegisterOp(PreluInit, PreluPrepare, PreluEval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/prelu.h b/tensorflow/lite/micro/kernels/prelu.h
index 571d1e88..e7bfbf17 100644
--- a/tensorflow/lite/micro/kernels/prelu.h
+++ b/tensorflow/lite/micro/kernels/prelu.h
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/c/common.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 TfLiteStatus CalculatePreluParams(const TfLiteTensor* input,
                                   const TfLiteTensor* alpha,
@@ -34,6 +34,6 @@ void BroadcastPrelu4DSlowFloat(const RuntimeShape& unextended_input1_shape,
 
 TfLiteStatus PreluPrepare(TfLiteContext* context, TfLiteNode* node);
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_PRELU_H_
diff --git a/tensorflow/lite/micro/kernels/prelu_common.cc b/tensorflow/lite/micro/kernels/prelu_common.cc
index 1a89cadf..e0a546cd 100644
--- a/tensorflow/lite/micro/kernels/prelu_common.cc
+++ b/tensorflow/lite/micro/kernels/prelu_common.cc
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/kernels/prelu.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 TfLiteStatus CalculatePreluParams(const TfLiteTensor* input,
                                   const TfLiteTensor* alpha,
@@ -102,4 +102,4 @@ TfLiteStatus PreluPrepare(TfLiteContext* context, TfLiteNode* node) {
   return kTfLiteOk;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/prelu_test.cc b/tensorflow/lite/micro/kernels/prelu_test.cc
index e4060347..b80584e0 100644
--- a/tensorflow/lite/micro/kernels/prelu_test.cc
+++ b/tensorflow/lite/micro/kernels/prelu_test.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -32,7 +32,7 @@ void ValidatePreluGoldens(TfLiteTensor* tensors, int tensors_size,
   int outputs_array_data[] = {1, 2};
   TfLiteIntArray* outputs_array = IntArrayFromInts(outputs_array_data);
 
-  const TFLMRegistration registration = tflite::Register_PRELU();
+  const TFLMRegistration registration = tflite_micro::Register_PRELU();
   micro::KernelRunner runner(registration, tensors, tensors_size, inputs_array,
                              outputs_array,
                              /*builtin_data=*/nullptr);
@@ -99,7 +99,7 @@ void TestPreluQuantized(int* input_dims_data, const float* input_data,
 }
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -122,7 +122,7 @@ TF_LITE_MICRO_TEST(FloatPreluActivationsOpTest) {
   };
   const int output_dims_count = 12;
   float output_data[output_dims_count];
-  tflite::testing::TestPreluFloat(input_shape, input_values, alpha_shape,
+  tflite_micro::testing::TestPreluFloat(input_shape, input_values, alpha_shape,
                                   alpha_values, golden, output_shape,
                                   output_data);
 }
@@ -151,7 +151,7 @@ TF_LITE_MICRO_TEST(QuantizedInt8PreluActivationsOpTest) {
   float scale = 2.0 / 255.0;
   int zero_point = 0;
   int8_t output_data[dims_count];
-  tflite::testing::TestPreluQuantized(
+  tflite_micro::testing::TestPreluQuantized(
       input_shape, input_values, input_quantized, scale, zero_point,
       alpha_shape, alpha_values, alpha_quantized, scale, zero_point, golden,
       golden_quantized, scale, zero_point, output_shape, output_data);
diff --git a/tensorflow/lite/micro/kernels/quantization_util_test.cc b/tensorflow/lite/micro/kernels/quantization_util_test.cc
index 76ee9eef..a300a17c 100644
--- a/tensorflow/lite/micro/kernels/quantization_util_test.cc
+++ b/tensorflow/lite/micro/kernels/quantization_util_test.cc
@@ -17,7 +17,7 @@ limitations under the License.
 
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 template <class FloatIn, class IntOut>
@@ -171,27 +171,27 @@ void RunSafeCastTests() {
 }
 
 }  // namespace
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(QuantizationUtilTest_SafeCast) {
-  tflite::RunSafeCastTests<float, int8_t>();
-  tflite::RunSafeCastTests<double, int8_t>();
-  tflite::RunSafeCastTests<float, int16_t>();
-  tflite::RunSafeCastTests<double, int16_t>();
-  tflite::RunSafeCastTests<float, int32_t>();
-  tflite::RunSafeCastTests<double, int32_t>();
-  tflite::RunSafeCastTests<float, int64_t>();
-  tflite::RunSafeCastTests<double, int64_t>();
-  tflite::RunSafeCastTests<float, uint8_t>();
-  tflite::RunSafeCastTests<double, uint8_t>();
-  tflite::RunSafeCastTests<float, uint16_t>();
-  tflite::RunSafeCastTests<double, uint16_t>();
-  tflite::RunSafeCastTests<float, uint32_t>();
-  tflite::RunSafeCastTests<double, uint32_t>();
-  tflite::RunSafeCastTests<float, uint64_t>();
-  tflite::RunSafeCastTests<double, uint64_t>();
+  tflite_micro::RunSafeCastTests<float, int8_t>();
+  tflite_micro::RunSafeCastTests<double, int8_t>();
+  tflite_micro::RunSafeCastTests<float, int16_t>();
+  tflite_micro::RunSafeCastTests<double, int16_t>();
+  tflite_micro::RunSafeCastTests<float, int32_t>();
+  tflite_micro::RunSafeCastTests<double, int32_t>();
+  tflite_micro::RunSafeCastTests<float, int64_t>();
+  tflite_micro::RunSafeCastTests<double, int64_t>();
+  tflite_micro::RunSafeCastTests<float, uint8_t>();
+  tflite_micro::RunSafeCastTests<double, uint8_t>();
+  tflite_micro::RunSafeCastTests<float, uint16_t>();
+  tflite_micro::RunSafeCastTests<double, uint16_t>();
+  tflite_micro::RunSafeCastTests<float, uint32_t>();
+  tflite_micro::RunSafeCastTests<double, uint32_t>();
+  tflite_micro::RunSafeCastTests<float, uint64_t>();
+  tflite_micro::RunSafeCastTests<double, uint64_t>();
 }
 
 // Example taken from http://www.tensorflow.org/performance/quantization
@@ -202,74 +202,74 @@ TF_LITE_MICRO_TEST(QuantizationUtilTest_SafeCast) {
 //  255       | 30.0
 //  128       | 10.0
 TF_LITE_MICRO_TEST(QuantizationUtilTest_ChooseQuantizationParams) {
-  tflite::QuantizationParams qp =
-      tflite::ChooseQuantizationParams<uint8_t>(-10.0, 30.0);
+  tflite_micro::QuantizationParams qp =
+      tflite_micro::ChooseQuantizationParams<uint8_t>(-10.0, 30.0);
   TF_LITE_MICRO_EXPECT_NEAR(qp.scale, 0.156863, 1e-5);
   TF_LITE_MICRO_EXPECT_EQ(qp.zero_point, 64);
 }
 
 TF_LITE_MICRO_TEST(
     QuantizationUtilTest_ChooseQuantizationParamsZeroPointOnMinBoundary) {
-  tflite::QuantizationParams qp =
-      tflite::ChooseQuantizationParams<uint8_t>(0.0, 30.0);
+  tflite_micro::QuantizationParams qp =
+      tflite_micro::ChooseQuantizationParams<uint8_t>(0.0, 30.0);
   TF_LITE_MICRO_EXPECT_NEAR(qp.scale, 0.117647, 1e-5);
   TF_LITE_MICRO_EXPECT_EQ(qp.zero_point, 0);
 }
 
 TF_LITE_MICRO_TEST(
     QuantizationUtilTest_ChooseQuantizationParamsEmptyRangeZero) {
-  tflite::QuantizationParams qp =
-      tflite::ChooseQuantizationParams<uint8_t>(0.0, 0.0);
+  tflite_micro::QuantizationParams qp =
+      tflite_micro::ChooseQuantizationParams<uint8_t>(0.0, 0.0);
   TF_LITE_MICRO_EXPECT_NEAR(qp.scale, 0.0, 1e-5);
   TF_LITE_MICRO_EXPECT_EQ(qp.zero_point, 0);
 }
 
 TF_LITE_MICRO_TEST(
     QuantizationUtilTest_ChooseQuantizationParamsZeroPointOnMaxBoundary) {
-  tflite::QuantizationParams qp =
-      tflite::ChooseQuantizationParams<uint8_t>(-10.0, 0.0);
+  tflite_micro::QuantizationParams qp =
+      tflite_micro::ChooseQuantizationParams<uint8_t>(-10.0, 0.0);
   TF_LITE_MICRO_EXPECT_NEAR(qp.scale, 0.039216, 1e-5);
   TF_LITE_MICRO_EXPECT_EQ(qp.zero_point, 255);
 }
 
 TF_LITE_MICRO_TEST(QuantizationUtilTest_IntegerFrExp) {
   int shift;
-  int64_t result = tflite::IntegerFrExp(0.0, &shift);
+  int64_t result = tflite_micro::IntegerFrExp(0.0, &shift);
   TF_LITE_MICRO_EXPECT_EQ(0, result);
   TF_LITE_MICRO_EXPECT_EQ(0, shift);
 
-  result = tflite::IntegerFrExp(1.0, &shift);
+  result = tflite_micro::IntegerFrExp(1.0, &shift);
   TF_LITE_MICRO_EXPECT_NEAR(0x40000000, result, 1ll);
   TF_LITE_MICRO_EXPECT_EQ(1, shift);
 
-  result = tflite::IntegerFrExp(0.25, &shift);
+  result = tflite_micro::IntegerFrExp(0.25, &shift);
   TF_LITE_MICRO_EXPECT_NEAR(0x40000000, result, 1ll);
   TF_LITE_MICRO_EXPECT_EQ(-1, shift);
 
-  result = tflite::IntegerFrExp(-1.0, &shift);
+  result = tflite_micro::IntegerFrExp(-1.0, &shift);
   TF_LITE_MICRO_EXPECT_NEAR(-(1 << 30), result, 1ll);
   TF_LITE_MICRO_EXPECT_EQ(1, shift);
 
-  result = tflite::IntegerFrExp(123.45, &shift);
+  result = tflite_micro::IntegerFrExp(123.45, &shift);
   TF_LITE_MICRO_EXPECT_NEAR(2071147315, result, 1ll);
   TF_LITE_MICRO_EXPECT_EQ(7, shift);
 
-  result = tflite::IntegerFrExp(static_cast<double>(NAN), &shift);
+  result = tflite_micro::IntegerFrExp(static_cast<double>(NAN), &shift);
   TF_LITE_MICRO_EXPECT_NEAR(0, result, 1);
   TF_LITE_MICRO_EXPECT_EQ(0x7fffffff, shift);
 
-  result = tflite::IntegerFrExp(static_cast<double>(INFINITY), &shift);
+  result = tflite_micro::IntegerFrExp(static_cast<double>(INFINITY), &shift);
   TF_LITE_MICRO_EXPECT_NEAR(std::numeric_limits<int64_t>::max(), result, 1);
   TF_LITE_MICRO_EXPECT_EQ(0x7fffffff, shift);
 
-  result = tflite::IntegerFrExp(-static_cast<double>(INFINITY), &shift);
+  result = tflite_micro::IntegerFrExp(-static_cast<double>(INFINITY), &shift);
   TF_LITE_MICRO_EXPECT_NEAR(std::numeric_limits<int64_t>::min(), result, 1);
   TF_LITE_MICRO_EXPECT_EQ(0x7fffffff, shift);
 }
 
 TF_LITE_MICRO_TEST(QuantizationUtilTest_IntegerFrExpVersusDouble) {
   int shift;
-  int32_t result = tflite::IntegerFrExp(0.0, &shift);
+  int32_t result = tflite_micro::IntegerFrExp(0.0, &shift);
   TF_LITE_MICRO_EXPECT_EQ(result, 0);
   TF_LITE_MICRO_EXPECT_EQ(shift, 0);
 
@@ -278,28 +278,28 @@ TF_LITE_MICRO_TEST(QuantizationUtilTest_IntegerFrExpVersusDouble) {
   TF_LITE_MICRO_EXPECT_EQ(double_result, 0);
   TF_LITE_MICRO_EXPECT_EQ(double_shift, 0);
 
-  result = tflite::IntegerFrExp(1.0, &shift);
+  result = tflite_micro::IntegerFrExp(1.0, &shift);
   TF_LITE_MICRO_EXPECT_NEAR(result, 0x40000000, 1);
   TF_LITE_MICRO_EXPECT_EQ(shift, 1);
   double_result = std::frexp(1.0, &double_shift);
   TF_LITE_MICRO_EXPECT_NEAR(double_result, 0.5, 1e-5);
   TF_LITE_MICRO_EXPECT_EQ(double_shift, 1);
 
-  result = tflite::IntegerFrExp(0.25, &shift);
+  result = tflite_micro::IntegerFrExp(0.25, &shift);
   TF_LITE_MICRO_EXPECT_NEAR(result, 0x40000000, 1);
   TF_LITE_MICRO_EXPECT_EQ(shift, -1);
   double_result = std::frexp(0.25, &double_shift);
   TF_LITE_MICRO_EXPECT_NEAR(double_result, 0.5, 1e-5);
   TF_LITE_MICRO_EXPECT_EQ(double_shift, -1);
 
-  result = tflite::IntegerFrExp(-1.0, &shift);
+  result = tflite_micro::IntegerFrExp(-1.0, &shift);
   TF_LITE_MICRO_EXPECT_NEAR(result, -(1 << 30), 1);
   TF_LITE_MICRO_EXPECT_EQ(shift, 1);
   double_result = std::frexp(-1.0, &double_shift);
   TF_LITE_MICRO_EXPECT_NEAR(double_result, -0.5, 1e-5);
   TF_LITE_MICRO_EXPECT_EQ(double_shift, 1);
 
-  result = tflite::IntegerFrExp(123.45, &shift);
+  result = tflite_micro::IntegerFrExp(123.45, &shift);
   TF_LITE_MICRO_EXPECT_NEAR(result, (0.964453 * (1LL << 31)), 1000);
   TF_LITE_MICRO_EXPECT_EQ(shift, 7);
   double_result = std::frexp(123.45, &double_shift);
@@ -308,71 +308,71 @@ TF_LITE_MICRO_TEST(QuantizationUtilTest_IntegerFrExpVersusDouble) {
 }
 
 TF_LITE_MICRO_TEST(QuantizationUtilTest_DoubleFromFractionAndShift) {
-  double result = tflite::DoubleFromFractionAndShift(0, 0);
+  double result = tflite_micro::DoubleFromFractionAndShift(0, 0);
   TF_LITE_MICRO_EXPECT_EQ(0, result);
 
-  result = tflite::DoubleFromFractionAndShift(0x40000000, 1);
+  result = tflite_micro::DoubleFromFractionAndShift(0x40000000, 1);
   TF_LITE_MICRO_EXPECT_NEAR(1.0, result, 1e-5);
 
-  result = tflite::DoubleFromFractionAndShift(0x40000000, 2);
+  result = tflite_micro::DoubleFromFractionAndShift(0x40000000, 2);
   TF_LITE_MICRO_EXPECT_NEAR(2.0, result, 1e-5);
 
   int shift;
-  int64_t fraction = tflite::IntegerFrExp(3.0, &shift);
-  result = tflite::DoubleFromFractionAndShift(fraction, shift);
+  int64_t fraction = tflite_micro::IntegerFrExp(3.0, &shift);
+  result = tflite_micro::DoubleFromFractionAndShift(fraction, shift);
   TF_LITE_MICRO_EXPECT_NEAR(3.0, result, 1e-5);
 
-  fraction = tflite::IntegerFrExp(123.45, &shift);
-  result = tflite::DoubleFromFractionAndShift(fraction, shift);
+  fraction = tflite_micro::IntegerFrExp(123.45, &shift);
+  result = tflite_micro::DoubleFromFractionAndShift(fraction, shift);
   TF_LITE_MICRO_EXPECT_NEAR(123.45, result, 1e-5);
 
-  fraction = tflite::IntegerFrExp(-23.232323, &shift);
-  result = tflite::DoubleFromFractionAndShift(fraction, shift);
+  fraction = tflite_micro::IntegerFrExp(-23.232323, &shift);
+  result = tflite_micro::DoubleFromFractionAndShift(fraction, shift);
   TF_LITE_MICRO_EXPECT_NEAR(-23.232323, result, 1e-5);
 
-  fraction = tflite::IntegerFrExp(static_cast<double>(NAN), &shift);
-  result = tflite::DoubleFromFractionAndShift(fraction, shift);
+  fraction = tflite_micro::IntegerFrExp(static_cast<double>(NAN), &shift);
+  result = tflite_micro::DoubleFromFractionAndShift(fraction, shift);
   TF_LITE_MICRO_EXPECT_TRUE(std::isnan(result));
 
-  fraction = tflite::IntegerFrExp(static_cast<double>(INFINITY), &shift);
-  result = tflite::DoubleFromFractionAndShift(fraction, shift);
+  fraction = tflite_micro::IntegerFrExp(static_cast<double>(INFINITY), &shift);
+  result = tflite_micro::DoubleFromFractionAndShift(fraction, shift);
   TF_LITE_MICRO_EXPECT_FALSE(std::isfinite(result));
 }
 
 TF_LITE_MICRO_TEST(QuantizationUtilTest_IntegerDoubleMultiply) {
-  TF_LITE_MICRO_EXPECT_NEAR(1.0, tflite::IntegerDoubleMultiply(1.0, 1.0), 1e-5);
-  TF_LITE_MICRO_EXPECT_NEAR(2.0, tflite::IntegerDoubleMultiply(1.0, 2.0), 1e-5);
-  TF_LITE_MICRO_EXPECT_NEAR(2.0, tflite::IntegerDoubleMultiply(2.0, 1.0), 1e-5);
-  TF_LITE_MICRO_EXPECT_NEAR(4.0, tflite::IntegerDoubleMultiply(2.0, 2.0), 1e-5);
-  TF_LITE_MICRO_EXPECT_NEAR(0.5, tflite::IntegerDoubleMultiply(1.0, 0.5), 1e-5);
-  TF_LITE_MICRO_EXPECT_NEAR(0.25, tflite::IntegerDoubleMultiply(0.5, 0.5),
+  TF_LITE_MICRO_EXPECT_NEAR(1.0, tflite_micro::IntegerDoubleMultiply(1.0, 1.0), 1e-5);
+  TF_LITE_MICRO_EXPECT_NEAR(2.0, tflite_micro::IntegerDoubleMultiply(1.0, 2.0), 1e-5);
+  TF_LITE_MICRO_EXPECT_NEAR(2.0, tflite_micro::IntegerDoubleMultiply(2.0, 1.0), 1e-5);
+  TF_LITE_MICRO_EXPECT_NEAR(4.0, tflite_micro::IntegerDoubleMultiply(2.0, 2.0), 1e-5);
+  TF_LITE_MICRO_EXPECT_NEAR(0.5, tflite_micro::IntegerDoubleMultiply(1.0, 0.5), 1e-5);
+  TF_LITE_MICRO_EXPECT_NEAR(0.25, tflite_micro::IntegerDoubleMultiply(0.5, 0.5),
                             1e-5);
-  TF_LITE_MICRO_EXPECT_NEAR(-1.0, tflite::IntegerDoubleMultiply(1.0, -1.0),
+  TF_LITE_MICRO_EXPECT_NEAR(-1.0, tflite_micro::IntegerDoubleMultiply(1.0, -1.0),
                             1e-5);
-  TF_LITE_MICRO_EXPECT_NEAR(-1.0, tflite::IntegerDoubleMultiply(-1.0, 1.0),
+  TF_LITE_MICRO_EXPECT_NEAR(-1.0, tflite_micro::IntegerDoubleMultiply(-1.0, 1.0),
                             1e-5);
-  TF_LITE_MICRO_EXPECT_NEAR(1.0, tflite::IntegerDoubleMultiply(-1.0, -1.0),
+  TF_LITE_MICRO_EXPECT_NEAR(1.0, tflite_micro::IntegerDoubleMultiply(-1.0, -1.0),
                             1e-5);
   TF_LITE_MICRO_EXPECT_NEAR(
-      15000000.0, tflite::IntegerDoubleMultiply(3000.0, 5000.0), 1e-5);
+      15000000.0, tflite_micro::IntegerDoubleMultiply(3000.0, 5000.0), 1e-5);
   TF_LITE_MICRO_EXPECT_TRUE(std::isnan(
-      tflite::IntegerDoubleMultiply(static_cast<double>(NAN), 5000.0)));
+      tflite_micro::IntegerDoubleMultiply(static_cast<double>(NAN), 5000.0)));
   TF_LITE_MICRO_EXPECT_TRUE(std::isnan(
-      tflite::IntegerDoubleMultiply(3000.0, static_cast<double>(NAN))));
+      tflite_micro::IntegerDoubleMultiply(3000.0, static_cast<double>(NAN))));
 }
 
 TF_LITE_MICRO_TEST(QuantizationUtilTest_IntegerDoubleCompare) {
-  TF_LITE_MICRO_EXPECT_EQ(-1, tflite::IntegerDoubleCompare(0.0, 1.0));
-  TF_LITE_MICRO_EXPECT_EQ(1, tflite::IntegerDoubleCompare(1.0, 0.0));
-  TF_LITE_MICRO_EXPECT_EQ(0, tflite::IntegerDoubleCompare(1.0, 1.0));
-  TF_LITE_MICRO_EXPECT_EQ(0, tflite::IntegerDoubleCompare(0.0, 0.0));
-  TF_LITE_MICRO_EXPECT_EQ(-1, tflite::IntegerDoubleCompare(-10.0, 10.0));
-  TF_LITE_MICRO_EXPECT_EQ(1, tflite::IntegerDoubleCompare(123.45, 10.0));
+  TF_LITE_MICRO_EXPECT_EQ(-1, tflite_micro::IntegerDoubleCompare(0.0, 1.0));
+  TF_LITE_MICRO_EXPECT_EQ(1, tflite_micro::IntegerDoubleCompare(1.0, 0.0));
+  TF_LITE_MICRO_EXPECT_EQ(0, tflite_micro::IntegerDoubleCompare(1.0, 1.0));
+  TF_LITE_MICRO_EXPECT_EQ(0, tflite_micro::IntegerDoubleCompare(0.0, 0.0));
+  TF_LITE_MICRO_EXPECT_EQ(-1, tflite_micro::IntegerDoubleCompare(-10.0, 10.0));
+  TF_LITE_MICRO_EXPECT_EQ(1, tflite_micro::IntegerDoubleCompare(123.45, 10.0));
   TF_LITE_MICRO_EXPECT_EQ(
-      1, tflite::IntegerDoubleCompare(static_cast<double>(NAN),
+      1, tflite_micro::IntegerDoubleCompare(static_cast<double>(NAN),
                                       static_cast<double>(INFINITY)));
   TF_LITE_MICRO_EXPECT_EQ(
-      1, tflite::IntegerDoubleCompare(static_cast<double>(INFINITY),
+      1, tflite_micro::IntegerDoubleCompare(static_cast<double>(INFINITY),
                                       static_cast<double>(NAN)));
 }
 
@@ -380,7 +380,7 @@ TF_LITE_MICRO_TEST(QuantizationUtilTest_PreprocessSoftmaxScaling) {
   auto quantize = [](double beta, double scale, int integer_bits) {
     int32_t q;
     int s;
-    tflite::PreprocessSoftmaxScaling(beta, scale, integer_bits, &q, &s);
+    tflite_micro::PreprocessSoftmaxScaling(beta, scale, integer_bits, &q, &s);
     return std::pair<int32_t, int>{q, s};
   };
 
@@ -407,10 +407,10 @@ TF_LITE_MICRO_TEST(QuantizationUtilTest_PreprocessSoftmaxScaling) {
 }
 
 TF_LITE_MICRO_TEST(QuantizationUtilTest_CalculateInputRadius) {
-  TF_LITE_MICRO_EXPECT_EQ(tflite::CalculateInputRadius(4, 27), 15);
-  TF_LITE_MICRO_EXPECT_EQ(tflite::CalculateInputRadius(3, 27), 14);
-  TF_LITE_MICRO_EXPECT_EQ(tflite::CalculateInputRadius(3, 28), 7);
-  TF_LITE_MICRO_EXPECT_EQ(tflite::CalculateInputRadius(4, 2), 503316480);
+  TF_LITE_MICRO_EXPECT_EQ(tflite_micro::CalculateInputRadius(4, 27), 15);
+  TF_LITE_MICRO_EXPECT_EQ(tflite_micro::CalculateInputRadius(3, 27), 14);
+  TF_LITE_MICRO_EXPECT_EQ(tflite_micro::CalculateInputRadius(3, 28), 7);
+  TF_LITE_MICRO_EXPECT_EQ(tflite_micro::CalculateInputRadius(4, 2), 503316480);
 }
 
 TF_LITE_MICRO_TEST(QuantizationUtilTest_QuantizeMultiplierArray) {
@@ -420,7 +420,7 @@ TF_LITE_MICRO_TEST(QuantizationUtilTest_QuantizeMultiplierArray) {
   const int size = 13;
   int32_t effective_scale_significand[size];
   int effective_scale_shift[size];
-  tflite::QuantizeMultiplierArray(weights, size, effective_scale_significand,
+  tflite_micro::QuantizeMultiplierArray(weights, size, effective_scale_significand,
                                   effective_scale_shift);
   const int32_t expected_effective_scale_significand[] = {
       -1073741824,  // float scale = -4
diff --git a/tensorflow/lite/micro/kernels/quantize.cc b/tensorflow/lite/micro/kernels/quantize.cc
index 1ac69424..74fd3a74 100644
--- a/tensorflow/lite/micro/kernels/quantize.cc
+++ b/tensorflow/lite/micro/kernels/quantize.cc
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 void* Init(TfLiteContext* context, const char* buffer, size_t length) {
@@ -34,8 +34,8 @@ void* Init(TfLiteContext* context, const char* buffer, size_t length) {
 }  // namespace
 
 TFLMRegistration Register_QUANTIZE() {
-  return tflite::micro::RegisterOp(Init, PrepareQuantizeReference,
+  return tflite_micro::micro::RegisterOp(Init, PrepareQuantizeReference,
                                    EvalQuantizeReference);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/quantize.h b/tensorflow/lite/micro/kernels/quantize.h
index ba93809a..e61074ac 100644
--- a/tensorflow/lite/micro/kernels/quantize.h
+++ b/tensorflow/lite/micro/kernels/quantize.h
@@ -18,10 +18,10 @@ limitations under the License.
 #include "tensorflow/lite/c/common.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 struct OpDataQuantizeReference {
-  tflite::QuantizationParams quantization_params;
+  tflite_micro::QuantizationParams quantization_params;
   // The scaling factor from input to output (aka the 'real multiplier') can
   // be represented as a fixed point multiplier plus a left shift.
   int32_t requantize_output_multiplier;
@@ -32,6 +32,6 @@ struct OpDataQuantizeReference {
 
 TfLiteStatus EvalQuantizeReference(TfLiteContext* context, TfLiteNode* node);
 TfLiteStatus PrepareQuantizeReference(TfLiteContext* context, TfLiteNode* node);
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_QUANTIZE_H_
diff --git a/tensorflow/lite/micro/kernels/quantize_common.cc b/tensorflow/lite/micro/kernels/quantize_common.cc
index cb04eafc..f1c5f4c0 100644
--- a/tensorflow/lite/micro/kernels/quantize_common.cc
+++ b/tensorflow/lite/micro/kernels/quantize_common.cc
@@ -26,7 +26,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_log.h"
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 TfLiteStatus PrepareQuantizeReference(TfLiteContext* context,
                                       TfLiteNode* node) {
@@ -93,24 +93,24 @@ TfLiteStatus EvalQuantizeReference(TfLiteContext* context, TfLiteNode* node) {
   TFLITE_DCHECK(node->user_data != nullptr);
   auto* data = static_cast<OpDataQuantizeReference*>(node->user_data);
 
-  const TfLiteEvalTensor* input = tflite::micro::GetEvalInput(context, node, 0);
-  TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, 0);
+  const TfLiteEvalTensor* input = tflite_micro::micro::GetEvalInput(context, node, 0);
+  TfLiteEvalTensor* output = tflite_micro::micro::GetEvalOutput(context, node, 0);
 
   if (input->type == kTfLiteFloat32) {
     switch (output->type) {
       case kTfLiteInt8:
         reference_ops::AffineQuantize(
-            data->quantization_params, tflite::micro::GetTensorShape(input),
-            tflite::micro::GetTensorData<float>(input),
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<int8_t>(output));
+            data->quantization_params, tflite_micro::micro::GetTensorShape(input),
+            tflite_micro::micro::GetTensorData<float>(input),
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int8_t>(output));
         break;
       case kTfLiteInt16:
         reference_ops::AffineQuantize(
-            data->quantization_params, tflite::micro::GetTensorShape(input),
-            tflite::micro::GetTensorData<float>(input),
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<int16_t>(output));
+            data->quantization_params, tflite_micro::micro::GetTensorShape(input),
+            tflite_micro::micro::GetTensorData<float>(input),
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int16_t>(output));
         return kTfLiteOk;
       default:
         MicroPrintf("Input %s, output %s not supported.",
@@ -123,17 +123,17 @@ TfLiteStatus EvalQuantizeReference(TfLiteContext* context, TfLiteNode* node) {
     switch (output->type) {
       case kTfLiteInt8:
         reference_ops::Requantize(
-            tflite::micro::GetTensorData<int32_t>(input), size,
+            tflite_micro::micro::GetTensorData<int32_t>(input), size,
             data->requantize_output_multiplier, data->requantize_output_shift,
             data->input_zero_point, data->quantization_params.zero_point,
-            tflite::micro::GetTensorData<int8_t>(output));
+            tflite_micro::micro::GetTensorData<int8_t>(output));
         break;
       case kTfLiteInt16:
         reference_ops::Requantize(
-            tflite::micro::GetTensorData<int32_t>(input), size,
+            tflite_micro::micro::GetTensorData<int32_t>(input), size,
             data->requantize_output_multiplier, data->requantize_output_shift,
             data->input_zero_point, data->quantization_params.zero_point,
-            tflite::micro::GetTensorData<int16_t>(output));
+            tflite_micro::micro::GetTensorData<int16_t>(output));
         break;
       default:
         MicroPrintf("Input %s, output %s not supported.",
@@ -146,24 +146,24 @@ TfLiteStatus EvalQuantizeReference(TfLiteContext* context, TfLiteNode* node) {
     switch (output->type) {
       case kTfLiteInt8:
         reference_ops::Requantize(
-            tflite::micro::GetTensorData<int16_t>(input), size,
+            tflite_micro::micro::GetTensorData<int16_t>(input), size,
             data->requantize_output_multiplier, data->requantize_output_shift,
             data->input_zero_point, data->quantization_params.zero_point,
-            tflite::micro::GetTensorData<int8_t>(output));
+            tflite_micro::micro::GetTensorData<int8_t>(output));
         break;
       case kTfLiteInt16:
         reference_ops::Requantize(
-            tflite::micro::GetTensorData<int16_t>(input), size,
+            tflite_micro::micro::GetTensorData<int16_t>(input), size,
             data->requantize_output_multiplier, data->requantize_output_shift,
             data->input_zero_point, data->quantization_params.zero_point,
-            tflite::micro::GetTensorData<int16_t>(output));
+            tflite_micro::micro::GetTensorData<int16_t>(output));
         return kTfLiteOk;
       case kTfLiteInt32:
         reference_ops::Requantize(
-            tflite::micro::GetTensorData<int16_t>(input), size,
+            tflite_micro::micro::GetTensorData<int16_t>(input), size,
             data->requantize_output_multiplier, data->requantize_output_shift,
             data->input_zero_point, data->quantization_params.zero_point,
-            tflite::micro::GetTensorData<int32_t>(output));
+            tflite_micro::micro::GetTensorData<int32_t>(output));
         return kTfLiteOk;
       default:
         MicroPrintf("Input %s, output %s not supported.",
@@ -178,31 +178,31 @@ TfLiteStatus EvalQuantizeReference(TfLiteContext* context, TfLiteNode* node) {
     switch (output->type) {
       case kTfLiteInt8:
         reference_ops::Requantize(
-            tflite::micro::GetTensorData<int8_t>(input), size,
+            tflite_micro::micro::GetTensorData<int8_t>(input), size,
             data->requantize_output_multiplier, data->requantize_output_shift,
             data->input_zero_point, data->quantization_params.zero_point,
-            tflite::micro::GetTensorData<int8_t>(output));
+            tflite_micro::micro::GetTensorData<int8_t>(output));
         break;
       case kTfLiteUInt8:
         reference_ops::Requantize(
-            tflite::micro::GetTensorData<int8_t>(input), size,
+            tflite_micro::micro::GetTensorData<int8_t>(input), size,
             data->requantize_output_multiplier, data->requantize_output_shift,
             data->input_zero_point, data->quantization_params.zero_point,
-            tflite::micro::GetTensorData<uint8_t>(output));
+            tflite_micro::micro::GetTensorData<uint8_t>(output));
         break;
       case kTfLiteInt16:
         reference_ops::Requantize(
-            tflite::micro::GetTensorData<int8_t>(input), size,
+            tflite_micro::micro::GetTensorData<int8_t>(input), size,
             data->requantize_output_multiplier, data->requantize_output_shift,
             data->input_zero_point, data->quantization_params.zero_point,
-            tflite::micro::GetTensorData<int16_t>(output));
+            tflite_micro::micro::GetTensorData<int16_t>(output));
         break;
       case kTfLiteInt32:
         reference_ops::Requantize(
-            tflite::micro::GetTensorData<int8_t>(input), size,
+            tflite_micro::micro::GetTensorData<int8_t>(input), size,
             data->requantize_output_multiplier, data->requantize_output_shift,
             data->input_zero_point, data->quantization_params.zero_point,
-            tflite::micro::GetTensorData<int32_t>(output));
+            tflite_micro::micro::GetTensorData<int32_t>(output));
         break;
       default:
         MicroPrintf("Input %s, output %s not supported.",
@@ -215,10 +215,10 @@ TfLiteStatus EvalQuantizeReference(TfLiteContext* context, TfLiteNode* node) {
     switch (output->type) {
       case kTfLiteInt8:
         reference_ops::Requantize(
-            tflite::micro::GetTensorData<uint8_t>(input), size,
+            tflite_micro::micro::GetTensorData<uint8_t>(input), size,
             data->requantize_output_multiplier, data->requantize_output_shift,
             data->input_zero_point, data->quantization_params.zero_point,
-            tflite::micro::GetTensorData<int8_t>(output));
+            tflite_micro::micro::GetTensorData<int8_t>(output));
         break;
       default:
         MicroPrintf("Input %s, output %s not supported.",
@@ -236,4 +236,4 @@ TfLiteStatus EvalQuantizeReference(TfLiteContext* context, TfLiteNode* node) {
   return kTfLiteOk;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/quantize_test.cc b/tensorflow/lite/micro/kernels/quantize_test.cc
index 98671089..a3352277 100644
--- a/tensorflow/lite/micro/kernels/quantize_test.cc
+++ b/tensorflow/lite/micro/kernels/quantize_test.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -115,7 +115,7 @@ void TestRequantize(int* input_dims_data, const float* input_data,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 TF_LITE_MICRO_TEST(QuantizeOpTestInt16) {
@@ -127,7 +127,7 @@ TF_LITE_MICRO_TEST(QuantizeOpTestInt16) {
   const int zero_point = -1;
   int16_t output[kLength];
   int16_t values_quantized[kLength];
-  tflite::testing::TestQuantizeFloat(
+  tflite_micro::testing::TestQuantizeFloat(
       dims, values, dims, values, values_quantized, scale, zero_point, output);
 }
 
@@ -140,7 +140,7 @@ TF_LITE_MICRO_TEST(QuantizeOpTestInt16NoScale) {
   const int zero_point = 0;
   int16_t output[kLength];
   int16_t values_quantized[kLength];
-  tflite::testing::TestQuantizeFloat(
+  tflite_micro::testing::TestQuantizeFloat(
       dims, values, dims, values, values_quantized, scale, zero_point, output);
 }
 
@@ -155,7 +155,7 @@ TF_LITE_MICRO_TEST(QuantizeOpTestInt16toInt16) {
   int16_t output_quantized[kLength];
   int16_t values_quantized[kLength];
   int16_t input_quantized[kLength];
-  tflite::testing::TestRequantize(dims, values, input_quantized, input_scale,
+  tflite_micro::testing::TestRequantize(dims, values, input_quantized, input_scale,
                                   input_zero_point, dims, values,
                                   values_quantized, output_scale,
                                   output_zero_point, output_quantized);
@@ -172,7 +172,7 @@ TF_LITE_MICRO_TEST(QuantizeOpTestInt16toInt16NoZeroPoint) {
   int16_t output_quantized[kLength];
   int16_t values_quantized[kLength];
   int16_t input_quantized[kLength];
-  tflite::testing::TestRequantize(dims, values, input_quantized, input_scale,
+  tflite_micro::testing::TestRequantize(dims, values, input_quantized, input_scale,
                                   input_zero_point, dims, values,
                                   values_quantized, output_scale,
                                   output_zero_point, output_quantized);
@@ -189,7 +189,7 @@ TF_LITE_MICRO_TEST(QuantizeOpTestInt8toInt8) {
   int8_t output_quantized[kLength];
   int8_t values_quantized[kLength];
   int8_t input_quantized[kLength];
-  tflite::testing::TestRequantize(dims, values, input_quantized, input_scale,
+  tflite_micro::testing::TestRequantize(dims, values, input_quantized, input_scale,
                                   input_zero_point, dims, values,
                                   values_quantized, output_scale,
                                   output_zero_point, output_quantized);
@@ -206,7 +206,7 @@ TF_LITE_MICRO_TEST(QuantizeOpTestInt8toInt8NoZeroPoint) {
   int8_t output_quantized[kLength];
   int8_t values_quantized[kLength];
   int8_t input_quantized[kLength];
-  tflite::testing::TestRequantize(dims, values, input_quantized, input_scale,
+  tflite_micro::testing::TestRequantize(dims, values, input_quantized, input_scale,
                                   input_zero_point, dims, values,
                                   values_quantized, output_scale,
                                   output_zero_point, output_quantized);
@@ -223,7 +223,7 @@ TF_LITE_MICRO_TEST(QuantizeOpTestInt8toInt16) {
   int16_t output_quantized[kLength];
   int16_t values_quantized[kLength];
   int8_t input_quantized[kLength];
-  tflite::testing::TestRequantize(dims, values, input_quantized, input_scale,
+  tflite_micro::testing::TestRequantize(dims, values, input_quantized, input_scale,
                                   input_zero_point, dims, values,
                                   values_quantized, output_scale,
                                   output_zero_point, output_quantized);
@@ -242,7 +242,7 @@ TF_LITE_MICRO_TEST(QuantizeOpTestInt32toInt16) {
   int16_t output_quantized[kLength];
   int16_t values_quantized[kLength];
   int32_t input_quantized[kLength];
-  tflite::testing::TestRequantize(dims, values, input_quantized, input_scale,
+  tflite_micro::testing::TestRequantize(dims, values, input_quantized, input_scale,
                                   input_zero_point, dims, values,
                                   values_quantized, output_scale,
                                   output_zero_point, output_quantized);
@@ -261,7 +261,7 @@ TF_LITE_MICRO_TEST(QuantizeOpTestInt32toInt8) {
   int8_t output_quantized[kLength];
   int8_t values_quantized[kLength];
   int32_t input_quantized[kLength];
-  tflite::testing::TestRequantize(dims, values, input_quantized, input_scale,
+  tflite_micro::testing::TestRequantize(dims, values, input_quantized, input_scale,
                                   input_zero_point, dims, values,
                                   values_quantized, output_scale,
                                   output_zero_point, output_quantized);
@@ -280,7 +280,7 @@ TF_LITE_MICRO_TEST(QuantizeOpTestInt16toInt8) {
   int8_t output_quantized[kLength];
   int8_t values_quantized[kLength];
   int16_t input_quantized[kLength];
-  tflite::testing::TestRequantize(dims, values, input_quantized, input_scale,
+  tflite_micro::testing::TestRequantize(dims, values, input_quantized, input_scale,
                                   input_zero_point, dims, values,
                                   values_quantized, output_scale,
                                   output_zero_point, output_quantized);
@@ -298,7 +298,7 @@ TF_LITE_MICRO_TEST(QuantizeOpTestInt8toUInt8Fast) {
   uint8_t output_quantized[kLength];
   uint8_t values_quantized[kLength];
   int8_t input_quantized[kLength];
-  tflite::testing::TestRequantize(dims, values, input_quantized, input_scale,
+  tflite_micro::testing::TestRequantize(dims, values, input_quantized, input_scale,
                                   input_zero_point, dims, values,
                                   values_quantized, output_scale,
                                   output_zero_point, output_quantized);
@@ -316,7 +316,7 @@ TF_LITE_MICRO_TEST(QuantizeOpTestInt8toUInt8Normal) {
   uint8_t output_quantized[kLength];
   uint8_t values_quantized[kLength];
   int8_t input_quantized[kLength];
-  tflite::testing::TestRequantize(dims, values, input_quantized, input_scale,
+  tflite_micro::testing::TestRequantize(dims, values, input_quantized, input_scale,
                                   input_zero_point, dims, values,
                                   values_quantized, output_scale,
                                   output_zero_point, output_quantized);
@@ -334,7 +334,7 @@ TF_LITE_MICRO_TEST(QuantizeOpTestUInt8toInt8Fast) {
   int8_t output_quantized[kLength];
   int8_t values_quantized[kLength];
   uint8_t input_quantized[kLength];
-  tflite::testing::TestRequantize(dims, values, input_quantized, input_scale,
+  tflite_micro::testing::TestRequantize(dims, values, input_quantized, input_scale,
                                   input_zero_point, dims, values,
                                   values_quantized, output_scale,
                                   output_zero_point, output_quantized);
@@ -352,7 +352,7 @@ TF_LITE_MICRO_TEST(QuantizeOpTestUInt8toInt8Normal) {
   int8_t output_quantized[kLength];
   int8_t values_quantized[kLength];
   uint8_t input_quantized[kLength];
-  tflite::testing::TestRequantize(dims, values, input_quantized, input_scale,
+  tflite_micro::testing::TestRequantize(dims, values, input_quantized, input_scale,
                                   input_zero_point, dims, values,
                                   values_quantized, output_scale,
                                   output_zero_point, output_quantized);
@@ -369,7 +369,7 @@ TF_LITE_MICRO_TEST(QuantizeOpTestInt8toInt32) {
   int32_t output_quantized[kLength];
   int32_t values_quantized[kLength];
   int8_t input_quantized[kLength];
-  tflite::testing::TestRequantize(dims, values, input_quantized, input_scale,
+  tflite_micro::testing::TestRequantize(dims, values, input_quantized, input_scale,
                                   input_zero_point, dims, values,
                                   values_quantized, output_scale,
                                   output_zero_point, output_quantized);
@@ -386,7 +386,7 @@ TF_LITE_MICRO_TEST(QuantizeOpTestInt16toInt32) {
   int32_t output_quantized[kLength];
   int32_t values_quantized[kLength];
   int16_t input_quantized[kLength];
-  tflite::testing::TestRequantize(dims, values, input_quantized, input_scale,
+  tflite_micro::testing::TestRequantize(dims, values, input_quantized, input_scale,
                                   input_zero_point, dims, values,
                                   values_quantized, output_scale,
                                   output_zero_point, output_quantized);
@@ -405,7 +405,7 @@ TF_LITE_MICRO_TEST(QuantizeOpTestInt16toInt8) {
   int8_t output_quantized[kLength];
   int8_t values_quantized[kLength];
   int16_t input_quantized[kLength];
-  tflite::testing::TestRequantize(dims, values, input_quantized, input_scale,
+  tflite_micro::testing::TestRequantize(dims, values, input_quantized, input_scale,
                                   input_zero_point, dims, values,
                                   values_quantized, output_scale,
                                   output_zero_point, output_quantized);
@@ -420,7 +420,7 @@ TF_LITE_MICRO_TEST(QuantizeOpTestInt8) {
   const int zero_point = -1;
   int16_t output[kLength];
   int16_t values_quantized[kLength];
-  tflite::testing::TestQuantizeFloat(
+  tflite_micro::testing::TestQuantizeFloat(
       dims, values, dims, values, values_quantized, scale, zero_point, output);
 }
 
@@ -432,7 +432,7 @@ TF_LITE_MICRO_TEST(QuantizeOpTestInt8NoZeroPoint) {
   const int zero_point = 0;
   int8_t output[kLength];
   int8_t values_quantized[kLength];
-  tflite::testing::TestQuantizeFloat(
+  tflite_micro::testing::TestQuantizeFloat(
       dims, values, dims, values, values_quantized, scale, zero_point, output);
 }
 
diff --git a/tensorflow/lite/micro/kernels/read_variable.cc b/tensorflow/lite/micro/kernels/read_variable.cc
index 87e720d1..a574e322 100644
--- a/tensorflow/lite/micro/kernels/read_variable.cc
+++ b/tensorflow/lite/micro/kernels/read_variable.cc
@@ -28,7 +28,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_resource_variable.h"
 #include "tensorflow/lite/schema/schema_generated.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -55,14 +55,14 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input_resource_id_tensor =
-      tflite::micro::GetEvalInput(context, node, kInputVariableId);
+      tflite_micro::micro::GetEvalInput(context, node, kInputVariableId);
   TFLITE_DCHECK(input_resource_id_tensor != nullptr);
 
   TfLiteEvalTensor* output_value =
-      tflite::micro::GetEvalOutput(context, node, kOutputValue);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputValue);
   TFLITE_DCHECK(output_value != nullptr);
 
-  tflite::MicroContext* micro_context = tflite::GetMicroContext(context);
+  tflite_micro::MicroContext* micro_context = tflite_micro::GetMicroContext(context);
   MicroGraph& graph_info = micro_context->graph();
 
   MicroResourceVariables* resources = graph_info.GetResourceVariables();
@@ -81,7 +81,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace.
 
 TFLMRegistration Register_READ_VARIABLE() {
-  return tflite::micro::RegisterOp(nullptr, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/reduce.cc b/tensorflow/lite/micro/kernels/reduce.cc
index ab24a82d..387add59 100644
--- a/tensorflow/lite/micro/kernels/reduce.cc
+++ b/tensorflow/lite/micro/kernels/reduce.cc
@@ -26,7 +26,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/reduce.h"
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 void* InitReduce(TfLiteContext* context, const char* buffer, size_t length) {
   return context->AllocatePersistentBuffer(context, sizeof(OpDataReduce));
@@ -58,15 +58,15 @@ TfLiteStatus EvalSum(TfLiteContext* context, TfLiteNode* node) {
 }
 
 TFLMRegistration Register_MEAN() {
-  return tflite::micro::RegisterOp(InitReduce, PrepareMeanOrSum, EvalMean);
+  return tflite_micro::micro::RegisterOp(InitReduce, PrepareMeanOrSum, EvalMean);
 }
 
 TFLMRegistration Register_REDUCE_MAX() {
-  return tflite::micro::RegisterOp(InitReduce, PrepareMax, EvalMax);
+  return tflite_micro::micro::RegisterOp(InitReduce, PrepareMax, EvalMax);
 }
 
 TFLMRegistration Register_SUM() {
-  return tflite::micro::RegisterOp(InitReduce, PrepareMeanOrSum, EvalSum);
+  return tflite_micro::micro::RegisterOp(InitReduce, PrepareMeanOrSum, EvalSum);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/reduce.h b/tensorflow/lite/micro/kernels/reduce.h
index 2daeef5f..9f70bd6c 100644
--- a/tensorflow/lite/micro/kernels/reduce.h
+++ b/tensorflow/lite/micro/kernels/reduce.h
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/types.h"
 #include "tensorflow/lite/micro/micro_common.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 extern const int kMaxNumberOfAxis;
 extern const int kMaxNumberOfReducedAxis;
@@ -60,6 +60,6 @@ TFLMRegistration Register_MEAN();
 TFLMRegistration Register_REDUCE_MAX();
 TFLMRegistration Register_SUM();
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_REDUCE_H_
diff --git a/tensorflow/lite/micro/kernels/reduce_common.cc b/tensorflow/lite/micro/kernels/reduce_common.cc
index 0dab49c2..9a71341e 100644
--- a/tensorflow/lite/micro/kernels/reduce_common.cc
+++ b/tensorflow/lite/micro/kernels/reduce_common.cc
@@ -26,7 +26,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_log.h"
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 const int kMaxNumberOfAxis = 5;
 const int kMaxNumberOfReducedAxis = 2;
@@ -125,7 +125,7 @@ TfLiteStatus PrepareMeanOrSumHelper(TfLiteContext* context, TfLiteNode* node,
 }
 
 void ResolveAxis(const int* axis_data, int axis_count,
-                 tflite::MeanParams* op_params) {
+                 tflite_micro::MeanParams* op_params) {
   int i = 0;
   for (; i < axis_count; ++i) {
     op_params->axis[i] = static_cast<int16_t>(axis_data[i]);
@@ -141,19 +141,19 @@ TfLiteStatus QuantizedMeanOrSum(TfLiteContext* context, TfLiteNode* node,
                                 int* temp_index, int* resolved_axis,
                                 int32_t* temp_sum, OpDataReduce* op_data,
                                 bool compute_sum) {
-  const TfLiteEvalTensor* input = tflite::micro::GetEvalInput(context, node, 0);
-  const TfLiteEvalTensor* axis = tflite::micro::GetEvalInput(context, node, 1);
-  TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, 0);
+  const TfLiteEvalTensor* input = tflite_micro::micro::GetEvalInput(context, node, 0);
+  const TfLiteEvalTensor* axis = tflite_micro::micro::GetEvalInput(context, node, 1);
+  TfLiteEvalTensor* output = tflite_micro::micro::GetEvalOutput(context, node, 0);
   TfLiteReducerParams* params =
       static_cast<TfLiteReducerParams*>(node->builtin_data);
 
   bool result = reference_ops::QuantizedMeanOrSumExtraArgs<T, int32_t>(
-      tflite::micro::GetTensorData<T>(input), op_data->input_zp,
+      tflite_micro::micro::GetTensorData<T>(input), op_data->input_zp,
       op_data->input_scale, &input->dims->data[0], input->dims->size,
-      tflite::micro::GetTensorData<T>(output), op_data->output_scale,
+      tflite_micro::micro::GetTensorData<T>(output), op_data->output_scale,
       op_data->multiplier, op_data->shift, op_data->output_zp,
       &output->dims->data[0], output->dims->size,
-      tflite::micro::GetTensorData<int>(axis), op_data->num_axis,
+      tflite_micro::micro::GetTensorData<int>(axis), op_data->num_axis,
       params->keep_dims, temp_index, resolved_axis, temp_sum, compute_sum);
   TF_LITE_ENSURE(context, result);
 
@@ -175,9 +175,9 @@ TfLiteStatus EvalIntegerMean(TfLiteContext* context, TfLiteNode* node,
 
 TfLiteStatus EvalMeanHelper(TfLiteContext* context, TfLiteNode* node,
                             OpDataReduce* op_data) {
-  const TfLiteEvalTensor* input = tflite::micro::GetEvalInput(context, node, 0);
-  const TfLiteEvalTensor* axis = tflite::micro::GetEvalInput(context, node, 1);
-  TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, 0);
+  const TfLiteEvalTensor* input = tflite_micro::micro::GetEvalInput(context, node, 0);
+  const TfLiteEvalTensor* axis = tflite_micro::micro::GetEvalInput(context, node, 1);
+  TfLiteEvalTensor* output = tflite_micro::micro::GetEvalOutput(context, node, 0);
   TfLiteReducerParams* params =
       reinterpret_cast<TfLiteReducerParams*>(node->builtin_data);
 
@@ -187,8 +187,8 @@ TfLiteStatus EvalMeanHelper(TfLiteContext* context, TfLiteNode* node,
 
   switch (input->type) {
     case kTfLiteFloat32: {
-      tflite::MeanParams op_params;
-      ResolveAxis(tflite::micro::GetTensorData<int>(axis), num_axis,
+      tflite_micro::MeanParams op_params;
+      ResolveAxis(tflite_micro::micro::GetTensorData<int>(axis), num_axis,
                   &op_params);
 
       // Special case mean implementation exists for 4D mean across axes 1
@@ -200,20 +200,20 @@ TfLiteStatus EvalMeanHelper(TfLiteContext* context, TfLiteNode* node,
 
       // Defer to specialized implementation for 4D Mean across axes 1 & 2.
       if (params->keep_dims && special_case_4d_axes_1_and_2) {
-        reference_ops::Mean(op_params, tflite::micro::GetTensorShape(input),
-                            tflite::micro::GetTensorData<float>(input),
-                            tflite::micro::GetTensorShape(output),
-                            tflite::micro::GetTensorData<float>(output));
+        reference_ops::Mean(op_params, tflite_micro::micro::GetTensorShape(input),
+                            tflite_micro::micro::GetTensorData<float>(input),
+                            tflite_micro::micro::GetTensorShape(output),
+                            tflite_micro::micro::GetTensorData<float>(output));
       } else {
         TF_LITE_ENSURE(
             context,
             reference_ops::Mean(
-                tflite::micro::GetTensorData<float>(input), input->dims->data,
-                input->dims->size, tflite::micro::GetTensorData<float>(output),
+                tflite_micro::micro::GetTensorData<float>(input), input->dims->data,
+                input->dims->size, tflite_micro::micro::GetTensorData<float>(output),
                 output->dims->data, output->dims->size,
-                tflite::micro::GetTensorData<int>(axis), num_axis,
+                tflite_micro::micro::GetTensorData<int>(axis), num_axis,
                 params->keep_dims, temp_index, resolved_axis,
-                tflite::micro::GetTensorData<float>(output)));
+                tflite_micro::micro::GetTensorData<float>(output)));
       }
     } break;
     case kTfLiteInt8: {
@@ -236,9 +236,9 @@ TfLiteStatus EvalMeanHelper(TfLiteContext* context, TfLiteNode* node,
 
 TfLiteStatus EvalMaxHelper(TfLiteContext* context, TfLiteNode* node,
                            OpDataReduce* op_data) {
-  const TfLiteEvalTensor* input = tflite::micro::GetEvalInput(context, node, 0);
-  const TfLiteEvalTensor* axis = tflite::micro::GetEvalInput(context, node, 1);
-  TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, 0);
+  const TfLiteEvalTensor* input = tflite_micro::micro::GetEvalInput(context, node, 0);
+  const TfLiteEvalTensor* axis = tflite_micro::micro::GetEvalInput(context, node, 1);
+  TfLiteEvalTensor* output = tflite_micro::micro::GetEvalOutput(context, node, 0);
   TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);
   TfLiteReducerParams* params =
       static_cast<TfLiteReducerParams*>(node->builtin_data);
@@ -254,10 +254,10 @@ TfLiteStatus EvalMaxHelper(TfLiteContext* context, TfLiteNode* node,
       TF_LITE_ENSURE(
           context,
           reference_ops::ReduceGeneric<float>(
-              tflite::micro::GetTensorData<float>(input), input->dims->data,
-              input->dims->size, tflite::micro::GetTensorData<float>(output),
+              tflite_micro::micro::GetTensorData<float>(input), input->dims->data,
+              input->dims->size, tflite_micro::micro::GetTensorData<float>(output),
               output->dims->data, output->dims->size,
-              tflite::micro::GetTensorData<int>(axis), num_axis,
+              tflite_micro::micro::GetTensorData<int>(axis), num_axis,
               params->keep_dims, temp_buffer, resolved_axis,
               std::numeric_limits<float>::lowest(),
               [](const float current, const float in) -> float {
@@ -271,10 +271,10 @@ TfLiteStatus EvalMaxHelper(TfLiteContext* context, TfLiteNode* node,
       TF_LITE_ENSURE(
           context,
           reference_ops::ReduceGeneric<int8_t>(
-              tflite::micro::GetTensorData<int8_t>(input), input->dims->data,
-              input->dims->size, tflite::micro::GetTensorData<int8_t>(output),
+              tflite_micro::micro::GetTensorData<int8_t>(input), input->dims->data,
+              input->dims->size, tflite_micro::micro::GetTensorData<int8_t>(output),
               output->dims->data, output->dims->size,
-              tflite::micro::GetTensorData<int>(axis), num_axis,
+              tflite_micro::micro::GetTensorData<int>(axis), num_axis,
               params->keep_dims, temp_buffer, resolved_axis,
               std::numeric_limits<int8_t>::lowest(),
               [](const int8_t current, const int8_t in) -> int8_t {
@@ -290,9 +290,9 @@ TfLiteStatus EvalMaxHelper(TfLiteContext* context, TfLiteNode* node,
 
 TfLiteStatus EvalSumHelper(TfLiteContext* context, TfLiteNode* node,
                            OpDataReduce* op_data) {
-  const TfLiteEvalTensor* input = tflite::micro::GetEvalInput(context, node, 0);
-  const TfLiteEvalTensor* axis = tflite::micro::GetEvalInput(context, node, 1);
-  TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, 0);
+  const TfLiteEvalTensor* input = tflite_micro::micro::GetEvalInput(context, node, 0);
+  const TfLiteEvalTensor* axis = tflite_micro::micro::GetEvalInput(context, node, 1);
+  TfLiteEvalTensor* output = tflite_micro::micro::GetEvalOutput(context, node, 0);
   TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);
   TfLiteReducerParams* params =
       static_cast<TfLiteReducerParams*>(node->builtin_data);
@@ -307,10 +307,10 @@ TfLiteStatus EvalSumHelper(TfLiteContext* context, TfLiteNode* node,
       TF_LITE_ENSURE(
           context,
           reference_ops::ReduceGeneric<float>(
-              tflite::micro::GetTensorData<float>(input), input->dims->data,
-              input->dims->size, tflite::micro::GetTensorData<float>(output),
+              tflite_micro::micro::GetTensorData<float>(input), input->dims->data,
+              input->dims->size, tflite_micro::micro::GetTensorData<float>(output),
               output->dims->data, output->dims->size,
-              tflite::micro::GetTensorData<int>(axis), num_axis,
+              tflite_micro::micro::GetTensorData<int>(axis), num_axis,
               params->keep_dims, temp_index, resolved_axis, /*init_value=*/0.f,
               [](const float current, const float in) -> float {
                 return in + current;
@@ -335,4 +335,4 @@ TfLiteStatus EvalSumHelper(TfLiteContext* context, TfLiteNode* node,
   return kTfLiteOk;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/reduce_test.cc b/tensorflow/lite/micro/kernels/reduce_test.cc
index 8e532376..3f7c49b8 100644
--- a/tensorflow/lite/micro/kernels/reduce_test.cc
+++ b/tensorflow/lite/micro/kernels/reduce_test.cc
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -151,7 +151,7 @@ void TestReduceOpQuantized(int* input_dims_data, const float* input_data,
   };
 
   // Quantize expected output
-  tflite::Quantize(expected_output_data, expected_output_data_quant,
+  tflite_micro::Quantize(expected_output_data, expected_output_data_quant,
                    output_dims_count, output_scale, output_zero_point);
 
   TF_LITE_MICRO_EXPECT_EQ(
@@ -163,26 +163,26 @@ void TestReduceOpQuantized(int* input_dims_data, const float* input_data,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(MeanFloat2DKeepDims) {
-  float output_data[tflite::testing::kOutputElements2D];
+  float output_data[tflite_micro::testing::kOutputElements2D];
 
   TfLiteReducerParams params = {true};
 
-  tflite::testing::TestReduceOpFloat(
-      tflite::testing::kInputShape2D, tflite::testing::kInputData2D,
-      tflite::testing::kAxisShape2D, tflite::testing::kAxisData2D,
-      tflite::testing::kOutputShape2D, output_data,
-      tflite::testing::kGoldenData2D, tflite::Register_MEAN(), &params);
+  tflite_micro::testing::TestReduceOpFloat(
+      tflite_micro::testing::kInputShape2D, tflite_micro::testing::kInputData2D,
+      tflite_micro::testing::kAxisShape2D, tflite_micro::testing::kAxisData2D,
+      tflite_micro::testing::kOutputShape2D, output_data,
+      tflite_micro::testing::kGoldenData2D, tflite_micro::Register_MEAN(), &params);
 }
 
 TF_LITE_MICRO_TEST(MeanInt82DKeepDims) {
-  int8_t expected_output_data_quant[tflite::testing::kOutputElements2D];
-  int8_t output_data_quant[tflite::testing::kOutputElements2D];
-  int8_t input_data_quant[tflite::testing::kInputElements2D];
+  int8_t expected_output_data_quant[tflite_micro::testing::kOutputElements2D];
+  int8_t output_data_quant[tflite_micro::testing::kOutputElements2D];
+  int8_t input_data_quant[tflite_micro::testing::kInputElements2D];
 
   float input_scale = 0.5f;
   int input_zero_point = 0;
@@ -193,19 +193,19 @@ TF_LITE_MICRO_TEST(MeanInt82DKeepDims) {
       true  // keep_dims
   };
 
-  tflite::testing::TestReduceOpQuantized<int8_t>(
-      tflite::testing::kInputShape2D, tflite::testing::kInputData2D,
+  tflite_micro::testing::TestReduceOpQuantized<int8_t>(
+      tflite_micro::testing::kInputShape2D, tflite_micro::testing::kInputData2D,
       input_data_quant, input_scale, input_zero_point,
-      tflite::testing::kAxisShape2D, tflite::testing::kAxisData2D,
-      tflite::testing::kOutputShape2D, tflite::testing::kGoldenData2D,
+      tflite_micro::testing::kAxisShape2D, tflite_micro::testing::kAxisData2D,
+      tflite_micro::testing::kOutputShape2D, tflite_micro::testing::kGoldenData2D,
       output_data_quant, expected_output_data_quant, output_scale,
-      output_zero_point, tflite::Register_MEAN(), &params, 1.0);
+      output_zero_point, tflite_micro::Register_MEAN(), &params, 1.0);
 }
 
 TF_LITE_MICRO_TEST(MeanInt162DKeepDims) {
-  int16_t expected_output_data_quant[tflite::testing::kOutputElements2D];
-  int16_t output_data_quant[tflite::testing::kOutputElements2D];
-  int16_t input_data_quant[tflite::testing::kInputElements2D];
+  int16_t expected_output_data_quant[tflite_micro::testing::kOutputElements2D];
+  int16_t output_data_quant[tflite_micro::testing::kOutputElements2D];
+  int16_t input_data_quant[tflite_micro::testing::kInputElements2D];
 
   float input_scale = 0.5f;
   int input_zero_point = 0;
@@ -216,31 +216,31 @@ TF_LITE_MICRO_TEST(MeanInt162DKeepDims) {
       true  // keep_dims
   };
 
-  tflite::testing::TestReduceOpQuantized<int16_t>(
-      tflite::testing::kInputShape2D, tflite::testing::kInputData2D,
+  tflite_micro::testing::TestReduceOpQuantized<int16_t>(
+      tflite_micro::testing::kInputShape2D, tflite_micro::testing::kInputData2D,
       input_data_quant, input_scale, input_zero_point,
-      tflite::testing::kAxisShape2D, tflite::testing::kAxisData2D,
-      tflite::testing::kOutputShape2D, tflite::testing::kGoldenData2D,
+      tflite_micro::testing::kAxisShape2D, tflite_micro::testing::kAxisData2D,
+      tflite_micro::testing::kOutputShape2D, tflite_micro::testing::kGoldenData2D,
       output_data_quant, expected_output_data_quant, output_scale,
-      output_zero_point, tflite::Register_MEAN(), &params, 1.0);
+      output_zero_point, tflite_micro::Register_MEAN(), &params, 1.0);
 }
 
 TF_LITE_MICRO_TEST(MeanFloat3DKeepDims) {
-  float output_data[tflite::testing::kOutputElements3D];
+  float output_data[tflite_micro::testing::kOutputElements3D];
 
   TfLiteReducerParams params = {true};
 
-  tflite::testing::TestReduceOpFloat(
-      tflite::testing::kInputShape3D, tflite::testing::kInputData3D,
-      tflite::testing::kAxisShape3D, tflite::testing::kAxisData3D,
-      tflite::testing::kOutputShape3D, output_data,
-      tflite::testing::kGoldenData3D, tflite::Register_MEAN(), &params);
+  tflite_micro::testing::TestReduceOpFloat(
+      tflite_micro::testing::kInputShape3D, tflite_micro::testing::kInputData3D,
+      tflite_micro::testing::kAxisShape3D, tflite_micro::testing::kAxisData3D,
+      tflite_micro::testing::kOutputShape3D, output_data,
+      tflite_micro::testing::kGoldenData3D, tflite_micro::Register_MEAN(), &params);
 }
 
 TF_LITE_MICRO_TEST(MeanInt83DKeepDims) {
-  int8_t expected_output_data_quant[tflite::testing::kOutputElements3D];
-  int8_t output_data_quant[tflite::testing::kOutputElements3D];
-  int8_t input_data_quant[tflite::testing::kInputElements3D];
+  int8_t expected_output_data_quant[tflite_micro::testing::kOutputElements3D];
+  int8_t output_data_quant[tflite_micro::testing::kOutputElements3D];
+  int8_t input_data_quant[tflite_micro::testing::kInputElements3D];
 
   float input_scale = 0.5f;
   int input_zero_point = 0;
@@ -251,19 +251,19 @@ TF_LITE_MICRO_TEST(MeanInt83DKeepDims) {
       true  // keep_dims
   };
 
-  tflite::testing::TestReduceOpQuantized<int8_t>(
-      tflite::testing::kInputShape3D, tflite::testing::kInputData3D,
+  tflite_micro::testing::TestReduceOpQuantized<int8_t>(
+      tflite_micro::testing::kInputShape3D, tflite_micro::testing::kInputData3D,
       input_data_quant, input_scale, input_zero_point,
-      tflite::testing::kAxisShape3D, tflite::testing::kAxisData3D,
-      tflite::testing::kOutputShape3D, tflite::testing::kGoldenData3D,
+      tflite_micro::testing::kAxisShape3D, tflite_micro::testing::kAxisData3D,
+      tflite_micro::testing::kOutputShape3D, tflite_micro::testing::kGoldenData3D,
       output_data_quant, expected_output_data_quant, output_scale,
-      output_zero_point, tflite::Register_MEAN(), &params, 1.0);
+      output_zero_point, tflite_micro::Register_MEAN(), &params, 1.0);
 }
 
 TF_LITE_MICRO_TEST(MeanInt163DKeepDims) {
-  int16_t expected_output_data_quant[tflite::testing::kOutputElements3D];
-  int16_t output_data_quant[tflite::testing::kOutputElements3D];
-  int16_t input_data_quant[tflite::testing::kInputElements3D];
+  int16_t expected_output_data_quant[tflite_micro::testing::kOutputElements3D];
+  int16_t output_data_quant[tflite_micro::testing::kOutputElements3D];
+  int16_t input_data_quant[tflite_micro::testing::kInputElements3D];
 
   float input_scale = 0.5f;
   int input_zero_point = 0;
@@ -274,33 +274,33 @@ TF_LITE_MICRO_TEST(MeanInt163DKeepDims) {
       true  // keep_dims
   };
 
-  tflite::testing::TestReduceOpQuantized<int16_t>(
-      tflite::testing::kInputShape3D, tflite::testing::kInputData3D,
+  tflite_micro::testing::TestReduceOpQuantized<int16_t>(
+      tflite_micro::testing::kInputShape3D, tflite_micro::testing::kInputData3D,
       input_data_quant, input_scale, input_zero_point,
-      tflite::testing::kAxisShape3D, tflite::testing::kAxisData3D,
-      tflite::testing::kOutputShape3D, tflite::testing::kGoldenData3D,
+      tflite_micro::testing::kAxisShape3D, tflite_micro::testing::kAxisData3D,
+      tflite_micro::testing::kOutputShape3D, tflite_micro::testing::kGoldenData3D,
       output_data_quant, expected_output_data_quant, output_scale,
-      output_zero_point, tflite::Register_MEAN(), &params, 1.0);
+      output_zero_point, tflite_micro::Register_MEAN(), &params, 1.0);
 }
 
 TF_LITE_MICRO_TEST(MeanFloat4DKeepDims) {
-  float output_data[tflite::testing::kOutputElements4D];
+  float output_data[tflite_micro::testing::kOutputElements4D];
 
   TfLiteReducerParams params = {
       true  // keep_dims
   };
 
-  tflite::testing::TestReduceOpFloat(
-      tflite::testing::kInputShape4D, tflite::testing::kInputData4D,
-      tflite::testing::kAxisShape4D, tflite::testing::kAxisData4D,
-      tflite::testing::kOutputShape4D, output_data,
-      tflite::testing::kGoldenData4D, tflite::Register_MEAN(), &params);
+  tflite_micro::testing::TestReduceOpFloat(
+      tflite_micro::testing::kInputShape4D, tflite_micro::testing::kInputData4D,
+      tflite_micro::testing::kAxisShape4D, tflite_micro::testing::kAxisData4D,
+      tflite_micro::testing::kOutputShape4D, output_data,
+      tflite_micro::testing::kGoldenData4D, tflite_micro::Register_MEAN(), &params);
 }
 
 TF_LITE_MICRO_TEST(MeanInt84DKeepDims) {
-  int8_t expected_output_data_quant[tflite::testing::kOutputElements4D];
-  int8_t output_data_quant[tflite::testing::kOutputElements4D];
-  int8_t input_data_quant[tflite::testing::kInputElements4D];
+  int8_t expected_output_data_quant[tflite_micro::testing::kOutputElements4D];
+  int8_t output_data_quant[tflite_micro::testing::kOutputElements4D];
+  int8_t input_data_quant[tflite_micro::testing::kInputElements4D];
 
   float input_scale = 0.5f;
   int input_zero_point = 0;
@@ -311,19 +311,19 @@ TF_LITE_MICRO_TEST(MeanInt84DKeepDims) {
       true  // keep_dims
   };
 
-  tflite::testing::TestReduceOpQuantized<int8_t>(
-      tflite::testing::kInputShape4D, tflite::testing::kInputData4D,
+  tflite_micro::testing::TestReduceOpQuantized<int8_t>(
+      tflite_micro::testing::kInputShape4D, tflite_micro::testing::kInputData4D,
       input_data_quant, input_scale, input_zero_point,
-      tflite::testing::kAxisShape4D, tflite::testing::kAxisData4D,
-      tflite::testing::kOutputShape4D, tflite::testing::kGoldenData4D,
+      tflite_micro::testing::kAxisShape4D, tflite_micro::testing::kAxisData4D,
+      tflite_micro::testing::kOutputShape4D, tflite_micro::testing::kGoldenData4D,
       output_data_quant, expected_output_data_quant, output_scale,
-      output_zero_point, tflite::Register_MEAN(), &params, 1.0);
+      output_zero_point, tflite_micro::Register_MEAN(), &params, 1.0);
 }
 
 TF_LITE_MICRO_TEST(MeanInt164DKeepDims) {
-  int16_t expected_output_data_quant[tflite::testing::kOutputElements4D];
-  int16_t output_data_quant[tflite::testing::kOutputElements4D];
-  int16_t input_data_quant[tflite::testing::kInputElements4D];
+  int16_t expected_output_data_quant[tflite_micro::testing::kOutputElements4D];
+  int16_t output_data_quant[tflite_micro::testing::kOutputElements4D];
+  int16_t input_data_quant[tflite_micro::testing::kInputElements4D];
 
   float input_scale = 0.5f;
   int input_zero_point = 0;
@@ -334,33 +334,33 @@ TF_LITE_MICRO_TEST(MeanInt164DKeepDims) {
       true  // keep_dims
   };
 
-  tflite::testing::TestReduceOpQuantized<int16_t>(
-      tflite::testing::kInputShape4D, tflite::testing::kInputData4D,
+  tflite_micro::testing::TestReduceOpQuantized<int16_t>(
+      tflite_micro::testing::kInputShape4D, tflite_micro::testing::kInputData4D,
       input_data_quant, input_scale, input_zero_point,
-      tflite::testing::kAxisShape4D, tflite::testing::kAxisData4D,
-      tflite::testing::kOutputShape4D, tflite::testing::kGoldenData4D,
+      tflite_micro::testing::kAxisShape4D, tflite_micro::testing::kAxisData4D,
+      tflite_micro::testing::kOutputShape4D, tflite_micro::testing::kGoldenData4D,
       output_data_quant, expected_output_data_quant, output_scale,
-      output_zero_point, tflite::Register_MEAN(), &params, 1.0);
+      output_zero_point, tflite_micro::Register_MEAN(), &params, 1.0);
 }
 
 TF_LITE_MICRO_TEST(MeanFloat4DWithoutKeepDims) {
   int kOutputShape4D[] = {2, 2, 2};
-  float output_data[tflite::testing::kOutputElements4D];
+  float output_data[tflite_micro::testing::kOutputElements4D];
   TfLiteReducerParams params = {
       false  // keep_dims
   };
 
-  tflite::testing::TestReduceOpFloat(
-      tflite::testing::kInputShape4D, tflite::testing::kInputData4D,
-      tflite::testing::kAxisShape4D, tflite::testing::kAxisData4D,
-      kOutputShape4D, output_data, tflite::testing::kGoldenData4D,
-      tflite::Register_MEAN(), &params);
+  tflite_micro::testing::TestReduceOpFloat(
+      tflite_micro::testing::kInputShape4D, tflite_micro::testing::kInputData4D,
+      tflite_micro::testing::kAxisShape4D, tflite_micro::testing::kAxisData4D,
+      kOutputShape4D, output_data, tflite_micro::testing::kGoldenData4D,
+      tflite_micro::Register_MEAN(), &params);
 }
 
 TF_LITE_MICRO_TEST(MeanInt84DWithoutKeepDims) {
-  int8_t expected_output_data_quant[tflite::testing::kOutputElements4D];
-  int8_t output_data_quant[tflite::testing::kOutputElements4D];
-  int8_t input_data_quant[tflite::testing::kInputElements4D];
+  int8_t expected_output_data_quant[tflite_micro::testing::kOutputElements4D];
+  int8_t output_data_quant[tflite_micro::testing::kOutputElements4D];
+  int8_t input_data_quant[tflite_micro::testing::kInputElements4D];
 
   int kOutputShape4D[] = {2, 2, 2};
   TfLiteReducerParams params = {
@@ -371,19 +371,19 @@ TF_LITE_MICRO_TEST(MeanInt84DWithoutKeepDims) {
   float output_scale = 0.5f;
   int output_zero_point = 0;
 
-  tflite::testing::TestReduceOpQuantized<int8_t>(
-      tflite::testing::kInputShape4D, tflite::testing::kInputData4D,
+  tflite_micro::testing::TestReduceOpQuantized<int8_t>(
+      tflite_micro::testing::kInputShape4D, tflite_micro::testing::kInputData4D,
       input_data_quant, input_scale, input_zero_point,
-      tflite::testing::kAxisShape4D, tflite::testing::kAxisData4D,
-      kOutputShape4D, tflite::testing::kGoldenData4D, output_data_quant,
+      tflite_micro::testing::kAxisShape4D, tflite_micro::testing::kAxisData4D,
+      kOutputShape4D, tflite_micro::testing::kGoldenData4D, output_data_quant,
       expected_output_data_quant, output_scale, output_zero_point,
-      tflite::Register_MEAN(), &params, 1.0);
+      tflite_micro::Register_MEAN(), &params, 1.0);
 }
 
 TF_LITE_MICRO_TEST(MeanInt164DWithoutKeepDims) {
-  int16_t expected_output_data_quant[tflite::testing::kOutputElements4D];
-  int16_t output_data_quant[tflite::testing::kOutputElements4D];
-  int16_t input_data_quant[tflite::testing::kInputElements4D];
+  int16_t expected_output_data_quant[tflite_micro::testing::kOutputElements4D];
+  int16_t output_data_quant[tflite_micro::testing::kOutputElements4D];
+  int16_t input_data_quant[tflite_micro::testing::kInputElements4D];
 
   int kOutputShape4D[] = {2, 2, 2};
   TfLiteReducerParams params = {
@@ -394,19 +394,19 @@ TF_LITE_MICRO_TEST(MeanInt164DWithoutKeepDims) {
   float output_scale = 0.5f;
   int output_zero_point = 0;
 
-  tflite::testing::TestReduceOpQuantized<int16_t>(
-      tflite::testing::kInputShape4D, tflite::testing::kInputData4D,
+  tflite_micro::testing::TestReduceOpQuantized<int16_t>(
+      tflite_micro::testing::kInputShape4D, tflite_micro::testing::kInputData4D,
       input_data_quant, input_scale, input_zero_point,
-      tflite::testing::kAxisShape4D, tflite::testing::kAxisData4D,
-      kOutputShape4D, tflite::testing::kGoldenData4D, output_data_quant,
+      tflite_micro::testing::kAxisShape4D, tflite_micro::testing::kAxisData4D,
+      kOutputShape4D, tflite_micro::testing::kGoldenData4D, output_data_quant,
       expected_output_data_quant, output_scale, output_zero_point,
-      tflite::Register_MEAN(), &params, 1.0);
+      tflite_micro::Register_MEAN(), &params, 1.0);
 }
 
 TF_LITE_MICRO_TEST(MeanInt164DWithoutKeepDimsDifferentScaleAndZeroPoint) {
-  int16_t expected_output_data_quant[tflite::testing::kOutputElements4D];
-  int16_t output_data_quant[tflite::testing::kOutputElements4D];
-  int16_t input_data_quant[tflite::testing::kInputElements4D];
+  int16_t expected_output_data_quant[tflite_micro::testing::kOutputElements4D];
+  int16_t output_data_quant[tflite_micro::testing::kOutputElements4D];
+  int16_t input_data_quant[tflite_micro::testing::kInputElements4D];
 
   int kOutputShape4D[] = {2, 2, 2};
   TfLiteReducerParams params = {
@@ -417,13 +417,13 @@ TF_LITE_MICRO_TEST(MeanInt164DWithoutKeepDimsDifferentScaleAndZeroPoint) {
   float output_scale = 0.7f;
   int output_zero_point = 0;
 
-  tflite::testing::TestReduceOpQuantized<int16_t>(
-      tflite::testing::kInputShape4D, tflite::testing::kInputData4D,
+  tflite_micro::testing::TestReduceOpQuantized<int16_t>(
+      tflite_micro::testing::kInputShape4D, tflite_micro::testing::kInputData4D,
       input_data_quant, input_scale, input_zero_point,
-      tflite::testing::kAxisShape4D, tflite::testing::kAxisData4D,
-      kOutputShape4D, tflite::testing::kGoldenData4D, output_data_quant,
+      tflite_micro::testing::kAxisShape4D, tflite_micro::testing::kAxisData4D,
+      kOutputShape4D, tflite_micro::testing::kGoldenData4D, output_data_quant,
       expected_output_data_quant, output_scale, output_zero_point,
-      tflite::Register_MEAN(), &params, 1.0);
+      tflite_micro::Register_MEAN(), &params, 1.0);
 }
 
 TF_LITE_MICRO_TEST(MeanFloat4DWithoutKeepDimsWithPrecision) {
@@ -438,10 +438,10 @@ TF_LITE_MICRO_TEST(MeanFloat4DWithoutKeepDimsWithPrecision) {
       false  // keep_dims
   };
 
-  tflite::testing::TestReduceOpFloat(
-      kInputShape4D, kInputData4D, tflite::testing::kAxisShape4D,
-      tflite::testing::kAxisData4D, kOutputShape4D, output_data, kGoldenData4D,
-      tflite::Register_MEAN(), &params);
+  tflite_micro::testing::TestReduceOpFloat(
+      kInputShape4D, kInputData4D, tflite_micro::testing::kAxisShape4D,
+      tflite_micro::testing::kAxisData4D, kOutputShape4D, output_data, kGoldenData4D,
+      tflite_micro::Register_MEAN(), &params);
 }
 
 TF_LITE_MICRO_TEST(FloatMaxOpTestNotKeepDims) {
@@ -457,9 +457,9 @@ TF_LITE_MICRO_TEST(FloatMaxOpTestNotKeepDims) {
 
   TfLiteReducerParams params = {false};
 
-  tflite::testing::TestReduceOpFloat(
+  tflite_micro::testing::TestReduceOpFloat(
       input_shape, input_data, axis_shape, axis_data, output_shape, output_data,
-      expected_output_data, tflite::Register_REDUCE_MAX(), &params);
+      expected_output_data, tflite_micro::Register_REDUCE_MAX(), &params);
 }
 
 TF_LITE_MICRO_TEST(FloatMaxOpTestKeepDims) {
@@ -475,9 +475,9 @@ TF_LITE_MICRO_TEST(FloatMaxOpTestKeepDims) {
 
   TfLiteReducerParams params = {true};
 
-  tflite::testing::TestReduceOpFloat(
+  tflite_micro::testing::TestReduceOpFloat(
       input_shape, input_data, axis_shape, axis_data, output_shape, output_data,
-      expected_output_data, tflite::Register_REDUCE_MAX(), &params);
+      expected_output_data, tflite_micro::Register_REDUCE_MAX(), &params);
 }
 
 TF_LITE_MICRO_TEST(Int8MaxOpTestKeepDims) {
@@ -497,11 +497,11 @@ TF_LITE_MICRO_TEST(Int8MaxOpTestKeepDims) {
   int8_t output_data_quant[2];
   int8_t expected_output_data_quant[2];
 
-  tflite::testing::TestReduceOpQuantized<int8_t>(
+  tflite_micro::testing::TestReduceOpQuantized<int8_t>(
       input_shape, input_data, input_data_quant, input_scale, input_zp,
       axis_shape, axis_data, output_shape, expected_output_data,
       output_data_quant, expected_output_data_quant, input_scale, input_zp,
-      tflite::Register_REDUCE_MAX(), &params);
+      tflite_micro::Register_REDUCE_MAX(), &params);
 }
 
 TF_LITE_MICRO_TEST(Int8MaxOpTestWithoutKeepDims) {
@@ -523,11 +523,11 @@ TF_LITE_MICRO_TEST(Int8MaxOpTestWithoutKeepDims) {
   int8_t output_data_quant[2];
   int8_t expected_output_data_quant[2];
 
-  tflite::testing::TestReduceOpQuantized<int8_t>(
+  tflite_micro::testing::TestReduceOpQuantized<int8_t>(
       input_shape, input_data, input_data_quant, input_scale, input_zp,
       axis_shape, axis_data, output_shape, expected_output_data,
       output_data_quant, expected_output_data_quant, output_scale, output_zp,
-      tflite::Register_REDUCE_MAX(), &params);
+      tflite_micro::Register_REDUCE_MAX(), &params);
 }
 
 TF_LITE_MICRO_TEST(MeanInt84DWithoutKeepDimsWithPrecision) {
@@ -548,12 +548,12 @@ TF_LITE_MICRO_TEST(MeanInt84DWithoutKeepDimsWithPrecision) {
   int8_t expected_output_data_quant[2];
   int8_t input_data_quant[12];
 
-  tflite::testing::TestReduceOpQuantized<int8_t>(
+  tflite_micro::testing::TestReduceOpQuantized<int8_t>(
       kInputShape4D, kInputData4D, input_data_quant, input_scale,
-      input_zero_point, tflite::testing::kAxisShape4D,
-      tflite::testing::kAxisData4D, kOutputShape4D, kGoldenData4D,
+      input_zero_point, tflite_micro::testing::kAxisShape4D,
+      tflite_micro::testing::kAxisData4D, kOutputShape4D, kGoldenData4D,
       output_data_quant, expected_output_data_quant, output_scale,
-      output_zero_point, tflite::Register_MEAN(), &params, 1.0);
+      output_zero_point, tflite_micro::Register_MEAN(), &params, 1.0);
 }
 
 TF_LITE_MICRO_TEST(SumFloatFlatten2ReduceDims) {
@@ -569,9 +569,9 @@ TF_LITE_MICRO_TEST(SumFloatFlatten2ReduceDims) {
 
   TfLiteReducerParams params = {false};
 
-  tflite::testing::TestReduceOpFloat(
+  tflite_micro::testing::TestReduceOpFloat(
       input_shape, input_data, axis_shape, axis_data, output_shape,
-      actual_output_data, expected_output, tflite::Register_SUM(), &params);
+      actual_output_data, expected_output, tflite_micro::Register_SUM(), &params);
 }
 
 TF_LITE_MICRO_TEST(SumFloatFlatten2NonReduceDims) {
@@ -588,9 +588,9 @@ TF_LITE_MICRO_TEST(SumFloatFlatten2NonReduceDims) {
 
   TfLiteReducerParams params = {false};
 
-  tflite::testing::TestReduceOpFloat(
+  tflite_micro::testing::TestReduceOpFloat(
       input_shape, input_data, axis_shape, axis_data, output_shape,
-      actual_output_data, expected_output, tflite::Register_SUM(), &params);
+      actual_output_data, expected_output, tflite_micro::Register_SUM(), &params);
 }
 
 TF_LITE_MICRO_TEST(SumFloatFlatten2MiddleDims) {
@@ -606,27 +606,27 @@ TF_LITE_MICRO_TEST(SumFloatFlatten2MiddleDims) {
 
   TfLiteReducerParams params = {false};
 
-  tflite::testing::TestReduceOpFloat(
+  tflite_micro::testing::TestReduceOpFloat(
       input_shape, input_data, axis_shape, axis_data, output_shape,
-      actual_output_data, expected_output, tflite::Register_SUM(), &params);
+      actual_output_data, expected_output, tflite_micro::Register_SUM(), &params);
 }
 
 TF_LITE_MICRO_TEST(SumFloat2DKeepDims) {
-  float output_data[tflite::testing::kOutputElements2D];
+  float output_data[tflite_micro::testing::kOutputElements2D];
 
   TfLiteReducerParams params = {true};
 
-  tflite::testing::TestReduceOpFloat(
-      tflite::testing::kInputShape2D, tflite::testing::kInputData2D,
-      tflite::testing::kAxisShape2D, tflite::testing::kAxisData2D,
-      tflite::testing::kOutputShape2D, output_data,
-      tflite::testing::kGoldenDataSum2D, tflite::Register_SUM(), &params);
+  tflite_micro::testing::TestReduceOpFloat(
+      tflite_micro::testing::kInputShape2D, tflite_micro::testing::kInputData2D,
+      tflite_micro::testing::kAxisShape2D, tflite_micro::testing::kAxisData2D,
+      tflite_micro::testing::kOutputShape2D, output_data,
+      tflite_micro::testing::kGoldenDataSum2D, tflite_micro::Register_SUM(), &params);
 }
 
 TF_LITE_MICRO_TEST(SumInt82DKeepDims) {
-  int8_t expected_output_data_quant[tflite::testing::kOutputElements2D];
-  int8_t output_data_quant[tflite::testing::kOutputElements2D];
-  int8_t input_data_quant[tflite::testing::kInputElements2D];
+  int8_t expected_output_data_quant[tflite_micro::testing::kOutputElements2D];
+  int8_t output_data_quant[tflite_micro::testing::kOutputElements2D];
+  int8_t input_data_quant[tflite_micro::testing::kInputElements2D];
 
   float input_scale = 0.5f;
   int input_zero_point = 0;
@@ -637,19 +637,19 @@ TF_LITE_MICRO_TEST(SumInt82DKeepDims) {
       true  // keep_dims
   };
 
-  tflite::testing::TestReduceOpQuantized<int8_t>(
-      tflite::testing::kInputShape2D, tflite::testing::kInputData2D,
+  tflite_micro::testing::TestReduceOpQuantized<int8_t>(
+      tflite_micro::testing::kInputShape2D, tflite_micro::testing::kInputData2D,
       input_data_quant, input_scale, input_zero_point,
-      tflite::testing::kAxisShape2D, tflite::testing::kAxisData2D,
-      tflite::testing::kOutputShape2D, tflite::testing::kGoldenDataSum2D,
+      tflite_micro::testing::kAxisShape2D, tflite_micro::testing::kAxisData2D,
+      tflite_micro::testing::kOutputShape2D, tflite_micro::testing::kGoldenDataSum2D,
       output_data_quant, expected_output_data_quant, output_scale,
-      output_zero_point, tflite::Register_SUM(), &params, 1.0);
+      output_zero_point, tflite_micro::Register_SUM(), &params, 1.0);
 }
 
 TF_LITE_MICRO_TEST(SumInt162DKeepDims) {
-  int16_t expected_output_data_quant[tflite::testing::kOutputElements2D];
-  int16_t output_data_quant[tflite::testing::kOutputElements2D];
-  int16_t input_data_quant[tflite::testing::kInputElements2D];
+  int16_t expected_output_data_quant[tflite_micro::testing::kOutputElements2D];
+  int16_t output_data_quant[tflite_micro::testing::kOutputElements2D];
+  int16_t input_data_quant[tflite_micro::testing::kInputElements2D];
 
   float input_scale = 0.5f;
   int input_zero_point = 0;
@@ -660,31 +660,31 @@ TF_LITE_MICRO_TEST(SumInt162DKeepDims) {
       true  // keep_dims
   };
 
-  tflite::testing::TestReduceOpQuantized<int16_t>(
-      tflite::testing::kInputShape2D, tflite::testing::kInputData2D,
+  tflite_micro::testing::TestReduceOpQuantized<int16_t>(
+      tflite_micro::testing::kInputShape2D, tflite_micro::testing::kInputData2D,
       input_data_quant, input_scale, input_zero_point,
-      tflite::testing::kAxisShape2D, tflite::testing::kAxisData2D,
-      tflite::testing::kOutputShape2D, tflite::testing::kGoldenDataSum2D,
+      tflite_micro::testing::kAxisShape2D, tflite_micro::testing::kAxisData2D,
+      tflite_micro::testing::kOutputShape2D, tflite_micro::testing::kGoldenDataSum2D,
       output_data_quant, expected_output_data_quant, output_scale,
-      output_zero_point, tflite::Register_SUM(), &params, 1.0);
+      output_zero_point, tflite_micro::Register_SUM(), &params, 1.0);
 }
 
 TF_LITE_MICRO_TEST(SumFloat3DKeepDims) {
-  float output_data[tflite::testing::kOutputElements3D];
+  float output_data[tflite_micro::testing::kOutputElements3D];
 
   TfLiteReducerParams params = {true};
 
-  tflite::testing::TestReduceOpFloat(
-      tflite::testing::kInputShape3D, tflite::testing::kInputData3D,
-      tflite::testing::kAxisShape3D, tflite::testing::kAxisData3D,
-      tflite::testing::kOutputShape3D, output_data,
-      tflite::testing::kGoldenDataSum3D, tflite::Register_SUM(), &params);
+  tflite_micro::testing::TestReduceOpFloat(
+      tflite_micro::testing::kInputShape3D, tflite_micro::testing::kInputData3D,
+      tflite_micro::testing::kAxisShape3D, tflite_micro::testing::kAxisData3D,
+      tflite_micro::testing::kOutputShape3D, output_data,
+      tflite_micro::testing::kGoldenDataSum3D, tflite_micro::Register_SUM(), &params);
 }
 
 TF_LITE_MICRO_TEST(SumInt83DKeepDims) {
-  int8_t expected_output_data_quant[tflite::testing::kOutputElements3D];
-  int8_t output_data_quant[tflite::testing::kOutputElements3D];
-  int8_t input_data_quant[tflite::testing::kInputElements3D];
+  int8_t expected_output_data_quant[tflite_micro::testing::kOutputElements3D];
+  int8_t output_data_quant[tflite_micro::testing::kOutputElements3D];
+  int8_t input_data_quant[tflite_micro::testing::kInputElements3D];
 
   float input_scale = 0.5f;
   int input_zero_point = 0;
@@ -695,19 +695,19 @@ TF_LITE_MICRO_TEST(SumInt83DKeepDims) {
       true  // keep_dims
   };
 
-  tflite::testing::TestReduceOpQuantized<int8_t>(
-      tflite::testing::kInputShape3D, tflite::testing::kInputData3D,
+  tflite_micro::testing::TestReduceOpQuantized<int8_t>(
+      tflite_micro::testing::kInputShape3D, tflite_micro::testing::kInputData3D,
       input_data_quant, input_scale, input_zero_point,
-      tflite::testing::kAxisShape3D, tflite::testing::kAxisData3D,
-      tflite::testing::kOutputShape3D, tflite::testing::kGoldenDataSum3D,
+      tflite_micro::testing::kAxisShape3D, tflite_micro::testing::kAxisData3D,
+      tflite_micro::testing::kOutputShape3D, tflite_micro::testing::kGoldenDataSum3D,
       output_data_quant, expected_output_data_quant, output_scale,
-      output_zero_point, tflite::Register_SUM(), &params, 1.0);
+      output_zero_point, tflite_micro::Register_SUM(), &params, 1.0);
 }
 
 TF_LITE_MICRO_TEST(SumInt163DKeepDims) {
-  int16_t expected_output_data_quant[tflite::testing::kOutputElements3D];
-  int16_t output_data_quant[tflite::testing::kOutputElements3D];
-  int16_t input_data_quant[tflite::testing::kInputElements3D];
+  int16_t expected_output_data_quant[tflite_micro::testing::kOutputElements3D];
+  int16_t output_data_quant[tflite_micro::testing::kOutputElements3D];
+  int16_t input_data_quant[tflite_micro::testing::kInputElements3D];
 
   float input_scale = 0.5f;
   int input_zero_point = 0;
@@ -718,33 +718,33 @@ TF_LITE_MICRO_TEST(SumInt163DKeepDims) {
       true  // keep_dims
   };
 
-  tflite::testing::TestReduceOpQuantized<int16_t>(
-      tflite::testing::kInputShape3D, tflite::testing::kInputData3D,
+  tflite_micro::testing::TestReduceOpQuantized<int16_t>(
+      tflite_micro::testing::kInputShape3D, tflite_micro::testing::kInputData3D,
       input_data_quant, input_scale, input_zero_point,
-      tflite::testing::kAxisShape3D, tflite::testing::kAxisData3D,
-      tflite::testing::kOutputShape3D, tflite::testing::kGoldenDataSum3D,
+      tflite_micro::testing::kAxisShape3D, tflite_micro::testing::kAxisData3D,
+      tflite_micro::testing::kOutputShape3D, tflite_micro::testing::kGoldenDataSum3D,
       output_data_quant, expected_output_data_quant, output_scale,
-      output_zero_point, tflite::Register_SUM(), &params, 1.0);
+      output_zero_point, tflite_micro::Register_SUM(), &params, 1.0);
 }
 
 TF_LITE_MICRO_TEST(SumFloat4DKeepDims) {
-  float output_data[tflite::testing::kOutputElements4D];
+  float output_data[tflite_micro::testing::kOutputElements4D];
 
   TfLiteReducerParams params = {
       true  // keep_dims
   };
 
-  tflite::testing::TestReduceOpFloat(
-      tflite::testing::kInputShape4D, tflite::testing::kInputData4D,
-      tflite::testing::kAxisShape4D, tflite::testing::kAxisData4D,
-      tflite::testing::kOutputShape4D, output_data,
-      tflite::testing::kGoldenDataSum4D, tflite::Register_SUM(), &params);
+  tflite_micro::testing::TestReduceOpFloat(
+      tflite_micro::testing::kInputShape4D, tflite_micro::testing::kInputData4D,
+      tflite_micro::testing::kAxisShape4D, tflite_micro::testing::kAxisData4D,
+      tflite_micro::testing::kOutputShape4D, output_data,
+      tflite_micro::testing::kGoldenDataSum4D, tflite_micro::Register_SUM(), &params);
 }
 
 TF_LITE_MICRO_TEST(SumInt84DKeepDims) {
-  int8_t expected_output_data_quant[tflite::testing::kOutputElements4D];
-  int8_t output_data_quant[tflite::testing::kOutputElements4D];
-  int8_t input_data_quant[tflite::testing::kInputElements4D];
+  int8_t expected_output_data_quant[tflite_micro::testing::kOutputElements4D];
+  int8_t output_data_quant[tflite_micro::testing::kOutputElements4D];
+  int8_t input_data_quant[tflite_micro::testing::kInputElements4D];
 
   float input_scale = 1.f;
   int input_zero_point = 0;
@@ -755,19 +755,19 @@ TF_LITE_MICRO_TEST(SumInt84DKeepDims) {
       true  // keep_dims
   };
 
-  tflite::testing::TestReduceOpQuantized<int8_t>(
-      tflite::testing::kInputShape4D, tflite::testing::kInputData4D,
+  tflite_micro::testing::TestReduceOpQuantized<int8_t>(
+      tflite_micro::testing::kInputShape4D, tflite_micro::testing::kInputData4D,
       input_data_quant, input_scale, input_zero_point,
-      tflite::testing::kAxisShape4D, tflite::testing::kAxisData4D,
-      tflite::testing::kOutputShape4D, tflite::testing::kGoldenDataSum4D,
+      tflite_micro::testing::kAxisShape4D, tflite_micro::testing::kAxisData4D,
+      tflite_micro::testing::kOutputShape4D, tflite_micro::testing::kGoldenDataSum4D,
       output_data_quant, expected_output_data_quant, output_scale,
-      output_zero_point, tflite::Register_SUM(), &params, 1.0);
+      output_zero_point, tflite_micro::Register_SUM(), &params, 1.0);
 }
 
 TF_LITE_MICRO_TEST(SumInt164DKeepDims) {
-  int16_t expected_output_data_quant[tflite::testing::kOutputElements4D];
-  int16_t output_data_quant[tflite::testing::kOutputElements4D];
-  int16_t input_data_quant[tflite::testing::kInputElements4D];
+  int16_t expected_output_data_quant[tflite_micro::testing::kOutputElements4D];
+  int16_t output_data_quant[tflite_micro::testing::kOutputElements4D];
+  int16_t input_data_quant[tflite_micro::testing::kInputElements4D];
 
   float input_scale = 0.5f;
   int input_zero_point = 0;
@@ -778,33 +778,33 @@ TF_LITE_MICRO_TEST(SumInt164DKeepDims) {
       true  // keep_dims
   };
 
-  tflite::testing::TestReduceOpQuantized<int16_t>(
-      tflite::testing::kInputShape4D, tflite::testing::kInputData4D,
+  tflite_micro::testing::TestReduceOpQuantized<int16_t>(
+      tflite_micro::testing::kInputShape4D, tflite_micro::testing::kInputData4D,
       input_data_quant, input_scale, input_zero_point,
-      tflite::testing::kAxisShape4D, tflite::testing::kAxisData4D,
-      tflite::testing::kOutputShape4D, tflite::testing::kGoldenDataSum4D,
+      tflite_micro::testing::kAxisShape4D, tflite_micro::testing::kAxisData4D,
+      tflite_micro::testing::kOutputShape4D, tflite_micro::testing::kGoldenDataSum4D,
       output_data_quant, expected_output_data_quant, output_scale,
-      output_zero_point, tflite::Register_SUM(), &params, 1.0);
+      output_zero_point, tflite_micro::Register_SUM(), &params, 1.0);
 }
 
 TF_LITE_MICRO_TEST(SumFloat4DWithoutKeepDims) {
   int kOutputShape4D[] = {2, 2, 2};
-  float output_data[tflite::testing::kOutputElements4D];
+  float output_data[tflite_micro::testing::kOutputElements4D];
   TfLiteReducerParams params = {
       false  // keep_dims
   };
 
-  tflite::testing::TestReduceOpFloat(
-      tflite::testing::kInputShape4D, tflite::testing::kInputData4D,
-      tflite::testing::kAxisShape4D, tflite::testing::kAxisData4D,
-      kOutputShape4D, output_data, tflite::testing::kGoldenDataSum4D,
-      tflite::Register_SUM(), &params);
+  tflite_micro::testing::TestReduceOpFloat(
+      tflite_micro::testing::kInputShape4D, tflite_micro::testing::kInputData4D,
+      tflite_micro::testing::kAxisShape4D, tflite_micro::testing::kAxisData4D,
+      kOutputShape4D, output_data, tflite_micro::testing::kGoldenDataSum4D,
+      tflite_micro::Register_SUM(), &params);
 }
 
 TF_LITE_MICRO_TEST(SumInt84DWithoutKeepDims) {
-  int8_t expected_output_data_quant[tflite::testing::kOutputElements4D];
-  int8_t output_data_quant[tflite::testing::kOutputElements4D];
-  int8_t input_data_quant[tflite::testing::kInputElements4D];
+  int8_t expected_output_data_quant[tflite_micro::testing::kOutputElements4D];
+  int8_t output_data_quant[tflite_micro::testing::kOutputElements4D];
+  int8_t input_data_quant[tflite_micro::testing::kInputElements4D];
 
   int kOutputShape4D[] = {2, 2, 2};
   TfLiteReducerParams params = {
@@ -815,19 +815,19 @@ TF_LITE_MICRO_TEST(SumInt84DWithoutKeepDims) {
   float output_scale = 1.f;
   int output_zero_point = 0;
 
-  tflite::testing::TestReduceOpQuantized<int8_t>(
-      tflite::testing::kInputShape4D, tflite::testing::kInputData4D,
+  tflite_micro::testing::TestReduceOpQuantized<int8_t>(
+      tflite_micro::testing::kInputShape4D, tflite_micro::testing::kInputData4D,
       input_data_quant, input_scale, input_zero_point,
-      tflite::testing::kAxisShape4D, tflite::testing::kAxisData4D,
-      kOutputShape4D, tflite::testing::kGoldenDataSum4D, output_data_quant,
+      tflite_micro::testing::kAxisShape4D, tflite_micro::testing::kAxisData4D,
+      kOutputShape4D, tflite_micro::testing::kGoldenDataSum4D, output_data_quant,
       expected_output_data_quant, output_scale, output_zero_point,
-      tflite::Register_SUM(), &params, 1.0);
+      tflite_micro::Register_SUM(), &params, 1.0);
 }
 
 TF_LITE_MICRO_TEST(SumInt164DWithoutKeepDims) {
-  int16_t expected_output_data_quant[tflite::testing::kOutputElements4D];
-  int16_t output_data_quant[tflite::testing::kOutputElements4D];
-  int16_t input_data_quant[tflite::testing::kInputElements4D];
+  int16_t expected_output_data_quant[tflite_micro::testing::kOutputElements4D];
+  int16_t output_data_quant[tflite_micro::testing::kOutputElements4D];
+  int16_t input_data_quant[tflite_micro::testing::kInputElements4D];
 
   int kOutputShape4D[] = {2, 2, 2};
   TfLiteReducerParams params = {
@@ -838,19 +838,19 @@ TF_LITE_MICRO_TEST(SumInt164DWithoutKeepDims) {
   float output_scale = 0.5f;
   int output_zero_point = 0;
 
-  tflite::testing::TestReduceOpQuantized<int16_t>(
-      tflite::testing::kInputShape4D, tflite::testing::kInputData4D,
+  tflite_micro::testing::TestReduceOpQuantized<int16_t>(
+      tflite_micro::testing::kInputShape4D, tflite_micro::testing::kInputData4D,
       input_data_quant, input_scale, input_zero_point,
-      tflite::testing::kAxisShape4D, tflite::testing::kAxisData4D,
-      kOutputShape4D, tflite::testing::kGoldenDataSum4D, output_data_quant,
+      tflite_micro::testing::kAxisShape4D, tflite_micro::testing::kAxisData4D,
+      kOutputShape4D, tflite_micro::testing::kGoldenDataSum4D, output_data_quant,
       expected_output_data_quant, output_scale, output_zero_point,
-      tflite::Register_SUM(), &params, 1.0);
+      tflite_micro::Register_SUM(), &params, 1.0);
 }
 
 TF_LITE_MICRO_TEST(SumInt164DWithoutKeepDimsDifferentScaleAndZeroPoint) {
-  int16_t expected_output_data_quant[tflite::testing::kOutputElements4D];
-  int16_t output_data_quant[tflite::testing::kOutputElements4D];
-  int16_t input_data_quant[tflite::testing::kInputElements4D];
+  int16_t expected_output_data_quant[tflite_micro::testing::kOutputElements4D];
+  int16_t output_data_quant[tflite_micro::testing::kOutputElements4D];
+  int16_t input_data_quant[tflite_micro::testing::kInputElements4D];
 
   int kOutputShape4D[] = {2, 2, 2};
   TfLiteReducerParams params = {
@@ -861,13 +861,13 @@ TF_LITE_MICRO_TEST(SumInt164DWithoutKeepDimsDifferentScaleAndZeroPoint) {
   float output_scale = 0.7f;
   int output_zero_point = 0;
 
-  tflite::testing::TestReduceOpQuantized<int16_t>(
-      tflite::testing::kInputShape4D, tflite::testing::kInputData4D,
+  tflite_micro::testing::TestReduceOpQuantized<int16_t>(
+      tflite_micro::testing::kInputShape4D, tflite_micro::testing::kInputData4D,
       input_data_quant, input_scale, input_zero_point,
-      tflite::testing::kAxisShape4D, tflite::testing::kAxisData4D,
-      kOutputShape4D, tflite::testing::kGoldenDataSum4D, output_data_quant,
+      tflite_micro::testing::kAxisShape4D, tflite_micro::testing::kAxisData4D,
+      kOutputShape4D, tflite_micro::testing::kGoldenDataSum4D, output_data_quant,
       expected_output_data_quant, output_scale, output_zero_point,
-      tflite::Register_SUM(), &params, 1.0);
+      tflite_micro::Register_SUM(), &params, 1.0);
 }
 
 TF_LITE_MICRO_TEST(SumFloatSize1) {
@@ -881,9 +881,9 @@ TF_LITE_MICRO_TEST(SumFloatSize1) {
 
   TfLiteReducerParams params = {false};
 
-  tflite::testing::TestReduceOpFloat(
+  tflite_micro::testing::TestReduceOpFloat(
       input_shape, input_data, axis_shape, axis_data, output_shape,
-      actual_output_data, expected_output, tflite::Register_SUM(), &params);
+      actual_output_data, expected_output, tflite_micro::Register_SUM(), &params);
 }
 
 TF_LITE_MICRO_TEST(SumFloat2DRedundantDims) {
@@ -897,9 +897,9 @@ TF_LITE_MICRO_TEST(SumFloat2DRedundantDims) {
 
   TfLiteReducerParams params = {false};
 
-  tflite::testing::TestReduceOpFloat(
+  tflite_micro::testing::TestReduceOpFloat(
       input_shape, input_data, axis_shape, axis_data, output_shape,
-      actual_output_data, expected_output, tflite::Register_SUM(), &params);
+      actual_output_data, expected_output, tflite_micro::Register_SUM(), &params);
 }
 
 TF_LITE_MICRO_TEST(SumFloatScalar) {
@@ -913,9 +913,9 @@ TF_LITE_MICRO_TEST(SumFloatScalar) {
 
   TfLiteReducerParams params = {false};
 
-  tflite::testing::TestReduceOpFloat(
+  tflite_micro::testing::TestReduceOpFloat(
       input_shape, input_data, axis_shape, axis_data, output_shape,
-      actual_output_data, expected_output, tflite::Register_SUM(), &params);
+      actual_output_data, expected_output, tflite_micro::Register_SUM(), &params);
 }
 
 TF_LITE_MICRO_TESTS_END
diff --git a/tensorflow/lite/micro/kernels/reshape.cc b/tensorflow/lite/micro/kernels/reshape.cc
index 4e6c530b..c5b1725b 100644
--- a/tensorflow/lite/micro/kernels/reshape.cc
+++ b/tensorflow/lite/micro/kernels/reshape.cc
@@ -26,14 +26,14 @@ limitations under the License.
 #include "tensorflow/lite/micro/memory_helpers.h"
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kReshapeInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kReshapeInputTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kReshapeOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kReshapeOutputTensor);
 
   // TODO(b/162522304): storing input bytes in OpData increases some models
   // significantly, possibly due to alignment issues.
@@ -52,7 +52,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_RESHAPE() {
-  return tflite::micro::RegisterOp(nullptr, PrepareReshapeReference, Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, PrepareReshapeReference, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/reshape.h b/tensorflow/lite/micro/kernels/reshape.h
index 02bda32a..bcb3e056 100644
--- a/tensorflow/lite/micro/kernels/reshape.h
+++ b/tensorflow/lite/micro/kernels/reshape.h
@@ -16,11 +16,11 @@ limitations under the License.
 #include "tensorflow/lite/c/builtin_op_data.h"
 #include "tensorflow/lite/c/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 constexpr int kReshapeInputTensor = 0;
 constexpr int kReshapeOutputTensor = 0;
 
 TfLiteStatus PrepareReshapeReference(TfLiteContext* context, TfLiteNode* node);
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/reshape_common.cc b/tensorflow/lite/micro/kernels/reshape_common.cc
index b86e2be0..ca8d89e7 100644
--- a/tensorflow/lite/micro/kernels/reshape_common.cc
+++ b/tensorflow/lite/micro/kernels/reshape_common.cc
@@ -25,7 +25,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/memory_helpers.h"
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -66,8 +66,8 @@ TfLiteStatus ReshapeOutput(TfLiteContext* context, TfLiteNode* node) {
   }
   if (stretch_dim != -1) {
     TfLiteEvalTensor* output_eval =
-        tflite::micro::GetEvalOutput(context, node, kReshapeOutputTensor);
-    TF_LITE_ENSURE_STATUS(tflite::micro::CreateWritableTensorDimsWithCopy(
+        tflite_micro::micro::GetEvalOutput(context, node, kReshapeOutputTensor);
+    TF_LITE_ENSURE_STATUS(tflite_micro::micro::CreateWritableTensorDimsWithCopy(
         context, output, output_eval));
     output_shape = output->dims;  // output tensor dims were moved
     output_shape->data[stretch_dim] = num_input_elements / num_output_elements;
@@ -91,4 +91,4 @@ TfLiteStatus PrepareReshapeReference(TfLiteContext* context, TfLiteNode* node) {
   return kTfLiteOk;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/reshape_test.cc b/tensorflow/lite/micro/kernels/reshape_test.cc
index d78d9fa9..f1912202 100644
--- a/tensorflow/lite/micro/kernels/reshape_test.cc
+++ b/tensorflow/lite/micro/kernels/reshape_test.cc
@@ -24,7 +24,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -38,7 +38,7 @@ void ValidateReshapeGoldens(TfLiteTensor* tensors, int tensors_size,
                             const size_t expected_output_len,
                             int* expected_dims, const size_t expected_dims_len,
                             bool expect_failure) {
-  const TFLMRegistration registration = tflite::Register_RESHAPE();
+  const TFLMRegistration registration = tflite_micro::Register_RESHAPE();
   micro::KernelRunner runner(registration, tensors, tensors_size, inputs_array,
                              outputs_array,
                              /*builtin_data=*/nullptr);
@@ -153,7 +153,7 @@ void TestReshapeQuantized(int* input_dims_data, const T* input_data,
 }
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -168,7 +168,7 @@ TF_LITE_MICRO_TEST(ReshapeWithMismatchedDimensionsShouldFail) {
   const float golden_output[] = {};
   const int golden_dims_len = 0;
   int golden_dims[] = {};
-  tflite::testing::TestReshape(
+  tflite_micro::testing::TestReshape(
       input_dims, input_data, shape_dims, shape_int32, output_dims, output_data,
       golden_output, golden_output_len, golden_dims, golden_dims_len, true);
 }
@@ -184,7 +184,7 @@ TF_LITE_MICRO_TEST(ReshapeWithManyDimensionsShouldSucceed) {
   const float golden_output[] = {3, 2};
   const int golden_dims_len = 9;
   int golden_dims[] = {1, 1, 1, 1, 1, 1, 1, 1, 2};
-  tflite::testing::TestReshape(
+  tflite_micro::testing::TestReshape(
       input_dims, input, shape_dims, shape_int32, output_dims, output_data,
       golden_output, golden_output_len, golden_dims, golden_dims_len, false);
 }
@@ -200,7 +200,7 @@ TF_LITE_MICRO_TEST(ReshapeWithTooManySpecialDimensionsShouldFail) {
   const float golden_output[] = {};
   const int golden_dims_len = 9;
   int golden_dims[] = {};
-  tflite::testing::TestReshape(
+  tflite_micro::testing::TestReshape(
       input_dims, input, shape_dims, shape_int32, output_dims, output_data,
       golden_output, golden_output_len, golden_dims, golden_dims_len, true);
 }
@@ -210,19 +210,19 @@ TF_LITE_MICRO_TEST(ReshapeWithTooManySpecialDimensionsShouldFail) {
 TF_LITE_MICRO_TEST(ReshapeWithInvalidShapeShouldFail) {
   int input_dims_data[] = {3, 1, 2, 2};
   TfLiteIntArray* input_dims =
-      tflite::testing::IntArrayFromInts(input_dims_data);
+      tflite_micro::testing::IntArrayFromInts(input_dims_data);
   const float input_data[] = {3.0f};
-  auto input_tensor = tflite::testing::CreateTensor(input_data, input_dims);
+  auto input_tensor = tflite_micro::testing::CreateTensor(input_data, input_dims);
   float output_data[4];
   int output_dims_data[6] = {2, 2, 1, 2, 2, 1};
   TfLiteIntArray* output_dims =
-      tflite::testing::IntArrayFromInts(output_dims_data);
-  auto output_tensor = tflite::testing::CreateTensor(output_data, output_dims);
+      tflite_micro::testing::IntArrayFromInts(output_dims_data);
+  auto output_tensor = tflite_micro::testing::CreateTensor(output_data, output_dims);
   const int expected_output[] = {};
   const int expected_output_len = 0;
   int expected_dims[] = {};
   const int expected_dims_len = 0;
-  tflite::testing::TestReshapeWithoutShape(
+  tflite_micro::testing::TestReshapeWithoutShape(
       &input_tensor, &output_tensor, expected_output, expected_output_len,
       expected_dims, expected_dims_len, true);
 }
@@ -247,19 +247,19 @@ TF_LITE_MICRO_TEST(ReshapeWithRegularShapesShouldSucceed) {
   const int16_t golden_output_int16[] = {1, 2, 3, 4, 5, 6, 7, 8};
   const int golden_dims_len = 3;
   int golden_dims[] = {2, 2, 2};
-  tflite::testing::TestReshape(input_dims, input_float, shape_dims, shape_int32,
+  tflite_micro::testing::TestReshape(input_dims, input_float, shape_dims, shape_int32,
                                output_dims, output_data_float,
                                golden_output_float, golden_output_len,
                                golden_dims, golden_dims_len, false);
-  tflite::testing::TestReshapeQuantized(
+  tflite_micro::testing::TestReshapeQuantized(
       input_dims, input_int8, shape_dims, shape_int32, output_dims,
       output_data_int8, golden_output_int8, golden_output_len, golden_dims,
       golden_dims_len, false);
-  tflite::testing::TestReshapeQuantized(
+  tflite_micro::testing::TestReshapeQuantized(
       input_dims, input_uint8, shape_dims, shape_int32, output_dims,
       output_data_uint8, golden_output_uint8, golden_output_len, golden_dims,
       golden_dims_len, false);
-  tflite::testing::TestReshapeQuantized(
+  tflite_micro::testing::TestReshapeQuantized(
       input_dims, input_int16, shape_dims, shape_int32, output_dims,
       output_data_int16, golden_output_int16, golden_output_len, golden_dims,
       golden_dims_len, false);
@@ -286,19 +286,19 @@ TF_LITE_MICRO_TEST(ReshapeWithStretchDimensionShouldSucceed) {
   const int16_t golden_output_int16[] = {1, 2, 3, 4, 5, 6, 7, 8};
   const int golden_dims_len = 3;
   int golden_dims[] = {2, 1, 4};
-  tflite::testing::TestReshape(input_dims, input_float, shape_dims, shape_int32,
+  tflite_micro::testing::TestReshape(input_dims, input_float, shape_dims, shape_int32,
                                output_dims, output_data_float,
                                golden_output_float, golden_output_len,
                                golden_dims, golden_dims_len, false);
-  tflite::testing::TestReshapeQuantized(
+  tflite_micro::testing::TestReshapeQuantized(
       input_dims, input_int8, shape_dims, shape_int32, output_dims,
       output_data_int8, golden_output_int8, golden_output_len, golden_dims,
       golden_dims_len, false);
-  tflite::testing::TestReshapeQuantized(
+  tflite_micro::testing::TestReshapeQuantized(
       input_dims, input_uint8, shape_dims, shape_int32, output_dims,
       output_data_uint8, golden_output_uint8, golden_output_len, golden_dims,
       golden_dims_len, false);
-  tflite::testing::TestReshapeQuantized(
+  tflite_micro::testing::TestReshapeQuantized(
       input_dims, input_int16, shape_dims, shape_int32, output_dims,
       output_data_int16, golden_output_int16, golden_output_len, golden_dims,
       golden_dims_len, false);
@@ -322,15 +322,15 @@ TF_LITE_MICRO_TEST(ReshapeWithScalarOutputShouldSucceed) {
   const uint8_t golden_output_uint8[] = {3};
   const int golden_dims_len = 0;
   int golden_dims[] = {};
-  tflite::testing::TestReshape(input_dims, input_float, shape_dims, shape_int32,
+  tflite_micro::testing::TestReshape(input_dims, input_float, shape_dims, shape_int32,
                                output_dims, output_data_float,
                                golden_output_float, golden_output_len,
                                golden_dims, golden_dims_len, false);
-  tflite::testing::TestReshapeQuantized(
+  tflite_micro::testing::TestReshapeQuantized(
       input_dims, input_int8, shape_dims, shape_int32, output_dims,
       output_data_int8, golden_output_int8, golden_output_len, golden_dims,
       golden_dims_len, false);
-  tflite::testing::TestReshapeQuantized(
+  tflite_micro::testing::TestReshapeQuantized(
       input_dims, input_uint8, shape_dims, shape_int32, output_dims,
       output_data_uint8, golden_output_uint8, golden_output_len, golden_dims,
       golden_dims_len, false);
@@ -339,8 +339,8 @@ TF_LITE_MICRO_TEST(ReshapeWithScalarOutputShouldSucceed) {
 // Some old models specify '[0]' as the new shape, indicating that both input
 // and output are scalars.
 TF_LITE_MICRO_TEST(ReshapeWithLegacyScalarOutputShouldSucceed) {
-  using tflite::testing::CreateTensor;
-  using tflite::testing::IntArrayFromInts;
+  using tflite_micro::testing::CreateTensor;
+  using tflite_micro::testing::IntArrayFromInts;
 
   int input_dims_data[] = {1, 1};
   TfLiteIntArray* input_dims = IntArrayFromInts(input_dims_data);
@@ -356,18 +356,18 @@ TF_LITE_MICRO_TEST(ReshapeWithLegacyScalarOutputShouldSucceed) {
   TfLiteIntArray* shape_dims = IntArrayFromInts(shape_dims_data);
 
   const int32_t shape_data[] = {0};
-  auto shape_tensor = tflite::testing::CreateTensor(shape_data, shape_dims);
+  auto shape_tensor = tflite_micro::testing::CreateTensor(shape_data, shape_dims);
   const float expected_output_with_shape[] = {};
   const int expected_output_with_shape_len = 0;
   const float expected_output_no_shape[] = {3};
   const int expected_output_no_shape_len = 1;
   int expected_dims[] = {};
   const int expected_dims_len = 0;
-  tflite::testing::TestReshapeWithShape<float>(
+  tflite_micro::testing::TestReshapeWithShape<float>(
       &input_tensor, &shape_tensor, &output_tensor, expected_output_with_shape,
       expected_output_with_shape_len, expected_dims, expected_dims_len, true);
 
-  tflite::testing::TestReshapeWithoutShape<float>(
+  tflite_micro::testing::TestReshapeWithoutShape<float>(
       &input_tensor, &output_tensor, expected_output_no_shape,
       expected_output_no_shape_len, expected_dims, expected_dims_len, false);
 }
diff --git a/tensorflow/lite/micro/kernels/resize_bilinear.cc b/tensorflow/lite/micro/kernels/resize_bilinear.cc
index e701e03b..2344daeb 100644
--- a/tensorflow/lite/micro/kernels/resize_bilinear.cc
+++ b/tensorflow/lite/micro/kernels/resize_bilinear.cc
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_log.h"
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 constexpr int kInputTensor = 0;
@@ -71,34 +71,34 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
       reinterpret_cast<TfLiteResizeBilinearParams*>(node->builtin_data);
 
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   const TfLiteEvalTensor* size =
-      tflite::micro::GetEvalInput(context, node, kSizeTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kSizeTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
   if (output->type == kTfLiteFloat32) {
-    tflite::ResizeBilinearParams op_params;
+    tflite_micro::ResizeBilinearParams op_params;
     op_params.align_corners = params->align_corners;
     op_params.half_pixel_centers = params->half_pixel_centers;
     reference_ops::ResizeBilinear(op_params,
-                                  tflite::micro::GetTensorShape(input),
-                                  tflite::micro::GetTensorData<float>(input),
-                                  tflite::micro::GetTensorShape(size),
-                                  tflite::micro::GetTensorData<int32_t>(size),
-                                  tflite::micro::GetTensorShape(output),
-                                  tflite::micro::GetTensorData<float>(output));
+                                  tflite_micro::micro::GetTensorShape(input),
+                                  tflite_micro::micro::GetTensorData<float>(input),
+                                  tflite_micro::micro::GetTensorShape(size),
+                                  tflite_micro::micro::GetTensorData<int32_t>(size),
+                                  tflite_micro::micro::GetTensorShape(output),
+                                  tflite_micro::micro::GetTensorData<float>(output));
   } else if (output->type == kTfLiteInt8) {
-    tflite::ResizeBilinearParams op_params;
+    tflite_micro::ResizeBilinearParams op_params;
     op_params.align_corners = params->align_corners;
     op_params.half_pixel_centers = params->half_pixel_centers;
     reference_ops::ResizeBilinearInteger(
-        op_params, tflite::micro::GetTensorShape(input),
-        tflite::micro::GetTensorData<int8_t>(input),
-        tflite::micro::GetTensorShape(size),
-        tflite::micro::GetTensorData<int32_t>(size),
-        tflite::micro::GetTensorShape(output),
-        tflite::micro::GetTensorData<int8_t>(output));
+        op_params, tflite_micro::micro::GetTensorShape(input),
+        tflite_micro::micro::GetTensorData<int8_t>(input),
+        tflite_micro::micro::GetTensorShape(size),
+        tflite_micro::micro::GetTensorData<int32_t>(size),
+        tflite_micro::micro::GetTensorShape(output),
+        tflite_micro::micro::GetTensorData<int8_t>(output));
   } else {
     MicroPrintf("Output type is %d, requires float or int8.", output->type);
     return kTfLiteError;
@@ -110,7 +110,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_RESIZE_BILINEAR() {
-  return tflite::micro::RegisterOp(nullptr, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/resize_bilinear_test.cc b/tensorflow/lite/micro/kernels/resize_bilinear_test.cc
index b52cebeb..5c3dddef 100644
--- a/tensorflow/lite/micro/kernels/resize_bilinear_test.cc
+++ b/tensorflow/lite/micro/kernels/resize_bilinear_test.cc
@@ -18,7 +18,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -89,7 +89,7 @@ void TestResizeBilinear(int* input_dims_data, const T* input_data,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -106,7 +106,7 @@ TF_LITE_MICRO_TEST(HorizontalResize) {
       false  /*half pixel centers*/
   };
 
-  tflite::testing::TestResizeBilinear(input_dims, input_data,
+  tflite_micro::testing::TestResizeBilinear(input_dims, input_data,
                                       expected_size_data, expected_output_data,
                                       output_dims, output_data, &params);
 }
@@ -124,7 +124,7 @@ TF_LITE_MICRO_TEST(HorizontalResizeInt8) {
       false  /*half pixel centers*/
   };
 
-  tflite::testing::TestResizeBilinear<int8_t>(
+  tflite_micro::testing::TestResizeBilinear<int8_t>(
       input_dims, input_data, expected_size_data, expected_output_data,
       output_dims, output_data, &params);
 }
@@ -142,7 +142,7 @@ TF_LITE_MICRO_TEST(VerticalResize) {
       false  /*half pixel centers*/
   };
 
-  tflite::testing::TestResizeBilinear(input_dims, input_data,
+  tflite_micro::testing::TestResizeBilinear(input_dims, input_data,
                                       expected_size_data, expected_output_data,
                                       output_dims, output_data, &params);
 }
@@ -160,7 +160,7 @@ TF_LITE_MICRO_TEST(VerticalResizeInt8) {
       false  /*half pixel centers*/
   };
 
-  tflite::testing::TestResizeBilinear<int8_t>(
+  tflite_micro::testing::TestResizeBilinear<int8_t>(
       input_dims, input_data, expected_size_data, expected_output_data,
       output_dims, output_data, &params);
 }
@@ -186,7 +186,7 @@ TF_LITE_MICRO_TEST(TwoDimensionalResize) {
       false  /*half pixel centers*/
   };
 
-  tflite::testing::TestResizeBilinear(input_dims, input_data,
+  tflite_micro::testing::TestResizeBilinear(input_dims, input_data,
                                       expected_size_data, expected_output_data,
                                       output_dims, output_data, &params);
 }
@@ -211,7 +211,7 @@ TF_LITE_MICRO_TEST(TwoDimensionalResizeInt8) {
       false  /*half pixel centers*/
   };
 
-  tflite::testing::TestResizeBilinear<int8_t>(
+  tflite_micro::testing::TestResizeBilinear<int8_t>(
       input_dims, input_data, expected_size_data, expected_output_data,
       output_dims, output_data, &params);
 }
@@ -241,7 +241,7 @@ TF_LITE_MICRO_TEST(TwoDimensionalResizeWithTwoBatches) {
       false  /*half pixel centers*/
   };
 
-  tflite::testing::TestResizeBilinear(input_dims, input_data,
+  tflite_micro::testing::TestResizeBilinear(input_dims, input_data,
                                       expected_size_data, expected_output_data,
                                       output_dims, output_data, &params);
 }
@@ -271,7 +271,7 @@ TF_LITE_MICRO_TEST(TwoDimensionalResizeWithTwoBatchesInt8) {
       false  /*half pixel centers*/
   };
 
-  tflite::testing::TestResizeBilinear<int8_t>(
+  tflite_micro::testing::TestResizeBilinear<int8_t>(
       input_dims, input_data, expected_size_data, expected_output_data,
       output_dims, output_data, &params, /*tolerance=*/1);
 }
@@ -296,7 +296,7 @@ TF_LITE_MICRO_TEST(ThreeDimensionalResize) {
       false  /*half pixel centers*/
   };
 
-  tflite::testing::TestResizeBilinear(input_dims, input_data,
+  tflite_micro::testing::TestResizeBilinear(input_dims, input_data,
                                       expected_size_data, expected_output_data,
                                       output_dims, output_data, &params);
 }
@@ -321,7 +321,7 @@ TF_LITE_MICRO_TEST(ThreeDimensionalResizeInt8) {
       false  /*half pixel centers*/
   };
 
-  tflite::testing::TestResizeBilinear<int8_t>(
+  tflite_micro::testing::TestResizeBilinear<int8_t>(
       input_dims, input_data, expected_size_data, expected_output_data,
       output_dims, output_data, &params, /*tolerance=*/1);
 }
diff --git a/tensorflow/lite/micro/kernels/resize_nearest_neighbor.cc b/tensorflow/lite/micro/kernels/resize_nearest_neighbor.cc
index 46b6ea16..580ef1d1 100644
--- a/tensorflow/lite/micro/kernels/resize_nearest_neighbor.cc
+++ b/tensorflow/lite/micro/kernels/resize_nearest_neighbor.cc
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -70,40 +70,40 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
       reinterpret_cast<TfLiteResizeNearestNeighborParams*>(node->builtin_data);
 
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   const TfLiteEvalTensor* size =
-      tflite::micro::GetEvalInput(context, node, kSizeTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kSizeTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
-  tflite::ResizeNearestNeighborParams op_params;
+  tflite_micro::ResizeNearestNeighborParams op_params;
   op_params.align_corners = params->align_corners;
   op_params.half_pixel_centers = false;
 
   if (output->type == kTfLiteFloat32) {
     reference_ops::ResizeNearestNeighbor(
-        op_params, tflite::micro::GetTensorShape(input),
-        tflite::micro::GetTensorData<int32_t>(input),
-        tflite::micro::GetTensorShape(size),
-        tflite::micro::GetTensorData<int32_t>(size),
-        tflite::micro::GetTensorShape(output),
-        tflite::micro::GetTensorData<int32_t>(output));
+        op_params, tflite_micro::micro::GetTensorShape(input),
+        tflite_micro::micro::GetTensorData<int32_t>(input),
+        tflite_micro::micro::GetTensorShape(size),
+        tflite_micro::micro::GetTensorData<int32_t>(size),
+        tflite_micro::micro::GetTensorShape(output),
+        tflite_micro::micro::GetTensorData<int32_t>(output));
   } else if (output->type == kTfLiteInt8) {
     reference_ops::ResizeNearestNeighbor(
-        op_params, tflite::micro::GetTensorShape(input),
-        tflite::micro::GetTensorData<int8_t>(input),
-        tflite::micro::GetTensorShape(size),
-        tflite::micro::GetTensorData<int32_t>(size),
-        tflite::micro::GetTensorShape(output),
-        tflite::micro::GetTensorData<int8_t>(output));
+        op_params, tflite_micro::micro::GetTensorShape(input),
+        tflite_micro::micro::GetTensorData<int8_t>(input),
+        tflite_micro::micro::GetTensorShape(size),
+        tflite_micro::micro::GetTensorData<int32_t>(size),
+        tflite_micro::micro::GetTensorShape(output),
+        tflite_micro::micro::GetTensorData<int8_t>(output));
   } else if (output->type == kTfLiteInt16) {
     reference_ops::ResizeNearestNeighbor(
-        op_params, tflite::micro::GetTensorShape(input),
-        tflite::micro::GetTensorData<int16_t>(input),
-        tflite::micro::GetTensorShape(size),
-        tflite::micro::GetTensorData<int32_t>(size),
-        tflite::micro::GetTensorShape(output),
-        tflite::micro::GetTensorData<int16_t>(output));
+        op_params, tflite_micro::micro::GetTensorShape(input),
+        tflite_micro::micro::GetTensorData<int16_t>(input),
+        tflite_micro::micro::GetTensorShape(size),
+        tflite_micro::micro::GetTensorData<int32_t>(size),
+        tflite_micro::micro::GetTensorShape(output),
+        tflite_micro::micro::GetTensorData<int16_t>(output));
   } else {
     MicroPrintf("Output tensor type %s (%d) not supported.",
                 TfLiteTypeGetName(output->type), output->type);
@@ -117,7 +117,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_RESIZE_NEAREST_NEIGHBOR() {
-  return tflite::micro::RegisterOp(nullptr, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/resize_nearest_neighbor_test.cc b/tensorflow/lite/micro/kernels/resize_nearest_neighbor_test.cc
index 3e06da8f..64c78771 100644
--- a/tensorflow/lite/micro/kernels/resize_nearest_neighbor_test.cc
+++ b/tensorflow/lite/micro/kernels/resize_nearest_neighbor_test.cc
@@ -18,7 +18,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -71,7 +71,7 @@ void TestResizeNearestNeighbor(int* input_dims_data, const T* input_data,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -83,7 +83,7 @@ TF_LITE_MICRO_TEST(HorizontalResize) {
   int output_dims[] = {4, 1, 1, 3, 1};
   float output_data[3];
 
-  tflite::testing::TestResizeNearestNeighbor<float>(
+  tflite_micro::testing::TestResizeNearestNeighbor<float>(
       input_dims, input_data, expected_size_data, expected_output_data,
       output_dims, output_data);
 }
@@ -96,7 +96,7 @@ TF_LITE_MICRO_TEST(HorizontalResizeInt8) {
   int output_dims[] = {4, 1, 1, 3, 1};
   int8_t output_data[3];
 
-  tflite::testing::TestResizeNearestNeighbor<int8_t>(
+  tflite_micro::testing::TestResizeNearestNeighbor<int8_t>(
       input_dims, input_data, expected_size_data, expected_output_data,
       output_dims, output_data);
 }
@@ -109,7 +109,7 @@ TF_LITE_MICRO_TEST(HorizontalResizeInt16) {
   int output_dims[] = {4, 1, 1, 3, 1};
   int16_t output_data[3];
 
-  tflite::testing::TestResizeNearestNeighbor<int16_t>(
+  tflite_micro::testing::TestResizeNearestNeighbor<int16_t>(
       input_dims, input_data, expected_size_data, expected_output_data,
       output_dims, output_data);
 }
@@ -122,7 +122,7 @@ TF_LITE_MICRO_TEST(VerticalResize) {
   int output_dims[] = {4, 1, 3, 1, 1};
   float output_data[3];
 
-  tflite::testing::TestResizeNearestNeighbor<float>(
+  tflite_micro::testing::TestResizeNearestNeighbor<float>(
       input_dims, input_data, expected_size_data, expected_output_data,
       output_dims, output_data);
 }
@@ -135,7 +135,7 @@ TF_LITE_MICRO_TEST(VerticalResizeInt8) {
   int output_dims[] = {4, 1, 3, 1, 1};
   int8_t output_data[3];
 
-  tflite::testing::TestResizeNearestNeighbor<int8_t>(
+  tflite_micro::testing::TestResizeNearestNeighbor<int8_t>(
       input_dims, input_data, expected_size_data, expected_output_data,
       output_dims, output_data);
 }
@@ -148,7 +148,7 @@ TF_LITE_MICRO_TEST(VerticalResizeInt16) {
   int output_dims[] = {4, 1, 3, 1, 1};
   int16_t output_data[3];
 
-  tflite::testing::TestResizeNearestNeighbor<int16_t>(
+  tflite_micro::testing::TestResizeNearestNeighbor<int16_t>(
       input_dims, input_data, expected_size_data, expected_output_data,
       output_dims, output_data);
 }
@@ -169,7 +169,7 @@ TF_LITE_MICRO_TEST(TwoDimensionalResize) {
   int output_dims[] = {4, 1, 3, 3, 1};
   float output_data[9];
 
-  tflite::testing::TestResizeNearestNeighbor<float>(
+  tflite_micro::testing::TestResizeNearestNeighbor<float>(
       input_dims, input_data, expected_size_data, expected_output_data,
       output_dims, output_data);
 }
@@ -189,7 +189,7 @@ TF_LITE_MICRO_TEST(TwoDimensionalResizeInt8) {
   int output_dims[] = {4, 1, 3, 3, 1};
   int8_t output_data[9];
 
-  tflite::testing::TestResizeNearestNeighbor<int8_t>(
+  tflite_micro::testing::TestResizeNearestNeighbor<int8_t>(
       input_dims, input_data, expected_size_data, expected_output_data,
       output_dims, output_data);
 }
@@ -209,7 +209,7 @@ TF_LITE_MICRO_TEST(TwoDimensionalResizeInt16) {
   int output_dims[] = {4, 1, 3, 3, 1};
   int16_t output_data[9];
 
-  tflite::testing::TestResizeNearestNeighbor<int16_t>(
+  tflite_micro::testing::TestResizeNearestNeighbor<int16_t>(
       input_dims, input_data, expected_size_data, expected_output_data,
       output_dims, output_data);
 }
@@ -234,7 +234,7 @@ TF_LITE_MICRO_TEST(TwoDimensionalResizeWithTwoBatches) {
   int output_dims[] = {4, 2, 3, 3, 1};
   float output_data[18];
 
-  tflite::testing::TestResizeNearestNeighbor<float>(
+  tflite_micro::testing::TestResizeNearestNeighbor<float>(
       input_dims, input_data, expected_size_data, expected_output_data,
       output_dims, output_data);
 }
@@ -259,7 +259,7 @@ TF_LITE_MICRO_TEST(TwoDimensionalResizeWithTwoBatchesInt8) {
   int output_dims[] = {4, 2, 3, 3, 1};
   int8_t output_data[18];
 
-  tflite::testing::TestResizeNearestNeighbor<int8_t>(
+  tflite_micro::testing::TestResizeNearestNeighbor<int8_t>(
       input_dims, input_data, expected_size_data, expected_output_data,
       output_dims, output_data);
 }
@@ -284,7 +284,7 @@ TF_LITE_MICRO_TEST(TwoDimensionalResizeWithTwoBatchesInt16) {
   int output_dims[] = {4, 2, 3, 3, 1};
   int16_t output_data[18];
 
-  tflite::testing::TestResizeNearestNeighbor<int16_t>(
+  tflite_micro::testing::TestResizeNearestNeighbor<int16_t>(
       input_dims, input_data, expected_size_data, expected_output_data,
       output_dims, output_data);
 }
@@ -304,7 +304,7 @@ TF_LITE_MICRO_TEST(ThreeDimensionalResize) {
   int output_dims[] = {4, 1, 3, 3, 2};
   float output_data[18];
 
-  tflite::testing::TestResizeNearestNeighbor<float>(
+  tflite_micro::testing::TestResizeNearestNeighbor<float>(
       input_dims, input_data, expected_size_data, expected_output_data,
       output_dims, output_data);
 }
@@ -324,7 +324,7 @@ TF_LITE_MICRO_TEST(ThreeDimensionalResizeInt8) {
   int output_dims[] = {4, 1, 3, 3, 2};
   int8_t output_data[18];
 
-  tflite::testing::TestResizeNearestNeighbor<int8_t>(
+  tflite_micro::testing::TestResizeNearestNeighbor<int8_t>(
       input_dims, input_data, expected_size_data, expected_output_data,
       output_dims, output_data);
 }
@@ -344,7 +344,7 @@ TF_LITE_MICRO_TEST(ThreeDimensionalResizeInt16) {
   int output_dims[] = {4, 1, 3, 3, 2};
   int16_t output_data[18];
 
-  tflite::testing::TestResizeNearestNeighbor<int16_t>(
+  tflite_micro::testing::TestResizeNearestNeighbor<int16_t>(
       input_dims, input_data, expected_size_data, expected_output_data,
       output_dims, output_data);
 }
diff --git a/tensorflow/lite/micro/kernels/round.cc b/tensorflow/lite/micro/kernels/round.cc
index a42349e2..91968ff2 100644
--- a/tensorflow/lite/micro/kernels/round.cc
+++ b/tensorflow/lite/micro/kernels/round.cc
@@ -20,7 +20,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 constexpr int kInputTensor = 0;
@@ -52,21 +52,21 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
-  reference_ops::Round(tflite::micro::GetTensorShape(input),
-                       tflite::micro::GetTensorData<float>(input),
-                       tflite::micro::GetTensorShape(output),
-                       tflite::micro::GetTensorData<float>(output));
+  reference_ops::Round(tflite_micro::micro::GetTensorShape(input),
+                       tflite_micro::micro::GetTensorData<float>(input),
+                       tflite_micro::micro::GetTensorShape(output),
+                       tflite_micro::micro::GetTensorData<float>(output));
 
   return kTfLiteOk;
 }
 }  // namespace
 
 TFLMRegistration Register_ROUND() {
-  return tflite::micro::RegisterOp(nullptr, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/round_test.cc b/tensorflow/lite/micro/kernels/round_test.cc
index fa09a9fc..f109a93c 100644
--- a/tensorflow/lite/micro/kernels/round_test.cc
+++ b/tensorflow/lite/micro/kernels/round_test.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -41,7 +41,7 @@ void TestRound(int* input_dims_data, const float* input_data,
   int outputs_array_data[] = {1, 1};
   TfLiteIntArray* outputs_array = IntArrayFromInts(outputs_array_data);
 
-  const TFLMRegistration registration = tflite::Register_ROUND();
+  const TFLMRegistration registration = tflite_micro::Register_ROUND();
   micro::KernelRunner runner(registration, tensors, tensors_size, inputs_array,
                              outputs_array, nullptr);
 
@@ -55,7 +55,7 @@ void TestRound(int* input_dims_data, const float* input_data,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -64,7 +64,7 @@ TF_LITE_MICRO_TEST(SingleDim) {
   const float input_data[] = {8.5, 0.0, 3.5, 4.2, -3.5, -4.5};
   const float golden[] = {8, 0, 4, 4, -4, -4};
   float output_data[6];
-  tflite::testing::TestRound(input_dims, input_data, golden, output_data);
+  tflite_micro::testing::TestRound(input_dims, input_data, golden, output_data);
 }
 
 TF_LITE_MICRO_TEST(MultiDims) {
@@ -73,7 +73,7 @@ TF_LITE_MICRO_TEST(MultiDims) {
                               -8.0001, -0.9999, -9.9999, -0.5,   -2.5, 1.5};
   const float golden[] = {0, 8, 1, 10, 0, 0, -8, -1, -10, -0, -2, 2};
   float output_data[12];
-  tflite::testing::TestRound(input_dims, input_data, golden, output_data);
+  tflite_micro::testing::TestRound(input_dims, input_data, golden, output_data);
 }
 
 TF_LITE_MICRO_TESTS_END
diff --git a/tensorflow/lite/micro/kernels/select.cc b/tensorflow/lite/micro/kernels/select.cc
index 90e64136..ef082cbc 100644
--- a/tensorflow/lite/micro/kernels/select.cc
+++ b/tensorflow/lite/micro/kernels/select.cc
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 constexpr int kInputTensorCondition = 0;
@@ -134,30 +134,30 @@ void CallSelect(const TfLiteEvalTensor* input_condition,
     select_func = reference_ops::Select<bool, T>;
   }
 
-  select_func(tflite::micro::GetTensorShape(input_condition),
-              tflite::micro::GetTensorData<bool>(input_condition),
-              tflite::micro::GetTensorShape(input_x),
-              tflite::micro::GetTensorData<T>(input_x),
-              tflite::micro::GetTensorShape(input_y),
-              tflite::micro::GetTensorData<T>(input_y),
-              tflite::micro::GetTensorShape(output),
-              tflite::micro::GetTensorData<T>(output));
+  select_func(tflite_micro::micro::GetTensorShape(input_condition),
+              tflite_micro::micro::GetTensorData<bool>(input_condition),
+              tflite_micro::micro::GetTensorShape(input_x),
+              tflite_micro::micro::GetTensorData<T>(input_x),
+              tflite_micro::micro::GetTensorShape(input_y),
+              tflite_micro::micro::GetTensorData<T>(input_y),
+              tflite_micro::micro::GetTensorShape(output),
+              tflite_micro::micro::GetTensorData<T>(output));
 }
 
 TfLiteStatus SelectEval(TfLiteContext* context, TfLiteNode* node) {
   OpData* data = static_cast<OpData*>(node->user_data);
 
   const TfLiteEvalTensor* input_condition =
-      tflite::micro::GetEvalInput(context, node, kInputTensorCondition);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensorCondition);
 
   const TfLiteEvalTensor* input_x =
-      tflite::micro::GetEvalInput(context, node, kInputTensorX);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensorX);
 
   const TfLiteEvalTensor* input_y =
-      tflite::micro::GetEvalInput(context, node, kInputTensorY);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensorY);
 
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
   switch (input_x->type) {
     case kTfLiteFloat32:
@@ -189,8 +189,8 @@ TfLiteStatus SelectEval(TfLiteContext* context, TfLiteNode* node) {
 // 1. Either the same shape (in which case the select is elementwise), or
 // 2. Broadcastable shapes between 'condition', 'x' and 'y'.
 TFLMRegistration Register_SELECT_V2() {
-  return tflite::micro::RegisterOp(tflite::SelectInit, tflite::SelectPrepare,
-                                   tflite::SelectEval);
+  return tflite_micro::micro::RegisterOp(tflite_micro::SelectInit, tflite_micro::SelectPrepare,
+                                   tflite_micro::SelectEval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/select_test.cc b/tensorflow/lite/micro/kernels/select_test.cc
index 1f92660d..51b12526 100644
--- a/tensorflow/lite/micro/kernels/select_test.cc
+++ b/tensorflow/lite/micro/kernels/select_test.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 
 typedef struct {
@@ -50,7 +50,7 @@ void TestSelect(int* input1_dims_data, const bool* input1_data,
   TfLiteIntArray* outputs_array = IntArrayFromInts(outputs_array_data);
 
   TfLiteSelectParams builtin_data;
-  const TFLMRegistration registration = tflite::Register_SELECT_V2();
+  const TFLMRegistration registration = tflite_micro::Register_SELECT_V2();
   micro::KernelRunner runner(registration, tensors, tensors_size, inputs_array,
                              outputs_array,
                              reinterpret_cast<void*>(&builtin_data));
@@ -78,7 +78,7 @@ void ExpectNear(int* dims, const T* expected_data, const T* output_data) {
 }
 
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -91,10 +91,10 @@ TF_LITE_MICRO_TEST(SelectFloat) {
   const float expected_output[] = {0.1f, 0.6f, 0.3, 0.8f};
 
   float output_data[4];
-  tflite::testing::TestSelect(inout_shape, input1_data, inout_shape,
+  tflite_micro::testing::TestSelect(inout_shape, input1_data, inout_shape,
                               input2_data, inout_shape, input3_data,
                               inout_shape, output_data);
-  tflite::testing::ExpectNear(inout_shape, expected_output, output_data);
+  tflite_micro::testing::ExpectNear(inout_shape, expected_output, output_data);
 }
 
 TF_LITE_MICRO_TEST(SelectInt8) {
@@ -106,10 +106,10 @@ TF_LITE_MICRO_TEST(SelectInt8) {
   const int8_t expected_output[] = {5, -2, 7, -8};
 
   int8_t output_data[4];
-  tflite::testing::TestSelect(inout_shape, input1_data, inout_shape,
+  tflite_micro::testing::TestSelect(inout_shape, input1_data, inout_shape,
                               input2_data, inout_shape, input3_data,
                               inout_shape, output_data);
-  tflite::testing::ExpectEqual(inout_shape, expected_output, output_data);
+  tflite_micro::testing::ExpectEqual(inout_shape, expected_output, output_data);
 }
 
 TF_LITE_MICRO_TEST(SelectInt16) {
@@ -121,10 +121,10 @@ TF_LITE_MICRO_TEST(SelectInt16) {
   const int16_t expected_output[] = {5, 2, 7, 8};
 
   int16_t output_data[4];
-  tflite::testing::TestSelect(inout_shape, input1_data, inout_shape,
+  tflite_micro::testing::TestSelect(inout_shape, input1_data, inout_shape,
                               input2_data, inout_shape, input3_data,
                               inout_shape, output_data);
-  tflite::testing::ExpectEqual(inout_shape, expected_output, output_data);
+  tflite_micro::testing::ExpectEqual(inout_shape, expected_output, output_data);
 }
 
 TF_LITE_MICRO_TEST(BroadcastSelectInt16OneDimensionConditionWithSingleValue) {
@@ -138,10 +138,10 @@ TF_LITE_MICRO_TEST(BroadcastSelectInt16OneDimensionConditionWithSingleValue) {
   const int16_t expected_output[] = {9, 10, 11, 12, 9, 10, 11, 12};
 
   int16_t output_data[8];
-  tflite::testing::TestSelect(input1_shape, input1_data, input2_shape,
+  tflite_micro::testing::TestSelect(input1_shape, input1_data, input2_shape,
                               input2_data, input3_shape, input3_data,
                               input2_shape, output_data);
-  tflite::testing::ExpectEqual(input2_shape, expected_output, output_data);
+  tflite_micro::testing::ExpectEqual(input2_shape, expected_output, output_data);
 }
 
 TF_LITE_MICRO_TEST(BroadcastSelectInt16LesserThan4D) {
@@ -154,10 +154,10 @@ TF_LITE_MICRO_TEST(BroadcastSelectInt16LesserThan4D) {
   const int16_t expected_output[] = {5, 2, 7, 4};
 
   int16_t output_data[4];
-  tflite::testing::TestSelect(input1_shape, input1_data, inout_shape,
+  tflite_micro::testing::TestSelect(input1_shape, input1_data, inout_shape,
                               input2_data, inout_shape, input3_data,
                               inout_shape, output_data);
-  tflite::testing::ExpectEqual(inout_shape, expected_output, output_data);
+  tflite_micro::testing::ExpectEqual(inout_shape, expected_output, output_data);
 }
 
 TF_LITE_MICRO_TEST(BroadcastSelectInt16OnFalseValue) {
@@ -170,10 +170,10 @@ TF_LITE_MICRO_TEST(BroadcastSelectInt16OnFalseValue) {
   const int16_t expected_output[] = {5, 6, 7, 8};
 
   int16_t output_data[4];
-  tflite::testing::TestSelect(input1_shape, input1_data, inout_shape,
+  tflite_micro::testing::TestSelect(input1_shape, input1_data, inout_shape,
                               input2_data, inout_shape, input3_data,
                               inout_shape, output_data);
-  tflite::testing::ExpectEqual(inout_shape, expected_output, output_data);
+  tflite_micro::testing::ExpectEqual(inout_shape, expected_output, output_data);
 }
 
 TF_LITE_MICRO_TEST(BroadcastSelectInt16) {
@@ -186,10 +186,10 @@ TF_LITE_MICRO_TEST(BroadcastSelectInt16) {
   const int16_t expected_output[] = {5, 2, 7, 4};
 
   int16_t output_data[4];
-  tflite::testing::TestSelect(input1_shape, input1_data, inout_shape,
+  tflite_micro::testing::TestSelect(input1_shape, input1_data, inout_shape,
                               input2_data, inout_shape, input3_data,
                               inout_shape, output_data);
-  tflite::testing::ExpectEqual(inout_shape, expected_output, output_data);
+  tflite_micro::testing::ExpectEqual(inout_shape, expected_output, output_data);
 }
 
 TF_LITE_MICRO_TEST(BroadcastSelectInt16OneDimensionConditionWithTwoValues) {
@@ -203,10 +203,10 @@ TF_LITE_MICRO_TEST(BroadcastSelectInt16OneDimensionConditionWithTwoValues) {
   const int16_t expected_output[] = {5, 1, 6, 2, 7, 3, 8, 4};
 
   int16_t output_data[8];
-  tflite::testing::TestSelect(input1_shape, input1_data, input_shape,
+  tflite_micro::testing::TestSelect(input1_shape, input1_data, input_shape,
                               input2_data, input_shape, input3_data,
                               output_shape, output_data);
-  tflite::testing::ExpectEqual(output_shape, expected_output, output_data);
+  tflite_micro::testing::ExpectEqual(output_shape, expected_output, output_data);
 }
 
 TF_LITE_MICRO_TEST(MixedFlatSizeOneInputsWithScalarInputConditionTensor) {
@@ -220,10 +220,10 @@ TF_LITE_MICRO_TEST(MixedFlatSizeOneInputsWithScalarInputConditionTensor) {
   const int16_t expected_output[] = {5};
 
   int16_t output_data[std::extent<decltype(expected_output)>::value];
-  tflite::testing::TestSelect(input1_shape, input1_data, input_shape,
+  tflite_micro::testing::TestSelect(input1_shape, input1_data, input_shape,
                               input2_data, input_shape, input3_data,
                               output_shape, output_data);
-  tflite::testing::ExpectEqual(output_shape, expected_output, output_data);
+  tflite_micro::testing::ExpectEqual(output_shape, expected_output, output_data);
 }
 
 TF_LITE_MICRO_TEST(MixedFlatSizeOneInputsWithScalarInputXTensor) {
@@ -237,10 +237,10 @@ TF_LITE_MICRO_TEST(MixedFlatSizeOneInputsWithScalarInputXTensor) {
   const int16_t expected_output[] = {1};
 
   int16_t output_data[std::extent<decltype(expected_output)>::value];
-  tflite::testing::TestSelect(input_shape, input1_data, input2_shape,
+  tflite_micro::testing::TestSelect(input_shape, input1_data, input2_shape,
                               input2_data, input_shape, input3_data,
                               output_shape, output_data);
-  tflite::testing::ExpectEqual(output_shape, expected_output, output_data);
+  tflite_micro::testing::ExpectEqual(output_shape, expected_output, output_data);
 }
 
 TF_LITE_MICRO_TEST(MixedFlatSizeOneInputsWithScalarInputYTensor) {
@@ -254,10 +254,10 @@ TF_LITE_MICRO_TEST(MixedFlatSizeOneInputsWithScalarInputYTensor) {
   const int16_t expected_output[] = {5};
 
   int16_t output_data[std::extent<decltype(expected_output)>::value];
-  tflite::testing::TestSelect(input_shape, input1_data, input_shape,
+  tflite_micro::testing::TestSelect(input_shape, input1_data, input_shape,
                               input2_data, input3_shape, input3_data,
                               output_shape, output_data);
-  tflite::testing::ExpectEqual(output_shape, expected_output, output_data);
+  tflite_micro::testing::ExpectEqual(output_shape, expected_output, output_data);
 }
 
 TF_LITE_MICRO_TESTS_END
diff --git a/tensorflow/lite/micro/kernels/shape.cc b/tensorflow/lite/micro/kernels/shape.cc
index a39bfc0e..df3517a4 100644
--- a/tensorflow/lite/micro/kernels/shape.cc
+++ b/tensorflow/lite/micro/kernels/shape.cc
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_log.h"
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 constexpr int kInputTensor = 0;
@@ -44,15 +44,15 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
   if (output->type != kTfLiteInt32) {
     MicroPrintf("Output type %s (%d) not supported.",
                 TfLiteTypeGetName(output->type), output->type);
     return kTfLiteError;
   } else {
-    ExtractShape(input, tflite::micro::GetTensorData<int32_t>(output));
+    ExtractShape(input, tflite_micro::micro::GetTensorData<int32_t>(output));
   }
 
   return kTfLiteOk;
@@ -61,7 +61,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_SHAPE() {
-  return tflite::micro::RegisterOp(nullptr, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/shape_test.cc b/tensorflow/lite/micro/kernels/shape_test.cc
index 413b7264..b57175e3 100644
--- a/tensorflow/lite/micro/kernels/shape_test.cc
+++ b/tensorflow/lite/micro/kernels/shape_test.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -31,7 +31,7 @@ void ValidateShape(TfLiteTensor* tensors, const int tensor_count,
   int outputs_array_data[] = {1, 1};
   TfLiteIntArray* outputs_array = IntArrayFromInts(outputs_array_data);
 
-  const TFLMRegistration registration = tflite::Register_SHAPE();
+  const TFLMRegistration registration = tflite_micro::Register_SHAPE();
   micro::KernelRunner runner(registration, tensors, tensor_count, inputs_array,
                              outputs_array, nullptr);
 
@@ -64,7 +64,7 @@ void TestShape(int* input_dims_data, const float* input_data,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -75,7 +75,7 @@ TF_LITE_MICRO_TEST(TestShape0) {
   int32_t expected_output_data[] = {5};
   int32_t output_data[1];
 
-  tflite::testing::TestShape(input_shape, input_values, output_dims,
+  tflite_micro::testing::TestShape(input_shape, input_values, output_dims,
                              expected_output_data, output_data);
 }
 
@@ -86,7 +86,7 @@ TF_LITE_MICRO_TEST(TestShape1) {
   int32_t expected_output_data[] = {4, 3};
   int32_t output_data[2];
 
-  tflite::testing::TestShape(input_shape, input_values, output_dims,
+  tflite_micro::testing::TestShape(input_shape, input_values, output_dims,
                              expected_output_data, output_data);
 }
 
@@ -97,7 +97,7 @@ TF_LITE_MICRO_TEST(TestShape2) {
   int32_t expected_output_data[] = {12, 1};
   int32_t output_data[2];
 
-  tflite::testing::TestShape(input_shape, input_values, output_dims,
+  tflite_micro::testing::TestShape(input_shape, input_values, output_dims,
                              expected_output_data, output_data);
 }
 
@@ -108,7 +108,7 @@ TF_LITE_MICRO_TEST(TestShape3) {
   int32_t expected_output_data[] = {2, 6};
   int32_t output_data[2];
 
-  tflite::testing::TestShape(input_shape, input_values, output_dims,
+  tflite_micro::testing::TestShape(input_shape, input_values, output_dims,
                              expected_output_data, output_data);
 }
 
@@ -119,7 +119,7 @@ TF_LITE_MICRO_TEST(TestShape4) {
   int32_t expected_output_data[] = {2, 2, 3};
   int32_t output_data[3];
 
-  tflite::testing::TestShape(input_shape, input_values, output_dims,
+  tflite_micro::testing::TestShape(input_shape, input_values, output_dims,
                              expected_output_data, output_data);
 }
 
@@ -130,7 +130,7 @@ TF_LITE_MICRO_TEST(TestShape5) {
   int32_t expected_output_data[] = {1};
   int32_t output_data[1];
 
-  tflite::testing::TestShape(input_shape, input_values, output_dims,
+  tflite_micro::testing::TestShape(input_shape, input_values, output_dims,
                              expected_output_data, output_data);
 }
 
diff --git a/tensorflow/lite/micro/kernels/slice.cc b/tensorflow/lite/micro/kernels/slice.cc
index 973da182..89d5fc04 100644
--- a/tensorflow/lite/micro/kernels/slice.cc
+++ b/tensorflow/lite/micro/kernels/slice.cc
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -39,8 +39,8 @@ void GetBeginAndSizeVectors(int dimensions, const TfLiteEvalTensor* begin,
                             int32_t* sizes) {
   int offset = kMaxDim - dimensions;
   for (int idx = 0; idx < dimensions; ++idx) {
-    begins[offset + idx] = tflite::micro::GetTensorData<T>(begin)[idx];
-    sizes[offset + idx] = tflite::micro::GetTensorData<T>(size)[idx];
+    begins[offset + idx] = tflite_micro::micro::GetTensorData<T>(begin)[idx];
+    sizes[offset + idx] = tflite_micro::micro::GetTensorData<T>(size)[idx];
   }
 }
 
@@ -83,15 +83,15 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   const TfLiteEvalTensor* begin =
-      tflite::micro::GetEvalInput(context, node, kBeginTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kBeginTensor);
   const TfLiteEvalTensor* size =
-      tflite::micro::GetEvalInput(context, node, kSizeTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kSizeTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
-  tflite::SliceParams op_params;
+  tflite_micro::SliceParams op_params;
   op_params.begin_count = kMaxDim;
   op_params.size_count = kMaxDim;
   for (int i = 0; i < kMaxDim; ++i) {
@@ -114,38 +114,38 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   switch (input->type) {
     case kTfLiteFloat32:
       reference_ops::Slice<float>(op_params,
-                                  tflite::micro::GetTensorShape(input),
-                                  tflite::micro::GetTensorData<float>(input),
-                                  tflite::micro::GetTensorShape(output),
-                                  tflite::micro::GetTensorData<float>(output));
+                                  tflite_micro::micro::GetTensorShape(input),
+                                  tflite_micro::micro::GetTensorData<float>(input),
+                                  tflite_micro::micro::GetTensorShape(output),
+                                  tflite_micro::micro::GetTensorData<float>(output));
       break;
     case kTfLiteInt32:
       reference_ops::Slice<int32_t>(
-          op_params, tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<int32_t>(input),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<int32_t>(output));
+          op_params, tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<int32_t>(input),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<int32_t>(output));
       break;
     case kTfLiteInt8:
       reference_ops::Slice<int8_t>(
-          op_params, tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<int8_t>(input),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<int8_t>(output));
+          op_params, tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<int8_t>(input),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<int8_t>(output));
       break;
     case kTfLiteInt16:
       reference_ops::Slice<int16_t>(
-          op_params, tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<int16_t>(input),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<int16_t>(output));
+          op_params, tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<int16_t>(input),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<int16_t>(output));
       break;
     case kTfLiteBool:
       reference_ops::Slice<bool>(op_params,
-                                 tflite::micro::GetTensorShape(input),
-                                 tflite::micro::GetTensorData<bool>(input),
-                                 tflite::micro::GetTensorShape(output),
-                                 tflite::micro::GetTensorData<bool>(output));
+                                 tflite_micro::micro::GetTensorShape(input),
+                                 tflite_micro::micro::GetTensorData<bool>(input),
+                                 tflite_micro::micro::GetTensorShape(output),
+                                 tflite_micro::micro::GetTensorData<bool>(output));
       break;
     default:
       MicroPrintf("Input tensor type %s (%d) not supported.",
@@ -158,7 +158,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_SLICE() {
-  return tflite::micro::RegisterOp(nullptr, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/slice_test.cc b/tensorflow/lite/micro/kernels/slice_test.cc
index e787c0c7..5978afcc 100644
--- a/tensorflow/lite/micro/kernels/slice_test.cc
+++ b/tensorflow/lite/micro/kernels/slice_test.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -50,7 +50,7 @@ void TestSlice(int* input_dims_data, const dataT* input_data,
   int outputs_array_data[] = {1, 3};
   TfLiteIntArray* outputs_array = IntArrayFromInts(outputs_array_data);
 
-  const TFLMRegistration registration = tflite::Register_SLICE();
+  const TFLMRegistration registration = tflite_micro::Register_SLICE();
   micro::KernelRunner runner(registration, tensors, tensors_size, inputs_array,
                              outputs_array, nullptr);
 
@@ -64,7 +64,7 @@ void TestSlice(int* input_dims_data, const dataT* input_data,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -79,7 +79,7 @@ TF_LITE_MICRO_TEST(In1D) {
   float expected_output_data[] = {2, 3};
   float output_data[2];
 
-  tflite::testing::TestSlice(input_shape, input_values, begin_shape,
+  tflite_micro::testing::TestSlice(input_shape, input_values, begin_shape,
                              begin_values, size_shape, size_values,
                              output_shape, expected_output_data, output_data);
 }
@@ -95,7 +95,7 @@ TF_LITE_MICRO_TEST(In2D) {
   float expected_output_data[] = {4, 5};
   float output_data[2];
 
-  tflite::testing::TestSlice(input_shape, input_values, begin_shape,
+  tflite_micro::testing::TestSlice(input_shape, input_values, begin_shape,
                              begin_values, size_shape, size_values,
                              output_shape, expected_output_data, output_data);
 }
@@ -111,7 +111,7 @@ TF_LITE_MICRO_TEST(In3D) {
   float expected_output_data[] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12};
   float output_data[12];
 
-  tflite::testing::TestSlice(input_shape, input_values, begin_shape,
+  tflite_micro::testing::TestSlice(input_shape, input_values, begin_shape,
                              begin_values, size_shape, size_values,
                              output_shape, expected_output_data, output_data);
 }
@@ -127,7 +127,7 @@ TF_LITE_MICRO_TEST(In5D) {
   float expected_output_data[] = {2, 3, 4};
   float output_data[3];
 
-  tflite::testing::TestSlice(input_shape, input_values, begin_shape,
+  tflite_micro::testing::TestSlice(input_shape, input_values, begin_shape,
                              begin_values, size_shape, size_values,
                              output_shape, expected_output_data, output_data);
 }
@@ -143,7 +143,7 @@ TF_LITE_MICRO_TEST(InputFloat) {
   float expected_output_data[] = {2, 3, 4};
   float output_data[3];
 
-  tflite::testing::TestSlice(input_shape, input_values, begin_shape,
+  tflite_micro::testing::TestSlice(input_shape, input_values, begin_shape,
                              begin_values, size_shape, size_values,
                              output_shape, expected_output_data, output_data);
 }
@@ -159,7 +159,7 @@ TF_LITE_MICRO_TEST(IndexInt64) {
   float expected_output_data[] = {2, 3, 4};
   float output_data[3];
 
-  tflite::testing::TestSlice(input_shape, input_values, begin_shape,
+  tflite_micro::testing::TestSlice(input_shape, input_values, begin_shape,
                              begin_values, size_shape, size_values,
                              output_shape, expected_output_data, output_data);
 }
@@ -178,7 +178,7 @@ TF_LITE_MICRO_TEST(InputInteger1) {
   int32_t expected_output_data[] = {3, 3, 3};
   int32_t output_data[3];
 
-  tflite::testing::TestSlice(input_shape, input_values, begin_shape,
+  tflite_micro::testing::TestSlice(input_shape, input_values, begin_shape,
                              begin_values, size_shape, size_values,
                              output_shape, expected_output_data, output_data);
 }
@@ -195,7 +195,7 @@ TF_LITE_MICRO_TEST(InputInteger2) {
   int32_t expected_output_data[] = {3, 3, 3, 4, 4, 4};
   int32_t output_data[6];
 
-  tflite::testing::TestSlice(input_shape, input_values, begin_shape,
+  tflite_micro::testing::TestSlice(input_shape, input_values, begin_shape,
                              begin_values, size_shape, size_values,
                              output_shape, expected_output_data, output_data);
 }
@@ -212,7 +212,7 @@ TF_LITE_MICRO_TEST(InputInteger3) {
   int32_t expected_output_data[] = {3, 3, 3, 5, 5, 5};
   int32_t output_data[6];
 
-  tflite::testing::TestSlice(input_shape, input_values, begin_shape,
+  tflite_micro::testing::TestSlice(input_shape, input_values, begin_shape,
                              begin_values, size_shape, size_values,
                              output_shape, expected_output_data, output_data);
 }
@@ -229,7 +229,7 @@ TF_LITE_MICRO_TEST(SizeMinus1) {
   int32_t expected_output_data[] = {3, 3, 3, 5, 5, 5};
   int32_t output_data[6];
 
-  tflite::testing::TestSlice(input_shape, input_values, begin_shape,
+  tflite_micro::testing::TestSlice(input_shape, input_values, begin_shape,
                              begin_values, size_shape, size_values,
                              output_shape, expected_output_data, output_data);
 }
@@ -246,7 +246,7 @@ TF_LITE_MICRO_TEST(BeginNonZeroSizeMinus1Axis1) {
   int32_t expected_output_data[] = {5, 6, 8, 9};
   int32_t output_data[4];
 
-  tflite::testing::TestSlice(input_shape, input_values, begin_shape,
+  tflite_micro::testing::TestSlice(input_shape, input_values, begin_shape,
                              begin_values, size_shape, size_values,
                              output_shape, expected_output_data, output_data);
 }
@@ -263,7 +263,7 @@ TF_LITE_MICRO_TEST(BeginNonZeroSizeMinus1Axis2) {
   int32_t expected_output_data[] = {3, 3, 5, 5};
   int32_t output_data[4];
 
-  tflite::testing::TestSlice(input_shape, input_values, begin_shape,
+  tflite_micro::testing::TestSlice(input_shape, input_values, begin_shape,
                              begin_values, size_shape, size_values,
                              output_shape, expected_output_data, output_data);
 }
@@ -280,7 +280,7 @@ TF_LITE_MICRO_TEST(BeginNonZeroSizeMinus1Axis3) {
   int32_t expected_output_data[] = {3, 3, 5, 5};
   int32_t output_data[4];
 
-  tflite::testing::TestSlice(input_shape, input_values, begin_shape,
+  tflite_micro::testing::TestSlice(input_shape, input_values, begin_shape,
                              begin_values, size_shape, size_values,
                              output_shape, expected_output_data, output_data);
 }
@@ -297,7 +297,7 @@ TF_LITE_MICRO_TEST(SliceInt8) {
   int8_t expected_output_data[] = {3, 3, 3, 5, 5, 5};
   int8_t output_data[6];
 
-  tflite::testing::TestSlice(input_shape, input_values, begin_shape,
+  tflite_micro::testing::TestSlice(input_shape, input_values, begin_shape,
                              begin_values, size_shape, size_values,
                              output_shape, expected_output_data, output_data);
 }
@@ -314,7 +314,7 @@ TF_LITE_MICRO_TEST(SliceInt16) {
   int16_t expected_output_data[] = {3, 3, 3, 5, 5, 5};
   int16_t output_data[6];
 
-  tflite::testing::TestSlice(input_shape, input_values, begin_shape,
+  tflite_micro::testing::TestSlice(input_shape, input_values, begin_shape,
                              begin_values, size_shape, size_values,
                              output_shape, expected_output_data, output_data);
 }
@@ -332,7 +332,7 @@ TF_LITE_MICRO_TEST(SliceBool) {
   bool expected_output_data[] = {true, false, true, false, false, true};
   bool output_data[6];
 
-  tflite::testing::TestSlice(input_shape, input_values, begin_shape,
+  tflite_micro::testing::TestSlice(input_shape, input_values, begin_shape,
                              begin_values, size_shape, size_values,
                              output_shape, expected_output_data, output_data);
 }
diff --git a/tensorflow/lite/micro/kernels/softmax.cc b/tensorflow/lite/micro/kernels/softmax.cc
index b154ecba..41c2596a 100644
--- a/tensorflow/lite/micro/kernels/softmax.cc
+++ b/tensorflow/lite/micro/kernels/softmax.cc
@@ -26,48 +26,48 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 void SoftmaxQuantized(const TfLiteEvalTensor* input, TfLiteEvalTensor* output,
                       const SoftmaxParams& op_data) {
   if (input->type == kTfLiteInt8) {
     if (output->type == kTfLiteInt16) {
-      tflite::reference_ops::Softmax(
-          op_data, tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<int8_t>(input),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<int16_t>(output));
+      tflite_micro::reference_ops::Softmax(
+          op_data, tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<int8_t>(input),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<int16_t>(output));
     } else {
-      tflite::reference_ops::Softmax(
-          op_data, tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<int8_t>(input),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<int8_t>(output));
+      tflite_micro::reference_ops::Softmax(
+          op_data, tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<int8_t>(input),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<int8_t>(output));
     }
   } else {
-    tflite::reference_ops::SoftmaxInt16(
-        op_data, tflite::micro::GetTensorShape(input),
-        tflite::micro::GetTensorData<int16_t>(input),
-        tflite::micro::GetTensorShape(output),
-        tflite::micro::GetTensorData<int16_t>(output));
+    tflite_micro::reference_ops::SoftmaxInt16(
+        op_data, tflite_micro::micro::GetTensorShape(input),
+        tflite_micro::micro::GetTensorData<int16_t>(input),
+        tflite_micro::micro::GetTensorShape(output),
+        tflite_micro::micro::GetTensorData<int16_t>(output));
   }
 }
 
 TfLiteStatus SoftmaxEval(TfLiteContext* context, TfLiteNode* node) {
-  const TfLiteEvalTensor* input = tflite::micro::GetEvalInput(context, node, 0);
-  TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, 0);
+  const TfLiteEvalTensor* input = tflite_micro::micro::GetEvalInput(context, node, 0);
+  TfLiteEvalTensor* output = tflite_micro::micro::GetEvalOutput(context, node, 0);
 
   TFLITE_DCHECK(node->user_data != nullptr);
   SoftmaxParams op_data = *static_cast<SoftmaxParams*>(node->user_data);
 
   switch (input->type) {
     case kTfLiteFloat32: {
-      tflite::reference_ops::Softmax(
-          op_data, tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<float>(input),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<float>(output));
+      tflite_micro::reference_ops::Softmax(
+          op_data, tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<float>(input),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<float>(output));
       return kTfLiteOk;
     }
     case kTfLiteInt8:
@@ -84,7 +84,7 @@ TfLiteStatus SoftmaxEval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_SOFTMAX() {
-  return tflite::micro::RegisterOp(SoftmaxInit, SoftmaxPrepare, SoftmaxEval);
+  return tflite_micro::micro::RegisterOp(SoftmaxInit, SoftmaxPrepare, SoftmaxEval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/softmax.h b/tensorflow/lite/micro/kernels/softmax.h
index fd972019..137158c4 100644
--- a/tensorflow/lite/micro/kernels/softmax.h
+++ b/tensorflow/lite/micro/kernels/softmax.h
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/types.h"
 #include "tensorflow/lite/micro/micro_common.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 void* SoftmaxInit(TfLiteContext* context, const char* buffer, size_t length);
 
@@ -62,6 +62,6 @@ inline TFLMRegistration Register_SOFTMAX_INT8() { return Register_SOFTMAX(); }
 inline TFLMRegistration Register_SOFTMAX_INT16() { return Register_SOFTMAX(); }
 #endif
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_SOFTMAX_H_
diff --git a/tensorflow/lite/micro/kernels/softmax_common.cc b/tensorflow/lite/micro/kernels/softmax_common.cc
index 62b8b290..55e36128 100644
--- a/tensorflow/lite/micro/kernels/softmax_common.cc
+++ b/tensorflow/lite/micro/kernels/softmax_common.cc
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/softmax.h"
 #include "tensorflow/lite/micro/micro_context.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 // Softmax parameter data that persists in user_data
@@ -120,13 +120,13 @@ TfLiteStatus CalculateSoftmaxParams(TfLiteContext* context,
       op_data->input_left_shift = input_left_shift;
     } else {
       int input_left_shift;
-      tflite::PreprocessSoftmaxScaling(
+      tflite_micro::PreprocessSoftmaxScaling(
           static_cast<double>(params->beta),
           static_cast<double>(input->params.scale), kScaledDiffIntegerBits,
           &op_data->input_multiplier, &input_left_shift);
       op_data->input_left_shift = input_left_shift;
       op_data->diff_min =
-          -1.0 * tflite::CalculateInputRadius(kScaledDiffIntegerBits,
+          -1.0 * tflite_micro::CalculateInputRadius(kScaledDiffIntegerBits,
                                               op_data->input_left_shift);
     }
   } else {
@@ -165,4 +165,4 @@ TfLiteStatus SoftmaxPrepare(TfLiteContext* context, TfLiteNode* node) {
   return ret_val;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/softmax_test.cc b/tensorflow/lite/micro/kernels/softmax_test.cc
index 057892bf..200649b6 100644
--- a/tensorflow/lite/micro/kernels/softmax_test.cc
+++ b/tensorflow/lite/micro/kernels/softmax_test.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -315,158 +315,158 @@ void TestSoftmaxQuantized(int* input_dims_data, const float* input_data,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(Softmax1DFloatShouldMatchGolden) {
-  float output_data[tflite::testing::flat_size_1d];
-  tflite::testing::TestSoftmaxFloat(
-      tflite::testing ::shape_1d, tflite::testing::input_data_1d,
-      tflite::testing::shape_1d, tflite::testing::golden_1d, output_data);
+  float output_data[tflite_micro::testing::flat_size_1d];
+  tflite_micro::testing::TestSoftmaxFloat(
+      tflite_micro::testing ::shape_1d, tflite_micro::testing::input_data_1d,
+      tflite_micro::testing::shape_1d, tflite_micro::testing::golden_1d, output_data);
 }
 
 TF_LITE_MICRO_TEST(Softmax1DQuantizedInt8ShouldMatchGolden) {
   const float input_scale = 0.1f;
   const int input_zero_point = 0;
 
-  int8_t input_quantized[tflite::testing::flat_size_1d];
-  int8_t golden_quantized[tflite::testing::flat_size_1d];
-  int8_t output_data[tflite::testing::flat_size_1d];
-  tflite::testing::TestSoftmaxQuantized(
-      tflite::testing::shape_1d, tflite::testing::input_data_1d,
-      input_quantized, input_scale, input_zero_point, tflite::testing::shape_1d,
-      tflite::testing::golden_1d, golden_quantized,
-      tflite::testing::output_scale_int8,
-      tflite::testing::output_zero_point_int8, output_data);
+  int8_t input_quantized[tflite_micro::testing::flat_size_1d];
+  int8_t golden_quantized[tflite_micro::testing::flat_size_1d];
+  int8_t output_data[tflite_micro::testing::flat_size_1d];
+  tflite_micro::testing::TestSoftmaxQuantized(
+      tflite_micro::testing::shape_1d, tflite_micro::testing::input_data_1d,
+      input_quantized, input_scale, input_zero_point, tflite_micro::testing::shape_1d,
+      tflite_micro::testing::golden_1d, golden_quantized,
+      tflite_micro::testing::output_scale_int8,
+      tflite_micro::testing::output_zero_point_int8, output_data);
 }
 
 TF_LITE_MICRO_TEST(Softmax1DQuantizedInt16ShouldMatchGolden) {
   const float input_scale = 0.1f;
   const int input_zero_point = 0;
 
-  int16_t input_quantized[tflite::testing::flat_size_1d];
-  int16_t golden_quantized[tflite::testing::flat_size_1d];
-  int16_t output_data[tflite::testing::flat_size_1d];
-  tflite::testing::TestSoftmaxQuantized(
-      tflite::testing::shape_1d, tflite::testing::input_data_1d,
-      input_quantized, input_scale, input_zero_point, tflite::testing::shape_1d,
-      tflite::testing::golden_1d, golden_quantized,
-      tflite::testing::output_scale_int16,
-      tflite::testing::output_zero_point_int16, output_data);
+  int16_t input_quantized[tflite_micro::testing::flat_size_1d];
+  int16_t golden_quantized[tflite_micro::testing::flat_size_1d];
+  int16_t output_data[tflite_micro::testing::flat_size_1d];
+  tflite_micro::testing::TestSoftmaxQuantized(
+      tflite_micro::testing::shape_1d, tflite_micro::testing::input_data_1d,
+      input_quantized, input_scale, input_zero_point, tflite_micro::testing::shape_1d,
+      tflite_micro::testing::golden_1d, golden_quantized,
+      tflite_micro::testing::output_scale_int16,
+      tflite_micro::testing::output_zero_point_int16, output_data);
 }
 
 TF_LITE_MICRO_TEST(Softmax2DFloatShouldMatchGolden) {
-  float output_data[tflite::testing::flat_size_2d];
-  tflite::testing::TestSoftmaxFloat(
-      tflite::testing ::shape_2d, tflite::testing::input_data_2d,
-      tflite::testing::shape_2d, tflite::testing::golden_2d, output_data);
+  float output_data[tflite_micro::testing::flat_size_2d];
+  tflite_micro::testing::TestSoftmaxFloat(
+      tflite_micro::testing ::shape_2d, tflite_micro::testing::input_data_2d,
+      tflite_micro::testing::shape_2d, tflite_micro::testing::golden_2d, output_data);
 }
 
 TF_LITE_MICRO_TEST(Softmax2DQuantizedInt8ShouldMatchGolden) {
   const float input_scale = 0.1f;
   const int input_zero_point = 0;
 
-  int8_t input_quantized[tflite::testing::flat_size_2d];
-  int8_t golden_quantized[tflite::testing::flat_size_2d];
-  int8_t output_data[tflite::testing::flat_size_2d];
-  tflite::testing::TestSoftmaxQuantized(
-      tflite::testing::shape_2d, tflite::testing::input_data_2d,
-      input_quantized, input_scale, input_zero_point, tflite::testing::shape_2d,
-      tflite::testing::golden_2d, golden_quantized,
-      tflite::testing::output_scale_int8,
-      tflite::testing::output_zero_point_int8, output_data);
+  int8_t input_quantized[tflite_micro::testing::flat_size_2d];
+  int8_t golden_quantized[tflite_micro::testing::flat_size_2d];
+  int8_t output_data[tflite_micro::testing::flat_size_2d];
+  tflite_micro::testing::TestSoftmaxQuantized(
+      tflite_micro::testing::shape_2d, tflite_micro::testing::input_data_2d,
+      input_quantized, input_scale, input_zero_point, tflite_micro::testing::shape_2d,
+      tflite_micro::testing::golden_2d, golden_quantized,
+      tflite_micro::testing::output_scale_int8,
+      tflite_micro::testing::output_zero_point_int8, output_data);
 }
 
 TF_LITE_MICRO_TEST(Softmax2DQuantizedInt16ShouldMatchGolden) {
   const float input_scale = 0.1f;
   const int input_zero_point = 0;
 
-  int16_t input_quantized[tflite::testing::flat_size_2d];
-  int16_t golden_quantized[tflite::testing::flat_size_2d];
-  int16_t output_data[tflite::testing::flat_size_2d];
-  tflite::testing::TestSoftmaxQuantized(
-      tflite::testing::shape_2d, tflite::testing::input_data_2d,
-      input_quantized, input_scale, input_zero_point, tflite::testing::shape_2d,
-      tflite::testing::golden_2d, golden_quantized,
-      tflite::testing::output_scale_int16,
-      tflite::testing::output_zero_point_int16, output_data);
+  int16_t input_quantized[tflite_micro::testing::flat_size_2d];
+  int16_t golden_quantized[tflite_micro::testing::flat_size_2d];
+  int16_t output_data[tflite_micro::testing::flat_size_2d];
+  tflite_micro::testing::TestSoftmaxQuantized(
+      tflite_micro::testing::shape_2d, tflite_micro::testing::input_data_2d,
+      input_quantized, input_scale, input_zero_point, tflite_micro::testing::shape_2d,
+      tflite_micro::testing::golden_2d, golden_quantized,
+      tflite_micro::testing::output_scale_int16,
+      tflite_micro::testing::output_zero_point_int16, output_data);
 }
 
 TF_LITE_MICRO_TEST(Softmax3DFloatShouldMatchGolden) {
-  float output_data[tflite::testing::flat_size_3d];
-  tflite::testing::TestSoftmaxFloat(
-      tflite::testing ::shape_3d, tflite::testing::input_data_3d,
-      tflite::testing::shape_3d, tflite::testing::golden_3d, output_data);
+  float output_data[tflite_micro::testing::flat_size_3d];
+  tflite_micro::testing::TestSoftmaxFloat(
+      tflite_micro::testing ::shape_3d, tflite_micro::testing::input_data_3d,
+      tflite_micro::testing::shape_3d, tflite_micro::testing::golden_3d, output_data);
 }
 
 TF_LITE_MICRO_TEST(Softmax3DQuantizedInt8ShouldMatchGolden) {
   const float input_scale = 0.1f;
   const int input_zero_point = 0;
 
-  int8_t input_quantized[tflite::testing::flat_size_3d];
-  int8_t golden_quantized[tflite::testing::flat_size_3d];
-  int8_t output_data[tflite::testing::flat_size_3d];
-  tflite::testing::TestSoftmaxQuantized(
-      tflite::testing::shape_3d, tflite::testing::input_data_3d,
-      input_quantized, input_scale, input_zero_point, tflite::testing::shape_3d,
-      tflite::testing::golden_3d, golden_quantized,
-      tflite::testing::output_scale_int8,
-      tflite::testing::output_zero_point_int8, output_data);
+  int8_t input_quantized[tflite_micro::testing::flat_size_3d];
+  int8_t golden_quantized[tflite_micro::testing::flat_size_3d];
+  int8_t output_data[tflite_micro::testing::flat_size_3d];
+  tflite_micro::testing::TestSoftmaxQuantized(
+      tflite_micro::testing::shape_3d, tflite_micro::testing::input_data_3d,
+      input_quantized, input_scale, input_zero_point, tflite_micro::testing::shape_3d,
+      tflite_micro::testing::golden_3d, golden_quantized,
+      tflite_micro::testing::output_scale_int8,
+      tflite_micro::testing::output_zero_point_int8, output_data);
 }
 
 TF_LITE_MICRO_TEST(Softmax3DQuantizedInt16ShouldMatchGolden) {
   const float input_scale = 0.1f;
   const int input_zero_point = 0;
 
-  int16_t input_quantized[tflite::testing::flat_size_3d];
-  int16_t golden_quantized[tflite::testing::flat_size_3d];
-  int16_t output_data[tflite::testing::flat_size_3d];
-  tflite::testing::TestSoftmaxQuantized(
-      tflite::testing::shape_3d, tflite::testing::input_data_3d,
-      input_quantized, input_scale, input_zero_point, tflite::testing::shape_3d,
-      tflite::testing::golden_3d, golden_quantized,
-      tflite::testing::output_scale_int16,
-      tflite::testing::output_zero_point_int16, output_data,
-      tflite::testing::tolerance_int16);
+  int16_t input_quantized[tflite_micro::testing::flat_size_3d];
+  int16_t golden_quantized[tflite_micro::testing::flat_size_3d];
+  int16_t output_data[tflite_micro::testing::flat_size_3d];
+  tflite_micro::testing::TestSoftmaxQuantized(
+      tflite_micro::testing::shape_3d, tflite_micro::testing::input_data_3d,
+      input_quantized, input_scale, input_zero_point, tflite_micro::testing::shape_3d,
+      tflite_micro::testing::golden_3d, golden_quantized,
+      tflite_micro::testing::output_scale_int16,
+      tflite_micro::testing::output_zero_point_int16, output_data,
+      tflite_micro::testing::tolerance_int16);
 }
 
 TF_LITE_MICRO_TEST(Softmax4DFloatShouldMatchGolden) {
-  float output_data[tflite::testing::flat_size_4d];
-  tflite::testing::TestSoftmaxFloat(
-      tflite::testing ::shape_4d, tflite::testing::input_data_4d,
-      tflite::testing::shape_4d, tflite::testing::golden_4d, output_data);
+  float output_data[tflite_micro::testing::flat_size_4d];
+  tflite_micro::testing::TestSoftmaxFloat(
+      tflite_micro::testing ::shape_4d, tflite_micro::testing::input_data_4d,
+      tflite_micro::testing::shape_4d, tflite_micro::testing::golden_4d, output_data);
 }
 
 TF_LITE_MICRO_TEST(Softmax4DQuantizedInt8ShouldMatchGolden) {
   const float input_scale = 0.1f;
   const int input_zero_point = 0;
 
-  int8_t input_quantized[tflite::testing::flat_size_4d];
-  int8_t golden_quantized[tflite::testing::flat_size_4d];
-  int8_t output_data[tflite::testing::flat_size_4d];
-  tflite::testing::TestSoftmaxQuantized(
-      tflite::testing::shape_4d, tflite::testing::input_data_4d,
-      input_quantized, input_scale, input_zero_point, tflite::testing::shape_4d,
-      tflite::testing::golden_4d, golden_quantized,
-      tflite::testing::output_scale_int8,
-      tflite::testing::output_zero_point_int8, output_data);
+  int8_t input_quantized[tflite_micro::testing::flat_size_4d];
+  int8_t golden_quantized[tflite_micro::testing::flat_size_4d];
+  int8_t output_data[tflite_micro::testing::flat_size_4d];
+  tflite_micro::testing::TestSoftmaxQuantized(
+      tflite_micro::testing::shape_4d, tflite_micro::testing::input_data_4d,
+      input_quantized, input_scale, input_zero_point, tflite_micro::testing::shape_4d,
+      tflite_micro::testing::golden_4d, golden_quantized,
+      tflite_micro::testing::output_scale_int8,
+      tflite_micro::testing::output_zero_point_int8, output_data);
 }
 
 TF_LITE_MICRO_TEST(Softmax4DQuantizedInt16ShouldMatchGolden) {
   const float input_scale = 0.1f;
   const int input_zero_point = 0;
 
-  int16_t input_quantized[tflite::testing::flat_size_4d];
-  int16_t golden_quantized[tflite::testing::flat_size_4d];
-  int16_t output_data[tflite::testing::flat_size_4d];
-  tflite::testing::TestSoftmaxQuantized(
-      tflite::testing::shape_4d, tflite::testing::input_data_4d,
-      input_quantized, input_scale, input_zero_point, tflite::testing::shape_4d,
-      tflite::testing::golden_4d, golden_quantized,
-      tflite::testing::output_scale_int16,
-      tflite::testing::output_zero_point_int16, output_data,
-      tflite::testing::tolerance_int16);
+  int16_t input_quantized[tflite_micro::testing::flat_size_4d];
+  int16_t golden_quantized[tflite_micro::testing::flat_size_4d];
+  int16_t output_data[tflite_micro::testing::flat_size_4d];
+  tflite_micro::testing::TestSoftmaxQuantized(
+      tflite_micro::testing::shape_4d, tflite_micro::testing::input_data_4d,
+      input_quantized, input_scale, input_zero_point, tflite_micro::testing::shape_4d,
+      tflite_micro::testing::golden_4d, golden_quantized,
+      tflite_micro::testing::output_scale_int16,
+      tflite_micro::testing::output_zero_point_int16, output_data,
+      tflite_micro::testing::tolerance_int16);
 }
 
 TF_LITE_MICRO_TEST(Softmax2DQuantizedInt8InputInt16OutputShouldMatchGolden) {
@@ -475,13 +475,13 @@ TF_LITE_MICRO_TEST(Softmax2DQuantizedInt8InputInt16OutputShouldMatchGolden) {
   const float output_scale = 1.0f / 65536.0f;
   const int output_zero_point = -32768;
 
-  int8_t input_quantized[tflite::testing::flat_size_2d];
-  int16_t golden_quantized[tflite::testing::flat_size_2d];
-  int16_t output_data[tflite::testing::flat_size_2d];
-  tflite::testing::TestSoftmaxQuantized(
-      tflite::testing::shape_2d, tflite::testing::input_data_2d,
-      input_quantized, input_scale, input_zero_point, tflite::testing::shape_2d,
-      tflite::testing::golden_2d, golden_quantized, output_scale,
+  int8_t input_quantized[tflite_micro::testing::flat_size_2d];
+  int16_t golden_quantized[tflite_micro::testing::flat_size_2d];
+  int16_t output_data[tflite_micro::testing::flat_size_2d];
+  tflite_micro::testing::TestSoftmaxQuantized(
+      tflite_micro::testing::shape_2d, tflite_micro::testing::input_data_2d,
+      input_quantized, input_scale, input_zero_point, tflite_micro::testing::shape_2d,
+      tflite_micro::testing::golden_2d, golden_quantized, output_scale,
       output_zero_point, output_data);
 }
 
diff --git a/tensorflow/lite/micro/kernels/space_to_batch_nd.cc b/tensorflow/lite/micro/kernels/space_to_batch_nd.cc
index 6b536eed..17a8b7bd 100644
--- a/tensorflow/lite/micro/kernels/space_to_batch_nd.cc
+++ b/tensorflow/lite/micro/kernels/space_to_batch_nd.cc
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_log.h"
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -73,36 +73,36 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
       *(static_cast<const SpaceToBatchParams*>(node->user_data));
 
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   const TfLiteEvalTensor* block_shape =
-      tflite::micro::GetEvalInput(context, node, kBlockShapeTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kBlockShapeTensor);
   const TfLiteEvalTensor* crops =
-      tflite::micro::GetEvalInput(context, node, kCropsTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kCropsTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
   switch (input->type) {  // Already know in/out types are same.
     case kTfLiteFloat32:
       reference_ops::SpaceToBatchND(
-          params, tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<float>(input),
-          tflite::micro::GetTensorShape(block_shape),
-          tflite::micro::GetTensorData<int32_t>(block_shape),
-          tflite::micro::GetTensorShape(crops),
-          tflite::micro::GetTensorData<int32_t>(crops),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<float>(output));
+          params, tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<float>(input),
+          tflite_micro::micro::GetTensorShape(block_shape),
+          tflite_micro::micro::GetTensorData<int32_t>(block_shape),
+          tflite_micro::micro::GetTensorShape(crops),
+          tflite_micro::micro::GetTensorData<int32_t>(crops),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<float>(output));
       break;
     case kTfLiteInt8:
       reference_ops::SpaceToBatchND(
-          params, tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<int8_t>(input),
-          tflite::micro::GetTensorShape(block_shape),
-          tflite::micro::GetTensorData<int32_t>(block_shape),
-          tflite::micro::GetTensorShape(crops),
-          tflite::micro::GetTensorData<int32_t>(crops),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<int8_t>(output));
+          params, tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<int8_t>(input),
+          tflite_micro::micro::GetTensorShape(block_shape),
+          tflite_micro::micro::GetTensorData<int32_t>(block_shape),
+          tflite_micro::micro::GetTensorShape(crops),
+          tflite_micro::micro::GetTensorData<int32_t>(crops),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<int8_t>(output));
       break;
     default:
       MicroPrintf("Type %s (%d) not supported.", TfLiteTypeGetName(input->type),
@@ -115,7 +115,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace.
 
 TFLMRegistration Register_SPACE_TO_BATCH_ND() {
-  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/space_to_batch_nd_test.cc b/tensorflow/lite/micro/kernels/space_to_batch_nd_test.cc
index eae185b1..e5d3505b 100644
--- a/tensorflow/lite/micro/kernels/space_to_batch_nd_test.cc
+++ b/tensorflow/lite/micro/kernels/space_to_batch_nd_test.cc
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -104,15 +104,15 @@ TfLiteStatus TestSpaceToBatchNdQuantized(
   constexpr int outputs_size = 1;
   constexpr int tensors_size = inputs_size + outputs_size;
   TfLiteTensor tensors[tensors_size] = {
-      tflite::testing::CreateQuantizedTensor(input_data, input_quantized,
+      tflite_micro::testing::CreateQuantizedTensor(input_data, input_quantized,
                                              input_dims, input_scale,
                                              input_zero_point),
-      tflite::testing::CreateTensor(block_shape_data, block_shape_dims),
-      tflite::testing::CreateTensor(crops_data, crops_dims),
-      tflite::testing::CreateQuantizedTensor(output_data, output_dims,
+      tflite_micro::testing::CreateTensor(block_shape_data, block_shape_dims),
+      tflite_micro::testing::CreateTensor(crops_data, crops_dims),
+      tflite_micro::testing::CreateQuantizedTensor(output_data, output_dims,
                                              output_scale, output_zero_point),
   };
-  tflite::Quantize(golden, golden_quantized, ElementCount(*output_dims),
+  tflite_micro::Quantize(golden, golden_quantized, ElementCount(*output_dims),
                    output_scale, output_zero_point);
 
   return ValidateSpaceToBatchNdGoldens(tensors, tensors_size, golden_quantized,
@@ -121,34 +121,34 @@ TfLiteStatus TestSpaceToBatchNdQuantized(
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(SpaceToBatchBasicFloat) {
-  float output[tflite::testing::kBasicInputOutputSize];
+  float output[tflite_micro::testing::kBasicInputOutputSize];
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteOk,
-      tflite::testing::TestSpaceToBatchNdFloat(
-          tflite::testing::basic_input_dims, tflite::testing::basic_input,
-          tflite::testing::basic_block_shape_dims,
-          tflite::testing::basic_block_shape, tflite::testing::basic_crops_dims,
-          tflite::testing::basic_crops, tflite::testing::basic_output_dims,
-          tflite::testing::basic_golden, output));
+      tflite_micro::testing::TestSpaceToBatchNdFloat(
+          tflite_micro::testing::basic_input_dims, tflite_micro::testing::basic_input,
+          tflite_micro::testing::basic_block_shape_dims,
+          tflite_micro::testing::basic_block_shape, tflite_micro::testing::basic_crops_dims,
+          tflite_micro::testing::basic_crops, tflite_micro::testing::basic_output_dims,
+          tflite_micro::testing::basic_golden, output));
 }
 
 TF_LITE_MICRO_TEST(SpaceToBatchBasicInt8) {
-  int8_t output[tflite::testing::kBasicInputOutputSize];
-  int8_t input_quantized[tflite::testing::kBasicInputOutputSize];
-  int8_t golden_quantized[tflite::testing::kBasicInputOutputSize];
+  int8_t output[tflite_micro::testing::kBasicInputOutputSize];
+  int8_t input_quantized[tflite_micro::testing::kBasicInputOutputSize];
+  int8_t golden_quantized[tflite_micro::testing::kBasicInputOutputSize];
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteOk,
-      tflite::testing::TestSpaceToBatchNdQuantized(
-          tflite::testing::basic_input_dims, tflite::testing::basic_input,
-          input_quantized, 1.0f, 0, tflite::testing::basic_block_shape_dims,
-          tflite::testing::basic_block_shape, tflite::testing::basic_crops_dims,
-          tflite::testing::basic_crops, tflite::testing::basic_output_dims,
-          tflite::testing::basic_golden, golden_quantized, 1.0f, 0, output));
+      tflite_micro::testing::TestSpaceToBatchNdQuantized(
+          tflite_micro::testing::basic_input_dims, tflite_micro::testing::basic_input,
+          input_quantized, 1.0f, 0, tflite_micro::testing::basic_block_shape_dims,
+          tflite_micro::testing::basic_block_shape, tflite_micro::testing::basic_crops_dims,
+          tflite_micro::testing::basic_crops, tflite_micro::testing::basic_output_dims,
+          tflite_micro::testing::basic_golden, golden_quantized, 1.0f, 0, output));
 }
 
 TF_LITE_MICRO_TESTS_END
diff --git a/tensorflow/lite/micro/kernels/space_to_depth.cc b/tensorflow/lite/micro/kernels/space_to_depth.cc
index eac6613a..72544346 100644
--- a/tensorflow/lite/micro/kernels/space_to_depth.cc
+++ b/tensorflow/lite/micro/kernels/space_to_depth.cc
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -121,7 +121,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_SPACE_TO_DEPTH() {
-  return tflite::micro::RegisterOp(nullptr, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/space_to_depth_test.cc b/tensorflow/lite/micro/kernels/space_to_depth_test.cc
index 1fd7d10e..a5c2f3e6 100644
--- a/tensorflow/lite/micro/kernels/space_to_depth_test.cc
+++ b/tensorflow/lite/micro/kernels/space_to_depth_test.cc
@@ -20,9 +20,9 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-using tflite::ElementCount;
-using tflite::testing::CreateTensor;
-using tflite::testing::IntArrayFromInts;
+using tflite_micro::ElementCount;
+using tflite_micro::testing::CreateTensor;
+using tflite_micro::testing::IntArrayFromInts;
 
 namespace {
 
@@ -63,7 +63,7 @@ void TestSpaceToDepth(const SpaceToDepthTest<T>& args) {
   TfLiteTensor tensors[] = {CreateTensor(args.input_data, input_dims),
                             CreateTensor(args.output_data, output_dims)};
 
-  const TFLMRegistration registration = tflite::Register_SPACE_TO_DEPTH();
+  const TFLMRegistration registration = tflite_micro::Register_SPACE_TO_DEPTH();
   constexpr int tensor_count = ArrayLength(tensors);
   constexpr int kInputIndex = 0;
   int input_indexes_data[] = {1, kInputIndex};
@@ -74,7 +74,7 @@ void TestSpaceToDepth(const SpaceToDepthTest<T>& args) {
   TfLiteSpaceToDepthParams op_params = {};
   op_params.block_size = args.block_size;
 
-  tflite::micro::KernelRunner runner(registration, tensors, tensor_count,
+  tflite_micro::micro::KernelRunner runner(registration, tensors, tensor_count,
                                      input_indexes, output_indexes,
                                      static_cast<void*>(&op_params));
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, runner.InitAndPrepare());
diff --git a/tensorflow/lite/micro/kernels/split.cc b/tensorflow/lite/micro/kernels/split.cc
index aa877201..c7090839 100644
--- a/tensorflow/lite/micro/kernels/split.cc
+++ b/tensorflow/lite/micro/kernels/split.cc
@@ -20,7 +20,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -30,7 +30,7 @@ TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   const int output_count = NumOutputs(node);
   const TfLiteIntArray* input_dims = input->dims;
   const TfLiteEvalTensor* output0 =
-      tflite::micro::GetEvalOutput(context, node, 0);
+      tflite_micro::micro::GetEvalOutput(context, node, 0);
   const TfLiteIntArray* output_dims = output0->dims;
 
   const int split_dimensions = input_dims->size;
@@ -52,11 +52,11 @@ TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
     base_inner_size *= input_dims->data[i];
   }
 
-  const T* input_ptr = tflite::micro::GetTensorData<T>(input);
+  const T* input_ptr = tflite_micro::micro::GetTensorData<T>(input);
   for (int k = 0; k < outer_size; ++k) {
     for (int i = 0; i < output_count; ++i) {
-      TfLiteEvalTensor* t = tflite::micro::GetEvalOutput(context, node, i);
-      T* output_data = tflite::micro::GetTensorData<T>(t);
+      TfLiteEvalTensor* t = tflite_micro::micro::GetEvalOutput(context, node, i);
+      T* output_data = tflite_micro::micro::GetTensorData<T>(t);
       const int copy_size = output_dims->data[axis] * base_inner_size;
       T* output_ptr = output_data + k * copy_size;
       for (int j = 0; j < copy_size; ++j) output_ptr[j] = input_ptr[j];
@@ -83,10 +83,10 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 }
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
-  const TfLiteEvalTensor* axis = tflite::micro::GetEvalInput(context, node, 0);
-  const TfLiteEvalTensor* input = tflite::micro::GetEvalInput(context, node, 1);
+  const TfLiteEvalTensor* axis = tflite_micro::micro::GetEvalInput(context, node, 0);
+  const TfLiteEvalTensor* input = tflite_micro::micro::GetEvalInput(context, node, 1);
 
-  int axis_value = tflite::micro::GetTensorData<int32_t>(axis)[0];
+  int axis_value = tflite_micro::micro::GetTensorData<int32_t>(axis)[0];
   if (axis_value < 0) {
     axis_value += input->dims->size;
   }
@@ -119,7 +119,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_SPLIT() {
-  return tflite::micro::RegisterOp(nullptr, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/split_test.cc b/tensorflow/lite/micro/kernels/split_test.cc
index e3bf37d0..cdaae07c 100644
--- a/tensorflow/lite/micro/kernels/split_test.cc
+++ b/tensorflow/lite/micro/kernels/split_test.cc
@@ -20,7 +20,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 
 void TestSplitTwoOutputsFloat(int* input_dims_data, const float* input_data,
@@ -130,7 +130,7 @@ void TestSplitFourOutputsFloat(
   int outputs_array_data[] = {4, 2, 3, 4, 5};
   TfLiteIntArray* outputs_array = IntArrayFromInts(outputs_array_data);
 
-  const TFLMRegistration registration = tflite::Register_SPLIT();
+  const TFLMRegistration registration = tflite_micro::Register_SPLIT();
   micro::KernelRunner runner(registration, tensors, tensors_size, inputs_array,
                              outputs_array, nullptr);
 
@@ -193,7 +193,7 @@ void TestSplitTwoOutputsQuantized(int* input_dims_data,
   int outputs_array_data[] = {2, 2, 3};
   TfLiteIntArray* outputs_array = IntArrayFromInts(outputs_array_data);
 
-  const TFLMRegistration registration = tflite::Register_SPLIT();
+  const TFLMRegistration registration = tflite_micro::Register_SPLIT();
   micro::KernelRunner runner(registration, tensors, tensors_size, inputs_array,
                              outputs_array, nullptr);
 
@@ -248,7 +248,7 @@ void TestSplitTwoOutputsQuantized32(
   int outputs_array_data[] = {2, 2, 3};
   TfLiteIntArray* outputs_array = IntArrayFromInts(outputs_array_data);
 
-  const TFLMRegistration registration = tflite::Register_SPLIT();
+  const TFLMRegistration registration = tflite_micro::Register_SPLIT();
   micro::KernelRunner runner(registration, tensors, tensors_size, inputs_array,
                              outputs_array, nullptr);
 
@@ -265,7 +265,7 @@ void TestSplitTwoOutputsQuantized32(
 }
 
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -284,7 +284,7 @@ TF_LITE_MICRO_TEST(TwoSplitFourDimensionalAxisZero) {
   constexpr int output2_dims_count = 8;
   float output1_data[output1_dims_count];
   float output2_data[output2_dims_count];
-  tflite::testing::TestSplitTwoOutputsFloat(
+  tflite_micro::testing::TestSplitTwoOutputsFloat(
       input_shape, input_data, axis_shape, axis_data, output1_shape, golden1,
       output2_shape, golden2, output1_data, output2_data);
 }
@@ -304,7 +304,7 @@ TF_LITE_MICRO_TEST(TwoSplitFourDimensionalAxisOne) {
   constexpr int output2_dims_count = 8;
   float output1_data[output1_dims_count];
   float output2_data[output2_dims_count];
-  tflite::testing::TestSplitTwoOutputsFloat(
+  tflite_micro::testing::TestSplitTwoOutputsFloat(
       input_shape, input_data, axis_shape, axis_data, output1_shape, golden1,
       output2_shape, golden2, output1_data, output2_data);
 }
@@ -324,7 +324,7 @@ TF_LITE_MICRO_TEST(TwoSplitFourDimensionalAxisTwo) {
   constexpr int output2_dims_count = 8;
   float output1_data[output1_dims_count];
   float output2_data[output2_dims_count];
-  tflite::testing::TestSplitTwoOutputsFloat(
+  tflite_micro::testing::TestSplitTwoOutputsFloat(
       input_shape, input_data, axis_shape, axis_data, output1_shape, golden1,
       output2_shape, golden2, output1_data, output2_data);
 }
@@ -344,7 +344,7 @@ TF_LITE_MICRO_TEST(TwoSplitFourDimensionalAxisThree) {
   constexpr int output2_dims_count = 8;
   float output1_data[output1_dims_count];
   float output2_data[output2_dims_count];
-  tflite::testing::TestSplitTwoOutputsFloat(
+  tflite_micro::testing::TestSplitTwoOutputsFloat(
       input_shape, input_data, axis_shape, axis_data, output1_shape, golden1,
       output2_shape, golden2, output1_data, output2_data);
 }
@@ -364,7 +364,7 @@ TF_LITE_MICRO_TEST(TwoSplitFourDimensionalNegativeAxis) {
   constexpr int output2_dims_count = 8;
   float output1_data[output1_dims_count];
   float output2_data[output2_dims_count];
-  tflite::testing::TestSplitTwoOutputsFloat(
+  tflite_micro::testing::TestSplitTwoOutputsFloat(
       input_shape, input_data, axis_shape, axis_data, output1_shape, golden1,
       output2_shape, golden2, output1_data, output2_data);
 }
@@ -391,7 +391,7 @@ TF_LITE_MICRO_TEST(FourSplit) {
   float output2_data[output2_dims_count];
   float output3_data[output3_dims_count];
   float output4_data[output4_dims_count];
-  tflite::testing::TestSplitFourOutputsFloat(
+  tflite_micro::testing::TestSplitFourOutputsFloat(
       input_shape, input_data, axis_shape, axis_data, output1_shape, golden1,
       output2_shape, golden2, output3_shape, golden3, output4_shape, golden4,
       output1_data, output2_data, output3_data, output4_data);
@@ -411,7 +411,7 @@ TF_LITE_MICRO_TEST(TwoSplitOneDimensional) {
   constexpr int output2_dims_count = 8;
   float output1_data[output1_dims_count];
   float output2_data[output2_dims_count];
-  tflite::testing::TestSplitTwoOutputsFloat(
+  tflite_micro::testing::TestSplitTwoOutputsFloat(
       input_shape, input_data, axis_shape, axis_data, output1_shape, golden1,
       output2_shape, golden2, output1_data, output2_data);
 }
@@ -431,7 +431,7 @@ TF_LITE_MICRO_TEST(TwoSplitFourDimensionalQuantized) {
   constexpr int output2_dims_count = 8;
   int8_t output1_data[output1_dims_count];
   int8_t output2_data[output2_dims_count];
-  tflite::testing::TestSplitTwoOutputsQuantized(
+  tflite_micro::testing::TestSplitTwoOutputsQuantized(
       input_shape, input_data, axis_shape, axis_data, output1_shape, golden1,
       output2_shape, golden2, output1_data, output2_data);
 }
@@ -451,7 +451,7 @@ TF_LITE_MICRO_TEST(TwoSplitFourDimensionalQuantized32) {
   constexpr int output2_dims_count = 8;
   int32_t output1_data[output1_dims_count];
   int32_t output2_data[output2_dims_count];
-  tflite::testing::TestSplitTwoOutputsQuantized32(
+  tflite_micro::testing::TestSplitTwoOutputsQuantized32(
       input_shape, input_data, axis_shape, axis_data, output1_shape, golden1,
       output2_shape, golden2, output1_data, output2_data);
 }
diff --git a/tensorflow/lite/micro/kernels/split_v.cc b/tensorflow/lite/micro/kernels/split_v.cc
index 6aed6f7f..b77647d2 100644
--- a/tensorflow/lite/micro/kernels/split_v.cc
+++ b/tensorflow/lite/micro/kernels/split_v.cc
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -30,7 +30,7 @@ TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
                        const TfLiteEvalTensor* input, int axis_value) {
   const TfLiteIntArray* input_dims = input->dims;
   const TfLiteEvalTensor* output0 =
-      tflite::micro::GetEvalOutput(context, node, 0);
+      tflite_micro::micro::GetEvalOutput(context, node, 0);
 
   const int split_dimensions = input_dims->size;
 
@@ -41,7 +41,7 @@ TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
   const int output_count = NumOutputs(node);
   for (int i = 0; i < output_count; i++) {
     split_size +=
-        tflite::micro::GetEvalOutput(context, node, i)->dims->data[axis_value];
+        tflite_micro::micro::GetEvalOutput(context, node, i)->dims->data[axis_value];
   }
   TFLITE_DCHECK_EQ(split_size, input_dims->data[axis_value]);
   int64_t outer_size = 1;
@@ -54,12 +54,12 @@ TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,
     base_inner_size *= input_dims->data[i];
   }
 
-  const T* input_ptr = tflite::micro::GetTensorData<T>(input);
+  const T* input_ptr = tflite_micro::micro::GetTensorData<T>(input);
   for (int k = 0; k < outer_size; ++k) {
     for (int i = 0; i < output_count; ++i) {
       TfLiteEvalTensor* output_tensor =
-          tflite::micro::GetEvalOutput(context, node, i);
-      T* output_data = tflite::micro::GetTensorData<T>(output_tensor);
+          tflite_micro::micro::GetEvalOutput(context, node, i);
+      T* output_data = tflite_micro::micro::GetTensorData<T>(output_tensor);
       const int copy_size =
           output_tensor->dims->data[axis_value] * base_inner_size;
       T* output_ptr = output_data + k * copy_size;
@@ -86,10 +86,10 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 }
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
-  const TfLiteEvalTensor* input = tflite::micro::GetEvalInput(context, node, 0);
-  const TfLiteEvalTensor* axis = tflite::micro::GetEvalInput(context, node, 2);
+  const TfLiteEvalTensor* input = tflite_micro::micro::GetEvalInput(context, node, 0);
+  const TfLiteEvalTensor* axis = tflite_micro::micro::GetEvalInput(context, node, 2);
 
-  int axis_value = tflite::micro::GetTensorData<int32_t>(axis)[0];
+  int axis_value = tflite_micro::micro::GetTensorData<int32_t>(axis)[0];
   if (axis_value < 0) {
     axis_value += input->dims->size;
   }
@@ -121,7 +121,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_SPLIT_V() {
-  return tflite::micro::RegisterOp(nullptr, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/split_v_test.cc b/tensorflow/lite/micro/kernels/split_v_test.cc
index 24efee02..f2ff1033 100644
--- a/tensorflow/lite/micro/kernels/split_v_test.cc
+++ b/tensorflow/lite/micro/kernels/split_v_test.cc
@@ -20,7 +20,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 
 template <int N>
@@ -97,7 +97,7 @@ void TestSplitVFloat(int* input_dims_data, const float* input_data,
 }
 
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -121,7 +121,7 @@ TF_LITE_MICRO_TEST(SPLIT_V_ThreeOutputs) {
   int output3_shape[] = {2, 2, 3};
   float output3_values[] = {7, 8, 9, 10, 11, 12};
 
-  tflite::testing::OutputTensors<3> output_tensors;
+  tflite_micro::testing::OutputTensors<3> output_tensors;
   output_tensors.data[0] = output1_data;
   output_tensors.data[1] = output2_data;
   output_tensors.data[2] = output3_data;
@@ -134,7 +134,7 @@ TF_LITE_MICRO_TEST(SPLIT_V_ThreeOutputs) {
   output_tensors.expected_output_data[1] = output2_values;
   output_tensors.expected_output_data[2] = output3_values;
 
-  tflite::testing::TestSplitVFloat(input_shape, input_values, axis_shape,
+  tflite_micro::testing::TestSplitVFloat(input_shape, input_values, axis_shape,
                                    axis_values, split_shape, split_values,
                                    output_tensors);
 }
@@ -157,7 +157,7 @@ TF_LITE_MICRO_TEST(SPLIT_V_FourDimensionalFloatAxis0) {
   int output2_shape[] = {4, 1, 2, 2, 2};
   float output2_values[] = {9, 10, 11, 12, 13, 14, 15, 16};
 
-  tflite::testing::OutputTensors<2> output_tensors;
+  tflite_micro::testing::OutputTensors<2> output_tensors;
 
   output_tensors.data[0] = output1_data;
   output_tensors.data[1] = output2_data;
@@ -168,7 +168,7 @@ TF_LITE_MICRO_TEST(SPLIT_V_FourDimensionalFloatAxis0) {
   output_tensors.expected_output_data[0] = output1_values;
   output_tensors.expected_output_data[1] = output2_values;
 
-  tflite::testing::TestSplitVFloat(input_shape, input_values, axis_shape,
+  tflite_micro::testing::TestSplitVFloat(input_shape, input_values, axis_shape,
                                    axis_values, split_shape, split_values,
                                    output_tensors);
 }
@@ -191,7 +191,7 @@ TF_LITE_MICRO_TEST(SPLIT_V_FourDimensionalFloatAxis1) {
   int output2_shape[] = {4, 2, 1, 2, 2};
   float output2_values[] = {5, 6, 7, 8, 13, 14, 15, 16};
 
-  tflite::testing::OutputTensors<2> output_tensors;
+  tflite_micro::testing::OutputTensors<2> output_tensors;
 
   output_tensors.data[0] = output1_data;
   output_tensors.data[1] = output2_data;
@@ -202,7 +202,7 @@ TF_LITE_MICRO_TEST(SPLIT_V_FourDimensionalFloatAxis1) {
   output_tensors.expected_output_data[0] = output1_values;
   output_tensors.expected_output_data[1] = output2_values;
 
-  tflite::testing::TestSplitVFloat(input_shape, input_values, axis_shape,
+  tflite_micro::testing::TestSplitVFloat(input_shape, input_values, axis_shape,
                                    axis_values, split_shape, split_values,
                                    output_tensors);
 }
@@ -225,7 +225,7 @@ TF_LITE_MICRO_TEST(SPLIT_VFourDimensionalFloatAxis2) {
   int output2_shape[] = {4, 2, 2, 1, 2};
   float output2_values[] = {3, 4, 7, 8, 11, 12, 15, 16};
 
-  tflite::testing::OutputTensors<2> output_tensors;
+  tflite_micro::testing::OutputTensors<2> output_tensors;
 
   output_tensors.data[0] = output1_data;
   output_tensors.data[1] = output2_data;
@@ -236,7 +236,7 @@ TF_LITE_MICRO_TEST(SPLIT_VFourDimensionalFloatAxis2) {
   output_tensors.expected_output_data[0] = output1_values;
   output_tensors.expected_output_data[1] = output2_values;
 
-  tflite::testing::TestSplitVFloat(input_shape, input_values, axis_shape,
+  tflite_micro::testing::TestSplitVFloat(input_shape, input_values, axis_shape,
                                    axis_values, split_shape, split_values,
                                    output_tensors);
 }
@@ -258,7 +258,7 @@ TF_LITE_MICRO_TEST(SPLIT_V_FourDimensionalFloatAxis3) {
   int output2_shape[] = {4, 2, 2, 2, 1};
   float output2_values[] = {2, 4, 6, 8, 10, 12, 14, 16};
 
-  tflite::testing::OutputTensors<2> output_tensors;
+  tflite_micro::testing::OutputTensors<2> output_tensors;
 
   output_tensors.data[0] = output1_data;
   output_tensors.data[1] = output2_data;
@@ -269,7 +269,7 @@ TF_LITE_MICRO_TEST(SPLIT_V_FourDimensionalFloatAxis3) {
   output_tensors.expected_output_data[0] = output1_values;
   output_tensors.expected_output_data[1] = output2_values;
 
-  tflite::testing::TestSplitVFloat(input_shape, input_values, axis_shape,
+  tflite_micro::testing::TestSplitVFloat(input_shape, input_values, axis_shape,
                                    axis_values, split_shape, split_values,
                                    output_tensors);
 }
@@ -292,7 +292,7 @@ TF_LITE_MICRO_TEST(SPLIT_V_FourDimensionalFloatNegativeAxis) {
   int output2_shape[] = {4, 1, 2, 2, 2};
   float output2_values[] = {9, 10, 11, 12, 13, 14, 15, 16};
 
-  tflite::testing::OutputTensors<2> output_tensors;
+  tflite_micro::testing::OutputTensors<2> output_tensors;
 
   output_tensors.data[0] = output1_data;
   output_tensors.data[1] = output2_data;
@@ -303,7 +303,7 @@ TF_LITE_MICRO_TEST(SPLIT_V_FourDimensionalFloatNegativeAxis) {
   output_tensors.expected_output_data[0] = output1_values;
   output_tensors.expected_output_data[1] = output2_values;
 
-  tflite::testing::TestSplitVFloat(input_shape, input_values, axis_shape,
+  tflite_micro::testing::TestSplitVFloat(input_shape, input_values, axis_shape,
                                    axis_values, split_shape, split_values,
                                    output_tensors);
 }
@@ -352,7 +352,7 @@ TF_LITE_MICRO_TEST(SPLIT_V_OneDimensionalFloatAxis0) {
   int output8_shape[] = {1, 1};
   float output8_values[] = {8};
 
-  tflite::testing::OutputTensors<8> output_tensors;
+  tflite_micro::testing::OutputTensors<8> output_tensors;
 
   output_tensors.data[0] = output1_data;
   output_tensors.data[1] = output2_data;
@@ -381,7 +381,7 @@ TF_LITE_MICRO_TEST(SPLIT_V_OneDimensionalFloatAxis0) {
   output_tensors.expected_output_data[6] = output7_values;
   output_tensors.expected_output_data[7] = output8_values;
 
-  tflite::testing::TestSplitVFloat(input_shape, input_values, axis_shape,
+  tflite_micro::testing::TestSplitVFloat(input_shape, input_values, axis_shape,
                                    axis_value, split_size_shape, split,
                                    output_tensors);
 }
@@ -429,7 +429,7 @@ TF_LITE_MICRO_TEST(SPLIT_V_OneDimensionalFloatTest2) {
   int output8_shape[] = {1, 0};
   float output8_values[1] = {};
 
-  tflite::testing::OutputTensors<8> output_tensors;
+  tflite_micro::testing::OutputTensors<8> output_tensors;
 
   output_tensors.data[0] = output1_data;
   output_tensors.data[1] = output2_data;
@@ -458,7 +458,7 @@ TF_LITE_MICRO_TEST(SPLIT_V_OneDimensionalFloatTest2) {
   output_tensors.expected_output_data[6] = output7_values;
   output_tensors.expected_output_data[7] = output8_values;
 
-  tflite::testing::TestSplitVFloat(input_shape, input_values, axis_shape,
+  tflite_micro::testing::TestSplitVFloat(input_shape, input_values, axis_shape,
                                    axis_value, split_size_shape, split,
                                    output_tensors);
 }
diff --git a/tensorflow/lite/micro/kernels/squared_difference.cc b/tensorflow/lite/micro/kernels/squared_difference.cc
index 0194d0c4..5103cb30 100644
--- a/tensorflow/lite/micro/kernels/squared_difference.cc
+++ b/tensorflow/lite/micro/kernels/squared_difference.cc
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_context.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 constexpr int kInputTensor1 = 0;
 constexpr int kInputTensor2 = 1;
@@ -188,20 +188,20 @@ void EvalQuantizedSquaredDifference(TfLiteContext* context, TfLiteNode* node,
   const auto* op_data = static_cast<const OpData*>(node->user_data);
   if (data->requires_broadcast) {
     reference_integer_ops::BroadcastBinaryFunction4DSlow(
-        op_data->arithmetic_params, tflite::micro::GetTensorShape(input1),
-        tflite::micro::GetTensorData<T>(input1),
-        tflite::micro::GetTensorShape(input2),
-        tflite::micro::GetTensorData<T>(input2),
-        tflite::micro::GetTensorShape(output),
-        tflite::micro::GetTensorData<T>(output),
+        op_data->arithmetic_params, tflite_micro::micro::GetTensorShape(input1),
+        tflite_micro::micro::GetTensorData<T>(input1),
+        tflite_micro::micro::GetTensorShape(input2),
+        tflite_micro::micro::GetTensorData<T>(input2),
+        tflite_micro::micro::GetTensorShape(output),
+        tflite_micro::micro::GetTensorData<T>(output),
         reference_integer_ops::CheckArithmeticParams, SquaredDifference);
   } else {
-    const int flat_size = tflite::micro::GetTensorShape(input1).FlatSize();
+    const int flat_size = tflite_micro::micro::GetTensorShape(input1).FlatSize();
     reference_integer_ops::ElementWise(
         flat_size, op_data->arithmetic_params,
-        tflite::micro::GetTensorData<T>(input1),
-        tflite::micro::GetTensorData<T>(input2),
-        tflite::micro::GetTensorData<T>(output),
+        tflite_micro::micro::GetTensorData<T>(input1),
+        tflite_micro::micro::GetTensorData<T>(input2),
+        tflite_micro::micro::GetTensorData<T>(output),
         reference_integer_ops::CheckArithmeticParams, SquaredDifference);
   }
 }
@@ -213,20 +213,20 @@ void EvalSquaredDifference(TfLiteContext* context, TfLiteNode* node,
                            TfLiteEvalTensor* output) {
   if (data->requires_broadcast) {
     reference_ops::BroadcastBinaryFunction4DSlow<T, T, T>(
-        tflite::micro::GetTensorShape(input1),
-        tflite::micro::GetTensorData<T>(input1),
-        tflite::micro::GetTensorShape(input2),
-        tflite::micro::GetTensorData<T>(input2),
-        tflite::micro::GetTensorShape(output),
-        tflite::micro::GetTensorData<T>(output), SquaredDifference<T>);
+        tflite_micro::micro::GetTensorShape(input1),
+        tflite_micro::micro::GetTensorData<T>(input1),
+        tflite_micro::micro::GetTensorShape(input2),
+        tflite_micro::micro::GetTensorData<T>(input2),
+        tflite_micro::micro::GetTensorShape(output),
+        tflite_micro::micro::GetTensorData<T>(output), SquaredDifference<T>);
   } else {
     reference_ops::BinaryFunction<T, T, T>(
-        tflite::micro::GetTensorShape(input1),
-        tflite::micro::GetTensorData<T>(input1),
-        tflite::micro::GetTensorShape(input2),
-        tflite::micro::GetTensorData<T>(input2),
-        tflite::micro::GetTensorShape(output),
-        tflite::micro::GetTensorData<T>(output), SquaredDifference<T>);
+        tflite_micro::micro::GetTensorShape(input1),
+        tflite_micro::micro::GetTensorData<T>(input1),
+        tflite_micro::micro::GetTensorShape(input2),
+        tflite_micro::micro::GetTensorData<T>(input2),
+        tflite_micro::micro::GetTensorShape(output),
+        tflite_micro::micro::GetTensorData<T>(output), SquaredDifference<T>);
   }
 }
 
@@ -234,11 +234,11 @@ TfLiteStatus SquaredDifferenceEval(TfLiteContext* context, TfLiteNode* node) {
   OpData* data = reinterpret_cast<OpData*>(node->user_data);
 
   const TfLiteEvalTensor* input1 =
-      tflite::micro::GetEvalInput(context, node, kInputTensor1);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor1);
   const TfLiteEvalTensor* input2 =
-      tflite::micro::GetEvalInput(context, node, kInputTensor2);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor2);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
   if (output->type == kTfLiteFloat32) {
     EvalSquaredDifference<float>(context, node, data, input1, input2, output);
@@ -263,8 +263,8 @@ TfLiteStatus SquaredDifferenceEval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_SQUARED_DIFFERENCE() {
-  return tflite::micro::RegisterOp(
+  return tflite_micro::micro::RegisterOp(
       SquaredDifferenceInit, SquaredDifferencePrepare, SquaredDifferenceEval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/squared_difference_test.cc b/tensorflow/lite/micro/kernels/squared_difference_test.cc
index c7d7e42a..fc1c7c20 100644
--- a/tensorflow/lite/micro/kernels/squared_difference_test.cc
+++ b/tensorflow/lite/micro/kernels/squared_difference_test.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -42,7 +42,7 @@ void ValidateSquaredDifferenceGoldens(TfLiteTensor* tensors, int tensors_size,
   int outputs_array_data[] = {1, 2};
   TfLiteIntArray* outputs_array = IntArrayFromInts(outputs_array_data);
 
-  const TFLMRegistration registration = tflite::Register_SQUARED_DIFFERENCE();
+  const TFLMRegistration registration = tflite_micro::Register_SQUARED_DIFFERENCE();
   micro::KernelRunner runner(registration, tensors, tensors_size, inputs_array,
                              outputs_array, /*builtin_data=*/nullptr);
 
@@ -124,7 +124,7 @@ void TestSquaredDifferenceQuantized(
   int outputs_array_data[] = {1, 2};
   TfLiteIntArray* outputs_array = IntArrayFromInts(outputs_array_data);
 
-  const TFLMRegistration registration = tflite::Register_SQUARED_DIFFERENCE();
+  const TFLMRegistration registration = tflite_micro::Register_SQUARED_DIFFERENCE();
   micro::KernelRunner runner(registration, tensors, tensors_size, inputs_array,
                              outputs_array, /*builtin_data=*/nullptr);
 
@@ -141,7 +141,7 @@ void TestSquaredDifferenceQuantized(
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -152,7 +152,7 @@ TF_LITE_MICRO_TEST(FloatSquaredDifferenceSameShape) {
   const float input2_values[] = {0.5, 0.2, -1.5, 0.5};
   const float golden_values[] = {0.49, 0.0, 0.09, 0.09};
   float output_data[data_size];
-  tflite::testing::TestSquaredDifference(
+  tflite_micro::testing::TestSquaredDifference(
       inout_shape, input1_values, inout_shape, input2_values, inout_shape,
       golden_values, output_data);
 }
@@ -163,11 +163,11 @@ TF_LITE_MICRO_TEST(FloatSquaredDifferenceVariousShapes) {
   const float input2_values[] = {1.0, 0.2, 0.6, 0.4, -1.0, -0.0};
   const float golden_values[] = {9.0, 0.0, 0.09, 0.16, 4.41, 4.0};
   float output_data[data_size];
-  for (int i = 0; i < tflite::testing::kNumTestShapes; ++i) {
-    tflite::testing::TestSquaredDifference(
-        tflite::testing::test_shape[i], input1_values,
-        tflite::testing::test_shape[i], input2_values,
-        tflite::testing::test_shape[i], golden_values, output_data);
+  for (int i = 0; i < tflite_micro::testing::kNumTestShapes; ++i) {
+    tflite_micro::testing::TestSquaredDifference(
+        tflite_micro::testing::test_shape[i], input1_values,
+        tflite_micro::testing::test_shape[i], input2_values,
+        tflite_micro::testing::test_shape[i], golden_values, output_data);
   }
 }
 
@@ -180,10 +180,10 @@ TF_LITE_MICRO_TEST(FloatSquaredDifferenceWithBroadcast) {
   const float input2_values[] = {0.1};
   const float golden_values[] = {0.09, 0.01, 0.16, 0.49, 0.0001, 1.0};
   float output_data[data_size];
-  for (int i = 0; i < tflite::testing::kNumTestShapes; ++i) {
-    tflite::testing::TestSquaredDifference(
-        tflite::testing::test_shape[i], input1_values, input2_shape,
-        input2_values, tflite::testing::test_shape[i], golden_values,
+  for (int i = 0; i < tflite_micro::testing::kNumTestShapes; ++i) {
+    tflite_micro::testing::TestSquaredDifference(
+        tflite_micro::testing::test_shape[i], input1_values, input2_shape,
+        input2_values, tflite_micro::testing::test_shape[i], golden_values,
         output_data);
   }
 }
@@ -195,7 +195,7 @@ TF_LITE_MICRO_TEST(IntegerSquaredDifferenceSameShape) {
   const int32_t input2_values[] = {5, -2, -3, 5};
   const int32_t golden_values[] = {49, 16, 144, 9};
   int32_t output_data[data_size];
-  tflite::testing::TestSquaredDifference(
+  tflite_micro::testing::TestSquaredDifference(
       inout_shape, input1_values, inout_shape, input2_values, inout_shape,
       golden_values, output_data);
 }
@@ -206,11 +206,11 @@ TF_LITE_MICRO_TEST(IntegerSquaredDifferenceVariousShapes) {
   const int32_t input2_values[] = {1, 2, 6, 5, -5, -20};
   const int32_t golden_values[] = {441, 0, 9, 9, 256, 0};
   int32_t output_data[data_size];
-  for (int i = 0; i < tflite::testing::kNumTestShapes; ++i) {
-    tflite::testing::TestSquaredDifference(
-        tflite::testing::test_shape[i], input1_values,
-        tflite::testing::test_shape[i], input2_values,
-        tflite::testing::test_shape[i], golden_values, output_data);
+  for (int i = 0; i < tflite_micro::testing::kNumTestShapes; ++i) {
+    tflite_micro::testing::TestSquaredDifference(
+        tflite_micro::testing::test_shape[i], input1_values,
+        tflite_micro::testing::test_shape[i], input2_values,
+        tflite_micro::testing::test_shape[i], golden_values, output_data);
   }
 }
 
@@ -223,10 +223,10 @@ TF_LITE_MICRO_TEST(IntegerSquaredDifferenceWithBroadcast) {
   const int32_t input2_values[] = {3};
   const int32_t golden_values[] = {529, 49, 16, 0, 4, 100};
   int32_t output_data[data_size];
-  for (int i = 0; i < tflite::testing::kNumTestShapes; ++i) {
-    tflite::testing::TestSquaredDifference(
-        tflite::testing::test_shape[i], input1_values, input2_shape,
-        input2_values, tflite::testing::test_shape[i], golden_values,
+  for (int i = 0; i < tflite_micro::testing::kNumTestShapes; ++i) {
+    tflite_micro::testing::TestSquaredDifference(
+        tflite_micro::testing::test_shape[i], input1_values, input2_shape,
+        input2_values, tflite_micro::testing::test_shape[i], golden_values,
         output_data);
   }
 }
@@ -242,7 +242,7 @@ TF_LITE_MICRO_TEST(QuantizedSquaredDifferenceSameShape) {
   int8_t input1_int8[data_size];
   int8_t input2_int8[data_size];
   int8_t output_int8[data_size];
-  tflite::testing::TestSquaredDifferenceQuantized(
+  tflite_micro::testing::TestSquaredDifferenceQuantized(
       inout_shape, input1_values, input1_int8, -1.2f, 0.8f, inout_shape,
       input2_values, input2_int8, -1.5f, 0.5f, inout_shape, output_int8, 0.0f,
       0.5f, output_dequantized, golden_values, 2.0f / 255.0f);
@@ -255,7 +255,7 @@ TF_LITE_MICRO_TEST(QuantizedSquaredDifferenceSameShape) {
   // -qmax).
   // TODO(b/269352046): understand the tolerance level
   // http://b/269352046#comment7
-  tflite::testing::TestSquaredDifferenceQuantized(
+  tflite_micro::testing::TestSquaredDifferenceQuantized(
       inout_shape, input1_values, input1_int16, -1.2f, 1.2f, inout_shape,
       input2_values, input2_int16, -1.5f, 1.5f, inout_shape, output_int16,
       -0.5f, 0.5f, output_dequantized, golden_values, 6.0f / 32768.0f,
@@ -272,11 +272,11 @@ TF_LITE_MICRO_TEST(QuantizedSquaredDifferenceVariousShapes) {
   int8_t input2_int8[data_size];
   int8_t output_int8[data_size];
   float output_dequantized[data_size];
-  for (int i = 0; i < tflite::testing::kNumTestShapes; ++i) {
-    tflite::testing::TestSquaredDifferenceQuantized(
-        tflite::testing::test_shape[i], input1_values, input1_int8, -2.0f, 1.7f,
-        tflite::testing::test_shape[i], input2_values, input2_int8, -1.0f, 1.0f,
-        tflite::testing::test_shape[i], output_int8, 0.0f, 9.0f,
+  for (int i = 0; i < tflite_micro::testing::kNumTestShapes; ++i) {
+    tflite_micro::testing::TestSquaredDifferenceQuantized(
+        tflite_micro::testing::test_shape[i], input1_values, input1_int8, -2.0f, 1.7f,
+        tflite_micro::testing::test_shape[i], input2_values, input2_int8, -1.0f, 1.0f,
+        tflite_micro::testing::test_shape[i], output_int8, 0.0f, 9.0f,
         output_dequantized, golden_values, 18.0f / 255.0f);
   }
 
@@ -286,11 +286,11 @@ TF_LITE_MICRO_TEST(QuantizedSquaredDifferenceVariousShapes) {
   int16_t output_int16[data_size];
   // Symmetrical quantization: (rmin == -rmax), requires narrow range (qmin =
   // -qmax).
-  for (int i = 0; i < tflite::testing::kNumTestShapes; ++i) {
-    tflite::testing::TestSquaredDifferenceQuantized(
-        tflite::testing::test_shape[i], input1_values, input1_int16, -2.0f,
-        2.0f, tflite::testing::test_shape[i], input2_values, input2_int16,
-        -1.0f, 1.0f, tflite::testing::test_shape[i], output_int16, -9.0f, 9.0f,
+  for (int i = 0; i < tflite_micro::testing::kNumTestShapes; ++i) {
+    tflite_micro::testing::TestSquaredDifferenceQuantized(
+        tflite_micro::testing::test_shape[i], input1_values, input1_int16, -2.0f,
+        2.0f, tflite_micro::testing::test_shape[i], input2_values, input2_int16,
+        -1.0f, 1.0f, tflite_micro::testing::test_shape[i], output_int16, -9.0f, 9.0f,
         output_dequantized, golden_values, 18.0f / 32768.0f,
         /*narrow_range=*/true);
   }
@@ -310,11 +310,11 @@ TF_LITE_MICRO_TEST(FloatSquaredDifferenceWithBroadcast) {
   int8_t input2_int8[data_size];
   int8_t output_int8[data_size];
   float output_dequantized[data_size];
-  for (int i = 0; i < tflite::testing::kNumTestShapes; ++i) {
-    tflite::testing::TestSquaredDifferenceQuantized(
-        tflite::testing::test_shape[i], input1_values, input1_int8, -0.2f, 1.1f,
+  for (int i = 0; i < tflite_micro::testing::kNumTestShapes; ++i) {
+    tflite_micro::testing::TestSquaredDifferenceQuantized(
+        tflite_micro::testing::test_shape[i], input1_values, input1_int8, -0.2f, 1.1f,
         input2_shape, input2_values, input2_int8, 0.0f, 1.0f,
-        tflite::testing::test_shape[i], output_int8, 0.0f, 1.0f,
+        tflite_micro::testing::test_shape[i], output_int8, 0.0f, 1.0f,
         output_dequantized, golden_values, 2.0f / 255.0f);
   }
 
@@ -322,11 +322,11 @@ TF_LITE_MICRO_TEST(FloatSquaredDifferenceWithBroadcast) {
   int16_t input1_int16[data_size];
   int16_t input2_int16[data_size];
   int16_t output_int16[data_size];
-  for (int i = 0; i < tflite::testing::kNumTestShapes; ++i) {
-    tflite::testing::TestSquaredDifferenceQuantized(
-        tflite::testing::test_shape[i], input1_values, input1_int16, -1.1f,
+  for (int i = 0; i < tflite_micro::testing::kNumTestShapes; ++i) {
+    tflite_micro::testing::TestSquaredDifferenceQuantized(
+        tflite_micro::testing::test_shape[i], input1_values, input1_int16, -1.1f,
         1.1f, input2_shape, input2_values, input2_int16, -1.0f, 1.0f,
-        tflite::testing::test_shape[i], output_int16, -1.0f, 1.0f,
+        tflite_micro::testing::test_shape[i], output_int16, -1.0f, 1.0f,
         output_dequantized, golden_values, 2.0f / 32768.0f,
         /*narrow_range=*/true);
   }
diff --git a/tensorflow/lite/micro/kernels/squeeze.cc b/tensorflow/lite/micro/kernels/squeeze.cc
index e556755f..03088aa1 100644
--- a/tensorflow/lite/micro/kernels/squeeze.cc
+++ b/tensorflow/lite/micro/kernels/squeeze.cc
@@ -24,7 +24,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/memory_helpers.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 struct SqueezeContext {
@@ -88,7 +88,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 }
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
-  const TfLiteEvalTensor* input = tflite::micro::GetEvalInput(context, node, 0);
+  const TfLiteEvalTensor* input = tflite_micro::micro::GetEvalInput(context, node, 0);
 
   if (input->type == kTfLiteString) {
     MicroPrintf("Type %s (%d) not supported.", TfLiteTypeGetName(input->type),
@@ -96,7 +96,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
     return kTfLiteError;
   }
 
-  TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, 0);
+  TfLiteEvalTensor* output = tflite_micro::micro::GetEvalOutput(context, node, 0);
   size_t input_byte_size;
   size_t output_byte_size;
   TF_LITE_ENSURE_OK(context,
@@ -112,7 +112,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_SQUEEZE() {
-  return tflite::micro::RegisterOp(nullptr, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/squeeze_test.cc b/tensorflow/lite/micro/kernels/squeeze_test.cc
index 149413bd..a6229a8b 100644
--- a/tensorflow/lite/micro/kernels/squeeze_test.cc
+++ b/tensorflow/lite/micro/kernels/squeeze_test.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -71,7 +71,7 @@ void TestSqueezeOp(int* input_dims_data, const int32_t* input_data,
 }
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -79,11 +79,11 @@ TF_LITE_MICRO_TEST(SqueezeAll) {
   int32_t output_data[24];
   TfLiteSqueezeParams squeeze_params = {{}, 0};
 
-  tflite::testing::TestSqueezeOp(tflite::testing::input_dims_data_common,
-                                 tflite::testing::input_data_common,
-                                 tflite::testing::output_dims_data_common,
-                                 output_data, tflite::testing::golden_common,
-                                 tflite::testing::expected_output_size_common,
+  tflite_micro::testing::TestSqueezeOp(tflite_micro::testing::input_dims_data_common,
+                                 tflite_micro::testing::input_data_common,
+                                 tflite_micro::testing::output_dims_data_common,
+                                 output_data, tflite_micro::testing::golden_common,
+                                 tflite_micro::testing::expected_output_size_common,
                                  &squeeze_params);
 }
 
@@ -92,22 +92,22 @@ TF_LITE_MICRO_TEST(SqueezeSelectedAxis) {
   TfLiteSqueezeParams squeeze_params = {{2}, 1};
   int output_dims_data_common[] = {2, 1, 24};
 
-  tflite::testing::TestSqueezeOp(
-      tflite::testing::input_dims_data_common,
-      tflite::testing::input_data_common, output_dims_data_common, output_data,
-      tflite::testing::golden_common,
-      tflite::testing::expected_output_size_common, &squeeze_params);
+  tflite_micro::testing::TestSqueezeOp(
+      tflite_micro::testing::input_dims_data_common,
+      tflite_micro::testing::input_data_common, output_dims_data_common, output_data,
+      tflite_micro::testing::golden_common,
+      tflite_micro::testing::expected_output_size_common, &squeeze_params);
 }
 
 TF_LITE_MICRO_TEST(SqueezeNegativeAxis) {
   int32_t output_data[24];
   TfLiteSqueezeParams squeeze_params = {{-1, 0}, 2};
 
-  tflite::testing::TestSqueezeOp(tflite::testing::input_dims_data_common,
-                                 tflite::testing::input_data_common,
-                                 tflite::testing::output_dims_data_common,
-                                 output_data, tflite::testing::golden_common,
-                                 tflite::testing::expected_output_size_common,
+  tflite_micro::testing::TestSqueezeOp(tflite_micro::testing::input_dims_data_common,
+                                 tflite_micro::testing::input_data_common,
+                                 tflite_micro::testing::output_dims_data_common,
+                                 output_data, tflite_micro::testing::golden_common,
+                                 tflite_micro::testing::expected_output_size_common,
                                  &squeeze_params);
 }
 
@@ -121,7 +121,7 @@ TF_LITE_MICRO_TEST(SqueezeAllDims) {
   int32_t output_data[24];
   TfLiteSqueezeParams squeeze_params = {{}, 0};
 
-  tflite::testing::TestSqueezeOp(input_dims_data, input_data, output_dims_data,
+  tflite_micro::testing::TestSqueezeOp(input_dims_data, input_data, output_dims_data,
                                  output_data, golden, expected_output_size,
                                  &squeeze_params);
 }
diff --git a/tensorflow/lite/micro/kernels/strided_slice.cc b/tensorflow/lite/micro/kernels/strided_slice.cc
index 743f45b2..95f54d11 100644
--- a/tensorflow/lite/micro/kernels/strided_slice.cc
+++ b/tensorflow/lite/micro/kernels/strided_slice.cc
@@ -25,7 +25,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/strided_slice.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -35,44 +35,44 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
       *(static_cast<const StridedSliceParams*>(node->user_data));
 
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kStridedSliceInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kStridedSliceInputTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kStridedSliceOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kStridedSliceOutputTensor);
   switch (output->type) {
     case kTfLiteFloat32:
       reference_ops::StridedSlice(op_params,
-                                  tflite::micro::GetTensorShape(input),
-                                  tflite::micro::GetTensorData<float>(input),
-                                  tflite::micro::GetTensorShape(output),
-                                  tflite::micro::GetTensorData<float>(output));
+                                  tflite_micro::micro::GetTensorShape(input),
+                                  tflite_micro::micro::GetTensorData<float>(input),
+                                  tflite_micro::micro::GetTensorShape(output),
+                                  tflite_micro::micro::GetTensorData<float>(output));
       break;
     case kTfLiteInt8:
       reference_ops::StridedSlice(op_params,
-                                  tflite::micro::GetTensorShape(input),
-                                  tflite::micro::GetTensorData<int8_t>(input),
-                                  tflite::micro::GetTensorShape(output),
-                                  tflite::micro::GetTensorData<int8_t>(output));
+                                  tflite_micro::micro::GetTensorShape(input),
+                                  tflite_micro::micro::GetTensorData<int8_t>(input),
+                                  tflite_micro::micro::GetTensorShape(output),
+                                  tflite_micro::micro::GetTensorData<int8_t>(output));
       break;
     case kTfLiteInt16:
       reference_ops::StridedSlice(
-          op_params, tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<int16_t>(input),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<int16_t>(output));
+          op_params, tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<int16_t>(input),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<int16_t>(output));
       break;
     case kTfLiteInt32:
       reference_ops::StridedSlice(
-          op_params, tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<int32_t>(input),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<int32_t>(output));
+          op_params, tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<int32_t>(input),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<int32_t>(output));
       break;
     case kTfLiteBool:
       reference_ops::StridedSlice(op_params,
-                                  tflite::micro::GetTensorShape(input),
-                                  tflite::micro::GetTensorData<bool>(input),
-                                  tflite::micro::GetTensorShape(output),
-                                  tflite::micro::GetTensorData<bool>(output));
+                                  tflite_micro::micro::GetTensorShape(input),
+                                  tflite_micro::micro::GetTensorData<bool>(input),
+                                  tflite_micro::micro::GetTensorShape(output),
+                                  tflite_micro::micro::GetTensorData<bool>(output));
       break;
     default:
       MicroPrintf("Type %s (%d) not supported.", TfLiteTypeGetName(input->type),
@@ -85,7 +85,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_STRIDED_SLICE() {
-  return tflite::micro::RegisterOp(StridedSliceInit, StridedSlicePrepare, Eval);
+  return tflite_micro::micro::RegisterOp(StridedSliceInit, StridedSlicePrepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/strided_slice.h b/tensorflow/lite/micro/kernels/strided_slice.h
index ea9413f3..b62cf746 100644
--- a/tensorflow/lite/micro/kernels/strided_slice.h
+++ b/tensorflow/lite/micro/kernels/strided_slice.h
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/c/common.h"
 #include "tensorflow/lite/micro/micro_common.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 constexpr int kStridedSliceInputTensor = 0;
 constexpr int kStridedSliceBeginTensor = 1;
@@ -35,6 +35,6 @@ void* StridedSliceInit(TfLiteContext* context, const char* buffer,
 
 TfLiteStatus StridedSlicePrepare(TfLiteContext* context, TfLiteNode* node);
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_STRIDED_SLICE_H_
diff --git a/tensorflow/lite/micro/kernels/strided_slice_common.cc b/tensorflow/lite/micro/kernels/strided_slice_common.cc
index 165e1f39..0ce3403d 100644
--- a/tensorflow/lite/micro/kernels/strided_slice_common.cc
+++ b/tensorflow/lite/micro/kernels/strided_slice_common.cc
@@ -25,7 +25,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/strided_slice.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -65,9 +65,9 @@ struct StridedSliceContext {
 // implementation, the 1-3D tensors are mapped to 4D.
 const int kMaxDim = 4;
 
-tflite::StridedSliceParams BuildStridedSliceParams(
+tflite_micro::StridedSliceParams BuildStridedSliceParams(
     StridedSliceContext* op_context) {
-  tflite::StridedSliceParams op_params{};
+  tflite_micro::StridedSliceParams op_params{};
   op_params.start_indices_count = op_context->dims;
   op_params.stop_indices_count = op_context->dims;
   op_params.strides_count = op_context->dims;
@@ -91,8 +91,8 @@ tflite::StridedSliceParams BuildStridedSliceParams(
 // long as the caller ensures the indexing tensors are present.
 TfLiteStatus CheckOutputSize(TfLiteContext* context,
                              StridedSliceContext* op_context) {
-  using ::tflite::strided_slice::StartForAxis;
-  using ::tflite::strided_slice::StopForAxis;
+  using ::tflite_micro::strided_slice::StartForAxis;
+  using ::tflite_micro::strided_slice::StopForAxis;
   TfLiteIntArray* output_shape = op_context->output->dims;
   int shape_size = 0;
   auto op_params = BuildStridedSliceParams(op_context);
@@ -146,4 +146,4 @@ TfLiteStatus StridedSlicePrepare(TfLiteContext* context, TfLiteNode* node) {
   return CheckOutputSize(context, &op_context);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/strided_slice_test.cc b/tensorflow/lite/micro/kernels/strided_slice_test.cc
index 16c3d9c8..1db088d7 100644
--- a/tensorflow/lite/micro/kernels/strided_slice_test.cc
+++ b/tensorflow/lite/micro/kernels/strided_slice_test.cc
@@ -20,7 +20,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -125,7 +125,7 @@ void TestStridedSliceQuantized(int* input_shape, int* begin_shape,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -144,7 +144,7 @@ TF_LITE_MICRO_TEST(UnsupportedInputSize) {
 
   TfLiteStridedSliceParams builtin_data = {};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, true);
@@ -165,7 +165,7 @@ TF_LITE_MICRO_TEST(In1D) {
 
   TfLiteStridedSliceParams builtin_data = {};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -186,7 +186,7 @@ TF_LITE_MICRO_TEST(In1D_EmptyOutput) {
 
   TfLiteStridedSliceParams builtin_data = {};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -207,7 +207,7 @@ TF_LITE_MICRO_TEST(In1D_NegativeBegin) {
 
   TfLiteStridedSliceParams builtin_data = {};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -228,7 +228,7 @@ TF_LITE_MICRO_TEST(In1D_OutOfRangeBegin) {
 
   TfLiteStridedSliceParams builtin_data = {};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -249,7 +249,7 @@ TF_LITE_MICRO_TEST(In1D_NegativeEnd) {
 
   TfLiteStridedSliceParams builtin_data = {};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -270,7 +270,7 @@ TF_LITE_MICRO_TEST(In1D_OutOfRangeEnd) {
 
   TfLiteStridedSliceParams builtin_data = {};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -291,7 +291,7 @@ TF_LITE_MICRO_TEST(In1D_BeginMask) {
 
   TfLiteStridedSliceParams builtin_data = {1, 0, 0, 0, 0, false};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -312,7 +312,7 @@ TF_LITE_MICRO_TEST(In1D_NegativeBeginNegativeStride) {
 
   TfLiteStridedSliceParams builtin_data = {};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -333,7 +333,7 @@ TF_LITE_MICRO_TEST(In1D_OutOfRangeBeginNegativeStride) {
 
   TfLiteStridedSliceParams builtin_data = {};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -354,7 +354,7 @@ TF_LITE_MICRO_TEST(In1D_NegativeEndNegativeStride) {
 
   TfLiteStridedSliceParams builtin_data = {};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -375,7 +375,7 @@ TF_LITE_MICRO_TEST(In1D_OutOfRangeEndNegativeStride) {
 
   TfLiteStridedSliceParams builtin_data = {};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -396,7 +396,7 @@ TF_LITE_MICRO_TEST(In1D_EndMask) {
 
   TfLiteStridedSliceParams builtin_data = {0, 1, 0, 0, 0, false};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -417,7 +417,7 @@ TF_LITE_MICRO_TEST(In1D_NegStride) {
 
   TfLiteStridedSliceParams builtin_data = {};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -438,7 +438,7 @@ TF_LITE_MICRO_TEST(In1D_EvenLenStride2) {
 
   TfLiteStridedSliceParams builtin_data = {};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -459,7 +459,7 @@ TF_LITE_MICRO_TEST(In1D_OddLenStride2) {
 
   TfLiteStridedSliceParams builtin_data = {};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -480,7 +480,7 @@ TF_LITE_MICRO_TEST(In2D_Identity) {
 
   TfLiteStridedSliceParams builtin_data = {};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -501,7 +501,7 @@ TF_LITE_MICRO_TEST(In2D) {
 
   TfLiteStridedSliceParams builtin_data = {};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -522,7 +522,7 @@ TF_LITE_MICRO_TEST(In2D_Stride2) {
 
   TfLiteStridedSliceParams builtin_data = {};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -543,7 +543,7 @@ TF_LITE_MICRO_TEST(In2D_NegStride) {
 
   TfLiteStridedSliceParams builtin_data = {};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -564,7 +564,7 @@ TF_LITE_MICRO_TEST(In2D_BeginMask) {
 
   TfLiteStridedSliceParams builtin_data = {1, 0, 0, 0, 0, false};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -585,7 +585,7 @@ TF_LITE_MICRO_TEST(In2D_EndMask) {
 
   TfLiteStridedSliceParams builtin_data = {0, 2, 0, 0, 0, false};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -606,7 +606,7 @@ TF_LITE_MICRO_TEST(In2D_NegStrideBeginMask) {
 
   TfLiteStridedSliceParams builtin_data = {2, 0, 0, 0, 0, false};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -627,7 +627,7 @@ TF_LITE_MICRO_TEST(In2D_NegStrideEndMask) {
 
   TfLiteStridedSliceParams builtin_data = {0, 2, 0, 0, 0, false};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -648,7 +648,7 @@ TF_LITE_MICRO_TEST(In3D_Identity) {
 
   TfLiteStridedSliceParams builtin_data = {};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -669,7 +669,7 @@ TF_LITE_MICRO_TEST(In3D_NegStride) {
 
   TfLiteStridedSliceParams builtin_data = {};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -690,7 +690,7 @@ TF_LITE_MICRO_TEST(In3D_Strided2) {
 
   TfLiteStridedSliceParams builtin_data = {};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -711,7 +711,7 @@ TF_LITE_MICRO_TEST(In1D_ShrinkAxisMask1) {
 
   TfLiteStridedSliceParams builtin_data = {};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -732,7 +732,7 @@ TF_LITE_MICRO_TEST(In1D_ShrinkAxisMask1_NegativeSlice) {
 
   TfLiteStridedSliceParams builtin_data = {0, 0, 0, 0, 1, false};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -753,7 +753,7 @@ TF_LITE_MICRO_TEST(In2D_ShrinkAxis3_NegativeSlice) {
 
   TfLiteStridedSliceParams builtin_data = {0, 0, 0, 0, 3, false};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -774,7 +774,7 @@ TF_LITE_MICRO_TEST(In2D_ShrinkAxis2_BeginEndAxis1_NegativeSlice) {
 
   TfLiteStridedSliceParams builtin_data = {1, 1, 0, 0, 2, false};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -795,7 +795,7 @@ TF_LITE_MICRO_TEST(In1D_BeginMaskShrinkAxisMask1) {
 
   TfLiteStridedSliceParams builtin_data = {1, 0, 0, 0, 1, false};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -816,7 +816,7 @@ TF_LITE_MICRO_TEST(In2D_ShrinkAxisMask1) {
 
   TfLiteStridedSliceParams builtin_data = {0, 0, 0, 0, 1, false};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -837,7 +837,7 @@ TF_LITE_MICRO_TEST(In2D_ShrinkAxisMask2) {
 
   TfLiteStridedSliceParams builtin_data = {0, 0, 0, 0, 2, false};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -858,7 +858,7 @@ TF_LITE_MICRO_TEST(In2D_ShrinkAxisMask3) {
 
   TfLiteStridedSliceParams builtin_data = {0, 0, 0, 0, 3, false};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -879,7 +879,7 @@ TF_LITE_MICRO_TEST(In3D_IdentityShrinkAxis1) {
 
   TfLiteStridedSliceParams builtin_data = {0, 0, 0, 0, 1, false};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -900,7 +900,7 @@ TF_LITE_MICRO_TEST(In3D_IdentityShrinkAxis2) {
 
   TfLiteStridedSliceParams builtin_data = {0, 0, 0, 0, 2, false};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -921,7 +921,7 @@ TF_LITE_MICRO_TEST(In3D_IdentityShrinkAxis3) {
 
   TfLiteStridedSliceParams builtin_data = {0, 0, 0, 0, 3, false};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -942,7 +942,7 @@ TF_LITE_MICRO_TEST(In3D_IdentityShrinkAxis4) {
 
   TfLiteStridedSliceParams builtin_data = {0, 0, 0, 0, 4, false};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -963,7 +963,7 @@ TF_LITE_MICRO_TEST(In3D_IdentityShrinkAxis5) {
 
   TfLiteStridedSliceParams builtin_data = {0, 0, 0, 0, 5, false};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -984,7 +984,7 @@ TF_LITE_MICRO_TEST(In3D_IdentityShrinkAxis6) {
 
   TfLiteStridedSliceParams builtin_data = {0, 0, 0, 0, 6, false};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -1005,7 +1005,7 @@ TF_LITE_MICRO_TEST(In3D_IdentityShrinkAxis7) {
 
   TfLiteStridedSliceParams builtin_data = {0, 0, 0, 0, 7, false};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -1027,7 +1027,7 @@ TF_LITE_MICRO_TEST(RunTwice) {
 
   TfLiteStridedSliceParams builtin_data = {1, 0, 0, 0, 0, false};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false, 2);
@@ -1048,7 +1048,7 @@ TF_LITE_MICRO_TEST(In3D_IdentityShrinkAxis1int8) {
 
   TfLiteStridedSliceParams builtin_data = {0, 0, 0, 0, 1, false};
 
-  tflite::testing::TestStridedSliceQuantized(
+  tflite_micro::testing::TestStridedSliceQuantized(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -1069,7 +1069,7 @@ TF_LITE_MICRO_TEST(In3D_IdentityShrinkAxis1int16) {
 
   TfLiteStridedSliceParams builtin_data = {0, 0, 0, 0, 1, false};
 
-  tflite::testing::TestStridedSliceQuantized(
+  tflite_micro::testing::TestStridedSliceQuantized(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -1090,7 +1090,7 @@ TF_LITE_MICRO_TEST(In3D_IdentityShrinkAxis1int32) {
 
   TfLiteStridedSliceParams builtin_data = {0, 0, 0, 0, 1, false};
 
-  tflite::testing::TestStridedSliceQuantized(
+  tflite_micro::testing::TestStridedSliceQuantized(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -1111,7 +1111,7 @@ TF_LITE_MICRO_TEST(In3D_Strided2int32) {
 
   TfLiteStridedSliceParams builtin_data = {};
 
-  tflite::testing::TestStridedSliceQuantized(
+  tflite_micro::testing::TestStridedSliceQuantized(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -1133,7 +1133,7 @@ TF_LITE_MICRO_TEST(In3D_Strided2bool) {
 
   TfLiteStridedSliceParams builtin_data = {};
 
-  tflite::testing::TestStridedSliceQuantized<bool>(
+  tflite_micro::testing::TestStridedSliceQuantized<bool>(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -1154,7 +1154,7 @@ TF_LITE_MICRO_TEST(MinusThreeMinusFourMinusOne) {
 
   TfLiteStridedSliceParams builtin_data = {0, 0, 0, 0, 0, false};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -1175,7 +1175,7 @@ TF_LITE_MICRO_TEST(MinusFourMinusThreeOne) {
 
   TfLiteStridedSliceParams builtin_data = {0, 0, 0, 0, 0, false};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -1196,7 +1196,7 @@ TF_LITE_MICRO_TEST(In3D_BackwardSmallBeginEndMask) {
 
   TfLiteStridedSliceParams builtin_data = {0, 1, 0, 0, 0, false};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -1217,7 +1217,7 @@ TF_LITE_MICRO_TEST(OneOneOne) {
 
   TfLiteStridedSliceParams builtin_data = {0, 0, 0, 0, 0, false};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false);
@@ -1238,7 +1238,7 @@ TF_LITE_MICRO_TEST(StrideOutOfBounds) {
 
   TfLiteStridedSliceParams builtin_data = {0, 0, 0, 0, 1, false};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false, 1, true);
@@ -1259,7 +1259,7 @@ TF_LITE_MICRO_TEST(OutOfBounds) {
 
   TfLiteStridedSliceParams builtin_data = {0, 0, 0, 0, 1, false};
 
-  tflite::testing::TestStridedSliceFloat(
+  tflite_micro::testing::TestStridedSliceFloat(
       input_shape, begin_shape, end_shape, strides_shape, &builtin_data,
       input_data, begin_data, end_data, strides_data, output_shape, output_data,
       golden, false, 1, true);
diff --git a/tensorflow/lite/micro/kernels/sub.cc b/tensorflow/lite/micro/kernels/sub.cc
index 930bc0ba..79ee8e9d 100644
--- a/tensorflow/lite/micro/kernels/sub.cc
+++ b/tensorflow/lite/micro/kernels/sub.cc
@@ -29,7 +29,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 void* SubInit(TfLiteContext* context, const char* buffer, size_t length) {
   TFLITE_DCHECK(context->AllocatePersistentBuffer != nullptr);
@@ -42,24 +42,24 @@ void EvalSub(TfLiteContext* context, TfLiteNode* node, TfLiteSubParams* params,
   float output_activation_min, output_activation_max;
   CalculateActivationRange(params->activation, &output_activation_min,
                            &output_activation_max);
-  tflite::ArithmeticParams op_params;
+  tflite_micro::ArithmeticParams op_params;
   SetActivationParams(output_activation_min, output_activation_max, &op_params);
   if (data->requires_broadcast) {
-    tflite::reference_ops::BroadcastSubSlow(
-        op_params, tflite::micro::GetTensorShape(input1),
-        tflite::micro::GetTensorData<float>(input1),
-        tflite::micro::GetTensorShape(input2),
-        tflite::micro::GetTensorData<float>(input2),
-        tflite::micro::GetTensorShape(output),
-        tflite::micro::GetTensorData<float>(output));
+    tflite_micro::reference_ops::BroadcastSubSlow(
+        op_params, tflite_micro::micro::GetTensorShape(input1),
+        tflite_micro::micro::GetTensorData<float>(input1),
+        tflite_micro::micro::GetTensorShape(input2),
+        tflite_micro::micro::GetTensorData<float>(input2),
+        tflite_micro::micro::GetTensorShape(output),
+        tflite_micro::micro::GetTensorData<float>(output));
   } else {
-    tflite::reference_ops::SubWithActivation(
-        op_params, tflite::micro::GetTensorShape(input1),
-        tflite::micro::GetTensorData<float>(input1),
-        tflite::micro::GetTensorShape(input2),
-        tflite::micro::GetTensorData<float>(input2),
-        tflite::micro::GetTensorShape(output),
-        tflite::micro::GetTensorData<float>(output));
+    tflite_micro::reference_ops::SubWithActivation(
+        op_params, tflite_micro::micro::GetTensorShape(input1),
+        tflite_micro::micro::GetTensorData<float>(input1),
+        tflite_micro::micro::GetTensorShape(input2),
+        tflite_micro::micro::GetTensorData<float>(input2),
+        tflite_micro::micro::GetTensorShape(output),
+        tflite_micro::micro::GetTensorData<float>(output));
   }
 }
 
@@ -68,7 +68,7 @@ TfLiteStatus EvalSubQuantized(TfLiteContext* context, TfLiteNode* node,
                               const TfLiteEvalTensor* input1,
                               const TfLiteEvalTensor* input2,
                               TfLiteEvalTensor* output) {
-  tflite::ArithmeticParams op_params;
+  tflite_micro::ArithmeticParams op_params;
   op_params.left_shift = data->left_shift;
   op_params.input1_offset = data->input1_offset;
   op_params.input1_multiplier = data->input1_multiplier;
@@ -82,47 +82,47 @@ TfLiteStatus EvalSubQuantized(TfLiteContext* context, TfLiteNode* node,
   SetActivationParams(data->output_activation_min, data->output_activation_max,
                       &op_params);
   bool need_broadcast = reference_ops::ProcessBroadcastShapes(
-      tflite::micro::GetTensorShape(input1),
-      tflite::micro::GetTensorShape(input2), &op_params);
+      tflite_micro::micro::GetTensorShape(input1),
+      tflite_micro::micro::GetTensorShape(input2), &op_params);
 
   switch (output->type) {
     case kTfLiteInt8: {
       if (need_broadcast) {
-        tflite::reference_ops::BroadcastQuantSubSlow(
-            op_params, tflite::micro::GetTensorShape(input1),
-            tflite::micro::GetTensorData<int8_t>(input1),
-            tflite::micro::GetTensorShape(input2),
-            tflite::micro::GetTensorData<int8_t>(input2),
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<int8_t>(output));
+        tflite_micro::reference_ops::BroadcastQuantSubSlow(
+            op_params, tflite_micro::micro::GetTensorShape(input1),
+            tflite_micro::micro::GetTensorData<int8_t>(input1),
+            tflite_micro::micro::GetTensorShape(input2),
+            tflite_micro::micro::GetTensorData<int8_t>(input2),
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int8_t>(output));
       } else {
-        tflite::reference_ops::Sub(
-            op_params, tflite::micro::GetTensorShape(input1),
-            tflite::micro::GetTensorData<int8_t>(input1),
-            tflite::micro::GetTensorShape(input2),
-            tflite::micro::GetTensorData<int8_t>(input2),
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<int8_t>(output));
+        tflite_micro::reference_ops::Sub(
+            op_params, tflite_micro::micro::GetTensorShape(input1),
+            tflite_micro::micro::GetTensorData<int8_t>(input1),
+            tflite_micro::micro::GetTensorShape(input2),
+            tflite_micro::micro::GetTensorData<int8_t>(input2),
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int8_t>(output));
       }
       break;
     }
     case kTfLiteInt16: {
       if (need_broadcast) {
-        tflite::reference_ops::BroadcastQuantSubSlow(
-            op_params, tflite::micro::GetTensorShape(input1),
-            tflite::micro::GetTensorData<int16_t>(input1),
-            tflite::micro::GetTensorShape(input2),
-            tflite::micro::GetTensorData<int16_t>(input2),
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<int16_t>(output));
+        tflite_micro::reference_ops::BroadcastQuantSubSlow(
+            op_params, tflite_micro::micro::GetTensorShape(input1),
+            tflite_micro::micro::GetTensorData<int16_t>(input1),
+            tflite_micro::micro::GetTensorShape(input2),
+            tflite_micro::micro::GetTensorData<int16_t>(input2),
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int16_t>(output));
       } else {
-        tflite::reference_ops::Sub(
-            op_params, tflite::micro::GetTensorShape(input1),
-            tflite::micro::GetTensorData<int16_t>(input1),
-            tflite::micro::GetTensorShape(input2),
-            tflite::micro::GetTensorData<int16_t>(input2),
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<int16_t>(output));
+        tflite_micro::reference_ops::Sub(
+            op_params, tflite_micro::micro::GetTensorShape(input1),
+            tflite_micro::micro::GetTensorData<int16_t>(input1),
+            tflite_micro::micro::GetTensorShape(input2),
+            tflite_micro::micro::GetTensorData<int16_t>(input2),
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int16_t>(output));
       }
       break;
     }
@@ -138,11 +138,11 @@ TfLiteStatus SubEval(TfLiteContext* context, TfLiteNode* node) {
   auto* params = reinterpret_cast<TfLiteSubParams*>(node->builtin_data);
 
   const TfLiteEvalTensor* input1 =
-      tflite::micro::GetEvalInput(context, node, kSubInputTensor1);
+      tflite_micro::micro::GetEvalInput(context, node, kSubInputTensor1);
   const TfLiteEvalTensor* input2 =
-      tflite::micro::GetEvalInput(context, node, kSubInputTensor2);
+      tflite_micro::micro::GetEvalInput(context, node, kSubInputTensor2);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kSubOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kSubOutputTensor);
 
   TFLITE_DCHECK(node->user_data != nullptr);
   const OpDataSub& data = *(static_cast<const OpDataSub*>(node->user_data));
@@ -162,7 +162,7 @@ TfLiteStatus SubEval(TfLiteContext* context, TfLiteNode* node) {
 }
 
 TFLMRegistration Register_SUB() {
-  return tflite::micro::RegisterOp(SubInit, SubPrepare, SubEval);
+  return tflite_micro::micro::RegisterOp(SubInit, SubPrepare, SubEval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/sub.h b/tensorflow/lite/micro/kernels/sub.h
index 29900221..130eddfe 100644
--- a/tensorflow/lite/micro/kernels/sub.h
+++ b/tensorflow/lite/micro/kernels/sub.h
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/c/builtin_op_data.h"
 #include "tensorflow/lite/c/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 extern const int kSubInputTensor1;
 extern const int kSubInputTensor2;
@@ -55,6 +55,6 @@ TfLiteStatus CalculateOpDataSub(TfLiteContext* context, TfLiteSubParams* params,
 
 TfLiteStatus SubPrepare(TfLiteContext* context, TfLiteNode* node);
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_SUB_H_
diff --git a/tensorflow/lite/micro/kernels/sub_common.cc b/tensorflow/lite/micro/kernels/sub_common.cc
index d6647462..da972ca5 100644
--- a/tensorflow/lite/micro/kernels/sub_common.cc
+++ b/tensorflow/lite/micro/kernels/sub_common.cc
@@ -27,7 +27,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/kernels/sub.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 const int kSubInputTensor1 = 0;
 const int kSubInputTensor2 = 1;
@@ -104,4 +104,4 @@ TfLiteStatus SubPrepare(TfLiteContext* context, TfLiteNode* node) {
   return kTfLiteOk;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/sub_test.cc b/tensorflow/lite/micro/kernels/sub_test.cc
index d5226ebc..f551ef87 100644
--- a/tensorflow/lite/micro/kernels/sub_test.cc
+++ b/tensorflow/lite/micro/kernels/sub_test.cc
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -72,7 +72,7 @@ void ValidateSubGoldens(TfLiteTensor* tensors, int tensors_size,
   int outputs_array_data[] = {1, 2};
   TfLiteIntArray* outputs_array = IntArrayFromInts(outputs_array_data);
 
-  const TFLMRegistration registration = tflite::Register_SUB();
+  const TFLMRegistration registration = tflite_micro::Register_SUB();
   micro::KernelRunner runner(registration, tensors, tensors_size, inputs_array,
                              outputs_array, &builtin_data);
 
@@ -123,16 +123,16 @@ void TestSubQuantized(int* input1_dims_data, const float* input1_data,
   constexpr int outputs_size = 1;
   constexpr int tensors_size = inputs_size + outputs_size;
   TfLiteTensor tensors[tensors_size] = {
-      tflite::testing::CreateQuantizedTensor(input1_data, input1_quantized,
+      tflite_micro::testing::CreateQuantizedTensor(input1_data, input1_quantized,
                                              input1_dims, input1_scale,
                                              input1_zero_point),
-      tflite::testing::CreateQuantizedTensor(input2_data, input2_quantized,
+      tflite_micro::testing::CreateQuantizedTensor(input2_data, input2_quantized,
                                              input2_dims, input2_scale,
                                              input2_zero_point),
-      tflite::testing::CreateQuantizedTensor(output_data, output_dims,
+      tflite_micro::testing::CreateQuantizedTensor(output_data, output_dims,
                                              output_scale, output_zero_point),
   };
-  tflite::Quantize(golden, golden_quantized, ElementCount(*output_dims),
+  tflite_micro::Quantize(golden, golden_quantized, ElementCount(*output_dims),
                    output_scale, output_zero_point);
 
   ValidateSubGoldens(tensors, tensors_size, golden_quantized, output_data,
@@ -141,7 +141,7 @@ void TestSubQuantized(int* input1_dims_data, const float* input1_data,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -152,7 +152,7 @@ TF_LITE_MICRO_TEST(FloatSubNoActivation) {
   const float input2_values[] = {0.1, 0.2, 0.3, 0.5};
   const float golden_values[] = {-2.1, 0.0, 0.4, 0.3};
   float output_data[output_dims_count];
-  tflite::testing::TestSubFloat(inout_shape, input1_values, inout_shape,
+  tflite_micro::testing::TestSubFloat(inout_shape, input1_values, inout_shape,
                                 input2_values, inout_shape, golden_values,
                                 kTfLiteActNone, output_data);
 }
@@ -165,7 +165,7 @@ TF_LITE_MICRO_TEST(FloatSubActivationRelu1) {
   const float golden_values[] = {-1.0, 0.0, 1.0, 0.3};
 
   float output_data[output_dims_count];
-  tflite::testing::TestSubFloat(inout_shape, input1_values, inout_shape,
+  tflite_micro::testing::TestSubFloat(inout_shape, input1_values, inout_shape,
                                 input2_values, inout_shape, golden_values,
                                 kTfLiteActReluN1To1, output_data);
 }
@@ -188,7 +188,7 @@ TF_LITE_MICRO_TEST(FloatSubVariousInputShapes) {
   };
 
   for (int i = 0; i < num_shapes; ++i) {
-    tflite::testing::TestSubFloat(test_shapes[i], input1_values, test_shapes[i],
+    tflite_micro::testing::TestSubFloat(test_shapes[i], input1_values, test_shapes[i],
                                   input2_values, test_shapes[i],
                                   expected_output, kTfLiteActNone, output_data);
   }
@@ -213,7 +213,7 @@ TF_LITE_MICRO_TEST(FloatSubWithScalarBroadcast) {
   };
 
   for (int i = 0; i < num_shapes; ++i) {
-    tflite::testing::TestSubFloat(test_shapes[i], input1_values, input2_shape,
+    tflite_micro::testing::TestSubFloat(test_shapes[i], input1_values, input2_shape,
                                   input2_values, test_shapes[i],
                                   expected_output, kTfLiteActNone, output_data);
   }
@@ -233,7 +233,7 @@ TF_LITE_MICRO_TEST(QuantizedSubNoActivationInt8) {
   int8_t golden_quantized[output_dims_count];
   int8_t output[output_dims_count];
 
-  tflite::testing::TestSubQuantized(
+  tflite_micro::testing::TestSubQuantized(
       inout_shape, input1_values, input1_quantized, scales[0], zero_points[0],
       inout_shape, input2_values, input2_quantized, scales[1], zero_points[1],
       inout_shape, golden_values, golden_quantized, scales[2], zero_points[2],
@@ -254,7 +254,7 @@ TF_LITE_MICRO_TEST(QuantizedSubActivationRelu1Int8) {
   int8_t golden_quantized[output_dims_count];
   int8_t output[output_dims_count];
 
-  tflite::testing::TestSubQuantized(
+  tflite_micro::testing::TestSubQuantized(
       inout_shape, input1_values, input1_quantized, scales[0], zero_points[0],
       inout_shape, input2_values, input2_quantized, scales[1], zero_points[1],
       inout_shape, golden_values, golden_quantized, scales[2], zero_points[2],
@@ -275,7 +275,7 @@ TF_LITE_MICRO_TEST(QuantizedSubActivationRelu1Int16) {
   int16_t golden_quantized[output_dims_count];
   int16_t output[output_dims_count];
 
-  tflite::testing::TestSubQuantized(
+  tflite_micro::testing::TestSubQuantized(
       inout_shape, input1_values, input1_quantized, scales[0], zero_points[0],
       inout_shape, input2_values, input2_quantized, scales[1], zero_points[1],
       inout_shape, golden_values, golden_quantized, scales[2], zero_points[2],
@@ -306,7 +306,7 @@ TF_LITE_MICRO_TEST(QuantizedSubVariousInputShapesInt8) {
   int8_t output[output_dims_count];
 
   for (int i = 0; i < num_shapes; i++) {
-    tflite::testing::TestSubQuantized(
+    tflite_micro::testing::TestSubQuantized(
         test_shapes[i], input1_values, input1_quantized, scales[0],
         zero_points[0], test_shapes[i], input2_values, input2_quantized,
         scales[1], zero_points[1], test_shapes[i], golden_values,
@@ -338,7 +338,7 @@ TF_LITE_MICRO_TEST(QuantizedSubVariousInputShapesInt16) {
   int16_t output[output_dims_count];
 
   for (int i = 0; i < num_shapes; i++) {
-    tflite::testing::TestSubQuantized(
+    tflite_micro::testing::TestSubQuantized(
         test_shapes[i], input1_values, input1_quantized, scales[0],
         zero_points[0], test_shapes[i], input2_values, input2_quantized,
         scales[1], zero_points[1], test_shapes[i], golden_values,
@@ -347,15 +347,15 @@ TF_LITE_MICRO_TEST(QuantizedSubVariousInputShapesInt16) {
 }
 
 TF_LITE_MICRO_TEST(QuantizedSubWithScalarBroadcastFloat) {
-  float output_float[tflite::testing::broadcast_output_dims_count];
-
-  for (int i = 0; i < tflite::testing::broadcast_num_shapes; ++i) {
-    tflite::testing::TestSubFloat(tflite::testing::broadcast_input1_shape,
-                                  tflite::testing::broadcast_input1_values,
-                                  tflite::testing::broadcast_input2_shapes[i],
-                                  tflite::testing::broadcast_input2_values,
-                                  tflite::testing::broadcast_output_shapes[i],
-                                  tflite::testing::broadcast_goldens[i],
+  float output_float[tflite_micro::testing::broadcast_output_dims_count];
+
+  for (int i = 0; i < tflite_micro::testing::broadcast_num_shapes; ++i) {
+    tflite_micro::testing::TestSubFloat(tflite_micro::testing::broadcast_input1_shape,
+                                  tflite_micro::testing::broadcast_input1_values,
+                                  tflite_micro::testing::broadcast_input2_shapes[i],
+                                  tflite_micro::testing::broadcast_input2_values,
+                                  tflite_micro::testing::broadcast_output_shapes[i],
+                                  tflite_micro::testing::broadcast_goldens[i],
                                   kTfLiteActNone, output_float);
   }
 }
@@ -386,7 +386,7 @@ TF_LITE_MICRO_TEST(QuantizedSubWithScalarBroadcastInt8) {
   int8_t output[output_dims_count];
 
   for (int i = 0; i < num_shapes; ++i) {
-    tflite::testing::TestSubQuantized(
+    tflite_micro::testing::TestSubQuantized(
         test_shapes[i], input1_values, input1_quantized, scales[0],
         zero_points[0], input2_shape, input2_values, input2_quantized,
         scales[1], zero_points[1], test_shapes[i], golden, golden_quantized,
@@ -420,7 +420,7 @@ TF_LITE_MICRO_TEST(QuantizedSubWithScalarBroadcastInt16) {
   int16_t output[output_dims_count];
 
   for (int i = 0; i < num_shapes; ++i) {
-    tflite::testing::TestSubQuantized(
+    tflite_micro::testing::TestSubQuantized(
         test_shapes[i], input1_values, input1_quantized, scales[0],
         zero_points[0], input2_shape, input2_values, input2_quantized,
         scales[1], zero_points[1], test_shapes[i], golden, golden_quantized,
@@ -431,19 +431,19 @@ TF_LITE_MICRO_TEST(QuantizedSubWithScalarBroadcastInt16) {
 TF_LITE_MICRO_TEST(QuantizedSubWithMixedBroadcastInt8) {
   const float scales[] = {0.1, 0.05, 0.1};
   const int zero_points[] = {-10, -5, 7};
-  int8_t input1_quantized[tflite::testing::broadcast_output_dims_count];
-  int8_t input2_quantized[tflite::testing::broadcast_output_dims_count];
-  int8_t golden_quantized[tflite::testing::broadcast_output_dims_count];
-  int8_t output[tflite::testing::broadcast_output_dims_count];
-
-  for (int i = 0; i < tflite::testing::broadcast_num_shapes; ++i) {
-    tflite::testing::TestSubQuantized(
-        tflite::testing::broadcast_input1_shape,
-        tflite::testing::broadcast_input1_values, input1_quantized, scales[0],
-        zero_points[0], tflite::testing::broadcast_input2_shapes[i],
-        tflite::testing::broadcast_input2_values, input2_quantized, scales[1],
-        zero_points[1], tflite::testing::broadcast_output_shapes[i],
-        tflite::testing::broadcast_goldens[i], golden_quantized, scales[2],
+  int8_t input1_quantized[tflite_micro::testing::broadcast_output_dims_count];
+  int8_t input2_quantized[tflite_micro::testing::broadcast_output_dims_count];
+  int8_t golden_quantized[tflite_micro::testing::broadcast_output_dims_count];
+  int8_t output[tflite_micro::testing::broadcast_output_dims_count];
+
+  for (int i = 0; i < tflite_micro::testing::broadcast_num_shapes; ++i) {
+    tflite_micro::testing::TestSubQuantized(
+        tflite_micro::testing::broadcast_input1_shape,
+        tflite_micro::testing::broadcast_input1_values, input1_quantized, scales[0],
+        zero_points[0], tflite_micro::testing::broadcast_input2_shapes[i],
+        tflite_micro::testing::broadcast_input2_values, input2_quantized, scales[1],
+        zero_points[1], tflite_micro::testing::broadcast_output_shapes[i],
+        tflite_micro::testing::broadcast_goldens[i], golden_quantized, scales[2],
         zero_points[2], kTfLiteActNone, output);
   }
 }
@@ -451,19 +451,19 @@ TF_LITE_MICRO_TEST(QuantizedSubWithMixedBroadcastInt8) {
 TF_LITE_MICRO_TEST(QuantizedSubWithMixedBroadcastInt16) {
   const float scales[] = {0.1, 0.05, 0.1};
   const int zero_points[] = {0, 0, 0};
-  int16_t input1_quantized[tflite::testing::broadcast_output_dims_count];
-  int16_t input2_quantized[tflite::testing::broadcast_output_dims_count];
-  int16_t golden_quantized[tflite::testing::broadcast_output_dims_count];
-  int16_t output[tflite::testing::broadcast_output_dims_count];
-
-  for (int i = 0; i < tflite::testing::broadcast_num_shapes; ++i) {
-    tflite::testing::TestSubQuantized(
-        tflite::testing::broadcast_input1_shape,
-        tflite::testing::broadcast_input1_values, input1_quantized, scales[0],
-        zero_points[0], tflite::testing::broadcast_input2_shapes[i],
-        tflite::testing::broadcast_input2_values, input2_quantized, scales[1],
-        zero_points[1], tflite::testing::broadcast_output_shapes[i],
-        tflite::testing::broadcast_goldens[i], golden_quantized, scales[2],
+  int16_t input1_quantized[tflite_micro::testing::broadcast_output_dims_count];
+  int16_t input2_quantized[tflite_micro::testing::broadcast_output_dims_count];
+  int16_t golden_quantized[tflite_micro::testing::broadcast_output_dims_count];
+  int16_t output[tflite_micro::testing::broadcast_output_dims_count];
+
+  for (int i = 0; i < tflite_micro::testing::broadcast_num_shapes; ++i) {
+    tflite_micro::testing::TestSubQuantized(
+        tflite_micro::testing::broadcast_input1_shape,
+        tflite_micro::testing::broadcast_input1_values, input1_quantized, scales[0],
+        zero_points[0], tflite_micro::testing::broadcast_input2_shapes[i],
+        tflite_micro::testing::broadcast_input2_values, input2_quantized, scales[1],
+        zero_points[1], tflite_micro::testing::broadcast_output_shapes[i],
+        tflite_micro::testing::broadcast_goldens[i], golden_quantized, scales[2],
         zero_points[2], kTfLiteActNone, output);
   }
 }
diff --git a/tensorflow/lite/micro/kernels/svdf.cc b/tensorflow/lite/micro/kernels/svdf.cc
index 0ffb4b07..22522d0a 100644
--- a/tensorflow/lite/micro/kernels/svdf.cc
+++ b/tensorflow/lite/micro/kernels/svdf.cc
@@ -29,7 +29,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_log.h"
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 void* Init(TfLiteContext* context, const char* buffer, size_t length) {
@@ -43,20 +43,20 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const OpDataSvdf& data = *(static_cast<const OpDataSvdf*>(node->user_data));
 
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kSvdfInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kSvdfInputTensor);
   const TfLiteEvalTensor* weights_feature =
-      tflite::micro::GetEvalInput(context, node, kSvdfWeightsFeatureTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kSvdfWeightsFeatureTensor);
   const TfLiteEvalTensor* weights_time =
-      tflite::micro::GetEvalInput(context, node, kSvdfWeightsTimeTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kSvdfWeightsTimeTensor);
   // TODO(#1751): account for optional bias tensor
   const TfLiteEvalTensor* bias =
       (NumInputs(node) == 5)
-          ? tflite::micro::GetEvalInput(context, node, kSvdfBiasTensor)
+          ? tflite_micro::micro::GetEvalInput(context, node, kSvdfBiasTensor)
           : nullptr;
-  TfLiteEvalTensor* activation_state = tflite::micro::GetMutableEvalInput(
+  TfLiteEvalTensor* activation_state = tflite_micro::micro::GetMutableEvalInput(
       context, node, kSvdfInputActivationStateTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kSvdfOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kSvdfOutputTensor);
 
   switch (weights_feature->type) {
     case kTfLiteFloat32: {
@@ -99,7 +99,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_SVDF() {
-  return tflite::micro::RegisterOp(Init, PrepareSvdf, Eval);
+  return tflite_micro::micro::RegisterOp(Init, PrepareSvdf, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/svdf.h b/tensorflow/lite/micro/kernels/svdf.h
index a05a9b4e..469f8c57 100644
--- a/tensorflow/lite/micro/kernels/svdf.h
+++ b/tensorflow/lite/micro/kernels/svdf.h
@@ -18,7 +18,7 @@ limitations under the License.
 #include "tensorflow/lite/c/builtin_op_data.h"
 #include "tensorflow/lite/micro/micro_common.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 struct OpDataSvdf {
   int32_t effective_scale_1_a;
@@ -95,6 +95,6 @@ TFLMRegistration Register_SVDF_INT8();
 inline TFLMRegistration Register_SVDF_INT8() { return Register_SVDF(); }
 
 #endif
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_SVDF_H_
diff --git a/tensorflow/lite/micro/kernels/svdf_common.cc b/tensorflow/lite/micro/kernels/svdf_common.cc
index d7dd963f..025047e6 100644
--- a/tensorflow/lite/micro/kernels/svdf_common.cc
+++ b/tensorflow/lite/micro/kernels/svdf_common.cc
@@ -27,7 +27,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/svdf.h"
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 /**
  * This version of SVDF is specific to TFLite Micro. It contains the following
@@ -74,7 +74,7 @@ void EvalIntegerSvdfReference(TfLiteContext* context, TfLiteNode* node,
       context->GetScratchBuffer(context, data.scratch_output_tensor_index));
 
   // Shift states.
-  T* const state_ptr = tflite::micro::GetTensorData<T>(activation_state_tensor);
+  T* const state_ptr = tflite_micro::micro::GetTensorData<T>(activation_state_tensor);
 
   // Left shift the activation_state.
   {
@@ -90,10 +90,10 @@ void EvalIntegerSvdfReference(TfLiteContext* context, TfLiteNode* node,
 
   // Feature matmul.
   {
-    T* state = tflite::micro::GetTensorData<T>(activation_state_tensor);
-    const int8_t* input = tflite::micro::GetTensorData<int8_t>(input_tensor);
+    T* state = tflite_micro::micro::GetTensorData<T>(activation_state_tensor);
+    const int8_t* input = tflite_micro::micro::GetTensorData<int8_t>(input_tensor);
     const int8_t* weight_feature =
-        tflite::micro::GetTensorData<int8_t>(weights_feature_tensor);
+        tflite_micro::micro::GetTensorData<int8_t>(weights_feature_tensor);
     const int32_t output_max = std::numeric_limits<T>::max();
     const int32_t output_min = std::numeric_limits<T>::min();
     T* result_in_batch = state + (n_memory - 1);
@@ -125,9 +125,9 @@ void EvalIntegerSvdfReference(TfLiteContext* context, TfLiteNode* node,
 
       // Perform batched vector dot product:
       const T* vector1_ptr =
-          tflite::micro::GetTensorData<T>(weights_time_tensor);
+          tflite_micro::micro::GetTensorData<T>(weights_time_tensor);
       const T* vector2_ptr =
-          tflite::micro::GetTensorData<T>(activation_state_tensor) +
+          tflite_micro::micro::GetTensorData<T>(activation_state_tensor) +
           b * n_memory * n_filter;
 
       for (int i = 0; i < n_filter; i++) {
@@ -148,7 +148,7 @@ void EvalIntegerSvdfReference(TfLiteContext* context, TfLiteNode* node,
     if (bias_tensor) {
       // Vector batch assign:
       const int32_t* bias_data =
-          tflite::micro::GetTensorData<int32_t>(bias_tensor);
+          tflite_micro::micro::GetTensorData<int32_t>(bias_tensor);
       for (int i = 0; i < n_batch; ++i) {
         int32_t* output_ptr = scratch_output_tensor + i * n_unit;
         const int32_t* bias_ptr = bias_data;
@@ -185,7 +185,7 @@ void EvalIntegerSvdfReference(TfLiteContext* context, TfLiteNode* node,
                                                  data.effective_scale_2_b);
       int32_t x3 = x2 + data.output_zero_point;
       int32_t x4 = std::min(std::max(output_min, x3), output_max);
-      tflite::micro::GetTensorData<int8_t>(output_tensor)[i] =
+      tflite_micro::micro::GetTensorData<int8_t>(output_tensor)[i] =
           static_cast<int8_t>(x4);
     }
   }
@@ -280,7 +280,7 @@ static inline void ApplyTimeWeightsBiasAndActivation(
     float* output_ptr_batch = output_ptr + b * num_units;
     for (int i = 0; i < num_units; ++i) {
       *output_ptr_batch =
-          tflite::ops::micro::ActivationValFloat(activation, *output_ptr_batch);
+          tflite_micro::ops::micro::ActivationValFloat(activation, *output_ptr_batch);
       ++output_ptr_batch;
     }
   }
@@ -300,14 +300,14 @@ void EvalFloatSvdfReference(
   const int memory_size = weights_time->dims->data[1];
 
   const float* weights_feature_ptr =
-      tflite::micro::GetTensorData<float>(weights_feature);
+      tflite_micro::micro::GetTensorData<float>(weights_feature);
   const float* weights_time_ptr =
-      tflite::micro::GetTensorData<float>(weights_time);
+      tflite_micro::micro::GetTensorData<float>(weights_time);
   // TODO(#1751): account for optional bias tensor
-  const float* bias_ptr = tflite::micro::GetTensorData<float>(bias);
-  const float* input_ptr = tflite::micro::GetTensorData<float>(input);
+  const float* bias_ptr = tflite_micro::micro::GetTensorData<float>(bias);
+  const float* input_ptr = tflite_micro::micro::GetTensorData<float>(input);
 
-  float* state_ptr = tflite::micro::GetTensorData<float>(activation_state);
+  float* state_ptr = tflite_micro::micro::GetTensorData<float>(activation_state);
 
   TFLITE_DCHECK(context != nullptr);
   TFLITE_DCHECK(context->GetScratchBuffer != nullptr);
@@ -315,7 +315,7 @@ void EvalFloatSvdfReference(
   float* scratch_ptr = static_cast<float*>(
       context->GetScratchBuffer(context, scratch_tensor_index));
 
-  float* output_ptr = tflite::micro::GetTensorData<float>(output);
+  float* output_ptr = tflite_micro::micro::GetTensorData<float>(output);
 
   // Left shift the activation_state.
   {
@@ -514,4 +514,4 @@ TfLiteStatus PrepareSvdf(TfLiteContext* context, TfLiteNode* node) {
   return kTfLiteOk;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/svdf_test.cc b/tensorflow/lite/micro/kernels/svdf_test.cc
index ae8fadc1..a7ce23fb 100644
--- a/tensorflow/lite/micro/kernels/svdf_test.cc
+++ b/tensorflow/lite/micro/kernels/svdf_test.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -644,9 +644,9 @@ inline void TestIntegerSVDF(
       CreateQuantizedTensor(output_data, output_dims, output_scale,
                             output_zero_point)};
 
-  tflite::Quantize(golden_output, golden_output_quantized, golden_output_len,
+  tflite_micro::Quantize(golden_output, golden_output_quantized, golden_output_len,
                    output_scale, output_zero_point);
-  tflite::Quantize(input_sequences_data, input_sequences_quantized,
+  tflite_micro::Quantize(input_sequences_data, input_sequences_quantized,
                    input_sequences_len, input_scale, input_zero_point);
 
   ValidateSVDFGoldens(batch_size, num_units, input_size, rank, tensors,
@@ -683,32 +683,32 @@ void SvdfQuantized2x2Input2x4OutputShouldMatchGolden() {
   int output_zero_point = 0;
 
   int8_t input_quantized[input_size_dims_count];
-  int8_t input_sequences_quantized[sizeof(tflite::testing::input_data_2x2x10) /
+  int8_t input_sequences_quantized[sizeof(tflite_micro::testing::input_data_2x2x10) /
                                    sizeof(float)];
   int8_t feature_weights_quantized
-      [sizeof(tflite::testing::feature_weights_data_2x2x10) / sizeof(float)];
-  T time_weights_quantized[sizeof(tflite::testing::time_weights_data_2x2x10) /
+      [sizeof(tflite_micro::testing::feature_weights_data_2x2x10) / sizeof(float)];
+  T time_weights_quantized[sizeof(tflite_micro::testing::time_weights_data_2x2x10) /
                            sizeof(float)];
   T activation_state_quantized[activation_state_dims_count];
   int32_t
-      bias_quantized[sizeof(tflite::testing::bias_data_2x2x10) / sizeof(float)];
-  int8_t golden_quantized[sizeof(tflite::testing::golden_output_2x2x10) /
+      bias_quantized[sizeof(tflite_micro::testing::bias_data_2x2x10) / sizeof(float)];
+  int8_t golden_quantized[sizeof(tflite_micro::testing::golden_output_2x2x10) /
                           sizeof(float)];
 
-  tflite::testing::TestIntegerSVDF(
+  tflite_micro::testing::TestIntegerSVDF(
       batch_size, num_units, input_size, memory_size, rank, kTfLiteActRelu,
       input_quantized, input_scale, input_zero_point,
-      tflite::testing::feature_weights_data_2x2x10, feature_weights_quantized,
-      feature_weights_scale, tflite::testing::time_weights_data_2x2x10,
+      tflite_micro::testing::feature_weights_data_2x2x10, feature_weights_quantized,
+      feature_weights_scale, tflite_micro::testing::time_weights_data_2x2x10,
       time_weights_quantized, time_weights_scale,
-      tflite::testing::bias_data_2x2x10, bias_quantized,
-      tflite::testing::initial_activation_state_data_2x2x10,
+      tflite_micro::testing::bias_data_2x2x10, bias_quantized,
+      tflite_micro::testing::initial_activation_state_data_2x2x10,
       activation_state_quantized, activation_state_scale, 0, output_data,
-      output_scale, output_zero_point, tflite::testing::input_data_2x2x10,
+      output_scale, output_zero_point, tflite_micro::testing::input_data_2x2x10,
       input_sequences_quantized,
-      sizeof(tflite::testing::input_data_2x2x10) / sizeof(float),
-      tflite::testing::golden_output_2x2x10, golden_quantized,
-      sizeof(tflite::testing::golden_output_2x2x10) / sizeof(float));
+      sizeof(tflite_micro::testing::input_data_2x2x10) / sizeof(float),
+      tflite_micro::testing::golden_output_2x2x10, golden_quantized,
+      sizeof(tflite_micro::testing::golden_output_2x2x10) / sizeof(float));
 }
 
 // Template parameter sets type of both time_weights and activation_state.
@@ -729,43 +729,43 @@ void SvdfQuantized1x16Input64x1OutputShouldMatchGolden() {
 
   float input_scale = 0.10075444;
   float feature_weights_scale = 0.00649388;
-  float time_weights_scale = tflite::testing::ScaleFromMinMax<T>(-.81, .81);
+  float time_weights_scale = tflite_micro::testing::ScaleFromMinMax<T>(-.81, .81);
   float activation_state_scale =
-      tflite::testing::ScaleFromMinMax<T>(-17.73, 17.73);
+      tflite_micro::testing::ScaleFromMinMax<T>(-17.73, 17.73);
   int activation_state_zero_point =
-      tflite::testing::ZeroPointFromMinMax<T>(-17.73, 17.73);
+      tflite_micro::testing::ZeroPointFromMinMax<T>(-17.73, 17.73);
   float output_scale = 0.051445257;
 
   int input_zero_point = 2;
   int output_zero_point = 0;
 
   int8_t input_quantized[input_dims_count];
-  int8_t input_sequences_quantized[sizeof(tflite::testing::input_data_16x1x1) /
+  int8_t input_sequences_quantized[sizeof(tflite_micro::testing::input_data_16x1x1) /
                                    sizeof(float)];
   int8_t feature_weights_quantized
-      [sizeof(tflite::testing::feature_weights_data_16x1x1) / sizeof(float)];
-  T time_weights_quantized[sizeof(tflite::testing::time_weights_data_16x1x1) /
+      [sizeof(tflite_micro::testing::feature_weights_data_16x1x1) / sizeof(float)];
+  T time_weights_quantized[sizeof(tflite_micro::testing::time_weights_data_16x1x1) /
                            sizeof(float)];
   T activation_state_quantized[activation_state_dims_count];
   int32_t
-      bias_quantized[sizeof(tflite::testing::bias_data_16x1x1) / sizeof(float)];
-  int8_t golden_quantized[sizeof(tflite::testing::golden_output_16x1x1) /
+      bias_quantized[sizeof(tflite_micro::testing::bias_data_16x1x1) / sizeof(float)];
+  int8_t golden_quantized[sizeof(tflite_micro::testing::golden_output_16x1x1) /
                           sizeof(float)];
 
-  tflite::testing::TestIntegerSVDF(
+  tflite_micro::testing::TestIntegerSVDF(
       batch_size, num_units, input_size, memory_size, rank, kTfLiteActNone,
       input_quantized, input_scale, input_zero_point,
-      tflite::testing::feature_weights_data_16x1x1, feature_weights_quantized,
-      feature_weights_scale, tflite::testing::time_weights_data_16x1x1,
+      tflite_micro::testing::feature_weights_data_16x1x1, feature_weights_quantized,
+      feature_weights_scale, tflite_micro::testing::time_weights_data_16x1x1,
       time_weights_quantized, time_weights_scale,
-      tflite::testing::bias_data_16x1x1, bias_quantized,
-      tflite::testing::initial_activation_state_data_16x1x1,
+      tflite_micro::testing::bias_data_16x1x1, bias_quantized,
+      tflite_micro::testing::initial_activation_state_data_16x1x1,
       activation_state_quantized, activation_state_scale,
       activation_state_zero_point, output_data, output_scale, output_zero_point,
-      tflite::testing::input_data_16x1x1, input_sequences_quantized,
-      sizeof(tflite::testing::input_data_16x1x1) / sizeof(float),
-      tflite::testing::golden_output_16x1x1, golden_quantized,
-      sizeof(tflite::testing::golden_output_16x1x1) / sizeof(float));
+      tflite_micro::testing::input_data_16x1x1, input_sequences_quantized,
+      sizeof(tflite_micro::testing::input_data_16x1x1) / sizeof(float),
+      tflite_micro::testing::golden_output_16x1x1, golden_quantized,
+      sizeof(tflite_micro::testing::golden_output_16x1x1) / sizeof(float));
 }
 
 template <typename T>
@@ -785,48 +785,48 @@ void SvdfQuantized1x16Input64x1OutputReluShouldMatchGolden() {
 
   float input_scale = 0.10075444;
   float feature_weights_scale = 0.00649388;
-  float time_weights_scale = tflite::testing::ScaleFromMinMax<T>(-.81, .81);
+  float time_weights_scale = tflite_micro::testing::ScaleFromMinMax<T>(-.81, .81);
   float activation_state_scale =
-      tflite::testing::ScaleFromMinMax<T>(-17.73, 17.73);
+      tflite_micro::testing::ScaleFromMinMax<T>(-17.73, 17.73);
   int activation_state_zero_point =
-      tflite::testing::ZeroPointFromMinMax<T>(-17.73, 17.73);
+      tflite_micro::testing::ZeroPointFromMinMax<T>(-17.73, 17.73);
   float output_scale = 0.051445257;
 
   int input_zero_point = 2;
   int output_zero_point = -128;
 
   int8_t input_quantized[input_dims_count];
-  int8_t input_sequences_quantized[sizeof(tflite::testing::input_data_16x1x1) /
+  int8_t input_sequences_quantized[sizeof(tflite_micro::testing::input_data_16x1x1) /
                                    sizeof(float)];
   int8_t feature_weights_quantized
-      [sizeof(tflite::testing::feature_weights_data_16x1x1) / sizeof(float)];
-  T time_weights_quantized[sizeof(tflite::testing::time_weights_data_16x1x1) /
+      [sizeof(tflite_micro::testing::feature_weights_data_16x1x1) / sizeof(float)];
+  T time_weights_quantized[sizeof(tflite_micro::testing::time_weights_data_16x1x1) /
                            sizeof(float)];
   T activation_state_quantized[activation_state_dims_count];
   int32_t
-      bias_quantized[sizeof(tflite::testing::bias_data_16x1x1) / sizeof(float)];
-  int8_t golden_quantized[sizeof(tflite::testing::golden_output_relu_16x1x1) /
+      bias_quantized[sizeof(tflite_micro::testing::bias_data_16x1x1) / sizeof(float)];
+  int8_t golden_quantized[sizeof(tflite_micro::testing::golden_output_relu_16x1x1) /
                           sizeof(float)];
 
-  tflite::testing::TestIntegerSVDF(
+  tflite_micro::testing::TestIntegerSVDF(
       batch_size, num_units, input_size, memory_size, rank, kTfLiteActRelu,
       input_quantized, input_scale, input_zero_point,
-      tflite::testing::feature_weights_data_16x1x1, feature_weights_quantized,
-      feature_weights_scale, tflite::testing::time_weights_data_16x1x1,
+      tflite_micro::testing::feature_weights_data_16x1x1, feature_weights_quantized,
+      feature_weights_scale, tflite_micro::testing::time_weights_data_16x1x1,
       time_weights_quantized, time_weights_scale,
-      tflite::testing::bias_data_16x1x1, bias_quantized,
-      tflite::testing::initial_activation_state_data_16x1x1,
+      tflite_micro::testing::bias_data_16x1x1, bias_quantized,
+      tflite_micro::testing::initial_activation_state_data_16x1x1,
       activation_state_quantized, activation_state_scale,
       activation_state_zero_point, output_data, output_scale, output_zero_point,
-      tflite::testing::input_data_16x1x1, input_sequences_quantized,
-      sizeof(tflite::testing::input_data_16x1x1) / sizeof(float),
-      tflite::testing::golden_output_relu_16x1x1, golden_quantized,
-      sizeof(tflite::testing::golden_output_relu_16x1x1) / sizeof(float));
+      tflite_micro::testing::input_data_16x1x1, input_sequences_quantized,
+      sizeof(tflite_micro::testing::input_data_16x1x1) / sizeof(float),
+      tflite_micro::testing::golden_output_relu_16x1x1, golden_quantized,
+      sizeof(tflite_micro::testing::golden_output_relu_16x1x1) / sizeof(float));
 }
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -846,8 +846,8 @@ TF_LITE_MICRO_TEST(SvdfFloat2x2Input2x4OutputShouldMatchGolden) {
   float activation_state_data[activation_state_dims_count];
 
   memcpy(activation_state_data,
-         tflite::testing::initial_activation_state_data_2x2x10,
-         sizeof(tflite::testing::initial_activation_state_data_2x2x10));
+         tflite_micro::testing::initial_activation_state_data_2x2x10,
+         sizeof(tflite_micro::testing::initial_activation_state_data_2x2x10));
 
   const int scratch_dims_count = batch_size * num_filters;
   float scratch_data[scratch_dims_count];
@@ -855,25 +855,25 @@ TF_LITE_MICRO_TEST(SvdfFloat2x2Input2x4OutputShouldMatchGolden) {
   const int output_dims_count = batch_size * num_units;
   float output_data[output_dims_count];
 
-  tflite::testing::TestSVDF(
+  tflite_micro::testing::TestSVDF(
       batch_size, num_units, input_size, memory_size, rank, kTfLiteActNone,
-      input_data, tflite::testing::feature_weights_data_2x2x10,
-      tflite::testing::time_weights_data_2x2x10, activation_state_data,
-      tflite::testing::bias_data_2x2x10, scratch_data, output_data,
-      tflite::testing::input_data_2x2x10,
-      sizeof(tflite::testing::input_data_2x2x10) / sizeof(float),
-      tflite::testing::golden_output_2x2x10);
+      input_data, tflite_micro::testing::feature_weights_data_2x2x10,
+      tflite_micro::testing::time_weights_data_2x2x10, activation_state_data,
+      tflite_micro::testing::bias_data_2x2x10, scratch_data, output_data,
+      tflite_micro::testing::input_data_2x2x10,
+      sizeof(tflite_micro::testing::input_data_2x2x10) / sizeof(float),
+      tflite_micro::testing::golden_output_2x2x10);
 }
 
 // Only reference kernels support full int8 svdf currently.
 #if !defined(HEXAGON)
 TF_LITE_MICRO_TEST(SvdfQuantized2x2Input2x4OutputShouldMatchGoldenInt8) {
-  tflite::testing::SvdfQuantized2x2Input2x4OutputShouldMatchGolden<int8_t>();
+  tflite_micro::testing::SvdfQuantized2x2Input2x4OutputShouldMatchGolden<int8_t>();
 }
 #endif
 
 TF_LITE_MICRO_TEST(SvdfQuantized2x2Input2x4OutputShouldMatchGoldenInt16) {
-  tflite::testing::SvdfQuantized2x2Input2x4OutputShouldMatchGolden<int16_t>();
+  tflite_micro::testing::SvdfQuantized2x2Input2x4OutputShouldMatchGolden<int16_t>();
 }
 
 TF_LITE_MICRO_TEST(SvdfFloat1x16Input64x1OutputShouldMatchGolden) {
@@ -895,16 +895,16 @@ TF_LITE_MICRO_TEST(SvdfFloat1x16Input64x1OutputShouldMatchGolden) {
 
   // Initialize activation state to starting values.
   memcpy(activation_state_data_mutable,
-         tflite::testing::initial_activation_state_data_16x1x1,
-         sizeof(tflite::testing::initial_activation_state_data_16x1x1));
+         tflite_micro::testing::initial_activation_state_data_16x1x1,
+         sizeof(tflite_micro::testing::initial_activation_state_data_16x1x1));
 
-  tflite::testing::TestSVDF(
+  tflite_micro::testing::TestSVDF(
       batch_size, num_units, input_size, memory_size, rank, kTfLiteActNone,
-      input_data, tflite::testing::feature_weights_data_16x1x1,
-      tflite::testing::time_weights_data_16x1x1, activation_state_data_mutable,
-      tflite::testing::bias_data_16x1x1, scratch_buffer, output_data,
-      tflite::testing::input_data_16x1x1, input_size,
-      tflite::testing::golden_output_16x1x1);
+      input_data, tflite_micro::testing::feature_weights_data_16x1x1,
+      tflite_micro::testing::time_weights_data_16x1x1, activation_state_data_mutable,
+      tflite_micro::testing::bias_data_16x1x1, scratch_buffer, output_data,
+      tflite_micro::testing::input_data_16x1x1, input_size,
+      tflite_micro::testing::golden_output_16x1x1);
 }
 
 TF_LITE_MICRO_TEST(SvdfFloat1x16Input64x1OutputReluShouldMatchGolden) {
@@ -926,39 +926,39 @@ TF_LITE_MICRO_TEST(SvdfFloat1x16Input64x1OutputReluShouldMatchGolden) {
 
   // Initialize activation state to starting values.
   memcpy(activation_state_data_mutable,
-         tflite::testing::initial_activation_state_data_16x1x1,
-         sizeof(tflite::testing::initial_activation_state_data_16x1x1));
+         tflite_micro::testing::initial_activation_state_data_16x1x1,
+         sizeof(tflite_micro::testing::initial_activation_state_data_16x1x1));
 
-  tflite::testing::TestSVDF(
+  tflite_micro::testing::TestSVDF(
       batch_size, num_units, input_size, memory_size, rank, kTfLiteActRelu,
-      input_data, tflite::testing::feature_weights_data_16x1x1,
-      tflite::testing::time_weights_data_16x1x1, activation_state_data_mutable,
-      tflite::testing::bias_data_16x1x1, scratch_buffer, output_data,
-      tflite::testing::input_data_16x1x1, input_size,
-      tflite::testing::golden_output_relu_16x1x1);
+      input_data, tflite_micro::testing::feature_weights_data_16x1x1,
+      tflite_micro::testing::time_weights_data_16x1x1, activation_state_data_mutable,
+      tflite_micro::testing::bias_data_16x1x1, scratch_buffer, output_data,
+      tflite_micro::testing::input_data_16x1x1, input_size,
+      tflite_micro::testing::golden_output_relu_16x1x1);
 }
 
 // Only reference kernels support full int8 svdf currently.
 #if !defined(HEXAGON)
 TF_LITE_MICRO_TEST(SvdfQuantized1x16Input64x1OutputShouldMatchGoldenInt8) {
-  tflite::testing::SvdfQuantized1x16Input64x1OutputShouldMatchGolden<int8_t>();
+  tflite_micro::testing::SvdfQuantized1x16Input64x1OutputShouldMatchGolden<int8_t>();
 }
 #endif
 
 TF_LITE_MICRO_TEST(SvdfQuantized1x16Input64x1OutputShouldMatchGoldenInt16) {
-  tflite::testing::SvdfQuantized1x16Input64x1OutputShouldMatchGolden<int16_t>();
+  tflite_micro::testing::SvdfQuantized1x16Input64x1OutputShouldMatchGolden<int16_t>();
 }
 
 // Only reference kernels support full int8 svdf currently.
 #if !defined(HEXAGON)
 TF_LITE_MICRO_TEST(SvdfQuantized1x16Input64x1OutputReluShouldMatchGoldenInt8) {
-  tflite::testing::SvdfQuantized1x16Input64x1OutputReluShouldMatchGolden<
+  tflite_micro::testing::SvdfQuantized1x16Input64x1OutputReluShouldMatchGolden<
       int8_t>();
 }
 #endif
 
 TF_LITE_MICRO_TEST(SvdfQuantized1x16Input64x1OutputReluShouldMatchGoldenInt16) {
-  tflite::testing::SvdfQuantized1x16Input64x1OutputReluShouldMatchGolden<
+  tflite_micro::testing::SvdfQuantized1x16Input64x1OutputReluShouldMatchGolden<
       int16_t>();
 }
 
diff --git a/tensorflow/lite/micro/kernels/tanh.cc b/tensorflow/lite/micro/kernels/tanh.cc
index e6958893..c0346759 100644
--- a/tensorflow/lite/micro/kernels/tanh.cc
+++ b/tensorflow/lite/micro/kernels/tanh.cc
@@ -27,7 +27,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_log.h"
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -149,37 +149,37 @@ TfLiteStatus TanhPrepare(TfLiteContext* context, TfLiteNode* node) {
 
 TfLiteStatus TanhEval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
   TFLITE_DCHECK(node->user_data != nullptr);
   const OpData& data = *(static_cast<const OpData*>(node->user_data));
 
   switch (input->type) {
     case kTfLiteFloat32: {
-      reference_ops::Tanh(tflite::micro::GetTensorShape(input),
-                          tflite::micro::GetTensorData<float>(input),
-                          tflite::micro::GetTensorShape(output),
-                          tflite::micro::GetTensorData<float>(output));
+      reference_ops::Tanh(tflite_micro::micro::GetTensorShape(input),
+                          tflite_micro::micro::GetTensorData<float>(input),
+                          tflite_micro::micro::GetTensorShape(output),
+                          tflite_micro::micro::GetTensorData<float>(output));
       return kTfLiteOk;
     } break;
     case kTfLiteInt16: {
       reference_integer_ops::Tanh(
           data.input_multiplier, data.input_left_shift,
-          tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<int16_t>(input),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<int16_t>(output));
+          tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<int16_t>(input),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<int16_t>(output));
       return kTfLiteOk;
     } break;
     case kTfLiteInt8: {
       reference_integer_ops::Tanh(
           data.input_zero_point, data.input_range_radius, data.input_multiplier,
-          data.input_left_shift, tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<int8_t>(input),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<int8_t>(output));
+          data.input_left_shift, tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<int8_t>(input),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<int8_t>(output));
       return kTfLiteOk;
     } break;
     default:
@@ -193,7 +193,7 @@ TfLiteStatus TanhEval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_TANH() {
-  return tflite::micro::RegisterOp(TanhInit, TanhPrepare, TanhEval);
+  return tflite_micro::micro::RegisterOp(TanhInit, TanhPrepare, TanhEval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/tanh_test.cc b/tensorflow/lite/micro/kernels/tanh_test.cc
index 00eb6555..5a0d52ec 100644
--- a/tensorflow/lite/micro/kernels/tanh_test.cc
+++ b/tensorflow/lite/micro/kernels/tanh_test.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -174,7 +174,7 @@ void TestTanhFloat(int input_dims_data[], const float* input_data,
   int outputs_array_data[] = {1, 1};
   TfLiteIntArray* outputs_array = IntArrayFromInts(outputs_array_data);
 
-  const TFLMRegistration registration = tflite::Register_TANH();
+  const TFLMRegistration registration = tflite_micro::Register_TANH();
   micro::KernelRunner runner(registration, tensors, tensors_size, inputs_array,
                              outputs_array, /*builtin_data=*/nullptr);
 
@@ -198,7 +198,7 @@ void TestTanhQuantized(int input_dims_data[], const float* input_data,
   TfLiteIntArray* output_dims = IntArrayFromInts(output_dims_data);
   const int output_elements_count = ElementCount(*output_dims);
 
-  tflite::Quantize(expected_output_data, expected_output_quantized,
+  tflite_micro::Quantize(expected_output_data, expected_output_quantized,
                    output_elements_count, output_scale, output_zero_point);
 
   constexpr int inputs_size = 1;
@@ -215,7 +215,7 @@ void TestTanhQuantized(int input_dims_data[], const float* input_data,
   int outputs_array_data[] = {1, 1};
   TfLiteIntArray* outputs_array = IntArrayFromInts(outputs_array_data);
 
-  const TFLMRegistration registration = tflite::Register_TANH();
+  const TFLMRegistration registration = tflite_micro::Register_TANH();
   micro::KernelRunner runner(registration, tensors, tensors_size, inputs_array,
                              outputs_array, /*builtin_data=*/nullptr);
 
@@ -230,20 +230,20 @@ void TestTanhQuantized(int input_dims_data[], const float* input_data,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(SimpleTestTanhFloat) {
-  using tflite::testing::tanh_input_vec_fp;
-  using tflite::testing::tanh_output_vec_fp;
-  using tflite::testing::tanh_vec_size;
+  using tflite_micro::testing::tanh_input_vec_fp;
+  using tflite_micro::testing::tanh_output_vec_fp;
+  using tflite_micro::testing::tanh_vec_size;
 
   int input_shape[] = {2, 1, tanh_vec_size};
   int output_shape[] = {2, 1, tanh_vec_size};
 
   float output_data[tanh_vec_size];
-  tflite::testing::TestTanhFloat(  //
+  tflite_micro::testing::TestTanhFloat(  //
       input_shape,                 // Input shape.
       tanh_input_vec_fp,           // Input data
       tanh_output_vec_fp,          // Expected results.
@@ -252,9 +252,9 @@ TF_LITE_MICRO_TEST(SimpleTestTanhFloat) {
 }
 
 TF_LITE_MICRO_TEST(SimpleTestTanhInt8) {
-  using tflite::testing::tanh_input_vec_fp;
-  using tflite::testing::tanh_output_vec_fp;
-  using tflite::testing::tanh_vec_size;
+  using tflite_micro::testing::tanh_input_vec_fp;
+  using tflite_micro::testing::tanh_output_vec_fp;
+  using tflite_micro::testing::tanh_vec_size;
 
   const float input_scale = 16 / 256.f;
   const int input_zero_point = 0;
@@ -267,7 +267,7 @@ TF_LITE_MICRO_TEST(SimpleTestTanhInt8) {
   int8_t input_quantized[tanh_vec_size];
   int8_t expected_output_quantized[tanh_vec_size];
   int8_t output_quantized[tanh_vec_size];
-  tflite::testing::TestTanhQuantized<int8_t>(         //
+  tflite_micro::testing::TestTanhQuantized<int8_t>(         //
       input_shape,                                    // Input shape.
       tanh_input_vec_fp, input_quantized,             // Input data.
       input_scale, input_zero_point,                  // Input quantized info.
@@ -280,9 +280,9 @@ TF_LITE_MICRO_TEST(SimpleTestTanhInt8) {
 }
 
 TF_LITE_MICRO_TEST(TestTanhInt16WideRange) {
-  using tflite::testing::tanh_int16_input_vec_fp;
-  using tflite::testing::tanh_int16_output_vec_fp;
-  using tflite::testing::tanh_int16_vec_size;
+  using tflite_micro::testing::tanh_int16_input_vec_fp;
+  using tflite_micro::testing::tanh_int16_output_vec_fp;
+  using tflite_micro::testing::tanh_int16_vec_size;
 
   const float input_scale = 32.f / 65536.f;
   const int input_zero_point = 0;
@@ -295,7 +295,7 @@ TF_LITE_MICRO_TEST(TestTanhInt16WideRange) {
   int16_t input_quantized[tanh_int16_vec_size];
   int16_t expected_output_quantized[tanh_int16_vec_size];
   int16_t output_quantized[tanh_int16_vec_size];
-  tflite::testing::TestTanhQuantized<int16_t>(  //
+  tflite_micro::testing::TestTanhQuantized<int16_t>(  //
       input_shape,                              // Input shape.
       tanh_int16_input_vec_fp,                  // Input data.
       input_quantized,                          // Quantized input data.
diff --git a/tensorflow/lite/micro/kernels/testdata/conv_test_data.cc b/tensorflow/lite/micro/kernels/testdata/conv_test_data.cc
index 094aab68..3a74bba0 100644
--- a/tensorflow/lite/micro/kernels/testdata/conv_test_data.cc
+++ b/tensorflow/lite/micro/kernels/testdata/conv_test_data.cc
@@ -17,7 +17,7 @@ limitations under the License.
 
 #include "tensorflow/lite/c/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // Kernel Conv Test Case: Int8Filter8x3x3x3PerChannelScaleRelu6ShouldMatchGolden
 const int8_t kConvInput1x32x32x3[1 * 32 * 32 * 3] = {
@@ -501,4 +501,4 @@ const int8_t kConvGoldenOutput4x4InputPaddingSame2x2[1 * 2 * 2 * 1] = {38, 24,
 const int8_t kConvGoldenOutput5x5InputPaddingSame3x3[1 * 3 * 3 * 1] = {
     -6, 25, 30, 58, 76, 7, 50, -11, -59};
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/testdata/conv_test_data.h b/tensorflow/lite/micro/kernels/testdata/conv_test_data.h
index bdac510a..4f7ef613 100644
--- a/tensorflow/lite/micro/kernels/testdata/conv_test_data.h
+++ b/tensorflow/lite/micro/kernels/testdata/conv_test_data.h
@@ -18,7 +18,7 @@ limitations under the License.
 
 #include "tensorflow/lite/c/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 extern const int8_t kConvInput1x32x32x3[];
 extern const int8_t kConvFilter8x3x3x3[];
 extern const int32_t kConvBiasQuantized8[];
@@ -32,6 +32,6 @@ extern const int32_t kConvZeroBias[];
 extern const int8_t kConvGoldenOutput4x4InputPaddingSame2x2[];
 extern const int8_t kConvGoldenOutput5x5InputPaddingSame3x3[];
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_CONV_TEST_DATA_H_
diff --git a/tensorflow/lite/micro/kernels/testdata/lstm_test_data.cc b/tensorflow/lite/micro/kernels/testdata/lstm_test_data.cc
index 0fc010bc..33ffc9aa 100644
--- a/tensorflow/lite/micro/kernels/testdata/lstm_test_data.cc
+++ b/tensorflow/lite/micro/kernels/testdata/lstm_test_data.cc
@@ -17,7 +17,7 @@ limitations under the License.
 
 #include <cstring>
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 
 namespace {
@@ -307,4 +307,4 @@ Create2x3x2X2Int16NodeContents(const float* input_data,
 }
 
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/testdata/lstm_test_data.h b/tensorflow/lite/micro/kernels/testdata/lstm_test_data.h
index 3edf4200..671bbb00 100644
--- a/tensorflow/lite/micro/kernels/testdata/lstm_test_data.h
+++ b/tensorflow/lite/micro/kernels/testdata/lstm_test_data.h
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/lstm_shared.h"
 #include "tensorflow/lite/micro/test_helpers.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 // Data structure to store all the data used to check output of internal gates
 // of one time step
@@ -421,15 +421,15 @@ CreateQuantizedGateData(
     const bool fold_zero_point) {
   GateData<WeightType, BiasType, input_dimension, state_dimension>
       quantized_gate_params;
-  tflite::SymmetricQuantize(gate_parameters.activation_weight,
+  tflite_micro::SymmetricQuantize(gate_parameters.activation_weight,
                             quantized_gate_params.activation_weight,
                             state_dimension * input_dimension,
                             gate_quantization_params.activation_weight.scale);
-  tflite::SymmetricQuantize(gate_parameters.recurrent_weight,
+  tflite_micro::SymmetricQuantize(gate_parameters.recurrent_weight,
                             quantized_gate_params.recurrent_weight,
                             state_dimension * state_dimension,
                             gate_quantization_params.recurrent_weight.scale);
-  tflite::SymmetricQuantize(gate_parameters.fused_bias,
+  tflite_micro::SymmetricQuantize(gate_parameters.fused_bias,
                             quantized_gate_params.fused_bias, state_dimension,
                             gate_quantization_params.bias.scale);
   // Note: steps below are not required for the generalized LSTM evaluation
@@ -441,7 +441,7 @@ CreateQuantizedGateData(
     std::memcpy(quantized_gate_params.activation_zp_folded_bias,
                 quantized_gate_params.fused_bias, 2 * sizeof(int32_t));
     // Pre-calculate bias - zero_point * weight (a constant).
-    tflite::tensor_utils::MatrixScalarMultiplyAccumulate(
+    tflite_micro::tensor_utils::MatrixScalarMultiplyAccumulate(
         quantized_gate_params.activation_weight,
         -1 * input_quantization_params.zero_point, 2, 2,
         quantized_gate_params.activation_zp_folded_bias);
@@ -451,7 +451,7 @@ CreateQuantizedGateData(
       quantized_gate_params.recurrent_zp_folded_bias[i] = 0;
     }
     // Calculate : -zero_point * weight since it is a constant
-    tflite::tensor_utils::MatrixScalarMultiplyAccumulate(
+    tflite_micro::tensor_utils::MatrixScalarMultiplyAccumulate(
         quantized_gate_params.recurrent_weight,
         -1 * output_quantization_params.zero_point, 2, 2,
         quantized_gate_params.recurrent_zp_folded_bias);
@@ -574,6 +574,6 @@ Create2x3x2X2Int16NodeContents(const float* input_data = nullptr,
                                const float* cell_state = nullptr);
 
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_TESTDATA_LSTM_TEST_DATA_H_
diff --git a/tensorflow/lite/micro/kernels/transpose.cc b/tensorflow/lite/micro/kernels/transpose.cc
index 710bfca4..1ce40795 100644
--- a/tensorflow/lite/micro/kernels/transpose.cc
+++ b/tensorflow/lite/micro/kernels/transpose.cc
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 constexpr int kInputTensor = 0;
@@ -74,7 +74,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* perm_tensor =
-      tflite::micro::GetEvalInput(context, node, kPermTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kPermTensor);
   const int32_t* perm_data = perm_tensor->data.i32;
   const int size = perm_tensor->dims->data[0];
   TransposeParams params;
@@ -87,21 +87,27 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   // on each cell. It's safe to implement per size of scalar type and this
   // trick keeps the total code size in a reasonable range.
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
   switch (input->type) {
     case kTfLiteFloat32:
-      reference_ops::Transpose(params, tflite::micro::GetTensorShape(input),
-                               tflite::micro::GetTensorData<float>(input),
-                               tflite::micro::GetTensorShape(output),
-                               tflite::micro::GetTensorData<float>(output));
+      reference_ops::Transpose(params, tflite_micro::micro::GetTensorShape(input),
+                               tflite_micro::micro::GetTensorData<float>(input),
+                               tflite_micro::micro::GetTensorShape(output),
+                               tflite_micro::micro::GetTensorData<float>(output));
+      break;
+    case kTfLiteInt16:
+      reference_ops::Transpose(params, tflite_micro::micro::GetTensorShape(input),
+                               tflite_micro::micro::GetTensorData<int16_t>(input),
+                               tflite_micro::micro::GetTensorShape(output),
+                               tflite_micro::micro::GetTensorData<int16_t>(output));
       break;
     case kTfLiteInt8:
-      reference_ops::Transpose(params, tflite::micro::GetTensorShape(input),
-                               tflite::micro::GetTensorData<int8_t>(input),
-                               tflite::micro::GetTensorShape(output),
-                               tflite::micro::GetTensorData<int8_t>(output));
+      reference_ops::Transpose(params, tflite_micro::micro::GetTensorShape(input),
+                               tflite_micro::micro::GetTensorData<int8_t>(input),
+                               tflite_micro::micro::GetTensorShape(output),
+                               tflite_micro::micro::GetTensorData<int8_t>(output));
       break;
     default:
       MicroPrintf(
@@ -117,6 +123,6 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_TRANSPOSE() {
-  return tflite::micro::RegisterOp(nullptr, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, Prepare, Eval);
 }
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/transpose_conv.cc b/tensorflow/lite/micro/kernels/transpose_conv.cc
index a2ac2b46..ed5b1b3e 100644
--- a/tensorflow/lite/micro/kernels/transpose_conv.cc
+++ b/tensorflow/lite/micro/kernels/transpose_conv.cc
@@ -26,7 +26,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 // For the TfLite transpose_conv implementation, input tensor 0 corresponds to
@@ -110,7 +110,7 @@ TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node,
     TF_LITE_ENSURE(context, output != nullptr);
     int output_channels = filter->dims->data[kConvQuantizedDimension];
 
-    TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(
+    TF_LITE_ENSURE_STATUS(tflite_micro::PopulateConvolutionQuantizationParams(
         context, input, filter, bias, output, kTfLiteActNone,
         &data->params.output_multiplier, &data->params.output_shift,
         &data->params.quantized_activation_min,
@@ -123,7 +123,7 @@ TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node,
     if (input->type == kTfLiteInt16) {
       TFLITE_DCHECK(filter->type == kTfLiteInt8);
       TFLITE_DCHECK(output->type == kTfLiteInt16);
-      if (bias->type == kTfLiteInt16) {
+      if (bias && (bias->type == kTfLiteInt16 || bias->type == kTfLiteInt32)) {
         TFLITE_DCHECK(
             context->RequestScratchBufferInArena(
                 context, GetTensorShape(bias).FlatSize() * sizeof(std::int64_t),
@@ -245,15 +245,15 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   const TfLiteEvalTensor* filter =
-      tflite::micro::GetEvalInput(context, node, kFilterTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kFilterTensor);
   const TfLiteEvalTensor* bias =
       (NumInputs(node) == 4)
-          ? tflite::micro::GetEvalInput(context, node, kBiasTensor)
+          ? tflite_micro::micro::GetEvalInput(context, node, kBiasTensor)
           : nullptr;
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
   TFLITE_DCHECK(node->user_data != nullptr);
   const OpData& data = *(static_cast<const OpData*>(node->user_data));
@@ -270,15 +270,15 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
                                &op_params.float_activation_max);
 
       reference_ops::TransposeConv(
-          op_params, tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<float>(input),
-          tflite::micro::GetTensorShape(filter),
-          tflite::micro::GetTensorData<float>(filter),
-          tflite::micro::GetTensorShape(bias),
-          tflite::micro::GetOptionalTensorData<float>(bias),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<float>(output),
-          tflite::micro::GetTensorShape(nullptr), nullptr);
+          op_params, tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<float>(input),
+          tflite_micro::micro::GetTensorShape(filter),
+          tflite_micro::micro::GetTensorData<float>(filter),
+          tflite_micro::micro::GetTensorShape(bias),
+          tflite_micro::micro::GetOptionalTensorData<float>(bias),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<float>(output),
+          tflite_micro::micro::GetTensorShape(nullptr), nullptr);
       break;
     }
     case kTfLiteInt8: {
@@ -286,15 +286,15 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
           context->GetScratchBuffer(context, data.scratch_buffer_index));
       reference_integer_ops::TransposeConv(
           data.params, data.per_channel_output_multiplier,
-          data.per_channel_output_shift, tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<int8_t>(input),
-          tflite::micro::GetTensorShape(filter),
-          tflite::micro::GetTensorData<int8_t>(filter),
-          tflite::micro::GetTensorShape(bias),
-          tflite::micro::GetOptionalTensorData<int32_t>(bias),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<int8_t>(output),
-          tflite::micro::GetTensorShape(nullptr), nullptr, scratch_buffer);
+          data.per_channel_output_shift, tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<int8_t>(input),
+          tflite_micro::micro::GetTensorShape(filter),
+          tflite_micro::micro::GetTensorData<int8_t>(filter),
+          tflite_micro::micro::GetTensorShape(bias),
+          tflite_micro::micro::GetOptionalTensorData<int32_t>(bias),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<int8_t>(output),
+          tflite_micro::micro::GetTensorShape(nullptr), nullptr, scratch_buffer);
       break;
     }
     case kTfLiteInt16: {
@@ -306,32 +306,50 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
         std::int64_t* bias_converted_buffer =
             static_cast<int64_t*>(context->GetScratchBuffer(
                 context, data.bias_converted_buffer_index));
-        for (int i = 0; i < tflite::micro::GetTensorShape(bias).FlatSize();
+        for (int i = 0; i < tflite_micro::micro::GetTensorShape(bias).FlatSize();
              i++) {
           bias_converted_buffer[i] = bias->data.i16[i];
         }
         reference_integer_ops::TransposeConv(
             data.params, data.per_channel_output_multiplier,
-            data.per_channel_output_shift, tflite::micro::GetTensorShape(input),
-            tflite::micro::GetTensorData<int16_t>(input),
-            tflite::micro::GetTensorShape(filter),
-            tflite::micro::GetTensorData<int8_t>(filter),
-            tflite::micro::GetTensorShape(bias), bias_converted_buffer,
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<int16_t>(output),
-            tflite::micro::GetTensorShape(nullptr), nullptr, scratch_buffer);
+            data.per_channel_output_shift, tflite_micro::micro::GetTensorShape(input),
+            tflite_micro::micro::GetTensorData<int16_t>(input),
+            tflite_micro::micro::GetTensorShape(filter),
+            tflite_micro::micro::GetTensorData<int8_t>(filter),
+            tflite_micro::micro::GetTensorShape(bias), bias_converted_buffer,
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int16_t>(output),
+            tflite_micro::micro::GetTensorShape(nullptr), nullptr, scratch_buffer);
+      } else if (bias != nullptr && bias->type == kTfLiteInt32) {
+        std::int64_t* bias_converted_buffer =
+            static_cast<int64_t*>(context->GetScratchBuffer(
+                context, data.bias_converted_buffer_index));
+        for (int i = 0; i < tflite_micro::micro::GetTensorShape(bias).FlatSize();
+             i++) {
+          bias_converted_buffer[i] = bias->data.i32[i];
+        }
+        reference_integer_ops::TransposeConv(
+            data.params, data.per_channel_output_multiplier,
+            data.per_channel_output_shift, tflite_micro::micro::GetTensorShape(input),
+            tflite_micro::micro::GetTensorData<int16_t>(input),
+            tflite_micro::micro::GetTensorShape(filter),
+            tflite_micro::micro::GetTensorData<int8_t>(filter),
+            tflite_micro::micro::GetTensorShape(bias), bias_converted_buffer,
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int16_t>(output),
+            tflite_micro::micro::GetTensorShape(nullptr), nullptr, scratch_buffer);
       } else {
         reference_integer_ops::TransposeConv(
             data.params, data.per_channel_output_multiplier,
-            data.per_channel_output_shift, tflite::micro::GetTensorShape(input),
-            tflite::micro::GetTensorData<int16_t>(input),
-            tflite::micro::GetTensorShape(filter),
-            tflite::micro::GetTensorData<int8_t>(filter),
-            tflite::micro::GetTensorShape(bias),
-            tflite::micro::GetOptionalTensorData<std::int64_t>(bias),
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<int16_t>(output),
-            tflite::micro::GetTensorShape(nullptr), nullptr, scratch_buffer);
+            data.per_channel_output_shift, tflite_micro::micro::GetTensorShape(input),
+            tflite_micro::micro::GetTensorData<int16_t>(input),
+            tflite_micro::micro::GetTensorShape(filter),
+            tflite_micro::micro::GetTensorData<int8_t>(filter),
+            tflite_micro::micro::GetTensorShape(bias),
+            tflite_micro::micro::GetOptionalTensorData<std::int64_t>(bias),
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int16_t>(output),
+            tflite_micro::micro::GetTensorShape(nullptr), nullptr, scratch_buffer);
       }
       break;
     }
@@ -346,7 +364,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_TRANSPOSE_CONV() {
-  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/transpose_conv_test.cc b/tensorflow/lite/micro/kernels/transpose_conv_test.cc
index 49d2c90f..a9e7358a 100644
--- a/tensorflow/lite/micro/kernels/transpose_conv_test.cc
+++ b/tensorflow/lite/micro/kernels/transpose_conv_test.cc
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -66,7 +66,7 @@ TfLiteStatus InvokeTransposeConv(TfLiteTensor* tensors, int tensors_size,
   int outputs_array_data[] = {1, 4};
   TfLiteIntArray* outputs_array = IntArrayFromInts(outputs_array_data);
 
-  const TFLMRegistration registration = tflite::Register_TRANSPOSE_CONV();
+  const TFLMRegistration registration = tflite_micro::Register_TRANSPOSE_CONV();
   micro::KernelRunner runner(registration, tensors, tensors_size, inputs_array,
                              outputs_array, conv_params);
 
@@ -149,7 +149,7 @@ TfLiteStatus TestTransposeConvQuantized(
   TfLiteTensor filter_tensor = CreateSymmetricPerChannelQuantizedTensor(
       filter_data, filter_quantized, filter_dims, filter_scales,
       filter_zero_points, &filter_quant, 0 /* quantized dimension */);
-  tflite::Quantize(expected_output_data, expected_output_quantized,
+  tflite_micro::Quantize(expected_output_data, expected_output_quantized,
                    output_dims_count, output_scale, 0);
 
   int output_shape_dims_data[] = {1, 0};
@@ -195,7 +195,7 @@ TfLiteStatus TestTransposeConvQuantized(
   TfLiteTensor filter_tensor = CreateSymmetricPerChannelQuantizedTensor(
       filter_data, filter_quantized, filter_dims, filter_scales,
       filter_zero_points, &filter_quant, 0 /* quantized dimension */);
-  tflite::Quantize(expected_output_data, expected_output_quantized,
+  tflite_micro::Quantize(expected_output_data, expected_output_quantized,
                    output_dims_count, output_scale, 0);
 
   int output_shape_dims_data[] = {1, 0};
@@ -223,25 +223,25 @@ TfLiteStatus TestTransposeConvQuantized(
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(SimpleTestFloat) {
-  float output_data[tflite::testing::kOutputElements];
+  float output_data[tflite_micro::testing::kOutputElements];
 
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteOk,
-      tflite::testing::TestTransposeConvFloat(
-          tflite::testing::kInputShape, tflite::testing::kInputData,
-          tflite::testing::kFilterShape, tflite::testing::kFilterData,
-          tflite::testing::kBiasShape, tflite::testing::kBiasData,
-          tflite::testing::kOutputShape, tflite::testing::kGoldenData,
-          &tflite::testing::common_conv_params, output_data));
+      tflite_micro::testing::TestTransposeConvFloat(
+          tflite_micro::testing::kInputShape, tflite_micro::testing::kInputData,
+          tflite_micro::testing::kFilterShape, tflite_micro::testing::kFilterData,
+          tflite_micro::testing::kBiasShape, tflite_micro::testing::kBiasData,
+          tflite_micro::testing::kOutputShape, tflite_micro::testing::kGoldenData,
+          &tflite_micro::testing::common_conv_params, output_data));
 }
 
 TF_LITE_MICRO_TEST(fusedRELUTest) {
-  float output_data[tflite::testing::kOutputElements];
+  float output_data[tflite_micro::testing::kOutputElements];
   float golden_data[] = {29,  24,  0, 0, 99,  72,  0,   0,
                          207, 186, 0, 0, 263, 292, 141, 0};
   int filter_shape[] = {4, 1, 3, 3, 1};
@@ -258,16 +258,16 @@ TF_LITE_MICRO_TEST(fusedRELUTest) {
                                   kTfLiteNoType};
 
   TF_LITE_MICRO_EXPECT_EQ(
-      kTfLiteOk, tflite::testing::TestTransposeConvFloat(
+      kTfLiteOk, tflite_micro::testing::TestTransposeConvFloat(
                      input_shape, input_data, filter_shape, filter_data,
-                     tflite::testing::kBiasShape, tflite::testing::kBiasData,
-                     tflite::testing::kOutputShape, golden_data, &conv_params,
+                     tflite_micro::testing::kBiasShape, tflite_micro::testing::kBiasData,
+                     tflite_micro::testing::kOutputShape, golden_data, &conv_params,
                      output_data));
 }
 
 TF_LITE_MICRO_TEST(AccuracyWithFusedActivationTest) {
   int output_shape[] = {4, 1, 3, 4, 1};
-  float output_data[tflite::testing::kOutputElements];
+  float output_data[tflite_micro::testing::kOutputElements];
   float golden_data[] = {1615, 1938, 0, 0, 2584, 1615, 0, 0, 323, 1292, 0, 0};
   int filter_shape[] = {4, 1, 3, 3, 1};
   float filter_data[] = {9, 5, 6, 9, 8, 5, 3, 1, 4};
@@ -282,9 +282,9 @@ TF_LITE_MICRO_TEST(AccuracyWithFusedActivationTest) {
                                   kTfLiteNoType};
 
   TF_LITE_MICRO_EXPECT_EQ(
-      kTfLiteOk, tflite::testing::TestTransposeConvFloat(
+      kTfLiteOk, tflite_micro::testing::TestTransposeConvFloat(
                      input_shape, input_data, filter_shape, filter_data,
-                     tflite::testing::kBiasShape, tflite::testing::kBiasData,
+                     tflite_micro::testing::kBiasShape, tflite_micro::testing::kBiasData,
                      output_shape, golden_data, &conv_params, output_data));
 }
 
@@ -312,13 +312,13 @@ TF_LITE_MICRO_TEST(MultiChannelBiasWithFusedActivationTest) {
 
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteOk,
-      tflite::testing::TestTransposeConvFloat(
+      tflite_micro::testing::TestTransposeConvFloat(
           input_shape, input_data, filter_shape, filter_data, bias_shape,
           bias_data, output_shape, golden_data, &conv_params, output_data));
 }
 
 TF_LITE_MICRO_TEST(SimpleTestQuantizedPerChannel) {
-  int8_t output_data[tflite::testing::kOutputElements];
+  int8_t output_data[tflite_micro::testing::kOutputElements];
 
   const float input_scale = 0.5f;
   const float output_scale = 30.0f;
@@ -326,28 +326,28 @@ TF_LITE_MICRO_TEST(SimpleTestQuantizedPerChannel) {
   const int input_zero_point = 0;
   const int output_zero_point = 0;
 
-  int8_t input_quantized[tflite::testing::kInputElements];
-  int8_t filter_quantized[tflite::testing::kFilterElements];
-  int32_t bias_quantized[tflite::testing::kBiasElements];
-  int8_t golden_quantized[tflite::testing::kOutputElements];
-  int zero_points[tflite::testing::kBiasElements + 1];
-  float scales[tflite::testing::kBiasElements + 1];
+  int8_t input_quantized[tflite_micro::testing::kInputElements];
+  int8_t filter_quantized[tflite_micro::testing::kFilterElements];
+  int32_t bias_quantized[tflite_micro::testing::kBiasElements];
+  int8_t golden_quantized[tflite_micro::testing::kOutputElements];
+  int zero_points[tflite_micro::testing::kBiasElements + 1];
+  float scales[tflite_micro::testing::kBiasElements + 1];
 
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteOk,
-      tflite::testing::TestTransposeConvQuantized(
-          tflite::testing::kInputShape, tflite::testing::kInputData,
+      tflite_micro::testing::TestTransposeConvQuantized(
+          tflite_micro::testing::kInputShape, tflite_micro::testing::kInputData,
           input_quantized, input_scale, input_zero_point,
-          tflite::testing::kFilterShape, tflite::testing::kFilterData,
-          filter_quantized, filter_scale, tflite::testing::kBiasShape,
-          tflite::testing::kBiasData, bias_quantized, scales, zero_points,
-          tflite::testing::kOutputShape, tflite::testing::kGoldenData,
+          tflite_micro::testing::kFilterShape, tflite_micro::testing::kFilterData,
+          filter_quantized, filter_scale, tflite_micro::testing::kBiasShape,
+          tflite_micro::testing::kBiasData, bias_quantized, scales, zero_points,
+          tflite_micro::testing::kOutputShape, tflite_micro::testing::kGoldenData,
           golden_quantized, output_scale, output_zero_point,
-          &tflite::testing::common_conv_params, output_data));
+          &tflite_micro::testing::common_conv_params, output_data));
 }
 
 TF_LITE_MICRO_TEST(SimpleTestQuantized16x8PerChannel) {
-  int16_t output_data[tflite::testing::kOutputElements];
+  int16_t output_data[tflite_micro::testing::kOutputElements];
 
   const float input_scale = 1.0f;
   const float output_scale = 1.0f;
@@ -355,28 +355,28 @@ TF_LITE_MICRO_TEST(SimpleTestQuantized16x8PerChannel) {
   const int input_zero_point = 0;
   const int output_zero_point = 0;
 
-  int16_t input_quantized[tflite::testing::kInputElements];
-  int8_t filter_quantized[tflite::testing::kFilterElements];
-  std::int64_t bias_quantized[tflite::testing::kBiasElements];
-  int16_t golden_quantized[tflite::testing::kOutputElements];
-  int zero_points[tflite::testing::kBiasElements + 1];
-  float scales[tflite::testing::kBiasElements + 1];
+  int16_t input_quantized[tflite_micro::testing::kInputElements];
+  int8_t filter_quantized[tflite_micro::testing::kFilterElements];
+  std::int64_t bias_quantized[tflite_micro::testing::kBiasElements];
+  int16_t golden_quantized[tflite_micro::testing::kOutputElements];
+  int zero_points[tflite_micro::testing::kBiasElements + 1];
+  float scales[tflite_micro::testing::kBiasElements + 1];
 
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteOk,
-      tflite::testing::TestTransposeConvQuantized(
-          tflite::testing::kInputShape, tflite::testing::kInputData,
+      tflite_micro::testing::TestTransposeConvQuantized(
+          tflite_micro::testing::kInputShape, tflite_micro::testing::kInputData,
           input_quantized, input_scale, input_zero_point,
-          tflite::testing::kFilterShape, tflite::testing::kFilterData,
-          filter_quantized, filter_scale, tflite::testing::kBiasShape,
-          tflite::testing::kBiasData, bias_quantized, scales, zero_points,
-          tflite::testing::kOutputShape, tflite::testing::kGoldenData,
+          tflite_micro::testing::kFilterShape, tflite_micro::testing::kFilterData,
+          filter_quantized, filter_scale, tflite_micro::testing::kBiasShape,
+          tflite_micro::testing::kBiasData, bias_quantized, scales, zero_points,
+          tflite_micro::testing::kOutputShape, tflite_micro::testing::kGoldenData,
           golden_quantized, output_scale, output_zero_point,
-          &tflite::testing::common_conv_params, output_data));
+          &tflite_micro::testing::common_conv_params, output_data));
 }
 
 TF_LITE_MICRO_TEST(SimpleTestQuantized16x8PerChannelWithInt16Bias) {
-  int16_t output_data[tflite::testing::kOutputElements];
+  int16_t output_data[tflite_micro::testing::kOutputElements];
 
   const float input_scale = 1.0f;
   const float output_scale = 1.0f;
@@ -384,41 +384,41 @@ TF_LITE_MICRO_TEST(SimpleTestQuantized16x8PerChannelWithInt16Bias) {
   const int input_zero_point = 0;
   const int output_zero_point = 0;
 
-  int16_t input_quantized[tflite::testing::kInputElements];
-  int8_t filter_quantized[tflite::testing::kFilterElements];
-  int16_t bias_quantized[tflite::testing::kBiasElements];
-  int16_t golden_quantized[tflite::testing::kOutputElements];
-  int zero_points[tflite::testing::kBiasElements + 1];
-  float scales[tflite::testing::kBiasElements + 1];
+  int16_t input_quantized[tflite_micro::testing::kInputElements];
+  int8_t filter_quantized[tflite_micro::testing::kFilterElements];
+  int16_t bias_quantized[tflite_micro::testing::kBiasElements];
+  int16_t golden_quantized[tflite_micro::testing::kOutputElements];
+  int zero_points[tflite_micro::testing::kBiasElements + 1];
+  float scales[tflite_micro::testing::kBiasElements + 1];
 
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteOk,
-      tflite::testing::TestTransposeConvQuantized(
-          tflite::testing::kInputShape, tflite::testing::kInputData,
+      tflite_micro::testing::TestTransposeConvQuantized(
+          tflite_micro::testing::kInputShape, tflite_micro::testing::kInputData,
           input_quantized, input_scale, input_zero_point,
-          tflite::testing::kFilterShape, tflite::testing::kFilterData,
-          filter_quantized, filter_scale, tflite::testing::kBiasShape,
-          tflite::testing::kBiasData, bias_quantized, scales, zero_points,
-          tflite::testing::kOutputShape, tflite::testing::kGoldenData,
+          tflite_micro::testing::kFilterShape, tflite_micro::testing::kFilterData,
+          filter_quantized, filter_scale, tflite_micro::testing::kBiasShape,
+          tflite_micro::testing::kBiasData, bias_quantized, scales, zero_points,
+          tflite_micro::testing::kOutputShape, tflite_micro::testing::kGoldenData,
           golden_quantized, output_scale, output_zero_point,
-          &tflite::testing::common_conv_params, output_data));
+          &tflite_micro::testing::common_conv_params, output_data));
 }
 
 TF_LITE_MICRO_TEST(InputOutputDifferentTypeIsError) {
-  using tflite::testing::CreateQuantizedTensor;
-  using tflite::testing::CreateTensor;
-  using tflite::testing::IntArrayFromInts;
-
-  TfLiteIntArray* input_dims = IntArrayFromInts(tflite::testing::kInputShape);
-  TfLiteIntArray* filter_dims = IntArrayFromInts(tflite::testing::kFilterShape);
-  TfLiteIntArray* bias_dims = IntArrayFromInts(tflite::testing::kBiasShape);
-  TfLiteIntArray* output_dims = IntArrayFromInts(tflite::testing::kOutputShape);
-  const int output_dims_count = tflite::ElementCount(*output_dims);
+  using tflite_micro::testing::CreateQuantizedTensor;
+  using tflite_micro::testing::CreateTensor;
+  using tflite_micro::testing::IntArrayFromInts;
+
+  TfLiteIntArray* input_dims = IntArrayFromInts(tflite_micro::testing::kInputShape);
+  TfLiteIntArray* filter_dims = IntArrayFromInts(tflite_micro::testing::kFilterShape);
+  TfLiteIntArray* bias_dims = IntArrayFromInts(tflite_micro::testing::kBiasShape);
+  TfLiteIntArray* output_dims = IntArrayFromInts(tflite_micro::testing::kOutputShape);
+  const int output_dims_count = tflite_micro::ElementCount(*output_dims);
   constexpr int inputs_size = 4;
   constexpr int outputs_size = 1;
   constexpr int tensors_size = inputs_size + outputs_size;
 
-  int8_t output_data[tflite::testing::kOutputElements];
+  int8_t output_data[tflite_micro::testing::kOutputElements];
 
   int output_shape_dims_data[] = {1, 0};
   int32_t* output_shape = nullptr;
@@ -426,35 +426,35 @@ TF_LITE_MICRO_TEST(InputOutputDifferentTypeIsError) {
 
   TfLiteTensor tensors[tensors_size] = {
       CreateTensor(output_shape, output_shape_dims),
-      CreateTensor(tflite::testing::kInputData, input_dims),
-      CreateTensor(tflite::testing::kFilterData, filter_dims),
-      CreateTensor(tflite::testing::kBiasData, bias_dims),
+      CreateTensor(tflite_micro::testing::kInputData, input_dims),
+      CreateTensor(tflite_micro::testing::kFilterData, filter_dims),
+      CreateTensor(tflite_micro::testing::kBiasData, bias_dims),
       CreateQuantizedTensor(output_data, output_dims, /*scale=*/1.0f,
                             /*zero_point=*/0),
   };
   TF_LITE_MICRO_EXPECT_EQ(
-      kTfLiteError, tflite::testing::InvokeTransposeConv(
+      kTfLiteError, tflite_micro::testing::InvokeTransposeConv(
                         tensors, tensors_size, output_dims_count,
-                        &tflite::testing::common_conv_params, output_data));
+                        &tflite_micro::testing::common_conv_params, output_data));
 }
 
 TF_LITE_MICRO_TEST(HybridModeIsError) {
-  using tflite::testing::CreateQuantizedTensor;
-  using tflite::testing::CreateTensor;
-  using tflite::testing::IntArrayFromInts;
+  using tflite_micro::testing::CreateQuantizedTensor;
+  using tflite_micro::testing::CreateTensor;
+  using tflite_micro::testing::IntArrayFromInts;
 
-  TfLiteIntArray* input_dims = IntArrayFromInts(tflite::testing::kInputShape);
-  TfLiteIntArray* filter_dims = IntArrayFromInts(tflite::testing::kFilterShape);
-  TfLiteIntArray* bias_dims = IntArrayFromInts(tflite::testing::kBiasShape);
-  TfLiteIntArray* output_dims = IntArrayFromInts(tflite::testing::kOutputShape);
-  const int output_dims_count = tflite::ElementCount(*output_dims);
+  TfLiteIntArray* input_dims = IntArrayFromInts(tflite_micro::testing::kInputShape);
+  TfLiteIntArray* filter_dims = IntArrayFromInts(tflite_micro::testing::kFilterShape);
+  TfLiteIntArray* bias_dims = IntArrayFromInts(tflite_micro::testing::kBiasShape);
+  TfLiteIntArray* output_dims = IntArrayFromInts(tflite_micro::testing::kOutputShape);
+  const int output_dims_count = tflite_micro::ElementCount(*output_dims);
 
   constexpr int inputs_size = 4;
   constexpr int outputs_size = 1;
   constexpr int tensors_size = inputs_size + outputs_size;
 
-  int8_t filter_data[tflite::testing::kFilterElements] = {};
-  float output_data[tflite::testing::kOutputElements];
+  int8_t filter_data[tflite_micro::testing::kFilterElements] = {};
+  float output_data[tflite_micro::testing::kOutputElements];
 
   int output_shape_dims_data[] = {1, 0};
   int32_t* output_shape = nullptr;
@@ -462,18 +462,18 @@ TF_LITE_MICRO_TEST(HybridModeIsError) {
 
   TfLiteTensor tensors[tensors_size] = {
       CreateTensor(output_shape, output_shape_dims),
-      CreateTensor(tflite::testing::kInputData, input_dims),
+      CreateTensor(tflite_micro::testing::kInputData, input_dims),
       CreateQuantizedTensor(filter_data, filter_dims,
                             /*scale=*/1.0f,
                             /*zero_point=*/0),
-      CreateTensor(tflite::testing::kBiasData, bias_dims),
+      CreateTensor(tflite_micro::testing::kBiasData, bias_dims),
       CreateTensor(output_data, output_dims),
   };
 
   TF_LITE_MICRO_EXPECT_EQ(
-      kTfLiteError, tflite::testing::InvokeTransposeConv(
+      kTfLiteError, tflite_micro::testing::InvokeTransposeConv(
                         tensors, tensors_size, output_dims_count,
-                        &tflite::testing::common_conv_params, output_data));
+                        &tflite_micro::testing::common_conv_params, output_data));
 }
 
 TF_LITE_MICRO_TESTS_END
diff --git a/tensorflow/lite/micro/kernels/transpose_test.cc b/tensorflow/lite/micro/kernels/transpose_test.cc
index 12bc431f..ddd35808 100644
--- a/tensorflow/lite/micro/kernels/transpose_test.cc
+++ b/tensorflow/lite/micro/kernels/transpose_test.cc
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -83,7 +83,7 @@ template <typename T>
 TfLiteStatus ValidateTranspose(TfLiteTensor* tensors, int tensors_size,
                                const T* expected_output_data, T* output_data,
                                int output_length,
-                               tflite::TransposeParams* params,
+                               tflite_micro::TransposeParams* params,
                                float tolerance = 1e-5) {
   TfLiteStatus status = InvokeTranspose(tensors, tensors_size, output_data,
                                         output_length, params);
@@ -131,7 +131,7 @@ void TestTranspose(int* input_dims_data, T* input_data, int* output_dims_data,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -143,9 +143,9 @@ TF_LITE_MICRO_TEST(1D) {
   int8_t output_data[3];
   const int8_t expected_output_data[] = {0, 1, 2};
 
-  tflite::TransposeParams params = {1, {0}};
+  tflite_micro::TransposeParams params = {1, {0}};
 
-  tflite::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
+  tflite_micro::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
                                  expected_output_data, output_data, &params);
 }
 
@@ -157,9 +157,9 @@ TF_LITE_MICRO_TEST(2DPerm1) {
   int8_t output_data[6];
   const int8_t expected_output_data[] = {0, 2, 4, 1, 3, 5};
 
-  tflite::TransposeParams params = {2, {1, 0}};
+  tflite_micro::TransposeParams params = {2, {1, 0}};
 
-  tflite::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
+  tflite_micro::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
                                  expected_output_data, output_data, &params);
 }
 
@@ -173,9 +173,9 @@ TF_LITE_MICRO_TEST(2D4x4KernelLeftOverRightSide) {
                                          2, 8,  14, 20, 3, 9,  15, 21,
                                          4, 10, 16, 22, 5, 11, 17, 23};
 
-  tflite::TransposeParams params = {2, {1, 0}};
+  tflite_micro::TransposeParams params = {2, {1, 0}};
 
-  tflite::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
+  tflite_micro::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
                                  expected_output_data, output_data, &params);
 }
 
@@ -189,9 +189,9 @@ TF_LITE_MICRO_TEST(2D4x4KernelLeftOverBottomSide) {
                                          9,  13, 17, 21, 2,  6,  10, 14,
                                          18, 22, 3,  7,  11, 15, 19, 23};
 
-  tflite::TransposeParams params = {2, {1, 0}};
+  tflite_micro::TransposeParams params = {2, {1, 0}};
 
-  tflite::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
+  tflite_micro::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
                                  expected_output_data, output_data, &params);
 }
 
@@ -205,9 +205,9 @@ TF_LITE_MICRO_TEST(3D) {
                                          9,  13, 17, 21, 2,  6,  10, 14,
                                          18, 22, 3,  7,  11, 15, 19, 23};
 
-  tflite::TransposeParams params = {3, {2, 0, 1}};
+  tflite_micro::TransposeParams params = {3, {2, 0, 1}};
 
-  tflite::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
+  tflite_micro::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
                                  expected_output_data, output_data, &params);
 }
 
@@ -219,9 +219,9 @@ TF_LITE_MICRO_TEST(1DNotShrinked) {
   float output_data[1];
   const float expected_output_data[] = {0};
 
-  tflite::TransposeParams params = {1, {0}};
+  tflite_micro::TransposeParams params = {1, {0}};
 
-  tflite::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
+  tflite_micro::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
                                  expected_output_data, output_data, &params);
 }
 
@@ -233,9 +233,9 @@ TF_LITE_MICRO_TEST(2DShrinkedOneTime) {
   float output_data[2];
   const float expected_output_data[] = {0, 1};
 
-  tflite::TransposeParams params = {2, {1, 0}};
+  tflite_micro::TransposeParams params = {2, {1, 0}};
 
-  tflite::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
+  tflite_micro::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
                                  expected_output_data, output_data, &params);
 }
 
@@ -247,9 +247,9 @@ TF_LITE_MICRO_TEST(2DShrinkedTwoTimes) {
   float output_data[1];
   const float expected_output_data[] = {0};
 
-  tflite::TransposeParams params = {2, {1, 0}};
+  tflite_micro::TransposeParams params = {2, {1, 0}};
 
-  tflite::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
+  tflite_micro::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
                                  expected_output_data, output_data, &params);
 }
 
@@ -261,9 +261,9 @@ TF_LITE_MICRO_TEST(3DShrinkedOneTime) {
   float output_data[6];
   const float expected_output_data[] = {0, 1, 2, 3, 4, 5};
 
-  tflite::TransposeParams params = {3, {0, 2, 1}};
+  tflite_micro::TransposeParams params = {3, {0, 2, 1}};
 
-  tflite::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
+  tflite_micro::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
                                  expected_output_data, output_data, &params);
 }
 
@@ -275,9 +275,9 @@ TF_LITE_MICRO_TEST(3DShrinkedTwoTimes) {
   float output_data[3];
   const float expected_output_data[] = {0, 1, 2};
 
-  tflite::TransposeParams params = {3, {1, 2, 0}};
+  tflite_micro::TransposeParams params = {3, {1, 2, 0}};
 
-  tflite::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
+  tflite_micro::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
                                  expected_output_data, output_data, &params);
 }
 
@@ -289,9 +289,9 @@ TF_LITE_MICRO_TEST(3DShrinkedAll) {
   float output_data[1];
   const float expected_output_data[] = {0};
 
-  tflite::TransposeParams params = {3, {1, 2, 0}};
+  tflite_micro::TransposeParams params = {3, {1, 2, 0}};
 
-  tflite::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
+  tflite_micro::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
                                  expected_output_data, output_data, &params);
 }
 
@@ -303,9 +303,9 @@ TF_LITE_MICRO_TEST(4DShrinkedOneTimes) {
   float output_data[12];
   const float expected_output_data[] = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11};
 
-  tflite::TransposeParams params = {4, {3, 0, 1, 2}};
+  tflite_micro::TransposeParams params = {4, {3, 0, 1, 2}};
 
-  tflite::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
+  tflite_micro::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
                                  expected_output_data, output_data, &params);
 }
 
@@ -317,9 +317,9 @@ TF_LITE_MICRO_TEST(4DShrinkedTwoTimes) {
   float output_data[6];
   const float expected_output_data[] = {0, 1, 2, 3, 4, 5};
 
-  tflite::TransposeParams params = {4, {0, 3, 1, 2}};
+  tflite_micro::TransposeParams params = {4, {0, 3, 1, 2}};
 
-  tflite::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
+  tflite_micro::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
                                  expected_output_data, output_data, &params);
 }
 
@@ -331,9 +331,9 @@ TF_LITE_MICRO_TEST(4DShrinkedThreeTimes) {
   float output_data[2];
   const float expected_output_data[] = {0, 1};
 
-  tflite::TransposeParams params = {4, {3, 2, 1, 0}};
+  tflite_micro::TransposeParams params = {4, {3, 2, 1, 0}};
 
-  tflite::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
+  tflite_micro::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
                                  expected_output_data, output_data, &params);
 }
 
@@ -345,9 +345,9 @@ TF_LITE_MICRO_TEST(4DShrinkedFourTimes) {
   float output_data[1];
   const float expected_output_data[] = {0};
 
-  tflite::TransposeParams params = {4, {2, 3, 1, 0}};
+  tflite_micro::TransposeParams params = {4, {2, 3, 1, 0}};
 
-  tflite::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
+  tflite_micro::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
                                  expected_output_data, output_data, &params);
 }
 
@@ -359,9 +359,9 @@ TF_LITE_MICRO_TEST(3DFlatten) {
   float output_data[12];
   const float expected_output_data[] = {0, 3, 1, 4, 2, 5, 6, 9, 7, 10, 8, 11};
 
-  tflite::TransposeParams params = {3, {0, 2, 1}};
+  tflite_micro::TransposeParams params = {3, {0, 2, 1}};
 
-  tflite::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
+  tflite_micro::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
                                  expected_output_data, output_data, &params);
 }
 
@@ -374,9 +374,9 @@ TF_LITE_MICRO_TEST(4DFlatten) {
   const float expected_output_data[] = {0, 2,  1, 3,  4,  6,  5,  7,
                                         8, 10, 9, 11, 12, 14, 13, 15};
 
-  tflite::TransposeParams params = {4, {0, 1, 3, 2}};
+  tflite_micro::TransposeParams params = {4, {0, 1, 3, 2}};
 
-  tflite::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
+  tflite_micro::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
                                  expected_output_data, output_data, &params);
 }
 
@@ -389,9 +389,9 @@ TF_LITE_MICRO_TEST(4DFlattenTwo) {
   const float expected_output_data[] = {0, 4,  1, 5,  2,  6,  3,  7,
                                         8, 12, 9, 13, 10, 14, 11, 15};
 
-  tflite::TransposeParams params = {4, {0, 2, 3, 1}};
+  tflite_micro::TransposeParams params = {4, {0, 2, 3, 1}};
 
-  tflite::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
+  tflite_micro::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
                                  expected_output_data, output_data, &params);
 }
 
@@ -400,16 +400,16 @@ TF_LITE_MICRO_TEST(3DDividedIntoTwo2DsOne) {
   float expected_output_data[24];
   int32_t shape[] = {2, 3, 4};
   int32_t perms[] = {1, 2, 0};
-  tflite::testing::RunTestPermutation(3, shape, perms, input_data,
+  tflite_micro::testing::RunTestPermutation(3, shape, perms, input_data,
                                       expected_output_data);
   int input_dims_data[] = {3, 2, 3, 4};
   int output_dims_data[] = {3, 2, 3, 4};
 
   float output_data[24];
 
-  tflite::TransposeParams params = {3, {1, 2, 0}};
+  tflite_micro::TransposeParams params = {3, {1, 2, 0}};
 
-  tflite::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
+  tflite_micro::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
                                  expected_output_data, output_data, &params);
 }
 
@@ -418,16 +418,16 @@ TF_LITE_MICRO_TEST(3DDividedIntoTwo2DsTwo) {
   float expected_output_data[24];
   int32_t shape[] = {2, 3, 4};
   int32_t perms[] = {2, 0, 1};
-  tflite::testing::RunTestPermutation(3, shape, perms, input_data,
+  tflite_micro::testing::RunTestPermutation(3, shape, perms, input_data,
                                       expected_output_data);
   int input_dims_data[] = {3, 2, 3, 4};
   int output_dims_data[] = {3, 2, 3, 4};
 
   float output_data[24];
 
-  tflite::TransposeParams params = {3, {2, 0, 1}};
+  tflite_micro::TransposeParams params = {3, {2, 0, 1}};
 
-  tflite::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
+  tflite_micro::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
                                  expected_output_data, output_data, &params);
 }
 
@@ -436,16 +436,16 @@ TF_LITE_MICRO_TEST(4DDividedIntoTwo2DsOne) {
   int32_t perms[] = {1, 2, 3, 0};
   float input_data[48];
   float expected_output_data[48];
-  tflite::testing::RunTestPermutation(4, shape, perms, input_data,
+  tflite_micro::testing::RunTestPermutation(4, shape, perms, input_data,
                                       expected_output_data);
   int input_dims_data[] = {4, 2, 3, 4, 2};
   int output_dims_data[] = {4, 2, 3, 4, 2};
 
   float output_data[48];
 
-  tflite::TransposeParams params = {4, {1, 2, 3, 0}};
+  tflite_micro::TransposeParams params = {4, {1, 2, 3, 0}};
 
-  tflite::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
+  tflite_micro::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
                                  expected_output_data, output_data, &params);
 }
 TF_LITE_MICRO_TEST(4DDividedIntoTwo2DsTwo) {
@@ -453,16 +453,16 @@ TF_LITE_MICRO_TEST(4DDividedIntoTwo2DsTwo) {
   int32_t perms[] = {2, 3, 0, 1};
   float input_data[48];
   float expected_output_data[48];
-  tflite::testing::RunTestPermutation(4, shape, perms, input_data,
+  tflite_micro::testing::RunTestPermutation(4, shape, perms, input_data,
                                       expected_output_data);
   int input_dims_data[] = {4, 2, 3, 4, 2};
   int output_dims_data[] = {4, 2, 3, 4, 2};
 
   float output_data[48];
 
-  tflite::TransposeParams params = {4, {2, 3, 0, 1}};
+  tflite_micro::TransposeParams params = {4, {2, 3, 0, 1}};
 
-  tflite::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
+  tflite_micro::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
                                  expected_output_data, output_data, &params);
 }
 
@@ -471,16 +471,16 @@ TF_LITE_MICRO_TEST(4DDividedIntoTwo2DsThree) {
   int32_t perms[] = {3, 0, 1, 2};
   float input_data[48];
   float expected_output_data[48];
-  tflite::testing::RunTestPermutation(4, shape, perms, input_data,
+  tflite_micro::testing::RunTestPermutation(4, shape, perms, input_data,
                                       expected_output_data);
   int input_dims_data[] = {4, 2, 3, 4, 2};
   int output_dims_data[] = {4, 2, 3, 4, 2};
 
   float output_data[48];
 
-  tflite::TransposeParams params = {4, {3, 0, 1, 2}};
+  tflite_micro::TransposeParams params = {4, {3, 0, 1, 2}};
 
-  tflite::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
+  tflite_micro::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
                                  expected_output_data, output_data, &params);
 }
 
@@ -489,16 +489,16 @@ TF_LITE_MICRO_TEST(5DDividedIntoTwo2DsOne) {
   int32_t perms[] = {1, 4, 2, 3, 0};
   float input_data[48];
   float expected_output_data[48];
-  tflite::testing::RunTestPermutation(5, shape, perms, input_data,
+  tflite_micro::testing::RunTestPermutation(5, shape, perms, input_data,
                                       expected_output_data);
   int input_dims_data[] = {5, 2, 3, 2, 2, 2};
   int output_dims_data[] = {5, 2, 3, 2, 2, 2};
 
   float output_data[48];
 
-  tflite::TransposeParams params = {5, {1, 4, 2, 3, 0}};
+  tflite_micro::TransposeParams params = {5, {1, 4, 2, 3, 0}};
 
-  tflite::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
+  tflite_micro::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
                                  expected_output_data, output_data, &params);
 }
 
@@ -507,16 +507,16 @@ TF_LITE_MICRO_TEST(5DDividedIntoTwo2DsTwo) {
   int32_t perms[] = {2, 3, 0, 4, 1};
   float input_data[48];
   float expected_output_data[48];
-  tflite::testing::RunTestPermutation(5, shape, perms, input_data,
+  tflite_micro::testing::RunTestPermutation(5, shape, perms, input_data,
                                       expected_output_data);
   int input_dims_data[] = {5, 2, 3, 2, 2, 2};
   int output_dims_data[] = {5, 2, 3, 2, 2, 2};
 
   float output_data[48];
 
-  tflite::TransposeParams params = {5, {2, 3, 0, 4, 1}};
+  tflite_micro::TransposeParams params = {5, {2, 3, 0, 4, 1}};
 
-  tflite::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
+  tflite_micro::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
                                  expected_output_data, output_data, &params);
 }
 
@@ -525,16 +525,16 @@ TF_LITE_MICRO_TEST(5DDividedIntoTwo2DsThree) {
   int32_t perms[] = {3, 0, 4, 1, 2};
   float input_data[48];
   float expected_output_data[48];
-  tflite::testing::RunTestPermutation(5, shape, perms, input_data,
+  tflite_micro::testing::RunTestPermutation(5, shape, perms, input_data,
                                       expected_output_data);
   int input_dims_data[] = {5, 2, 3, 2, 2, 2};
   int output_dims_data[] = {5, 2, 3, 2, 2, 2};
 
   float output_data[48];
 
-  tflite::TransposeParams params = {5, {3, 0, 4, 1, 2}};
+  tflite_micro::TransposeParams params = {5, {3, 0, 4, 1, 2}};
 
-  tflite::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
+  tflite_micro::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
                                  expected_output_data, output_data, &params);
 }
 
@@ -546,9 +546,9 @@ TF_LITE_MICRO_TEST(SimpleTestNoReorder) {
   float output_data[6];
   const float expected_output_data[] = {0, 1, 2, 3, 4, 5};
 
-  tflite::TransposeParams params = {4, {0, 1, 2, 3}};
+  tflite_micro::TransposeParams params = {4, {0, 1, 2, 3}};
 
-  tflite::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
+  tflite_micro::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
                                  expected_output_data, output_data, &params);
 }
 
@@ -560,9 +560,9 @@ TF_LITE_MICRO_TEST(SimpleTestWithReorder) {
   float output_data[6];
   const float expected_output_data[] = {0, 3, 1, 4, 2, 5};
 
-  tflite::TransposeParams params = {4, {2, 1, 3, 0}};
+  tflite_micro::TransposeParams params = {4, {2, 1, 3, 0}};
 
-  tflite::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
+  tflite_micro::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
                                  expected_output_data, output_data, &params);
 }
 
@@ -582,9 +582,9 @@ TF_LITE_MICRO_TEST(ComplexTestWithReorder) {
       15, 16, 17, 18, 19, 35, 36, 37, 38, 39, 55,  56,  57,  58,  59,
       75, 76, 77, 78, 79, 95, 96, 97, 98, 99, 115, 116, 117, 118, 119};
 
-  tflite::TransposeParams params = {4, {2, 0, 1, 3}};
+  tflite_micro::TransposeParams params = {4, {2, 0, 1, 3}};
 
-  tflite::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
+  tflite_micro::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
                                  expected_output_data, output_data, &params);
 }
 
@@ -604,9 +604,9 @@ TF_LITE_MICRO_TEST(Complex5DTestWithReorder) {
       70, 75, 71, 76, 72, 77,  73,  78,  74,  79,  90,  95,  91,  96,  92,
       97, 93, 98, 94, 99, 110, 115, 111, 116, 112, 117, 113, 118, 114, 119};
 
-  tflite::TransposeParams params = {5, {2, 0, 1, 4, 3}};
+  tflite_micro::TransposeParams params = {5, {2, 0, 1, 4, 3}};
 
-  tflite::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
+  tflite_micro::testing::TestTranspose(input_dims_data, input_data, output_dims_data,
                                  expected_output_data, output_data, &params);
 }
 
diff --git a/tensorflow/lite/micro/kernels/unidirectional_sequence_lstm.cc b/tensorflow/lite/micro/kernels/unidirectional_sequence_lstm.cc
index b2966805..698dfebd 100644
--- a/tensorflow/lite/micro/kernels/unidirectional_sequence_lstm.cc
+++ b/tensorflow/lite/micro/kernels/unidirectional_sequence_lstm.cc
@@ -27,7 +27,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/lstm_eval.h"
 #include "tensorflow/lite/micro/kernels/lstm_shared.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 /*Helper Functions*/
@@ -161,8 +161,8 @@ TfLiteStatus UnidirectionalSequenceLstmEval(TfLiteContext* context,
 }  // namespace
 
 TFLMRegistration Register_UNIDIRECTIONAL_SEQUENCE_LSTM() {
-  return tflite::micro::RegisterOp(UnidirectionalSequenceLstmInit,
+  return tflite_micro::micro::RegisterOp(UnidirectionalSequenceLstmInit,
                                    UnidirectionalSequenceLstmPrepare,
                                    UnidirectionalSequenceLstmEval);
 }
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/unidirectional_sequence_lstm.h b/tensorflow/lite/micro/kernels/unidirectional_sequence_lstm.h
index 16aa23b9..a66c7566 100644
--- a/tensorflow/lite/micro/kernels/unidirectional_sequence_lstm.h
+++ b/tensorflow/lite/micro/kernels/unidirectional_sequence_lstm.h
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/c/common.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // This is the most generic TFLMRegistration. The actual supported types
 // may still be target dependent. The only requirement is that every
@@ -42,6 +42,6 @@ inline TFLMRegistration Register_UNIDIRECTIONAL_SEQUENCE_LSTM_INT8() {
 }
 #endif
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_UNIDIRECTIONAL_SEQUENCE_LSTM_H_
diff --git a/tensorflow/lite/micro/kernels/unidirectional_sequence_lstm_test.cc b/tensorflow/lite/micro/kernels/unidirectional_sequence_lstm_test.cc
index c85e56fe..e3f219e5 100644
--- a/tensorflow/lite/micro/kernels/unidirectional_sequence_lstm_test.cc
+++ b/tensorflow/lite/micro/kernels/unidirectional_sequence_lstm_test.cc
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -64,8 +64,8 @@ void TestUnidirectionalLSTMInteger(
   int input_zero_points[2] = {1, -21};
   float input_scales[2] = {1, 0.004705882165580988};
   TfLiteAffineQuantization input_quant = {
-      tflite::testing::FloatArrayFromFloats(input_scales),
-      tflite::testing::IntArrayFromInts(input_zero_points), 0};
+      tflite_micro::testing::FloatArrayFromFloats(input_scales),
+      tflite_micro::testing::IntArrayFromInts(input_zero_points), 0};
   int intermediate_dim[2] = {1, 0};
   for (int i = 0; i < 5; ++i) {
     tensors[kLstmIntermediateTensorBase + i] =
@@ -145,51 +145,51 @@ void TestUnidirectionalLSTMFloat(
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 // TODO(b/230666079) enable below tests for xtensa when the xtensa
 // kernel is reconciled with reference kernel
 #if !defined(XTENSA)
 TF_LITE_MICRO_TEST(TestUnidirectionalLSTMFloat) {
-  const tflite::testing::LstmEvalCheckData<12, 4, 12> kernel_eval_data =
-      tflite::testing::Get2X2LstmEvalCheckData();
-  tflite::testing::LstmNodeContent<float, float, float, float, 2, 3, 2, 2>
-      float_node_contents = tflite::testing::Create2x3x2X2FloatNodeContents(
+  const tflite_micro::testing::LstmEvalCheckData<12, 4, 12> kernel_eval_data =
+      tflite_micro::testing::Get2X2LstmEvalCheckData();
+  tflite_micro::testing::LstmNodeContent<float, float, float, float, 2, 3, 2, 2>
+      float_node_contents = tflite_micro::testing::Create2x3x2X2FloatNodeContents(
           kernel_eval_data.input_data, kernel_eval_data.hidden_state);
 
   const float tolerance = 1e-6;
-  tflite::testing::TestUnidirectionalLSTMFloat(kernel_eval_data, tolerance,
+  tflite_micro::testing::TestUnidirectionalLSTMFloat(kernel_eval_data, tolerance,
                                                tolerance, float_node_contents);
 }
 
 TF_LITE_MICRO_TEST(TestUnidirectionalLSTMInt8) {
-  const tflite::testing::LstmEvalCheckData<12, 4, 12> kernel_eval_data =
-      tflite::testing::Get2X2LstmEvalCheckData();
-  tflite::testing::LstmNodeContent<int8_t, int8_t, int32_t, int16_t, 2, 3, 2, 2>
-      int8_node_contents = tflite::testing::Create2x3x2X2Int8NodeContents(
+  const tflite_micro::testing::LstmEvalCheckData<12, 4, 12> kernel_eval_data =
+      tflite_micro::testing::Get2X2LstmEvalCheckData();
+  tflite_micro::testing::LstmNodeContent<int8_t, int8_t, int32_t, int16_t, 2, 3, 2, 2>
+      int8_node_contents = tflite_micro::testing::Create2x3x2X2Int8NodeContents(
           kernel_eval_data.input_data, kernel_eval_data.hidden_state);
 
   const float hidden_state_tolerance = 1e-2;
   // cell state degrade due to integer overflow
   const float cell_state_tolerance = 1e-2;
-  tflite::testing::TestUnidirectionalLSTMInteger(
+  tflite_micro::testing::TestUnidirectionalLSTMInteger(
       kernel_eval_data, hidden_state_tolerance, cell_state_tolerance,
       int8_node_contents);
 }
 
 TF_LITE_MICRO_TEST(TestUnidirectionalLSTMInt16) {
-  const tflite::testing::LstmEvalCheckData<12, 4, 12> kernel_eval_data =
-      tflite::testing::Get2X2LstmEvalCheckData();
-  tflite::testing::LstmNodeContent<int16_t, int8_t, int64_t, int16_t, 2, 3, 2,
+  const tflite_micro::testing::LstmEvalCheckData<12, 4, 12> kernel_eval_data =
+      tflite_micro::testing::Get2X2LstmEvalCheckData();
+  tflite_micro::testing::LstmNodeContent<int16_t, int8_t, int64_t, int16_t, 2, 3, 2,
                                    2>
-      int16_node_contents = tflite::testing::Create2x3x2X2Int16NodeContents(
+      int16_node_contents = tflite_micro::testing::Create2x3x2X2Int16NodeContents(
           kernel_eval_data.input_data, kernel_eval_data.hidden_state);
 
   const float hidden_state_tolerance = 1e-3;  // actually very close to 1e-4
   // cell state degrade due to integer overflow
   const float cell_state_tolerance = 1e-2;
-  tflite::testing::TestUnidirectionalLSTMInteger(
+  tflite_micro::testing::TestUnidirectionalLSTMInteger(
       kernel_eval_data, hidden_state_tolerance, cell_state_tolerance,
       int16_node_contents);
 }
diff --git a/tensorflow/lite/micro/kernels/unpack.cc b/tensorflow/lite/micro/kernels/unpack.cc
index 3ce4c33f..e2db17ed 100644
--- a/tensorflow/lite/micro/kernels/unpack.cc
+++ b/tensorflow/lite/micro/kernels/unpack.cc
@@ -20,7 +20,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -31,7 +31,7 @@ TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
                         const TfLiteEvalTensor* input, int output_count,
                         int axis) {
   const TfLiteEvalTensor* output0 =
-      tflite::micro::GetEvalOutput(context, node, 0);
+      tflite_micro::micro::GetEvalOutput(context, node, 0);
   const TfLiteIntArray* input_dims = input->dims;
   const TfLiteIntArray* output_dims = output0->dims;
   const int dimensions = input_dims->size;
@@ -56,11 +56,11 @@ TfLiteStatus UnpackImpl(TfLiteContext* context, TfLiteNode* node,
   }
   TFLITE_DCHECK_EQ(output_size, copy_size * outer_size);
 
-  const T* input_data = tflite::micro::GetTensorData<T>(input);
+  const T* input_data = tflite_micro::micro::GetTensorData<T>(input);
 
   for (int i = 0; i < output_count; ++i) {
-    TfLiteEvalTensor* t = tflite::micro::GetEvalOutput(context, node, i);
-    T* output_data = tflite::micro::GetTensorData<T>(t);
+    TfLiteEvalTensor* t = tflite_micro::micro::GetEvalOutput(context, node, i);
+    T* output_data = tflite_micro::micro::GetTensorData<T>(t);
     for (int k = 0; k < outer_size; ++k) {
       T* output_ptr = output_data + copy_size * k;
       int loc = k * output_count * copy_size + i * copy_size;
@@ -77,7 +77,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
       reinterpret_cast<TfLiteUnpackParams*>(node->builtin_data);
 
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
 
   switch (input->type) {
     case kTfLiteFloat32: {
@@ -86,6 +86,9 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
     case kTfLiteInt32: {
       return UnpackImpl<int32_t>(context, node, input, data->num, data->axis);
     }
+    case kTfLiteInt16: {
+      return UnpackImpl<int16_t>(context, node, input, data->num, data->axis);
+    }
     case kTfLiteInt8: {
       return UnpackImpl<int8_t>(context, node, input, data->num, data->axis);
     }
@@ -102,7 +105,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_UNPACK() {
-  return tflite::micro::RegisterOp(nullptr, nullptr, Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, nullptr, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/unpack_test.cc b/tensorflow/lite/micro/kernels/unpack_test.cc
index 2e16822a..234b47ac 100644
--- a/tensorflow/lite/micro/kernels/unpack_test.cc
+++ b/tensorflow/lite/micro/kernels/unpack_test.cc
@@ -20,7 +20,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 
 void TestUnpackThreeOutputsFloat(
@@ -69,7 +69,7 @@ void TestUnpackThreeOutputsFloat(
   int outputs_array_data[] = {3, 1, 2, 3};
   TfLiteIntArray* outputs_array = IntArrayFromInts(outputs_array_data);
 
-  const TFLMRegistration registration = tflite::Register_UNPACK();
+  const TFLMRegistration registration = tflite_micro::Register_UNPACK();
   micro::KernelRunner runner(registration, tensors, tensors_size, inputs_array,
                              outputs_array,
                              reinterpret_cast<void*>(&builtin_data));
@@ -119,7 +119,7 @@ void TestUnpackOneOutputFloat(int* input_dims_data, const float* input_data,
   int outputs_array_data[] = {1, 1};
   TfLiteIntArray* outputs_array = IntArrayFromInts(outputs_array_data);
 
-  const TFLMRegistration registration = tflite::Register_UNPACK();
+  const TFLMRegistration registration = tflite_micro::Register_UNPACK();
   micro::KernelRunner runner(registration, tensors, tensors_size, inputs_array,
                              outputs_array,
                              reinterpret_cast<void*>(&builtin_data));
@@ -178,7 +178,7 @@ void TestUnpackThreeOutputsQuantized32(
   int outputs_array_data[] = {3, 1, 2, 3};
   TfLiteIntArray* outputs_array = IntArrayFromInts(outputs_array_data);
 
-  const TFLMRegistration registration = tflite::Register_UNPACK();
+  const TFLMRegistration registration = tflite_micro::Register_UNPACK();
   micro::KernelRunner runner(registration, tensors, tensors_size, inputs_array,
                              outputs_array,
                              reinterpret_cast<void*>(&builtin_data));
@@ -200,7 +200,7 @@ void TestUnpackThreeOutputsQuantized32(
 }
 
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -219,7 +219,7 @@ TF_LITE_MICRO_TEST(UnpackFloatThreeOutputs) {
   float output1_data[output1_dims_count];
   float output2_data[output2_dims_count];
   float output3_data[output3_dims_count];
-  tflite::testing::TestUnpackThreeOutputsFloat(
+  tflite_micro::testing::TestUnpackThreeOutputsFloat(
       input_shape, input_values, 0, output1_shape, output1_golden,
       output2_shape, output2_golden, output3_shape, output3_golden,
       output1_data, output2_data, output3_data);
@@ -240,7 +240,7 @@ TF_LITE_MICRO_TEST(UnpackFloatThreeOutputsNegativeAxisTwo) {
   float output1_data[output1_dims_count];
   float output2_data[output2_dims_count];
   float output3_data[output3_dims_count];
-  tflite::testing::TestUnpackThreeOutputsFloat(
+  tflite_micro::testing::TestUnpackThreeOutputsFloat(
       input_shape, input_values, -2, output1_shape, output1_golden,
       output2_shape, output2_golden, output3_shape, output3_golden,
       output1_data, output2_data, output3_data);
@@ -253,7 +253,7 @@ TF_LITE_MICRO_TEST(UnpackFloatOneOutput) {
   const float golden[] = {1, 2, 3, 4, 5, 6};
   constexpr int output_dims_count = 6;
   float output_data[output_dims_count];
-  tflite::testing::TestUnpackOneOutputFloat(input_shape, input_values, 0,
+  tflite_micro::testing::TestUnpackOneOutputFloat(input_shape, input_values, 0,
                                             output_shape, golden, output_data);
 }
 
@@ -272,7 +272,7 @@ TF_LITE_MICRO_TEST(UnpackQuantized32ThreeOutputs) {
   int32_t output1_data[output1_dims_count];
   int32_t output2_data[output2_dims_count];
   int32_t output3_data[output3_dims_count];
-  tflite::testing::TestUnpackThreeOutputsQuantized32(
+  tflite_micro::testing::TestUnpackThreeOutputsQuantized32(
       input_shape, input_values, 0, output1_shape, output1_golden,
       output2_shape, output2_golden, output3_shape, output3_golden,
       output1_data, output2_data, output3_data);
diff --git a/tensorflow/lite/micro/kernels/var_handle.cc b/tensorflow/lite/micro/kernels/var_handle.cc
index 06087f79..18d600d4 100644
--- a/tensorflow/lite/micro/kernels/var_handle.cc
+++ b/tensorflow/lite/micro/kernels/var_handle.cc
@@ -28,7 +28,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_resource_variable.h"
 #include "tensorflow/lite/schema/schema_generated.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -46,7 +46,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   const auto* params =
       reinterpret_cast<const TfLiteVarHandleParams*>(node->builtin_data);
 
-  tflite::MicroContext* micro_context = tflite::GetMicroContext(context);
+  tflite_micro::MicroContext* micro_context = tflite_micro::GetMicroContext(context);
   MicroGraph& graph_info = micro_context->graph();
 
   MicroResourceVariables* resources = graph_info.GetResourceVariables();
@@ -62,7 +62,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
     return kTfLiteError;
   }
 
-  TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, 0);
+  TfLiteEvalTensor* output = tflite_micro::micro::GetEvalOutput(context, node, 0);
   TFLITE_DCHECK(output != nullptr);
 
   // Assign saved resource_id so this output tensor will always return the
@@ -75,7 +75,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   OpData* op_data = reinterpret_cast<OpData*>(node->user_data);
 
-  TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, 0);
+  TfLiteEvalTensor* output = tflite_micro::micro::GetEvalOutput(context, node, 0);
   TFLITE_DCHECK(output != nullptr);
 
   // Assign saved resource_id so this output tensor will always return the
@@ -87,7 +87,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace.
 
 TFLMRegistration Register_VAR_HANDLE() {
-  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/while.cc b/tensorflow/lite/micro/kernels/while.cc
index 097a3421..6042f0eb 100644
--- a/tensorflow/lite/micro/kernels/while.cc
+++ b/tensorflow/lite/micro/kernels/while.cc
@@ -27,7 +27,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_graph.h"
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -50,7 +50,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   op_data->body_subgraph_index = params->body_subgraph_index;
 
   // The first input is the condition.
-  tflite::MicroContext* micro_context = tflite::GetMicroContext(context);
+  tflite_micro::MicroContext* micro_context = tflite_micro::GetMicroContext(context);
 
   size_t num_inputs = node->inputs->size;
   size_t num_outputs = node->outputs->size;
@@ -77,11 +77,11 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const OpData* op_data = reinterpret_cast<OpData*>(node->user_data);
 
-  tflite::MicroContext* micro_context = tflite::GetMicroContext(context);
+  tflite_micro::MicroContext* micro_context = tflite_micro::GetMicroContext(context);
   MicroGraph* graph_info = &micro_context->graph();
 
   TF_LITE_ENSURE_OK(context,
-                    tflite::micro::CopyOpInputsToSubgraphInputs(
+                    tflite_micro::micro::CopyOpInputsToSubgraphInputs(
                         context, node, graph_info, op_data->cond_subgraph_index,
                         /*first_tensor_idx=*/0));
 
@@ -93,25 +93,25 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   bool cond_value = cond_subgraph_output->data.b[0];
 
   TF_LITE_ENSURE_OK(context,
-                    tflite::micro::CopyOpInputsToSubgraphInputs(
+                    tflite_micro::micro::CopyOpInputsToSubgraphInputs(
                         context, node, graph_info, op_data->body_subgraph_index,
                         /*first_tensor_idx=*/0));
   TF_LITE_ENSURE_OK(context,
-                    tflite::micro::CopyOpInputsToOpOutputs(context, node));
+                    tflite_micro::micro::CopyOpInputsToOpOutputs(context, node));
 
   while (cond_value == true) {
     // Copy output of this iteration back to the body input.
     TF_LITE_ENSURE_OK(
-        context, tflite::micro::CopyOpOutputsToSubgraphInputs(
+        context, tflite_micro::micro::CopyOpOutputsToSubgraphInputs(
                      context, node, graph_info, op_data->body_subgraph_index));
     TF_LITE_ENSURE_OK(context,
                       graph_info->InvokeSubgraph(op_data->body_subgraph_index));
 
     TF_LITE_ENSURE_OK(
-        context, tflite::micro::CopySubgraphOutputsToOpOutputs(
+        context, tflite_micro::micro::CopySubgraphOutputsToOpOutputs(
                      context, node, graph_info, op_data->body_subgraph_index));
     TF_LITE_ENSURE_OK(
-        context, tflite::micro::CopyOpOutputsToSubgraphInputs(
+        context, tflite_micro::micro::CopyOpOutputsToSubgraphInputs(
                      context, node, graph_info, op_data->cond_subgraph_index));
     TF_LITE_ENSURE_OK(context,
                       graph_info->InvokeSubgraph(op_data->cond_subgraph_index));
@@ -127,7 +127,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace.
 
 TFLMRegistration Register_WHILE() {
-  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/while_test.cc b/tensorflow/lite/micro/kernels/while_test.cc
index 536f9033..85e97f6f 100644
--- a/tensorflow/lite/micro/kernels/while_test.cc
+++ b/tensorflow/lite/micro/kernels/while_test.cc
@@ -29,13 +29,13 @@ TF_LITE_MICRO_TEST(WhileShouldNeverInvokeConditionFalse) {
   constexpr int kArenaSize = 5000;
   uint8_t arena[kArenaSize];
 
-  const tflite::Model* model =
-      tflite::testing::GetSimpleModelWithSubgraphsAndWhile();
-  tflite::MicroMutableOpResolver<3> resolver;
+  const tflite_micro::Model* model =
+      tflite_micro::testing::GetSimpleModelWithSubgraphsAndWhile();
+  tflite_micro::MicroMutableOpResolver<3> resolver;
   resolver.AddWhile();
   resolver.AddAdd();
   resolver.AddLess();
-  tflite::MicroInterpreter interpreter(model, resolver, arena, kArenaSize);
+  tflite_micro::MicroInterpreter interpreter(model, resolver, arena, kArenaSize);
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, interpreter.AllocateTensors());
   TfLiteTensor* input0 = interpreter.input(0);
   TfLiteTensor* input1 = interpreter.input(1);
@@ -54,13 +54,13 @@ TF_LITE_MICRO_TEST(WhileShouldInvokeOnce) {
   constexpr int kArenaSize = 5000;
   uint8_t arena[kArenaSize];
 
-  const tflite::Model* model =
-      tflite::testing::GetSimpleModelWithSubgraphsAndWhile();
-  tflite::MicroMutableOpResolver<3> resolver;
+  const tflite_micro::Model* model =
+      tflite_micro::testing::GetSimpleModelWithSubgraphsAndWhile();
+  tflite_micro::MicroMutableOpResolver<3> resolver;
   resolver.AddWhile();
   resolver.AddAdd();
   resolver.AddLess();
-  tflite::MicroInterpreter interpreter(model, resolver, arena, kArenaSize);
+  tflite_micro::MicroInterpreter interpreter(model, resolver, arena, kArenaSize);
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, interpreter.AllocateTensors());
   TfLiteTensor* input0 = interpreter.input(0);
   TfLiteTensor* input1 = interpreter.input(1);
diff --git a/tensorflow/lite/micro/kernels/xtensa/add.cc b/tensorflow/lite/micro/kernels/xtensa/add.cc
index 423e7982..8e68d8d0 100644
--- a/tensorflow/lite/micro/kernels/xtensa/add.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/add.cc
@@ -29,7 +29,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/memory_helpers.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   TF_LITE_ENSURE_OK(context, AddPrepare(context, node));
@@ -46,45 +46,45 @@ TfLiteStatus EvalAdd(TfLiteContext* context, TfLiteNode* node,
                      const TfLiteEvalTensor* input2, TfLiteEvalTensor* output) {
   switch (output->type) {
     case kTfLiteFloat32: {
-      tflite::ArithmeticParams op_params;
+      tflite_micro::ArithmeticParams op_params;
       SetActivationParams(data->output_activation_min_f32,
                           data->output_activation_max_f32, &op_params);
       if (data->requires_broadcast) {
         reference_ops::BroadcastAdd4DSlow(
-            op_params, tflite::micro::GetTensorShape(input1),
-            tflite::micro::GetTensorData<float>(input1),
-            tflite::micro::GetTensorShape(input2),
-            tflite::micro::GetTensorData<float>(input2),
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<float>(output));
+            op_params, tflite_micro::micro::GetTensorShape(input1),
+            tflite_micro::micro::GetTensorData<float>(input1),
+            tflite_micro::micro::GetTensorShape(input2),
+            tflite_micro::micro::GetTensorData<float>(input2),
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<float>(output));
       } else {
-        reference_ops::Add(op_params, tflite::micro::GetTensorShape(input1),
-                           tflite::micro::GetTensorData<float>(input1),
-                           tflite::micro::GetTensorShape(input2),
-                           tflite::micro::GetTensorData<float>(input2),
-                           tflite::micro::GetTensorShape(output),
-                           tflite::micro::GetTensorData<float>(output));
+        reference_ops::Add(op_params, tflite_micro::micro::GetTensorShape(input1),
+                           tflite_micro::micro::GetTensorData<float>(input1),
+                           tflite_micro::micro::GetTensorShape(input2),
+                           tflite_micro::micro::GetTensorData<float>(input2),
+                           tflite_micro::micro::GetTensorShape(output),
+                           tflite_micro::micro::GetTensorData<float>(output));
       }
     } break;
     case kTfLiteInt32: {
-      tflite::ArithmeticParams op_params;
+      tflite_micro::ArithmeticParams op_params;
       SetActivationParams(std::numeric_limits<int32_t>::lowest(),
                           std::numeric_limits<int32_t>::max(), &op_params);
       if (data->requires_broadcast) {
         reference_ops::BroadcastAdd4DSlow(
-            op_params, tflite::micro::GetTensorShape(input1),
-            tflite::micro::GetTensorData<int32_t>(input1),
-            tflite::micro::GetTensorShape(input2),
-            tflite::micro::GetTensorData<int32_t>(input2),
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<int32_t>(output));
+            op_params, tflite_micro::micro::GetTensorShape(input1),
+            tflite_micro::micro::GetTensorData<int32_t>(input1),
+            tflite_micro::micro::GetTensorShape(input2),
+            tflite_micro::micro::GetTensorData<int32_t>(input2),
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int32_t>(output));
       } else {
-        reference_ops::Add(op_params, tflite::micro::GetTensorShape(input1),
-                           tflite::micro::GetTensorData<int32_t>(input1),
-                           tflite::micro::GetTensorShape(input2),
-                           tflite::micro::GetTensorData<int32_t>(input2),
-                           tflite::micro::GetTensorShape(output),
-                           tflite::micro::GetTensorData<int32_t>(output));
+        reference_ops::Add(op_params, tflite_micro::micro::GetTensorShape(input1),
+                           tflite_micro::micro::GetTensorData<int32_t>(input1),
+                           tflite_micro::micro::GetTensorShape(input2),
+                           tflite_micro::micro::GetTensorData<int32_t>(input2),
+                           tflite_micro::micro::GetTensorShape(output),
+                           tflite_micro::micro::GetTensorData<int32_t>(output));
       }
     } break;
     default:
@@ -100,7 +100,7 @@ TfLiteStatus EvalAddQuantized(TfLiteContext* context, TfLiteNode* node,
                               const TfLiteEvalTensor* input1,
                               const TfLiteEvalTensor* input2,
                               TfLiteEvalTensor* output) {
-  tflite::ArithmeticParams op_params;
+  tflite_micro::ArithmeticParams op_params;
   op_params.left_shift = data->left_shift;
   op_params.input1_offset = data->input1_offset;
   op_params.input1_multiplier = data->input1_multiplier;
@@ -115,8 +115,8 @@ TfLiteStatus EvalAddQuantized(TfLiteContext* context, TfLiteNode* node,
                       &op_params);
 #if !(defined(HIFI3) || defined(HIFI4))
   bool need_broadcast = reference_ops::ProcessBroadcastShapes(
-      tflite::micro::GetTensorShape(input1),
-      tflite::micro::GetTensorShape(input2), &op_params);
+      tflite_micro::micro::GetTensorShape(input1),
+      tflite_micro::micro::GetTensorShape(input2), &op_params);
 #endif  // !defined(HIFI3) && !defined(HIFI4)
 
   switch (output->type) {
@@ -129,22 +129,22 @@ TfLiteStatus EvalAddQuantized(TfLiteContext* context, TfLiteNode* node,
 #elif defined(HIFI3) || defined(HIFI4)  // defined(VISION_P6)
       int err;
       const RuntimeShape extended_input1_shape =
-          RuntimeShape::ExtendedShape(4, tflite::micro::GetTensorShape(input1));
+          RuntimeShape::ExtendedShape(4, tflite_micro::micro::GetTensorShape(input1));
       const RuntimeShape extended_input2_shape =
-          RuntimeShape::ExtendedShape(4, tflite::micro::GetTensorShape(input2));
+          RuntimeShape::ExtendedShape(4, tflite_micro::micro::GetTensorShape(input2));
       const RuntimeShape extended_output_shape =
-          RuntimeShape::ExtendedShape(4, tflite::micro::GetTensorShape(output));
+          RuntimeShape::ExtendedShape(4, tflite_micro::micro::GetTensorShape(output));
 
       err = xa_nn_elm_add_broadcast_4D_asym8sxasym8s_asym8s(
-          tflite::micro::GetTensorData<int8_t>(output),
+          tflite_micro::micro::GetTensorData<int8_t>(output),
           extended_output_shape.DimsData(), op_params.output_offset,
           op_params.output_shift, op_params.output_multiplier,
           op_params.quantized_activation_min,
           op_params.quantized_activation_max,
-          tflite::micro::GetTensorData<int8_t>(input1),
+          tflite_micro::micro::GetTensorData<int8_t>(input1),
           extended_input1_shape.DimsData(), op_params.input1_offset,
           op_params.input1_shift, op_params.input1_multiplier,
-          tflite::micro::GetTensorData<int8_t>(input2),
+          tflite_micro::micro::GetTensorData<int8_t>(input2),
           extended_input2_shape.DimsData(), op_params.input2_offset,
           op_params.input2_shift, op_params.input2_multiplier,
           op_params.left_shift);
@@ -153,20 +153,20 @@ TfLiteStatus EvalAddQuantized(TfLiteContext* context, TfLiteNode* node,
 #else                                   // defined(VISION_P6)
       if (need_broadcast) {
         reference_integer_ops::BroadcastAdd4DSlow(
-            op_params, tflite::micro::GetTensorShape(input1),
-            tflite::micro::GetTensorData<int8_t>(input1),
-            tflite::micro::GetTensorShape(input2),
-            tflite::micro::GetTensorData<int8_t>(input2),
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<int8_t>(output));
+            op_params, tflite_micro::micro::GetTensorShape(input1),
+            tflite_micro::micro::GetTensorData<int8_t>(input1),
+            tflite_micro::micro::GetTensorShape(input2),
+            tflite_micro::micro::GetTensorData<int8_t>(input2),
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int8_t>(output));
       } else {
         reference_integer_ops::Add(
-            op_params, tflite::micro::GetTensorShape(input1),
-            tflite::micro::GetTensorData<int8_t>(input1),
-            tflite::micro::GetTensorShape(input2),
-            tflite::micro::GetTensorData<int8_t>(input2),
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<int8_t>(output));
+            op_params, tflite_micro::micro::GetTensorShape(input1),
+            tflite_micro::micro::GetTensorData<int8_t>(input1),
+            tflite_micro::micro::GetTensorShape(input2),
+            tflite_micro::micro::GetTensorData<int8_t>(input2),
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int8_t>(output));
       }
 #endif                                  // defined(VISION_P6)
       break;
@@ -175,22 +175,22 @@ TfLiteStatus EvalAddQuantized(TfLiteContext* context, TfLiteNode* node,
 #if defined(HIFI3) || defined(HIFI4)
       int err;
       const RuntimeShape extended_input1_shape =
-          RuntimeShape::ExtendedShape(4, tflite::micro::GetTensorShape(input1));
+          RuntimeShape::ExtendedShape(4, tflite_micro::micro::GetTensorShape(input1));
       const RuntimeShape extended_input2_shape =
-          RuntimeShape::ExtendedShape(4, tflite::micro::GetTensorShape(input2));
+          RuntimeShape::ExtendedShape(4, tflite_micro::micro::GetTensorShape(input2));
       const RuntimeShape extended_output_shape =
-          RuntimeShape::ExtendedShape(4, tflite::micro::GetTensorShape(output));
+          RuntimeShape::ExtendedShape(4, tflite_micro::micro::GetTensorShape(output));
 
       err = xa_nn_elm_add_broadcast_4D_asym16sxasym16s_asym16s(
-          tflite::micro::GetTensorData<int16_t>(output),
+          tflite_micro::micro::GetTensorData<int16_t>(output),
           extended_output_shape.DimsData(), op_params.output_offset,
           op_params.output_shift, op_params.output_multiplier,
           op_params.quantized_activation_min,
           op_params.quantized_activation_max,
-          tflite::micro::GetTensorData<int16_t>(input1),
+          tflite_micro::micro::GetTensorData<int16_t>(input1),
           extended_input1_shape.DimsData(), op_params.input1_offset,
           op_params.input1_shift, op_params.input1_multiplier,
-          tflite::micro::GetTensorData<int16_t>(input2),
+          tflite_micro::micro::GetTensorData<int16_t>(input2),
           extended_input2_shape.DimsData(), op_params.input2_offset,
           op_params.input2_shift, op_params.input2_multiplier,
           op_params.left_shift);
@@ -199,19 +199,19 @@ TfLiteStatus EvalAddQuantized(TfLiteContext* context, TfLiteNode* node,
 #else   // defined(HIFI3) || defined(HIFI4)
       if (need_broadcast) {
         reference_ops::BroadcastAdd4DSlow(
-            op_params, tflite::micro::GetTensorShape(input1),
-            tflite::micro::GetTensorData<int16_t>(input1),
-            tflite::micro::GetTensorShape(input2),
-            tflite::micro::GetTensorData<int16_t>(input2),
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<int16_t>(output));
+            op_params, tflite_micro::micro::GetTensorShape(input1),
+            tflite_micro::micro::GetTensorData<int16_t>(input1),
+            tflite_micro::micro::GetTensorShape(input2),
+            tflite_micro::micro::GetTensorData<int16_t>(input2),
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int16_t>(output));
       } else {
-        reference_ops::Add(op_params, tflite::micro::GetTensorShape(input1),
-                           tflite::micro::GetTensorData<int16_t>(input1),
-                           tflite::micro::GetTensorShape(input2),
-                           tflite::micro::GetTensorData<int16_t>(input2),
-                           tflite::micro::GetTensorShape(output),
-                           tflite::micro::GetTensorData<int16_t>(output),
+        reference_ops::Add(op_params, tflite_micro::micro::GetTensorShape(input1),
+                           tflite_micro::micro::GetTensorData<int16_t>(input1),
+                           tflite_micro::micro::GetTensorShape(input2),
+                           tflite_micro::micro::GetTensorData<int16_t>(input2),
+                           tflite_micro::micro::GetTensorShape(output),
+                           tflite_micro::micro::GetTensorData<int16_t>(output),
                            false);
       }
 #endif  // defined(HIFI3) || defined(HIFI4)
@@ -247,11 +247,11 @@ TfLiteStatus AddEval(TfLiteContext* context, TfLiteNode* node) {
   const OpDataAdd* data = static_cast<const OpDataAdd*>(node->user_data);
 
   const TfLiteEvalTensor* input1 =
-      tflite::micro::GetEvalInput(context, node, kAddInputTensor1);
+      tflite_micro::micro::GetEvalInput(context, node, kAddInputTensor1);
   const TfLiteEvalTensor* input2 =
-      tflite::micro::GetEvalInput(context, node, kAddInputTensor2);
+      tflite_micro::micro::GetEvalInput(context, node, kAddInputTensor2);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kAddOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kAddOutputTensor);
 
   if (output->type == kTfLiteFloat32 || output->type == kTfLiteInt32) {
     TF_LITE_ENSURE_OK(
@@ -269,7 +269,7 @@ TfLiteStatus AddEval(TfLiteContext* context, TfLiteNode* node) {
 }
 
 TFLMRegistration Register_ADD() {
-  return tflite::micro::RegisterOp(AddInit, Prepare, AddEval);
+  return tflite_micro::micro::RegisterOp(AddInit, Prepare, AddEval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/xtensa/add_vision.cc b/tensorflow/lite/micro/kernels/xtensa/add_vision.cc
index 4e207130..5da70aef 100644
--- a/tensorflow/lite/micro/kernels/xtensa/add_vision.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/add_vision.cc
@@ -27,7 +27,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa.h"
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa_add.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 TfLiteStatus AddPrepareVision(TfLiteContext* context, TfLiteNode* node) {
   TFLITE_DCHECK(node->user_data != nullptr);
@@ -111,11 +111,11 @@ TfLiteStatus AddEvalQuantizedVision(TfLiteContext* context, TfLiteNode* node,
   const uint32_t output_size = NumElements(output->dims);
 
   xiAdd(data.p_context, data.context_size,
-        const_cast<int8_t*>(tflite::micro::GetTensorData<int8_t>(input1)),
+        const_cast<int8_t*>(tflite_micro::micro::GetTensorData<int8_t>(input1)),
         input1_size,
-        const_cast<int8_t*>(tflite::micro::GetTensorData<int8_t>(input2)),
-        input2_size, tflite::micro::GetTensorData<int8_t>(output), output_size);
+        const_cast<int8_t*>(tflite_micro::micro::GetTensorData<int8_t>(input2)),
+        input2_size, tflite_micro::micro::GetTensorData<int8_t>(output), output_size);
   return kTfLiteOk;
 }
-}  // namespace tflite
+}  // namespace tflite_micro
 #endif  // defined(VISION_P6)
diff --git a/tensorflow/lite/micro/kernels/xtensa/conv.cc b/tensorflow/lite/micro/kernels/xtensa/conv.cc
index 0955b12f..a12441c3 100644
--- a/tensorflow/lite/micro/kernels/xtensa/conv.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/conv.cc
@@ -29,7 +29,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa_conv.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
@@ -37,32 +37,32 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   TFLITE_DCHECK(node->builtin_data != nullptr);
 
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kConvInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kConvInputTensor);
 
   const auto& params =
       *(reinterpret_cast<TfLiteConvParams*>(node->builtin_data));
   const auto& op_data = *(reinterpret_cast<XtensaConvOpData*>(node->user_data));
 
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kConvOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kConvOutputTensor);
   const TfLiteEvalTensor* filter =
-      tflite::micro::GetEvalInput(context, node, kConvWeightsTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kConvWeightsTensor);
   const TfLiteEvalTensor* bias =
-      tflite::micro::GetEvalInput(context, node, kConvBiasTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kConvBiasTensor);
 
   switch (input->type) {
     case kTfLiteFloat32: {
-      tflite::reference_ops::Conv(
+      tflite_micro::reference_ops::Conv(
           ConvParamsFloat(params, op_data.reference_op_data),
-          tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<float>(input),
-          tflite::micro::GetTensorShape(filter),
-          tflite::micro::GetTensorData<float>(filter),
-          tflite::micro::GetTensorShape(bias),
-          tflite::micro::GetOptionalTensorData<float>(bias),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<float>(output),
-          tflite::micro::GetTensorShape(nullptr), nullptr);
+          tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<float>(input),
+          tflite_micro::micro::GetTensorShape(filter),
+          tflite_micro::micro::GetTensorData<float>(filter),
+          tflite_micro::micro::GetTensorShape(bias),
+          tflite_micro::micro::GetOptionalTensorData<float>(bias),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<float>(output),
+          tflite_micro::micro::GetTensorShape(nullptr), nullptr);
       break;
     }
     case kTfLiteInt8: {
@@ -117,7 +117,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_CONV_2D() {
-  return tflite::micro::RegisterOp(ConvInitXtensa, ConvPrepareXtensa, Eval);
+  return tflite_micro::micro::RegisterOp(ConvInitXtensa, ConvPrepareXtensa, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/xtensa/conv_common_xtensa.cc b/tensorflow/lite/micro/kernels/xtensa/conv_common_xtensa.cc
index 3063e777..160bf491 100644
--- a/tensorflow/lite/micro/kernels/xtensa/conv_common_xtensa.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/conv_common_xtensa.cc
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa.h"
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa_conv.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 void* ConvInitXtensa(TfLiteContext* context, const char* buffer,
                      size_t length) {
@@ -53,4 +53,4 @@ TfLiteStatus ConvPrepareXtensa(TfLiteContext* context, TfLiteNode* node) {
   return kTfLiteOk;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/xtensa/conv_hifi.cc b/tensorflow/lite/micro/kernels/xtensa/conv_hifi.cc
index 45c8f4bf..dd074dcb 100644
--- a/tensorflow/lite/micro/kernels/xtensa/conv_hifi.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/conv_hifi.cc
@@ -28,7 +28,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa.h"
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa_conv.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 TfLiteStatus ConvPrepareHifi(TfLiteContext* context, TfLiteNode* node) {
   XtensaConvOpData* data = static_cast<XtensaConvOpData*>(node->user_data);
@@ -123,8 +123,8 @@ TfLiteStatus ConvEvalHifiInt16(TfLiteContext* context, TfLiteNode* node,
                                const TfLiteEvalTensor* filter,
                                const TfLiteEvalTensor* bias,
                                TfLiteEvalTensor* output) {
-  const RuntimeShape& input_shape = tflite::micro::GetTensorShape(input);
-  const RuntimeShape& filter_shape = tflite::micro::GetTensorShape(filter);
+  const RuntimeShape& input_shape = tflite_micro::micro::GetTensorShape(input);
+  const RuntimeShape& filter_shape = tflite_micro::micro::GetTensorShape(filter);
   const int stride_width = params.stride_width;
   const int stride_height = params.stride_height;
   const int pad_width = data.reference_op_data.padding.width;
@@ -134,7 +134,7 @@ TfLiteStatus ConvEvalHifiInt16(TfLiteContext* context, TfLiteNode* node,
   const int32_t output_activation_max =
       data.reference_op_data.output_activation_max;
 
-  const RuntimeShape& output_shape = tflite::micro::GetTensorShape(output);
+  const RuntimeShape& output_shape = tflite_micro::micro::GetTensorShape(output);
   const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
   const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
@@ -145,10 +145,10 @@ TfLiteStatus ConvEvalHifiInt16(TfLiteContext* context, TfLiteNode* node,
   const int output_height = output_shape.Dims(1);
   const int output_width = output_shape.Dims(2);
 
-  const int16_t* input_data = tflite::micro::GetTensorData<int16_t>(input);
-  const int8_t* filter_data = tflite::micro::GetTensorData<int8_t>(filter);
-  const int64_t* bias_data = tflite::micro::GetTensorData<int64_t>(bias);
-  int16_t* output_data = tflite::micro::GetTensorData<int16_t>(output);
+  const int16_t* input_data = tflite_micro::micro::GetTensorData<int16_t>(input);
+  const int8_t* filter_data = tflite_micro::micro::GetTensorData<int8_t>(filter);
+  const int64_t* bias_data = tflite_micro::micro::GetTensorData<int64_t>(bias);
+  int16_t* output_data = tflite_micro::micro::GetTensorData<int16_t>(output);
 
   int output_data_format = 0;
   int out_length = output_height * output_width * output_depth;
@@ -219,8 +219,8 @@ TfLiteStatus ConvEvalHifiInt8(TfLiteContext* context, TfLiteNode* node,
                               const TfLiteEvalTensor* filter,
                               const TfLiteEvalTensor* bias,
                               TfLiteEvalTensor* output) {
-  const RuntimeShape& input_shape = tflite::micro::GetTensorShape(input);
-  const RuntimeShape& filter_shape = tflite::micro::GetTensorShape(filter);
+  const RuntimeShape& input_shape = tflite_micro::micro::GetTensorShape(input);
+  const RuntimeShape& filter_shape = tflite_micro::micro::GetTensorShape(filter);
   const int32_t input_offset = -data.reference_op_data.input_zero_point;
   const int32_t output_offset = data.reference_op_data.output_zero_point;
   const int stride_width = params.stride_width;
@@ -232,7 +232,7 @@ TfLiteStatus ConvEvalHifiInt8(TfLiteContext* context, TfLiteNode* node,
   const int32_t output_activation_max =
       data.reference_op_data.output_activation_max;
 
-  const RuntimeShape& output_shape = tflite::micro::GetTensorShape(output);
+  const RuntimeShape& output_shape = tflite_micro::micro::GetTensorShape(output);
   const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
   const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
@@ -243,21 +243,21 @@ TfLiteStatus ConvEvalHifiInt8(TfLiteContext* context, TfLiteNode* node,
   const int output_height = output_shape.Dims(1);
   const int output_width = output_shape.Dims(2);
 
-  const int8_t* input_data = tflite::micro::GetTensorData<int8_t>(input);
-  const int32_t* bias_data = tflite::micro::GetTensorData<int32_t>(bias);
-  int8_t* output_data = tflite::micro::GetTensorData<int8_t>(output);
+  const int8_t* input_data = tflite_micro::micro::GetTensorData<int8_t>(input);
+  const int32_t* bias_data = tflite_micro::micro::GetTensorData<int32_t>(bias);
+  int8_t* output_data = tflite_micro::micro::GetTensorData<int8_t>(output);
 
   const int8_t* filter_data;
   if (filter->type == kTfLiteInt4) {
     int8_t* unpacked_filter_data =
         static_cast<int8_t*>(context->GetScratchBuffer(
             context, data.reference_op_data.filter_buffer_index));
-    tflite::tensor_utils::UnpackDenseInt4IntoInt8(
-        tflite::micro::GetTensorData<int8_t>(filter),
-        tflite::micro::GetTensorShape(filter).FlatSize(), unpacked_filter_data);
+    tflite_micro::tensor_utils::UnpackDenseInt4IntoInt8(
+        tflite_micro::micro::GetTensorData<int8_t>(filter),
+        tflite_micro::micro::GetTensorShape(filter).FlatSize(), unpacked_filter_data);
     filter_data = unpacked_filter_data;
   } else {
-    filter_data = tflite::micro::GetTensorData<int8_t>(filter);
+    filter_data = tflite_micro::micro::GetTensorData<int8_t>(filter);
   }
 
   int output_data_format = 0;
@@ -324,5 +324,5 @@ TfLiteStatus ConvEvalHifiInt8(TfLiteContext* context, TfLiteNode* node,
   return kTfLiteOk;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
 #endif  // defined(HIFI3) || defined(HIFI4) || defined(HIFI5)
diff --git a/tensorflow/lite/micro/kernels/xtensa/conv_int16_reference.cc b/tensorflow/lite/micro/kernels/xtensa/conv_int16_reference.cc
index 2492d4b3..a16a22b4 100644
--- a/tensorflow/lite/micro/kernels/xtensa/conv_int16_reference.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/conv_int16_reference.cc
@@ -25,7 +25,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 TfLiteStatus ConvReferenceEvalInt16(TfLiteContext* context, TfLiteNode* node) {
   TFLITE_DCHECK(node->user_data != nullptr);
@@ -35,40 +35,40 @@ TfLiteStatus ConvReferenceEvalInt16(TfLiteContext* context, TfLiteNode* node) {
   const auto& op_data = *(reinterpret_cast<OpDataConv*>(node->user_data));
 
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kConvOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kConvOutputTensor);
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kConvInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kConvInputTensor);
   const TfLiteEvalTensor* filter =
-      tflite::micro::GetEvalInput(context, node, kConvWeightsTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kConvWeightsTensor);
   const TfLiteEvalTensor* bias =
       (NumInputs(node) == 3)
-          ? tflite::micro::GetEvalInput(context, node, kConvBiasTensor)
+          ? tflite_micro::micro::GetEvalInput(context, node, kConvBiasTensor)
           : nullptr;
 
   if (bias == nullptr || bias->type == kTfLiteInt32) {
     reference_integer_ops::ConvPerChannel(
         ConvParamsQuantized(params, op_data),
         op_data.per_channel_output_multiplier, op_data.per_channel_output_shift,
-        tflite::micro::GetTensorShape(input),
-        tflite::micro::GetTensorData<int16_t>(input),
-        tflite::micro::GetTensorShape(filter),
-        tflite::micro::GetTensorData<int8_t>(filter),
-        tflite::micro::GetTensorShape(bias),
-        tflite::micro::GetOptionalTensorData<std::int32_t>(bias),
-        tflite::micro::GetTensorShape(output),
-        tflite::micro::GetTensorData<int16_t>(output));
+        tflite_micro::micro::GetTensorShape(input),
+        tflite_micro::micro::GetTensorData<int16_t>(input),
+        tflite_micro::micro::GetTensorShape(filter),
+        tflite_micro::micro::GetTensorData<int8_t>(filter),
+        tflite_micro::micro::GetTensorShape(bias),
+        tflite_micro::micro::GetOptionalTensorData<std::int32_t>(bias),
+        tflite_micro::micro::GetTensorShape(output),
+        tflite_micro::micro::GetTensorData<int16_t>(output));
   } else if (bias->type == kTfLiteInt64) {
     reference_integer_ops::ConvPerChannel(
         ConvParamsQuantized(params, op_data),
         op_data.per_channel_output_multiplier, op_data.per_channel_output_shift,
-        tflite::micro::GetTensorShape(input),
-        tflite::micro::GetTensorData<int16_t>(input),
-        tflite::micro::GetTensorShape(filter),
-        tflite::micro::GetTensorData<int8_t>(filter),
-        tflite::micro::GetTensorShape(bias),
-        tflite::micro::GetOptionalTensorData<std::int64_t>(bias),
-        tflite::micro::GetTensorShape(output),
-        tflite::micro::GetTensorData<int16_t>(output));
+        tflite_micro::micro::GetTensorShape(input),
+        tflite_micro::micro::GetTensorData<int16_t>(input),
+        tflite_micro::micro::GetTensorShape(filter),
+        tflite_micro::micro::GetTensorData<int8_t>(filter),
+        tflite_micro::micro::GetTensorShape(bias),
+        tflite_micro::micro::GetOptionalTensorData<std::int64_t>(bias),
+        tflite_micro::micro::GetTensorShape(output),
+        tflite_micro::micro::GetTensorData<int16_t>(output));
   } else {
     MicroPrintf("Bias type %s (%d) not supported.",
                 TfLiteTypeGetName(bias->type), bias->type);
@@ -78,4 +78,4 @@ TfLiteStatus ConvReferenceEvalInt16(TfLiteContext* context, TfLiteNode* node) {
   return kTfLiteOk;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/xtensa/conv_int8_int16.cc b/tensorflow/lite/micro/kernels/xtensa/conv_int8_int16.cc
index d46546d2..b113e585 100644
--- a/tensorflow/lite/micro/kernels/xtensa/conv_int8_int16.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/conv_int8_int16.cc
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa.h"
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa_conv.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 TfLiteStatus EvalInt8(TfLiteContext* context, TfLiteNode* node) {
@@ -34,13 +34,13 @@ TfLiteStatus EvalInt8(TfLiteContext* context, TfLiteNode* node) {
       *(reinterpret_cast<TfLiteConvParams*>(node->builtin_data));
 
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kConvInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kConvInputTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kConvOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kConvOutputTensor);
   const TfLiteEvalTensor* filter =
-      tflite::micro::GetEvalInput(context, node, kConvWeightsTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kConvWeightsTensor);
   const TfLiteEvalTensor* bias =
-      tflite::micro::GetEvalInput(context, node, kConvBiasTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kConvBiasTensor);
 
 #if defined(HIFI3) || defined(HIFI4) || defined(HIFI5)
   return ConvEvalHifiInt8(context, node, params, op_data, input, filter, bias,
@@ -60,13 +60,13 @@ TfLiteStatus EvalInt16(TfLiteContext* context, TfLiteNode* node) {
       *(reinterpret_cast<TfLiteConvParams*>(node->builtin_data));
 
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kConvInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kConvInputTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kConvOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kConvOutputTensor);
   const TfLiteEvalTensor* filter =
-      tflite::micro::GetEvalInput(context, node, kConvWeightsTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kConvWeightsTensor);
   const TfLiteEvalTensor* bias =
-      tflite::micro::GetEvalInput(context, node, kConvBiasTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kConvBiasTensor);
 
   return ConvEvalHifiInt16(context, node, params, op_data, input, filter, bias,
                            output);
@@ -78,12 +78,12 @@ TfLiteStatus EvalInt16(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_CONV_2D_INT8() {
-  return tflite::micro::RegisterOp(ConvInitXtensa, ConvPrepareXtensa, EvalInt8);
+  return tflite_micro::micro::RegisterOp(ConvInitXtensa, ConvPrepareXtensa, EvalInt8);
 }
 
 TFLMRegistration Register_CONV_2D_INT16() {
-  return tflite::micro::RegisterOp(ConvInitXtensa, ConvPrepareXtensa,
+  return tflite_micro::micro::RegisterOp(ConvInitXtensa, ConvPrepareXtensa,
                                    EvalInt16);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/xtensa/conv_int8_reference.cc b/tensorflow/lite/micro/kernels/xtensa/conv_int8_reference.cc
index 6ac07bab..d3fe134f 100644
--- a/tensorflow/lite/micro/kernels/xtensa/conv_int8_reference.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/conv_int8_reference.cc
@@ -25,7 +25,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/conv.h"
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 TfLiteStatus ConvReferenceEvalInt8(TfLiteContext* context, TfLiteNode* node) {
   TFLITE_DCHECK(node->user_data != nullptr);
@@ -35,38 +35,38 @@ TfLiteStatus ConvReferenceEvalInt8(TfLiteContext* context, TfLiteNode* node) {
   const auto& op_data = *(reinterpret_cast<OpDataConv*>(node->user_data));
 
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kConvOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kConvOutputTensor);
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kConvInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kConvInputTensor);
   const TfLiteEvalTensor* filter =
-      tflite::micro::GetEvalInput(context, node, kConvWeightsTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kConvWeightsTensor);
   const TfLiteEvalTensor* bias =
       (NumInputs(node) == 3)
-          ? tflite::micro::GetEvalInput(context, node, kConvBiasTensor)
+          ? tflite_micro::micro::GetEvalInput(context, node, kConvBiasTensor)
           : nullptr;
 
   const int8_t* filter_data;
   if (filter->type == kTfLiteInt4) {
     int8_t* unpacked_filter_data = static_cast<int8_t*>(
         context->GetScratchBuffer(context, op_data.filter_buffer_index));
-    tflite::tensor_utils::UnpackDenseInt4IntoInt8(
-        tflite::micro::GetTensorData<int8_t>(filter),
-        tflite::micro::GetTensorShape(filter).FlatSize(), unpacked_filter_data);
+    tflite_micro::tensor_utils::UnpackDenseInt4IntoInt8(
+        tflite_micro::micro::GetTensorData<int8_t>(filter),
+        tflite_micro::micro::GetTensorShape(filter).FlatSize(), unpacked_filter_data);
     filter_data = unpacked_filter_data;
   } else {
-    filter_data = tflite::micro::GetTensorData<int8_t>(filter);
+    filter_data = tflite_micro::micro::GetTensorData<int8_t>(filter);
   }
 
   reference_integer_ops::ConvPerChannel(
       ConvParamsQuantized(params, op_data),
       op_data.per_channel_output_multiplier, op_data.per_channel_output_shift,
-      tflite::micro::GetTensorShape(input),
-      tflite::micro::GetTensorData<int8_t>(input),
-      tflite::micro::GetTensorShape(filter), filter_data,
-      tflite::micro::GetTensorShape(bias),
-      tflite::micro::GetOptionalTensorData<int32_t>(bias),
-      tflite::micro::GetTensorShape(output),
-      tflite::micro::GetTensorData<int8_t>(output));
+      tflite_micro::micro::GetTensorShape(input),
+      tflite_micro::micro::GetTensorData<int8_t>(input),
+      tflite_micro::micro::GetTensorShape(filter), filter_data,
+      tflite_micro::micro::GetTensorShape(bias),
+      tflite_micro::micro::GetOptionalTensorData<int32_t>(bias),
+      tflite_micro::micro::GetTensorShape(output),
+      tflite_micro::micro::GetTensorData<int8_t>(output));
 
   return kTfLiteOk;
 }
@@ -75,8 +75,8 @@ TfLiteStatus ConvReferenceEvalInt8(TfLiteContext* context, TfLiteNode* node) {
 // since the optimized conv implementation currently adds a lot to
 // the binary size (~30KB to text section).
 TFLMRegistration Register_CONV_2D_INT8REF() {
-  return tflite::micro::RegisterOp(ConvInit, ConvPrepare,
+  return tflite_micro::micro::RegisterOp(ConvInit, ConvPrepare,
                                    ConvReferenceEvalInt8);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/xtensa/conv_vision.cc b/tensorflow/lite/micro/kernels/xtensa/conv_vision.cc
index 812ab60e..4dfd4e03 100644
--- a/tensorflow/lite/micro/kernels/xtensa/conv_vision.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/conv_vision.cc
@@ -30,7 +30,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa_conv.h"
 #include "tensorflow/lite/micro/micro_arena_constants.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 TfLiteStatus ConvPrepareVision(TfLiteContext* context, TfLiteNode* node) {
   MicroContext* micro_context = GetMicroContext(context);
@@ -94,10 +94,10 @@ TfLiteStatus ConvPrepareVision(TfLiteContext* context, TfLiteNode* node) {
   if (filter->type == kTfLiteInt4) {
     const size_t bytes_unpacked = filter->bytes * 2;
     filter_int8.data.data = micro_context->AllocateTempBuffer(
-        bytes_unpacked, tflite::MicroArenaBufferAlignment());
+        bytes_unpacked, tflite_micro::MicroArenaBufferAlignment());
     filter_int8.dims = filter->dims;
     filter_int8.type = kTfLiteInt8;
-    tflite::tensor_utils::UnpackDenseInt4IntoInt8(
+    tflite_micro::tensor_utils::UnpackDenseInt4IntoInt8(
         GetTensorData<int8_t>(filter), GetTensorShape(filter).FlatSize(),
         GetTensorData<int8_t>(&filter_int8));
   } else {
@@ -170,8 +170,8 @@ TfLiteStatus ConvEvalVision(TfLiteContext* context, TfLiteNode* node,
   const int num_channels = filter->dims->data[kConvQuantizedDimension];
 
   xiConv(data.p_context, data.context_size,
-         const_cast<int8_t*>(tflite::micro::GetTensorData<int8_t>(input)),
-         input_size, tflite::micro::GetTensorData<int8_t>(output), output_size,
+         const_cast<int8_t*>(tflite_micro::micro::GetTensorData<int8_t>(input)),
+         input_size, tflite_micro::micro::GetTensorData<int8_t>(output), output_size,
          data.reorder_coefficient_bias, data.reorder_coefficient_bias_size,
          data.reference_op_data.per_channel_output_multiplier,
          data.per_channel_output_shift_int8, num_channels);
@@ -179,5 +179,5 @@ TfLiteStatus ConvEvalVision(TfLiteContext* context, TfLiteNode* node,
   return kTfLiteOk;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
 #endif  // defined(VISION_P6)
diff --git a/tensorflow/lite/micro/kernels/xtensa/depthwise_conv.cc b/tensorflow/lite/micro/kernels/xtensa/depthwise_conv.cc
index 8536ff79..9d8e3aca 100644
--- a/tensorflow/lite/micro/kernels/xtensa/depthwise_conv.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/depthwise_conv.cc
@@ -31,7 +31,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa_depthwise_conv.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 void* Init(TfLiteContext* context, const char* buffer, size_t length) {
@@ -80,17 +80,17 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
       *(reinterpret_cast<XtensaDepthwiseConvOpData*>(node->user_data));
 
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kDepthwiseConvOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kDepthwiseConvOutputTensor);
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kDepthwiseConvInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kDepthwiseConvInputTensor);
   const TfLiteEvalTensor* filter =
-      tflite::micro::GetEvalInput(context, node, kDepthwiseConvWeightsTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kDepthwiseConvWeightsTensor);
   const TfLiteEvalTensor* bias =
       (NumInputs(node) == 3)
-          ? tflite::micro::GetEvalInput(context, node, kDepthwiseConvBiasTensor)
+          ? tflite_micro::micro::GetEvalInput(context, node, kDepthwiseConvBiasTensor)
           : nullptr;
 
-  TfLiteEvalTensor filter_int8 = tflite::micro::MakeUnpackedInt4Tensor(
+  TfLiteEvalTensor filter_int8 = tflite_micro::micro::MakeUnpackedInt4Tensor(
       context, op_data.reference_op_data.filter_buffer_index, filter);
 
   switch (input->type) {  // Already know in/out types are same.
@@ -108,14 +108,14 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
               DepthwiseConvParamsQuantized(params, op_data.reference_op_data),
               op_data.reference_op_data.per_channel_output_multiplier,
               op_data.reference_op_data.per_channel_output_shift,
-              tflite::micro::GetTensorShape(input),
-              tflite::micro::GetTensorData<int8_t>(input),
-              tflite::micro::GetTensorShape(filter),
-              tflite::micro::GetTensorData<int8_t>(&filter_int8),
-              tflite::micro::GetTensorShape(bias),
-              tflite::micro::GetOptionalTensorData<int32_t>(bias),
-              tflite::micro::GetTensorShape(output),
-              tflite::micro::GetTensorData<int8_t>(output));
+              tflite_micro::micro::GetTensorShape(input),
+              tflite_micro::micro::GetTensorData<int8_t>(input),
+              tflite_micro::micro::GetTensorShape(filter),
+              tflite_micro::micro::GetTensorData<int8_t>(&filter_int8),
+              tflite_micro::micro::GetTensorShape(bias),
+              tflite_micro::micro::GetOptionalTensorData<int32_t>(bias),
+              tflite_micro::micro::GetTensorShape(output),
+              tflite_micro::micro::GetTensorData<int8_t>(output));
 #endif  // defined(HIFI3) || defined(HIFI4) || defined(HIFI5)
           break;
         }
@@ -133,14 +133,14 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
               DepthwiseConvParamsQuantized(params, op_data.reference_op_data),
               op_data.reference_op_data.per_channel_output_multiplier,
               op_data.reference_op_data.per_channel_output_shift,
-              tflite::micro::GetTensorShape(input),
-              tflite::micro::GetTensorData<int16_t>(input),
-              tflite::micro::GetTensorShape(filter),
-              tflite::micro::GetTensorData<int8_t>(&filter_int8),
-              tflite::micro::GetTensorShape(bias),
-              tflite::micro::GetOptionalTensorData<int64_t>(bias),
-              tflite::micro::GetTensorShape(output),
-              tflite::micro::GetTensorData<int16_t>(output));
+              tflite_micro::micro::GetTensorShape(input),
+              tflite_micro::micro::GetTensorData<int16_t>(input),
+              tflite_micro::micro::GetTensorShape(filter),
+              tflite_micro::micro::GetTensorData<int8_t>(&filter_int8),
+              tflite_micro::micro::GetTensorShape(bias),
+              tflite_micro::micro::GetOptionalTensorData<int64_t>(bias),
+              tflite_micro::micro::GetTensorShape(output),
+              tflite_micro::micro::GetTensorData<int16_t>(output));
           break;
         }
         default:
@@ -162,7 +162,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_DEPTHWISE_CONV_2D() {
-  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/xtensa/depthwise_conv_hifi.cc b/tensorflow/lite/micro/kernels/xtensa/depthwise_conv_hifi.cc
index 8c2052b2..df79009d 100644
--- a/tensorflow/lite/micro/kernels/xtensa/depthwise_conv_hifi.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/depthwise_conv_hifi.cc
@@ -29,7 +29,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa_depthwise_conv.h"
 
 #if defined(HIFI3) || defined(HIFI4) || defined(HIFI5)
-namespace tflite {
+namespace tflite_micro {
 TfLiteStatus DepthwiseConvPrepareHifi(TfLiteContext* context,
                                       TfLiteNode* node) {
   XtensaDepthwiseConvOpData* data =
@@ -112,10 +112,10 @@ TfLiteStatus DepthwiseConvEvalHifi(TfLiteContext* context, TfLiteNode* node,
         data.reference_op_data.output_activation_max;
     TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
 
-    const RuntimeShape& input_shape = tflite::micro::GetTensorShape(input);
-    const RuntimeShape& filter_shape = tflite::micro::GetTensorShape(filter);
-    const RuntimeShape& output_shape = tflite::micro::GetTensorShape(output);
-    const RuntimeShape& bias_shape = tflite::micro::GetTensorShape(bias);
+    const RuntimeShape& input_shape = tflite_micro::micro::GetTensorShape(input);
+    const RuntimeShape& filter_shape = tflite_micro::micro::GetTensorShape(filter);
+    const RuntimeShape& output_shape = tflite_micro::micro::GetTensorShape(output);
+    const RuntimeShape& bias_shape = tflite_micro::micro::GetTensorShape(bias);
     TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
     TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
     TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
@@ -132,10 +132,10 @@ TfLiteStatus DepthwiseConvEvalHifi(TfLiteContext* context, TfLiteNode* node,
     TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);
     TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
 
-    const int8_t* input_data = tflite::micro::GetTensorData<int8_t>(input);
-    const int8_t* filter_data = tflite::micro::GetTensorData<int8_t>(filter);
-    const int32_t* bias_data = tflite::micro::GetTensorData<int32_t>(bias);
-    int8_t* output_data = tflite::micro::GetTensorData<int8_t>(output);
+    const int8_t* input_data = tflite_micro::micro::GetTensorData<int8_t>(input);
+    const int8_t* filter_data = tflite_micro::micro::GetTensorData<int8_t>(filter);
+    const int32_t* bias_data = tflite_micro::micro::GetTensorData<int32_t>(bias);
+    int8_t* output_data = tflite_micro::micro::GetTensorData<int8_t>(output);
 
     int32_t input_data_format = 0;
     int32_t output_data_format = 0;
@@ -175,16 +175,16 @@ TfLiteStatus DepthwiseConvEvalHifi(TfLiteContext* context, TfLiteNode* node,
       DepthwiseConvParamsQuantized(params, data.reference_op_data),
       data.reference_op_data.per_channel_output_multiplier,
       data.reference_op_data.per_channel_output_shift,
-      tflite::micro::GetTensorShape(input),
-      tflite::micro::GetTensorData<int8_t>(input),
-      tflite::micro::GetTensorShape(filter),
-      tflite::micro::GetTensorData<int8_t>(filter),
-      tflite::micro::GetTensorShape(bias),
-      tflite::micro::GetTensorData<int32_t>(bias),
-      tflite::micro::GetTensorShape(output),
-      tflite::micro::GetTensorData<int8_t>(output));
+      tflite_micro::micro::GetTensorShape(input),
+      tflite_micro::micro::GetTensorData<int8_t>(input),
+      tflite_micro::micro::GetTensorShape(filter),
+      tflite_micro::micro::GetTensorData<int8_t>(filter),
+      tflite_micro::micro::GetTensorShape(bias),
+      tflite_micro::micro::GetTensorData<int32_t>(bias),
+      tflite_micro::micro::GetTensorShape(output),
+      tflite_micro::micro::GetTensorData<int8_t>(output));
 
   return kTfLiteOk;
 }
-}  // namespace tflite
+}  // namespace tflite_micro
 #endif  // defined(HIFI3) ||defined(HIFI4) || defined(HIFI5)
diff --git a/tensorflow/lite/micro/kernels/xtensa/depthwise_conv_vision.cc b/tensorflow/lite/micro/kernels/xtensa/depthwise_conv_vision.cc
index 35fa8cf1..6f6127a4 100644
--- a/tensorflow/lite/micro/kernels/xtensa/depthwise_conv_vision.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/depthwise_conv_vision.cc
@@ -29,7 +29,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa_depthwise_conv.h"
 #include "tensorflow/lite/micro/micro_arena_constants.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 TfLiteStatus DepthwiseConvPrepareVision(TfLiteContext* context,
                                         TfLiteNode* node) {
@@ -124,10 +124,10 @@ TfLiteStatus DepthwiseConvPrepareVision(TfLiteContext* context,
   if (filter->type == kTfLiteInt4) {
     const size_t bytes_unpacked = filter->bytes * 2;
     filter_int8.data.data = micro_context->AllocateTempBuffer(
-        bytes_unpacked, tflite::MicroArenaBufferAlignment());
+        bytes_unpacked, tflite_micro::MicroArenaBufferAlignment());
     filter_int8.dims = filter->dims;
     filter_int8.type = kTfLiteInt8;
-    tflite::tensor_utils::UnpackDenseInt4IntoInt8(
+    tflite_micro::tensor_utils::UnpackDenseInt4IntoInt8(
         GetTensorData<int8_t>(filter), GetTensorShape(filter).FlatSize(),
         GetTensorData<int8_t>(&filter_int8));
 
@@ -167,8 +167,8 @@ TfLiteStatus DepthwiseConvEvalVision(TfLiteContext* context, TfLiteNode* node,
   const int num_channels = filter->dims->data[kDepthwiseConvQuantizedDimension];
   xiDepthwiseConv(
       data.p_context, data.context_size,
-      const_cast<int8_t*>(tflite::micro::GetTensorData<int8_t>(input)),
-      input_size, tflite::micro::GetTensorData<int8_t>(output), output_size,
+      const_cast<int8_t*>(tflite_micro::micro::GetTensorData<int8_t>(input)),
+      input_size, tflite_micro::micro::GetTensorData<int8_t>(output), output_size,
       data.reorder_coefficient_bias, data.reorder_coefficient_bias_size,
       data.reference_op_data.per_channel_output_multiplier,
       data.per_channel_output_shift_int8, num_channels,
@@ -176,5 +176,5 @@ TfLiteStatus DepthwiseConvEvalVision(TfLiteContext* context, TfLiteNode* node,
       data.reference_op_data.padding.height);
   return kTfLiteOk;
 }
-}  // namespace tflite
+}  // namespace tflite_micro
 #endif  // defined(VISION_P6)
diff --git a/tensorflow/lite/micro/kernels/xtensa/fully_connected.cc b/tensorflow/lite/micro/kernels/xtensa/fully_connected.cc
index df545800..09fd0e85 100644
--- a/tensorflow/lite/micro/kernels/xtensa/fully_connected.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/fully_connected.cc
@@ -27,7 +27,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa_fully_connected.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -37,13 +37,13 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
       static_cast<const TfLiteFullyConnectedParams*>(node->builtin_data);
 
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kFullyConnectedInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kFullyConnectedInputTensor);
   const TfLiteEvalTensor* filter =
-      tflite::micro::GetEvalInput(context, node, kFullyConnectedWeightsTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kFullyConnectedWeightsTensor);
   const TfLiteEvalTensor* bias =
-      tflite::micro::GetEvalInput(context, node, kFullyConnectedBiasTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kFullyConnectedBiasTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kFullyConnectedOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kFullyConnectedOutputTensor);
 
   TFLITE_DCHECK(node->user_data != nullptr);
 
@@ -53,16 +53,16 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   // Checks in Prepare ensure input, output and filter types are all the same.
   switch (input->type) {
     case kTfLiteFloat32: {
-      tflite::reference_ops::FullyConnected(
+      tflite_micro::reference_ops::FullyConnected(
           FullyConnectedParamsFloat(params->activation),
-          tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<float>(input),
-          tflite::micro::GetTensorShape(filter),
-          tflite::micro::GetTensorData<float>(filter),
-          tflite::micro::GetTensorShape(bias),
-          tflite::micro::GetOptionalTensorData<float>(bias),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<float>(output));
+          tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<float>(input),
+          tflite_micro::micro::GetTensorShape(filter),
+          tflite_micro::micro::GetTensorData<float>(filter),
+          tflite_micro::micro::GetTensorShape(bias),
+          tflite_micro::micro::GetOptionalTensorData<float>(bias),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<float>(output));
       break;
     }
 
@@ -88,16 +88,16 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
     case kTfLiteInt16: {
       switch (filter->type) {
         case kTfLiteInt8: {
-          tflite::reference_integer_ops::FullyConnected(
+          tflite_micro::reference_integer_ops::FullyConnected(
               FullyConnectedParamsQuantized(data),
-              tflite::micro::GetTensorShape(input),
-              tflite::micro::GetTensorData<int16_t>(input),
-              tflite::micro::GetTensorShape(filter),
-              tflite::micro::GetTensorData<int8_t>(filter),
-              tflite::micro::GetTensorShape(bias),
-              tflite::micro::GetOptionalTensorData<int64_t>(bias),
-              tflite::micro::GetTensorShape(output),
-              tflite::micro::GetTensorData<int16_t>(output));
+              tflite_micro::micro::GetTensorShape(input),
+              tflite_micro::micro::GetTensorData<int16_t>(input),
+              tflite_micro::micro::GetTensorShape(filter),
+              tflite_micro::micro::GetTensorData<int8_t>(filter),
+              tflite_micro::micro::GetTensorShape(bias),
+              tflite_micro::micro::GetOptionalTensorData<int64_t>(bias),
+              tflite_micro::micro::GetTensorShape(output),
+              tflite_micro::micro::GetTensorData<int16_t>(output));
           break;
         }
         default: {
@@ -121,12 +121,12 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_FULLY_CONNECTED() {
-  return tflite::micro::RegisterOp(XtensaInitFullyConnected,
+  return tflite_micro::micro::RegisterOp(XtensaInitFullyConnected,
                                    XtensaPrepareFullyConnected, Eval);
 }
 
 TFLMInferenceRegistration RegisterInference_FULLY_CONNECTED() {
-  return tflite::micro::RegisterOp(Eval);
+  return tflite_micro::micro::RegisterOp(Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/xtensa/fully_connected_common_xtensa.cc b/tensorflow/lite/micro/kernels/xtensa/fully_connected_common_xtensa.cc
index cf87c5ff..43b1bc17 100644
--- a/tensorflow/lite/micro/kernels/xtensa/fully_connected_common_xtensa.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/fully_connected_common_xtensa.cc
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa.h"
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa_fully_connected.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 void* XtensaInitFullyConnected(TfLiteContext* context, const char* buffer,
                                size_t length) {
@@ -133,4 +133,4 @@ TfLiteStatus XtensaPrepareFullyConnected(TfLiteContext* context,
   return kTfLiteOk;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/xtensa/fully_connected_int8.cc b/tensorflow/lite/micro/kernels/xtensa/fully_connected_int8.cc
index f850c0c0..b5100f06 100644
--- a/tensorflow/lite/micro/kernels/xtensa/fully_connected_int8.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/fully_connected_int8.cc
@@ -25,7 +25,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa.h"
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa_fully_connected.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 TfLiteStatus XtensaEvalFullyConnectedQuantizedInt8(
     TfLiteContext* context, TfLiteNode* node, const OpDataFullyConnected& data,
@@ -33,38 +33,38 @@ TfLiteStatus XtensaEvalFullyConnectedQuantizedInt8(
     const TfLiteEvalTensor* bias, TfLiteEvalTensor* output) {
 #if !defined(VISION_P6)
   const int32_t* bias_data =
-      tflite::micro::GetOptionalTensorData<int32_t>(bias);
+      tflite_micro::micro::GetOptionalTensorData<int32_t>(bias);
 
   // P6 Vision will handle INT4 filters as a reference operation.
   // For all other architectures, unpack INT4 here.
-  const int8_t* filter_data = tflite::micro::GetTensorData<int8_t>(filter);
+  const int8_t* filter_data = tflite_micro::micro::GetTensorData<int8_t>(filter);
   if (filter->type == kTfLiteInt4) {
     int8_t* unpacked_filter_data = static_cast<int8_t*>(
         context->GetScratchBuffer(context, data.filter_buffer_index));
 
-    tflite::tensor_utils::UnpackDenseInt4IntoInt8(
-        tflite::micro::GetTensorData<int8_t>(filter),
-        tflite::micro::GetTensorShape(filter).FlatSize(), unpacked_filter_data);
+    tflite_micro::tensor_utils::UnpackDenseInt4IntoInt8(
+        tflite_micro::micro::GetTensorData<int8_t>(filter),
+        tflite_micro::micro::GetTensorShape(filter).FlatSize(), unpacked_filter_data);
     filter_data = unpacked_filter_data;
   }
 #endif  // !defined(VISION_P6)
 
 #if defined(HIFIMINI)
   FullyConnectedEvalHifimini(FullyConnectedParamsQuantized(data),
-                             tflite::micro::GetTensorShape(input),
-                             tflite::micro::GetTensorData<int8_t>(input),
-                             tflite::micro::GetTensorShape(filter), filter_data,
-                             tflite::micro::GetTensorShape(bias), bias_data,
-                             tflite::micro::GetTensorShape(output),
-                             tflite::micro::GetTensorData<int8_t>(output));
+                             tflite_micro::micro::GetTensorShape(input),
+                             tflite_micro::micro::GetTensorData<int8_t>(input),
+                             tflite_micro::micro::GetTensorShape(filter), filter_data,
+                             tflite_micro::micro::GetTensorShape(bias), bias_data,
+                             tflite_micro::micro::GetTensorShape(output),
+                             tflite_micro::micro::GetTensorData<int8_t>(output));
 #elif defined(HIFI3) || defined(HIFI4) || defined(HIFI5)
-  const RuntimeShape& output_shape = tflite::micro::GetTensorShape(output);
+  const RuntimeShape& output_shape = tflite_micro::micro::GetTensorShape(output);
   const int num_batches =
       FlatSizeSkipDim(output_shape, output_shape.DimensionsCount() - 1);
   const int output_depth =
       output_shape.Dims(output_shape.DimensionsCount() - 1);
 
-  const RuntimeShape& filter_shape = tflite::micro::GetTensorShape(filter);
+  const RuntimeShape& filter_shape = tflite_micro::micro::GetTensorShape(filter);
   const int filter_dim_count = filter_shape.DimensionsCount();
   const int accum_depth = filter_shape.Dims(filter_dim_count - 1);
 
@@ -73,16 +73,16 @@ TfLiteStatus XtensaEvalFullyConnectedQuantizedInt8(
     TF_LITE_ENSURE_EQ(
         context,
         xa_nn_fully_connected_sym8sxasym8s_asym8s(
-            (tflite::micro::GetTensorData<int8_t>(output) + b * output_depth),
+            (tflite_micro::micro::GetTensorData<int8_t>(output) + b * output_depth),
             filter_data,
-            (tflite::micro::GetTensorData<int8_t>(input) + b * accum_depth),
+            (tflite_micro::micro::GetTensorData<int8_t>(input) + b * accum_depth),
             bias_data, accum_depth, output_depth, op_params.input_offset,
             op_params.output_multiplier, op_params.output_shift,
             op_params.output_offset),
         0);
   }
 
-  int8_t* output_arr = tflite::micro::GetTensorData<int8_t>(output);
+  int8_t* output_arr = tflite_micro::micro::GetTensorData<int8_t>(output);
   TF_LITE_ENSURE_EQ(context,
                     xa_nn_vec_activation_min_max_8_8(
                         output_arr, output_arr, data.output_activation_min,
@@ -97,12 +97,12 @@ TfLiteStatus XtensaEvalFullyConnectedQuantizedInt8(
                            output);
 #else
   reference_integer_ops::FullyConnected(
-      FullyConnectedParamsQuantized(data), tflite::micro::GetTensorShape(input),
-      tflite::micro::GetTensorData<int8_t>(input),
-      tflite::micro::GetTensorShape(filter), filter_data,
-      tflite::micro::GetTensorShape(bias), bias_data,
-      tflite::micro::GetTensorShape(output),
-      tflite::micro::GetTensorData<int8_t>(output));
+      FullyConnectedParamsQuantized(data), tflite_micro::micro::GetTensorShape(input),
+      tflite_micro::micro::GetTensorData<int8_t>(input),
+      tflite_micro::micro::GetTensorShape(filter), filter_data,
+      tflite_micro::micro::GetTensorShape(bias), bias_data,
+      tflite_micro::micro::GetTensorShape(output),
+      tflite_micro::micro::GetTensorData<int8_t>(output));
 #endif  // defined(HIFI3) || defined(HIFI4) || defined(HIFI5)
 
   return kTfLiteOk;
@@ -116,14 +116,14 @@ TfLiteStatus EvalInt8(TfLiteContext* context, TfLiteNode* node) {
       *(static_cast<const OpDataFullyConnected*>(node->user_data));
 
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kFullyConnectedInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kFullyConnectedInputTensor);
   const TfLiteEvalTensor* filter =
-      tflite::micro::GetEvalInput(context, node, kFullyConnectedWeightsTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kFullyConnectedWeightsTensor);
   const TfLiteEvalTensor* bias =
-      tflite::micro::GetEvalInput(context, node, kFullyConnectedBiasTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kFullyConnectedBiasTensor);
 
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kFullyConnectedOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kFullyConnectedOutputTensor);
 
   return XtensaEvalFullyConnectedQuantizedInt8(context, node, data, input,
                                                filter, bias, output);
@@ -132,8 +132,8 @@ TfLiteStatus EvalInt8(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_FULLY_CONNECTED_INT8() {
-  return tflite::micro::RegisterOp(XtensaInitFullyConnected,
+  return tflite_micro::micro::RegisterOp(XtensaInitFullyConnected,
                                    XtensaPrepareFullyConnected, EvalInt8);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/xtensa/fully_connected_vision.cc b/tensorflow/lite/micro/kernels/xtensa/fully_connected_vision.cc
index 14bb9a12..ca0e7f21 100644
--- a/tensorflow/lite/micro/kernels/xtensa/fully_connected_vision.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/fully_connected_vision.cc
@@ -29,7 +29,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa_fully_connected.h"
 #include "tensorflow/lite/micro/micro_arena_constants.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 void NormalizeFCDims(uint32_t* dims, int rank) {
   if (rank < 4) {
@@ -97,10 +97,10 @@ TfLiteStatus FullyConnectedPrepareVision(TfLiteContext* context,
   if (filter->type == kTfLiteInt4) {
     const size_t bytes_unpacked = filter->bytes * 2;
     filter_int8.data.data = micro_context->AllocateTempBuffer(
-        bytes_unpacked, tflite::MicroArenaBufferAlignment());
+        bytes_unpacked, tflite_micro::MicroArenaBufferAlignment());
     filter_int8.dims = filter->dims;
     filter_int8.type = kTfLiteInt8;
-    tflite::tensor_utils::UnpackDenseInt4IntoInt8(
+    tflite_micro::tensor_utils::UnpackDenseInt4IntoInt8(
         GetTensorData<int8_t>(filter), GetTensorShape(filter).FlatSize(),
         GetTensorData<int8_t>(&filter_int8));
 
@@ -171,11 +171,11 @@ TfLiteStatus FullyConnectedEvalVision(TfLiteContext* context, TfLiteNode* node,
 
   xiFullyConnected(
       data.p_context, data.context_size,
-      const_cast<int8_t*>(tflite::micro::GetTensorData<int8_t>(input)),
-      input_size, tflite::micro::GetTensorData<int8_t>(output), output_size,
+      const_cast<int8_t*>(tflite_micro::micro::GetTensorData<int8_t>(input)),
+      input_size, tflite_micro::micro::GetTensorData<int8_t>(output), output_size,
       data.reorder_coefficient_bias, data.reorder_coefficient_bias_size, NULL,
       NULL, num_channels);
   return kTfLiteOk;
 }
-}  // namespace tflite
+}  // namespace tflite_micro
 #endif  // defined(VISION_P6)
diff --git a/tensorflow/lite/micro/kernels/xtensa/hifimini/fixedpoint_utils.h b/tensorflow/lite/micro/kernels/xtensa/hifimini/fixedpoint_utils.h
index 42bf971b..7d814717 100644
--- a/tensorflow/lite/micro/kernels/xtensa/hifimini/fixedpoint_utils.h
+++ b/tensorflow/lite/micro/kernels/xtensa/hifimini/fixedpoint_utils.h
@@ -26,7 +26,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/compatibility.h"
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // INT24 MIN/MAX
 #define INT24_MIN -8388608
@@ -134,6 +134,6 @@ inline int CreateQConstantForInt24(int integer_bits, float f) {
   return static_cast<int>(raw);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
 #endif  // defined(HIFIMINI)
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_XTENSA_HIFIMINI_FIXEDPOINT_UTILS_H_
diff --git a/tensorflow/lite/micro/kernels/xtensa/hifimini/fully_connected.cc b/tensorflow/lite/micro/kernels/xtensa/hifimini/fully_connected.cc
index b63c5001..5df2382d 100644
--- a/tensorflow/lite/micro/kernels/xtensa/hifimini/fully_connected.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/hifimini/fully_connected.cc
@@ -28,7 +28,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/hifimini/fixedpoint_utils.h"
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 void FullyConnectedEvalHifimini(
     const FullyConnectedParams& params, const RuntimeShape& input_shape,
@@ -114,5 +114,5 @@ void FullyConnectedEvalHifimini(
   }
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
 #endif  // defined(HIFIMINI)
diff --git a/tensorflow/lite/micro/kernels/xtensa/hifimini/svdf.cc b/tensorflow/lite/micro/kernels/xtensa/hifimini/svdf.cc
index 08ef4d9b..270294ee 100644
--- a/tensorflow/lite/micro/kernels/xtensa/hifimini/svdf.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/hifimini/svdf.cc
@@ -31,7 +31,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa.h"
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa_svdf.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 /**
  * This version of SVDF is specific to TFLite Micro. It contains only a full
@@ -67,7 +67,7 @@ TfLiteStatus EvalIntegerSvdfHifimini(
 
   // Shift states.
   int16_t* const state_ptr =
-      tflite::micro::GetTensorData<int16_t>(activation_state_tensor);
+      tflite_micro::micro::GetTensorData<int16_t>(activation_state_tensor);
 
   // Left shift the activation_state.
   {
@@ -83,9 +83,9 @@ TfLiteStatus EvalIntegerSvdfHifimini(
 
   // Feature matmul.
   {
-    const int8_t* input = tflite::micro::GetTensorData<int8_t>(input_tensor);
+    const int8_t* input = tflite_micro::micro::GetTensorData<int8_t>(input_tensor);
     const int8_t* weight_feature =
-        tflite::micro::GetTensorData<int8_t>(weights_feature_tensor);
+        tflite_micro::micro::GetTensorData<int8_t>(weights_feature_tensor);
     int16_t* result_in_batch = state_ptr + (n_memory - 1);
 
     ae_q56s output_int16_max_56 = AE_CVTQ48A32S(INT16_MAX);
@@ -153,7 +153,7 @@ TfLiteStatus EvalIntegerSvdfHifimini(
 
       // Perform batched vector dot product:
       const int16_t* vector1_ptr =
-          tflite::micro::GetTensorData<int16_t>(weights_time_tensor);
+          tflite_micro::micro::GetTensorData<int16_t>(weights_time_tensor);
       const int16_t* vector2_ptr = state_ptr + b * n_memory * n_filter;
 
       const ae_p16x2s* offset_vector1 =
@@ -186,7 +186,7 @@ TfLiteStatus EvalIntegerSvdfHifimini(
     if (bias_tensor) {
       // Vector batch assign:
       const int32_t* bias_data =
-          tflite::micro::GetTensorData<int32_t>(bias_tensor);
+          tflite_micro::micro::GetTensorData<int32_t>(bias_tensor);
       for (int i = 0; i < n_batch; ++i) {
         int32_t* output_ptr = scratch_output_tensor + i * n_unit;
         const int32_t* bias_ptr = bias_data;
@@ -227,11 +227,11 @@ TfLiteStatus EvalIntegerSvdfHifimini(
       // Cap min/max and convert to int32_t (already aligned to 32bit):
       x_56 = AE_MAXQ56S(x_56, output_int8_min_56);
       x_56 = AE_MINQ56S(x_56, output_int8_max_56);
-      tflite::micro::GetTensorData<int8_t>(output_tensor)[i] =
+      tflite_micro::micro::GetTensorData<int8_t>(output_tensor)[i] =
           static_cast<int8_t>(AE_TRUNCA32Q48(x_56));
     }
   }
   return kTfLiteOk;
 }
-}  // namespace tflite
+}  // namespace tflite_micro
 #endif  // defined(HIFIMINI)
diff --git a/tensorflow/lite/micro/kernels/xtensa/leaky_relu.cc b/tensorflow/lite/micro/kernels/xtensa/leaky_relu.cc
index 61ab9853..817f56ca 100644
--- a/tensorflow/lite/micro/kernels/xtensa/leaky_relu.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/leaky_relu.cc
@@ -25,7 +25,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 template <typename T>
 void QuantizeLeakyRelu(const LeakyReluOpData& data,
@@ -40,10 +40,10 @@ void QuantizeLeakyRelu(const LeakyReluOpData& data,
   op_params.output_multiplier_identity = data.output_multiplier_identity;
   op_params.output_shift_identity = data.output_shift_identity;
   reference_ops::QuantizeLeakyRelu(op_params,
-                                   tflite::micro::GetTensorShape(input),
-                                   tflite::micro::GetTensorData<T>(input),
-                                   tflite::micro::GetTensorShape(output),
-                                   tflite::micro::GetTensorData<T>(output));
+                                   tflite_micro::micro::GetTensorShape(input),
+                                   tflite_micro::micro::GetTensorData<T>(input),
+                                   tflite_micro::micro::GetTensorShape(output),
+                                   tflite_micro::micro::GetTensorData<T>(output));
 }
 
 void* LeakyReluInit(TfLiteContext* context, const char* buffer, size_t length) {
@@ -53,9 +53,9 @@ void* LeakyReluInit(TfLiteContext* context, const char* buffer, size_t length) {
 
 TfLiteStatus LeakyReluEval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
   const LeakyReluOpData& data = *static_cast<LeakyReluOpData*>(node->user_data);
 
   switch (input->type) {
@@ -65,10 +65,10 @@ TfLiteStatus LeakyReluEval(TfLiteContext* context, TfLiteNode* node) {
           static_cast<TfLiteLeakyReluParams*>(node->builtin_data);
 
       op_params.alpha = params->alpha;
-      reference_ops::LeakyRelu(op_params, tflite::micro::GetTensorShape(input),
-                               tflite::micro::GetTensorData<float>(input),
-                               tflite::micro::GetTensorShape(output),
-                               tflite::micro::GetTensorData<float>(output));
+      reference_ops::LeakyRelu(op_params, tflite_micro::micro::GetTensorShape(input),
+                               tflite_micro::micro::GetTensorData<float>(input),
+                               tflite_micro::micro::GetTensorShape(output),
+                               tflite_micro::micro::GetTensorData<float>(output));
       return kTfLiteOk;
     } break;
     case kTfLiteInt8: {
@@ -77,12 +77,12 @@ TfLiteStatus LeakyReluEval(TfLiteContext* context, TfLiteNode* node) {
     } break;
     case kTfLiteInt16: {
 #if defined(HIFI3) || defined(HIFI4)
-      const RuntimeShape& input_shape = tflite::micro::GetTensorShape(input);
-      const RuntimeShape& output_shape = tflite::micro::GetTensorShape(output);
+      const RuntimeShape& input_shape = tflite_micro::micro::GetTensorShape(input);
+      const RuntimeShape& output_shape = tflite_micro::micro::GetTensorShape(output);
       const int flat_size = MatchingFlatSize(input_shape, output_shape);
       int32_t err = xa_nn_vec_leaky_relu_asym16s_asym16s(
-          tflite::micro::GetTensorData<int16_t>(output),
-          tflite::micro::GetTensorData<int16_t>(input), data.input_zero_point,
+          tflite_micro::micro::GetTensorData<int16_t>(output),
+          tflite_micro::micro::GetTensorData<int16_t>(input), data.input_zero_point,
           data.output_multiplier_alpha, data.output_shift_alpha,
           data.output_multiplier_identity, data.output_shift_identity,
           data.output_zero_point, flat_size);
@@ -102,8 +102,8 @@ TfLiteStatus LeakyReluEval(TfLiteContext* context, TfLiteNode* node) {
 }
 
 TFLMRegistration Register_LEAKY_RELU() {
-  return tflite::micro::RegisterOp(LeakyReluInit, LeakyReluPrepare,
+  return tflite_micro::micro::RegisterOp(LeakyReluInit, LeakyReluPrepare,
                                    LeakyReluEval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/xtensa/logistic.cc b/tensorflow/lite/micro/kernels/xtensa/logistic.cc
index 2ddf82ef..049e3b95 100644
--- a/tensorflow/lite/micro/kernels/xtensa/logistic.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/logistic.cc
@@ -28,7 +28,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 void* LogisticInit(TfLiteContext* context, const char* buffer, size_t length) {
@@ -38,9 +38,9 @@ void* LogisticInit(TfLiteContext* context, const char* buffer, size_t length) {
 
 TfLiteStatus LogisticEval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kLogisticInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kLogisticInputTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kLogisticOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kLogisticOutputTensor);
 
   TFLITE_DCHECK(node->user_data != nullptr);
   OpDataLogistic* data = static_cast<OpDataLogistic*>(node->user_data);
@@ -55,33 +55,33 @@ TfLiteStatus LogisticEval(TfLiteContext* context, TfLiteNode* node) {
   switch (input->type) {
     case kTfLiteFloat32: {
 #if HIFI_VFPU && (defined(HIFI3) || defined(HIFI4) || defined(HIFI5))
-      const RuntimeShape& input_shape = tflite::micro::GetTensorShape(input);
-      const RuntimeShape& output_shape = tflite::micro::GetTensorShape(output);
+      const RuntimeShape& input_shape = tflite_micro::micro::GetTensorShape(input);
+      const RuntimeShape& output_shape = tflite_micro::micro::GetTensorShape(output);
       const int flat_size = MatchingFlatSize(input_shape, output_shape);
 
-      const float* inp_data_ptr = tflite::micro::GetTensorData<float>(input);
-      float* out_data_ptr = tflite::micro::GetTensorData<float>(output);
+      const float* inp_data_ptr = tflite_micro::micro::GetTensorData<float>(input);
+      float* out_data_ptr = tflite_micro::micro::GetTensorData<float>(output);
 
       TF_LITE_ENSURE_EQ(
           context,
           xa_nn_vec_sigmoid_f32_f32(out_data_ptr, inp_data_ptr, flat_size), 0);
 #else
-      reference_ops::Logistic(tflite::micro::GetTensorShape(input),
-                              tflite::micro::GetTensorData<float>(input),
-                              tflite::micro::GetTensorShape(output),
-                              tflite::micro::GetTensorData<float>(output));
+      reference_ops::Logistic(tflite_micro::micro::GetTensorShape(input),
+                              tflite_micro::micro::GetTensorData<float>(input),
+                              tflite_micro::micro::GetTensorShape(output),
+                              tflite_micro::micro::GetTensorData<float>(output));
 #endif  // HIFI_VFPU && (defined(HIFI3) || defined(HIFI4) || defined(HIFI5))
       break;
     }
     case kTfLiteInt8: {
 #if defined(HIFI3) || defined(HIFI4) || defined(HIFI5)
-      const RuntimeShape& input_shape = tflite::micro::GetTensorShape(input);
-      const RuntimeShape& output_shape = tflite::micro::GetTensorShape(output);
+      const RuntimeShape& input_shape = tflite_micro::micro::GetTensorShape(input);
+      const RuntimeShape& output_shape = tflite_micro::micro::GetTensorShape(output);
       const int flat_size = MatchingFlatSize(input_shape, output_shape);
 
       const int8_t* input_data_ptr =
-          tflite::micro::GetTensorData<int8_t>(input);
-      int8_t* output_data_ptr = tflite::micro::GetTensorData<int8_t>(output);
+          tflite_micro::micro::GetTensorData<int8_t>(input);
+      int8_t* output_data_ptr = tflite_micro::micro::GetTensorData<int8_t>(output);
 
       TF_LITE_ENSURE_EQ(
           context,
@@ -94,8 +94,8 @@ TfLiteStatus LogisticEval(TfLiteContext* context, TfLiteNode* node) {
       reference_integer_ops::Logistic(
           data->input_zero_point, data->input_range_radius,
           data->input_multiplier, data->input_left_shift,
-          NumElements(input->dims), tflite::micro::GetTensorData<int8_t>(input),
-          tflite::micro::GetTensorData<int8_t>(output));
+          NumElements(input->dims), tflite_micro::micro::GetTensorData<int8_t>(input),
+          tflite_micro::micro::GetTensorData<int8_t>(output));
 #endif  // defined(HIFI3) || defined(HIFI4) || defined(HIFI5)
       break;
     }
@@ -105,8 +105,8 @@ TfLiteStatus LogisticEval(TfLiteContext* context, TfLiteNode* node) {
           reference_integer_ops::Logistic(
               data->input_multiplier, data->input_left_shift,
               NumElements(input->dims),
-              tflite::micro::GetTensorData<int16_t>(input),
-              tflite::micro::GetTensorData<int16_t>(output));
+              tflite_micro::micro::GetTensorData<int16_t>(input),
+              tflite_micro::micro::GetTensorData<int16_t>(output));
           break;
         default:
           MicroPrintf("Input %s, output %s not supported.",
@@ -129,6 +129,6 @@ TfLiteStatus LogisticEval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_LOGISTIC() {
-  return tflite::micro::RegisterOp(LogisticInit, LogisticPrepare, LogisticEval);
+  return tflite_micro::micro::RegisterOp(LogisticInit, LogisticPrepare, LogisticEval);
 }
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/xtensa/lstm_eval.cc b/tensorflow/lite/micro/kernels/xtensa/lstm_eval.cc
index 9065388e..fc4afc71 100644
--- a/tensorflow/lite/micro/kernels/xtensa/lstm_eval.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/lstm_eval.cc
@@ -30,7 +30,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/op_macros.h"
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace ops {
 namespace micro {
 namespace lstm_eval {
@@ -864,66 +864,66 @@ TfLiteStatus EvalInteger8x8_16(
     for (int t = 0; t < max_time; t++) {
       const int t_rel = t;
       int8_t* output_ptr =
-          tflite::micro::GetTensorData<int8_t>(output) + t_rel * output_step;
+          tflite_micro::micro::GetTensorData<int8_t>(output) + t_rel * output_step;
       const int8_t* input_ptr =
-          tflite::micro::GetTensorData<int8_t>(input) + t_rel * input_step;
+          tflite_micro::micro::GetTensorData<int8_t>(input) + t_rel * input_step;
       LstmStepInteger8x8_16(
           input_ptr,
-          tflite::micro::GetTensorData<int8_t>(input_to_input_weights),
+          tflite_micro::micro::GetTensorData<int8_t>(input_to_input_weights),
           integer_lstm_param->effective_input_to_input_scale_a,
           integer_lstm_param->effective_input_to_input_scale_b,
-          tflite::micro::GetTensorData<int8_t>(input_to_forget_weights),
+          tflite_micro::micro::GetTensorData<int8_t>(input_to_forget_weights),
           integer_lstm_param->effective_input_to_forget_scale_a,
           integer_lstm_param->effective_input_to_forget_scale_b,
-          tflite::micro::GetTensorData<int8_t>(input_to_cell_weights),
+          tflite_micro::micro::GetTensorData<int8_t>(input_to_cell_weights),
           integer_lstm_param->effective_input_to_cell_scale_a,
           integer_lstm_param->effective_input_to_cell_scale_b,
-          tflite::micro::GetTensorData<int8_t>(input_to_output_weights),
+          tflite_micro::micro::GetTensorData<int8_t>(input_to_output_weights),
           integer_lstm_param->effective_input_to_output_scale_a,
           integer_lstm_param->effective_input_to_output_scale_b,
-          tflite::micro::GetTensorData<int8_t>(recurrent_to_input_weights),
+          tflite_micro::micro::GetTensorData<int8_t>(recurrent_to_input_weights),
           integer_lstm_param->effective_recurrent_to_input_scale_a,
           integer_lstm_param->effective_recurrent_to_input_scale_b,
-          tflite::micro::GetTensorData<int8_t>(recurrent_to_forget_weights),
+          tflite_micro::micro::GetTensorData<int8_t>(recurrent_to_forget_weights),
           integer_lstm_param->effective_recurrent_to_forget_scale_a,
           integer_lstm_param->effective_recurrent_to_forget_scale_b,
-          tflite::micro::GetTensorData<int8_t>(recurrent_to_cell_weights),
+          tflite_micro::micro::GetTensorData<int8_t>(recurrent_to_cell_weights),
           integer_lstm_param->effective_recurrent_to_cell_scale_a,
           integer_lstm_param->effective_recurrent_to_cell_scale_b,
-          tflite::micro::GetTensorData<int8_t>(recurrent_to_output_weights),
+          tflite_micro::micro::GetTensorData<int8_t>(recurrent_to_output_weights),
           integer_lstm_param->effective_recurrent_to_output_scale_a,
           integer_lstm_param->effective_recurrent_to_output_scale_b,
-          tflite::micro::GetTensorData<int16_t>(cell_to_input_weights),
+          tflite_micro::micro::GetTensorData<int16_t>(cell_to_input_weights),
           integer_lstm_param->effective_cell_to_input_scale_a,
           integer_lstm_param->effective_cell_to_input_scale_b,
-          tflite::micro::GetTensorData<int16_t>(cell_to_forget_weights),
+          tflite_micro::micro::GetTensorData<int16_t>(cell_to_forget_weights),
           integer_lstm_param->effective_cell_to_forget_scale_a,
           integer_lstm_param->effective_cell_to_forget_scale_b,
-          tflite::micro::GetTensorData<int16_t>(cell_to_output_weights),
+          tflite_micro::micro::GetTensorData<int16_t>(cell_to_output_weights),
           integer_lstm_param->effective_cell_to_output_scale_a,
           integer_lstm_param->effective_cell_to_output_scale_b,
-          tflite::micro::GetTensorData<int8_t>(projection_weights),
+          tflite_micro::micro::GetTensorData<int8_t>(projection_weights),
           integer_lstm_param->effective_proj_scale_a,
           integer_lstm_param->effective_proj_scale_b,
           integer_lstm_param->hidden_zp,
           integer_lstm_param->effective_hidden_scale_a,
           integer_lstm_param->effective_hidden_scale_b,
-          tflite::micro::GetTensorData<int16_t>(input_layer_norm_coefficients),
+          tflite_micro::micro::GetTensorData<int16_t>(input_layer_norm_coefficients),
           integer_lstm_param->layer_norm_input_scale_a,
           integer_lstm_param->layer_norm_input_scale_b,
-          tflite::micro::GetTensorData<int16_t>(forget_layer_norm_coefficients),
+          tflite_micro::micro::GetTensorData<int16_t>(forget_layer_norm_coefficients),
           integer_lstm_param->layer_norm_forget_scale_a,
           integer_lstm_param->layer_norm_forget_scale_b,
-          tflite::micro::GetTensorData<int16_t>(cell_layer_norm_coefficients),
+          tflite_micro::micro::GetTensorData<int16_t>(cell_layer_norm_coefficients),
           integer_lstm_param->layer_norm_cell_scale_a,
           integer_lstm_param->layer_norm_cell_scale_b,
-          tflite::micro::GetTensorData<int16_t>(output_layer_norm_coefficients),
+          tflite_micro::micro::GetTensorData<int16_t>(output_layer_norm_coefficients),
           integer_lstm_param->layer_norm_output_scale_a,
           integer_lstm_param->layer_norm_output_scale_b,
-          tflite::micro::GetTensorData<int32_t>(input_gate_bias),
-          tflite::micro::GetTensorData<int32_t>(forget_gate_bias),
-          tflite::micro::GetTensorData<int32_t>(cell_gate_bias),
-          tflite::micro::GetTensorData<int32_t>(output_gate_bias),
+          tflite_micro::micro::GetTensorData<int32_t>(input_gate_bias),
+          tflite_micro::micro::GetTensorData<int32_t>(forget_gate_bias),
+          tflite_micro::micro::GetTensorData<int32_t>(cell_gate_bias),
+          tflite_micro::micro::GetTensorData<int32_t>(output_gate_bias),
           integer_lstm_param->quantized_cell_clip,
           integer_lstm_param->quantized_proj_clip,
           integer_lstm_param->cell_scale,
@@ -940,8 +940,8 @@ TfLiteStatus EvalInteger8x8_16(
           integer_lstm_param->input_to_input_effective_bias.get(),
           integer_lstm_param->recurrent_to_input_effective_bias.get(),
           integer_lstm_param->projection_effective_bias.get(), n_batch, n_cell,
-          n_input, n_output, tflite::micro::GetTensorData<int8_t>(output_state),
-          output_state_zp, tflite::micro::GetTensorData<int16_t>(cell_state),
+          n_input, n_output, tflite_micro::micro::GetTensorData<int8_t>(output_state),
+          output_state_zp, tflite_micro::micro::GetTensorData<int16_t>(cell_state),
           output_ptr, (int16_t*)(scratch0), (int16_t*)(scratch1),
           (int16_t*)(scratch2), (int16_t*)(scratch3), (int8_t*)(scratch4),
           (int32_t*)(scratch5));
@@ -955,78 +955,78 @@ TfLiteStatus EvalInteger8x8_16(
         // backwards.
         const int t_rel = forward_sequence ? t : max_time - t - 1;
         const int time_offset = b * max_time + t_rel;
-        const int8_t* input_ptr = tflite::micro::GetTensorData<int8_t>(input) +
+        const int8_t* input_ptr = tflite_micro::micro::GetTensorData<int8_t>(input) +
                                   time_offset * input_step;
-        int8_t* output_ptr = tflite::micro::GetTensorData<int8_t>(output) +
+        int8_t* output_ptr = tflite_micro::micro::GetTensorData<int8_t>(output) +
                              time_offset * output_step;
 
         // Offset the {output,cell}_state pointers to the right batch.
         int8_t* output_state_ptr =
-            tflite::micro::GetTensorData<int8_t>(output_state) +
+            tflite_micro::micro::GetTensorData<int8_t>(output_state) +
             b * output_batch_leading_dim;
         int16_t* cell_state_ptr =
-            tflite::micro::GetTensorData<int16_t>(cell_state) + b * n_cell;
+            tflite_micro::micro::GetTensorData<int16_t>(cell_state) + b * n_cell;
 
         LstmStepInteger8x8_16(
             input_ptr,
-            tflite::micro::GetTensorData<int8_t>(input_to_input_weights),
+            tflite_micro::micro::GetTensorData<int8_t>(input_to_input_weights),
             integer_lstm_param->effective_input_to_input_scale_a,
             integer_lstm_param->effective_input_to_input_scale_b,
-            tflite::micro::GetTensorData<int8_t>(input_to_forget_weights),
+            tflite_micro::micro::GetTensorData<int8_t>(input_to_forget_weights),
             integer_lstm_param->effective_input_to_forget_scale_a,
             integer_lstm_param->effective_input_to_forget_scale_b,
-            tflite::micro::GetTensorData<int8_t>(input_to_cell_weights),
+            tflite_micro::micro::GetTensorData<int8_t>(input_to_cell_weights),
             integer_lstm_param->effective_input_to_cell_scale_a,
             integer_lstm_param->effective_input_to_cell_scale_b,
-            tflite::micro::GetTensorData<int8_t>(input_to_output_weights),
+            tflite_micro::micro::GetTensorData<int8_t>(input_to_output_weights),
             integer_lstm_param->effective_input_to_output_scale_a,
             integer_lstm_param->effective_input_to_output_scale_b,
-            tflite::micro::GetTensorData<int8_t>(recurrent_to_input_weights),
+            tflite_micro::micro::GetTensorData<int8_t>(recurrent_to_input_weights),
             integer_lstm_param->effective_recurrent_to_input_scale_a,
             integer_lstm_param->effective_recurrent_to_input_scale_b,
-            tflite::micro::GetTensorData<int8_t>(recurrent_to_forget_weights),
+            tflite_micro::micro::GetTensorData<int8_t>(recurrent_to_forget_weights),
             integer_lstm_param->effective_recurrent_to_forget_scale_a,
             integer_lstm_param->effective_recurrent_to_forget_scale_b,
-            tflite::micro::GetTensorData<int8_t>(recurrent_to_cell_weights),
+            tflite_micro::micro::GetTensorData<int8_t>(recurrent_to_cell_weights),
             integer_lstm_param->effective_recurrent_to_cell_scale_a,
             integer_lstm_param->effective_recurrent_to_cell_scale_b,
-            tflite::micro::GetTensorData<int8_t>(recurrent_to_output_weights),
+            tflite_micro::micro::GetTensorData<int8_t>(recurrent_to_output_weights),
             integer_lstm_param->effective_recurrent_to_output_scale_a,
             integer_lstm_param->effective_recurrent_to_output_scale_b,
-            tflite::micro::GetTensorData<int16_t>(cell_to_input_weights),
+            tflite_micro::micro::GetTensorData<int16_t>(cell_to_input_weights),
             integer_lstm_param->effective_cell_to_input_scale_a,
             integer_lstm_param->effective_cell_to_input_scale_b,
-            tflite::micro::GetTensorData<int16_t>(cell_to_forget_weights),
+            tflite_micro::micro::GetTensorData<int16_t>(cell_to_forget_weights),
             integer_lstm_param->effective_cell_to_forget_scale_a,
             integer_lstm_param->effective_cell_to_forget_scale_b,
-            tflite::micro::GetTensorData<int16_t>(cell_to_output_weights),
+            tflite_micro::micro::GetTensorData<int16_t>(cell_to_output_weights),
             integer_lstm_param->effective_cell_to_output_scale_a,
             integer_lstm_param->effective_cell_to_output_scale_b,
-            tflite::micro::GetTensorData<int8_t>(projection_weights),
+            tflite_micro::micro::GetTensorData<int8_t>(projection_weights),
             integer_lstm_param->effective_proj_scale_a,
             integer_lstm_param->effective_proj_scale_b,
             integer_lstm_param->hidden_zp,
             integer_lstm_param->effective_hidden_scale_a,
             integer_lstm_param->effective_hidden_scale_b,
-            tflite::micro::GetTensorData<int16_t>(
+            tflite_micro::micro::GetTensorData<int16_t>(
                 input_layer_norm_coefficients),
             integer_lstm_param->layer_norm_input_scale_a,
             integer_lstm_param->layer_norm_input_scale_b,
-            tflite::micro::GetTensorData<int16_t>(
+            tflite_micro::micro::GetTensorData<int16_t>(
                 forget_layer_norm_coefficients),
             integer_lstm_param->layer_norm_forget_scale_a,
             integer_lstm_param->layer_norm_forget_scale_b,
-            tflite::micro::GetTensorData<int16_t>(cell_layer_norm_coefficients),
+            tflite_micro::micro::GetTensorData<int16_t>(cell_layer_norm_coefficients),
             integer_lstm_param->layer_norm_cell_scale_a,
             integer_lstm_param->layer_norm_cell_scale_b,
-            tflite::micro::GetTensorData<int16_t>(
+            tflite_micro::micro::GetTensorData<int16_t>(
                 output_layer_norm_coefficients),
             integer_lstm_param->layer_norm_output_scale_a,
             integer_lstm_param->layer_norm_output_scale_b,
-            tflite::micro::GetTensorData<int32_t>(input_gate_bias),
-            tflite::micro::GetTensorData<int32_t>(forget_gate_bias),
-            tflite::micro::GetTensorData<int32_t>(cell_gate_bias),
-            tflite::micro::GetTensorData<int32_t>(output_gate_bias),
+            tflite_micro::micro::GetTensorData<int32_t>(input_gate_bias),
+            tflite_micro::micro::GetTensorData<int32_t>(forget_gate_bias),
+            tflite_micro::micro::GetTensorData<int32_t>(cell_gate_bias),
+            tflite_micro::micro::GetTensorData<int32_t>(output_gate_bias),
             integer_lstm_param->quantized_cell_clip,
             integer_lstm_param->quantized_proj_clip,
             integer_lstm_param->cell_scale,
@@ -1113,82 +1113,82 @@ TfLiteStatus EvalInteger8x8_8(
   for (int t = 0; t < max_time; t++) {
     const int t_rel = t;
     int8_t* output_ptr =
-        tflite::micro::GetTensorData<int8_t>(output) + t_rel * output_step;
+        tflite_micro::micro::GetTensorData<int8_t>(output) + t_rel * output_step;
     // Input can be int8 asymmetric or int16 symmetric.
     const int8_t* input_ptr =
-        tflite::micro::GetTensorData<int8_t>(input) + t_rel * input_step;
+        tflite_micro::micro::GetTensorData<int8_t>(input) + t_rel * input_step;
     lstm_eval::LstmStepInteger8x8_8(
         input_ptr, input_zp,
 
-        tflite::micro::GetTensorData<int8_t>(input_to_input_weights),
+        tflite_micro::micro::GetTensorData<int8_t>(input_to_input_weights),
         integer_lstm_param->effective_input_to_input_scale_a,
         integer_lstm_param->effective_input_to_input_scale_b,
 
-        tflite::micro::GetTensorData<int8_t>(input_to_forget_weights),
+        tflite_micro::micro::GetTensorData<int8_t>(input_to_forget_weights),
         integer_lstm_param->effective_input_to_forget_scale_a,
         integer_lstm_param->effective_input_to_forget_scale_b,
 
-        tflite::micro::GetTensorData<int8_t>(input_to_cell_weights),
+        tflite_micro::micro::GetTensorData<int8_t>(input_to_cell_weights),
         integer_lstm_param->effective_input_to_cell_scale_a,
         integer_lstm_param->effective_input_to_cell_scale_b,
 
-        tflite::micro::GetTensorData<int8_t>(input_to_output_weights),
+        tflite_micro::micro::GetTensorData<int8_t>(input_to_output_weights),
         integer_lstm_param->effective_input_to_output_scale_a,
         integer_lstm_param->effective_input_to_output_scale_b,
 
-        tflite::micro::GetTensorData<int8_t>(recurrent_to_input_weights),
+        tflite_micro::micro::GetTensorData<int8_t>(recurrent_to_input_weights),
         integer_lstm_param->effective_recurrent_to_input_scale_a,
         integer_lstm_param->effective_recurrent_to_input_scale_b,
 
-        tflite::micro::GetTensorData<int8_t>(recurrent_to_forget_weights),
+        tflite_micro::micro::GetTensorData<int8_t>(recurrent_to_forget_weights),
         integer_lstm_param->effective_recurrent_to_forget_scale_a,
         integer_lstm_param->effective_recurrent_to_forget_scale_b,
 
-        tflite::micro::GetTensorData<int8_t>(recurrent_to_cell_weights),
+        tflite_micro::micro::GetTensorData<int8_t>(recurrent_to_cell_weights),
         integer_lstm_param->effective_recurrent_to_cell_scale_a,
         integer_lstm_param->effective_recurrent_to_cell_scale_b,
 
-        tflite::micro::GetTensorData<int8_t>(recurrent_to_output_weights),
+        tflite_micro::micro::GetTensorData<int8_t>(recurrent_to_output_weights),
         integer_lstm_param->effective_recurrent_to_output_scale_a,
         integer_lstm_param->effective_recurrent_to_output_scale_b,
 
-        tflite::micro::GetTensorData<int8_t>(cell_to_input_weights),
+        tflite_micro::micro::GetTensorData<int8_t>(cell_to_input_weights),
         integer_lstm_param->effective_cell_to_input_scale_a,
         integer_lstm_param->effective_cell_to_input_scale_b,
 
-        tflite::micro::GetTensorData<int8_t>(cell_to_forget_weights),
+        tflite_micro::micro::GetTensorData<int8_t>(cell_to_forget_weights),
         integer_lstm_param->effective_cell_to_forget_scale_a,
         integer_lstm_param->effective_cell_to_forget_scale_b,
 
-        tflite::micro::GetTensorData<int8_t>(cell_to_output_weights),
+        tflite_micro::micro::GetTensorData<int8_t>(cell_to_output_weights),
         integer_lstm_param->effective_cell_to_output_scale_a,
         integer_lstm_param->effective_cell_to_output_scale_b,
 
-        tflite::micro::GetTensorData<int8_t>(projection_weights),
+        tflite_micro::micro::GetTensorData<int8_t>(projection_weights),
         integer_lstm_param->effective_proj_scale_a,
         integer_lstm_param->effective_proj_scale_b,
 
-        tflite::micro::GetTensorData<int16_t>(input_layer_norm_coefficients),
+        tflite_micro::micro::GetTensorData<int16_t>(input_layer_norm_coefficients),
         integer_lstm_param->layer_norm_input_scale_a,
         integer_lstm_param->layer_norm_input_scale_b,
 
-        tflite::micro::GetTensorData<int16_t>(forget_layer_norm_coefficients),
+        tflite_micro::micro::GetTensorData<int16_t>(forget_layer_norm_coefficients),
         integer_lstm_param->layer_norm_forget_scale_a,
         integer_lstm_param->layer_norm_forget_scale_b,
 
-        tflite::micro::GetTensorData<int16_t>(cell_layer_norm_coefficients),
+        tflite_micro::micro::GetTensorData<int16_t>(cell_layer_norm_coefficients),
         integer_lstm_param->layer_norm_cell_scale_a,
         integer_lstm_param->layer_norm_cell_scale_b,
 
-        tflite::micro::GetTensorData<int16_t>(output_layer_norm_coefficients),
+        tflite_micro::micro::GetTensorData<int16_t>(output_layer_norm_coefficients),
         integer_lstm_param->layer_norm_output_scale_a,
         integer_lstm_param->layer_norm_output_scale_b,
 
-        tflite::micro::GetTensorData<int32_t>(input_gate_bias),
-        tflite::micro::GetTensorData<int32_t>(forget_gate_bias),
-        tflite::micro::GetTensorData<int32_t>(cell_gate_bias),
-        tflite::micro::GetTensorData<int32_t>(output_gate_bias),
-        tflite::micro::GetTensorData<int32_t>(projection_bias),
+        tflite_micro::micro::GetTensorData<int32_t>(input_gate_bias),
+        tflite_micro::micro::GetTensorData<int32_t>(forget_gate_bias),
+        tflite_micro::micro::GetTensorData<int32_t>(cell_gate_bias),
+        tflite_micro::micro::GetTensorData<int32_t>(output_gate_bias),
+        tflite_micro::micro::GetTensorData<int32_t>(projection_bias),
 
         params, integer_lstm_param->intermediate_scale_a,
         integer_lstm_param->intermediate_scale_b,
@@ -1196,16 +1196,16 @@ TfLiteStatus EvalInteger8x8_8(
         integer_lstm_param->quantized_cell_clip,
         integer_lstm_param->quantized_proj_clip, n_batch, n_cell, n_input,
         n_output, output_batch_leading_dim,
-        tflite::micro::GetTensorData<int8_t>(output_state), output_state_zp,
-        tflite::micro::GetTensorData<int16_t>(cell_state), output_ptr,
-        tflite::micro::GetTensorData<int8_t>(scratch0),
-        tflite::micro::GetTensorData<int8_t>(scratch1),
-        tflite::micro::GetTensorData<int16_t>(scratch2),
-        tflite::micro::GetTensorData<int16_t>(scratch3),
-        tflite::micro::GetTensorData<int16_t>(scratch4),
-        tflite::micro::GetTensorData<int16_t>(scratch5),
-        tflite::micro::GetTensorData<int16_t>(scratch6),
-        tflite::micro::GetTensorData<int16_t>(scratch7));
+        tflite_micro::micro::GetTensorData<int8_t>(output_state), output_state_zp,
+        tflite_micro::micro::GetTensorData<int16_t>(cell_state), output_ptr,
+        tflite_micro::micro::GetTensorData<int8_t>(scratch0),
+        tflite_micro::micro::GetTensorData<int8_t>(scratch1),
+        tflite_micro::micro::GetTensorData<int16_t>(scratch2),
+        tflite_micro::micro::GetTensorData<int16_t>(scratch3),
+        tflite_micro::micro::GetTensorData<int16_t>(scratch4),
+        tflite_micro::micro::GetTensorData<int16_t>(scratch5),
+        tflite_micro::micro::GetTensorData<int16_t>(scratch6),
+        tflite_micro::micro::GetTensorData<int16_t>(scratch7));
   }
 
   return kTfLiteOk;
@@ -1214,4 +1214,4 @@ TfLiteStatus EvalInteger8x8_8(
 }  // namespace lstm_eval
 }  // namespace micro
 }  // namespace ops
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/xtensa/lstm_eval.h b/tensorflow/lite/micro/kernels/xtensa/lstm_eval.h
index 5dd746aa..ebf9700d 100644
--- a/tensorflow/lite/micro/kernels/xtensa/lstm_eval.h
+++ b/tensorflow/lite/micro/kernels/xtensa/lstm_eval.h
@@ -24,7 +24,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/reference/portable_tensor_utils_impl.h"
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace ops {
 namespace micro {
 namespace lstm_eval {
@@ -212,5 +212,5 @@ TfLiteStatus EvalInteger8x8_8(
 }  // namespace lstm_eval
 }  // namespace micro
 }  // namespace ops
-}  // namespace tflite
+}  // namespace tflite_micro
 #endif  // TENSORFLOW_LITE_KERNELS_LSTM_EVAL_H_
diff --git a/tensorflow/lite/micro/kernels/xtensa/lstm_eval_hifi.cc b/tensorflow/lite/micro/kernels/xtensa/lstm_eval_hifi.cc
index 2b49f26e..e6996670 100644
--- a/tensorflow/lite/micro/kernels/xtensa/lstm_eval_hifi.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/lstm_eval_hifi.cc
@@ -17,7 +17,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/lstm_eval.h"
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace ops {
 namespace micro {
 namespace lstm_eval {
@@ -459,4 +459,4 @@ void xa_nn_elm_mul_16x16_asym8s(int8_t* output, const int16_t* input_1,
 }  // namespace lstm_eval
 }  // namespace micro
 }  // namespace ops
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/xtensa/lstm_shared.h b/tensorflow/lite/micro/kernels/xtensa/lstm_shared.h
index 4bcff1aa..4d854c68 100644
--- a/tensorflow/lite/micro/kernels/xtensa/lstm_shared.h
+++ b/tensorflow/lite/micro/kernels/xtensa/lstm_shared.h
@@ -15,7 +15,7 @@ limitations under the License.
 #ifndef TENSORFLOW_LITE_KERNELS_LSTM_SHARED_H_
 #define TENSORFLOW_LITE_KERNELS_LSTM_SHARED_H_
 
-namespace tflite {
+namespace tflite_micro {
 namespace ops {
 namespace micro {
 namespace lstm {
@@ -74,5 +74,5 @@ constexpr int kOutputTensor = 0;
 }  // namespace lstm
 }  // namespace micro
 }  // namespace ops
-}  // namespace tflite
+}  // namespace tflite_micro
 #endif  // TENSORFLOW_LITE_KERNELS_LSTM_SHARED_H_
diff --git a/tensorflow/lite/micro/kernels/xtensa/pad.cc b/tensorflow/lite/micro/kernels/xtensa/pad.cc
index 86d0ab3a..b2616269 100644
--- a/tensorflow/lite/micro/kernels/xtensa/pad.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/pad.cc
@@ -26,7 +26,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa_pad.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -159,31 +159,31 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 #endif
 
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, /*index=*/0);
+      tflite_micro::micro::GetEvalInput(context, node, /*index=*/0);
   const TfLiteEvalTensor* constant_values =
       NumInputs(node) == 3
-          ? tflite::micro::GetEvalInput(context, node, /*index=*/2)
+          ? tflite_micro::micro::GetEvalInput(context, node, /*index=*/2)
           : nullptr;
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, /*index=*/0);
+      tflite_micro::micro::GetEvalOutput(context, node, /*index=*/0);
 
   switch (input->type) {
     case kTfLiteFloat32: {
       float pad_value =
           constant_values == nullptr
               ? 0.f
-              : *tflite::micro::GetTensorData<float>(constant_values);
+              : *tflite_micro::micro::GetTensorData<float>(constant_values);
       if (data->params.resizing_category == ResizingCategory::kImageStyle) {
         reference_ops::PadImageStyle(
-            data->params, tflite::micro::GetTensorShape(input),
-            tflite::micro::GetTensorData<float>(input), &pad_value,
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<float>(output));
+            data->params, tflite_micro::micro::GetTensorShape(input),
+            tflite_micro::micro::GetTensorData<float>(input), &pad_value,
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<float>(output));
       } else {
-        reference_ops::Pad(data->params, tflite::micro::GetTensorShape(input),
-                           tflite::micro::GetTensorData<float>(input),
-                           &pad_value, tflite::micro::GetTensorShape(output),
-                           tflite::micro::GetTensorData<float>(output));
+        reference_ops::Pad(data->params, tflite_micro::micro::GetTensorShape(input),
+                           tflite_micro::micro::GetTensorData<float>(input),
+                           &pad_value, tflite_micro::micro::GetTensorShape(output),
+                           tflite_micro::micro::GetTensorData<float>(output));
       }
     } break;
     case kTfLiteInt8: {
@@ -194,19 +194,19 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
       if (constant_values == nullptr) {
         pad_value = static_cast<uint8_t>(data->output_zero_point);
       } else {
-        pad_value = *tflite::micro::GetTensorData<int8_t>(constant_values);
+        pad_value = *tflite_micro::micro::GetTensorData<int8_t>(constant_values);
       }
       if (data->params.resizing_category == ResizingCategory::kImageStyle) {
         reference_ops::PadImageStyle(
-            data->params, tflite::micro::GetTensorShape(input),
-            tflite::micro::GetTensorData<int8_t>(input), &pad_value,
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<int8_t>(output));
+            data->params, tflite_micro::micro::GetTensorShape(input),
+            tflite_micro::micro::GetTensorData<int8_t>(input), &pad_value,
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int8_t>(output));
       } else {
-        reference_ops::Pad(data->params, tflite::micro::GetTensorShape(input),
-                           tflite::micro::GetTensorData<int8_t>(input),
-                           &pad_value, tflite::micro::GetTensorShape(output),
-                           tflite::micro::GetTensorData<int8_t>(output));
+        reference_ops::Pad(data->params, tflite_micro::micro::GetTensorShape(input),
+                           tflite_micro::micro::GetTensorData<int8_t>(input),
+                           &pad_value, tflite_micro::micro::GetTensorShape(output),
+                           tflite_micro::micro::GetTensorData<int8_t>(output));
       }
 #endif
     } break;
@@ -214,30 +214,30 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
       int16_t pad_value =
           constant_values == nullptr
               ? 0
-              : *tflite::micro::GetTensorData<int16_t>(constant_values);
+              : *tflite_micro::micro::GetTensorData<int16_t>(constant_values);
 #if defined(HIFI3) || defined(HIFI4)
       /* NNLib currently only supports up to 4D input tensors */
-      if (tflite::micro::GetTensorShape(input).DimensionsCount() == 4) {
+      if (tflite_micro::micro::GetTensorShape(input).DimensionsCount() == 4) {
         const TfLiteEvalTensor* paddings =
-            tflite::micro::GetEvalInput(context, node, /*index=*/1);
+            tflite_micro::micro::GetEvalInput(context, node, /*index=*/1);
         int32_t err = xa_nn_pad_16_16(
-            tflite::micro::GetTensorData<int16_t>(output),
-            tflite::micro::GetTensorShape(output).DimsData(),
-            tflite::micro::GetTensorData<int16_t>(input),
-            tflite::micro::GetTensorShape(input).DimsData(),
-            tflite::micro::GetTensorData<int32_t>(paddings),
-            tflite::micro::GetTensorShape(paddings).DimsData(),
-            tflite::micro::GetTensorShape(output).DimensionsCount(),
-            tflite::micro::GetTensorShape(input).DimensionsCount(),
-            tflite::micro::GetTensorShape(paddings).DimensionsCount(),
+            tflite_micro::micro::GetTensorData<int16_t>(output),
+            tflite_micro::micro::GetTensorShape(output).DimsData(),
+            tflite_micro::micro::GetTensorData<int16_t>(input),
+            tflite_micro::micro::GetTensorShape(input).DimsData(),
+            tflite_micro::micro::GetTensorData<int32_t>(paddings),
+            tflite_micro::micro::GetTensorShape(paddings).DimsData(),
+            tflite_micro::micro::GetTensorShape(output).DimensionsCount(),
+            tflite_micro::micro::GetTensorShape(input).DimensionsCount(),
+            tflite_micro::micro::GetTensorShape(paddings).DimensionsCount(),
             pad_value);
         if (err != 0) return kTfLiteError;
       } else {
 #endif  // defined(HIFI3) || defined(HIFI4)
-        reference_ops::Pad(data->params, tflite::micro::GetTensorShape(input),
-                           tflite::micro::GetTensorData<int16_t>(input),
-                           &pad_value, tflite::micro::GetTensorShape(output),
-                           tflite::micro::GetTensorData<int16_t>(output));
+        reference_ops::Pad(data->params, tflite_micro::micro::GetTensorShape(input),
+                           tflite_micro::micro::GetTensorData<int16_t>(input),
+                           &pad_value, tflite_micro::micro::GetTensorShape(output),
+                           tflite_micro::micro::GetTensorData<int16_t>(output));
 #if defined(HIFI3) || defined(HIFI4)
       }
 #endif  // defined(HIFI3) || defined(HIFI4)
@@ -246,11 +246,11 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
       int32_t pad_value =
           constant_values == nullptr
               ? 0
-              : *tflite::micro::GetTensorData<int32_t>(constant_values);
-      reference_ops::Pad(data->params, tflite::micro::GetTensorShape(input),
-                         tflite::micro::GetTensorData<int32_t>(input),
-                         &pad_value, tflite::micro::GetTensorShape(output),
-                         tflite::micro::GetTensorData<int32_t>(output));
+              : *tflite_micro::micro::GetTensorData<int32_t>(constant_values);
+      reference_ops::Pad(data->params, tflite_micro::micro::GetTensorShape(input),
+                         tflite_micro::micro::GetTensorData<int32_t>(input),
+                         &pad_value, tflite_micro::micro::GetTensorShape(output),
+                         tflite_micro::micro::GetTensorData<int32_t>(output));
     } break;
     default:
 
@@ -264,12 +264,12 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_PAD() {
-  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, Eval);
 }
 
 // Also register Pad as PadV2.
 TFLMRegistration Register_PADV2() {
-  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/xtensa/pad_vision.cc b/tensorflow/lite/micro/kernels/xtensa/pad_vision.cc
index f15e2f24..0dddbbe6 100644
--- a/tensorflow/lite/micro/kernels/xtensa/pad_vision.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/pad_vision.cc
@@ -27,7 +27,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa.h"
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa_pad.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 inline void OperandDims4D(uint32_t* dims, TfLiteTensor* opnd) {
   for (int i = NumDimensions(opnd) - 1, j = 0; i >= 0; i--, j++) {
@@ -99,9 +99,9 @@ TfLiteStatus PadEvalVision(const XtensaPadData& data,
   const uint32_t output_size = NumElements(output->dims);
 
   xiPad(data.p_context, data.context_size,
-        const_cast<int8_t*>(tflite::micro::GetTensorData<int8_t>(input)),
-        input_size, tflite::micro::GetTensorData<int8_t>(output), output_size);
+        const_cast<int8_t*>(tflite_micro::micro::GetTensorData<int8_t>(input)),
+        input_size, tflite_micro::micro::GetTensorData<int8_t>(output), output_size);
   return kTfLiteOk;
 }
-}  // namespace tflite
+}  // namespace tflite_micro
 #endif  // defined(VISION_P6)
diff --git a/tensorflow/lite/micro/kernels/xtensa/pooling.cc b/tensorflow/lite/micro/kernels/xtensa/pooling.cc
index 172d0587..16471a87 100644
--- a/tensorflow/lite/micro/kernels/xtensa/pooling.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/pooling.cc
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa_pooling.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -134,25 +134,25 @@ TfLiteStatus MaxEval(TfLiteContext* context, TfLiteNode* node) {
 
 TFLMRegistration Register_AVERAGE_POOL_2D() {
 #if defined(HIFI5)
-  return tflite::micro::RegisterOp(XtensaPoolingInit, AveragePrepareHifi,
+  return tflite_micro::micro::RegisterOp(XtensaPoolingInit, AveragePrepareHifi,
                                    AverageEval);
 #elif defined(VISION_P6)
-  return tflite::micro::RegisterOp(XtensaPoolingInit, AvgPoolingPrepareVision,
+  return tflite_micro::micro::RegisterOp(XtensaPoolingInit, AvgPoolingPrepareVision,
                                    AverageEval);
 #else
-  return tflite::micro::RegisterOp(XtensaPoolingInit, PoolingPrepare,
+  return tflite_micro::micro::RegisterOp(XtensaPoolingInit, PoolingPrepare,
                                    AverageEval);
 #endif
 }
 
 TFLMRegistration Register_MAX_POOL_2D() {
 #if defined(HIFI5)
-  return tflite::micro::RegisterOp(XtensaPoolingInit, MaxPrepareHifi, MaxEval);
+  return tflite_micro::micro::RegisterOp(XtensaPoolingInit, MaxPrepareHifi, MaxEval);
 #elif defined(VISION_P6)
-  return tflite::micro::RegisterOp(XtensaPoolingInit, MaxPoolingPrepareVision,
+  return tflite_micro::micro::RegisterOp(XtensaPoolingInit, MaxPoolingPrepareVision,
                                    MaxEval);
 #else
-  return tflite::micro::RegisterOp(XtensaPoolingInit, PoolingPrepare, MaxEval);
+  return tflite_micro::micro::RegisterOp(XtensaPoolingInit, PoolingPrepare, MaxEval);
 #endif
 }
 
@@ -162,4 +162,4 @@ TFLMRegistration Register_AVERAGE_POOL_2D_INT16() {
 
 TFLMRegistration Register_MAX_POOL_2D_INT16() { return Register_MAX_POOL_2D(); }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/xtensa/pooling_int8.cc b/tensorflow/lite/micro/kernels/xtensa/pooling_int8.cc
index 84246d64..c8c0ee16 100644
--- a/tensorflow/lite/micro/kernels/xtensa/pooling_int8.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/pooling_int8.cc
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa_pooling.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -157,8 +157,8 @@ TfLiteStatus AverageEvalQuantizedHifi(TfLiteContext* context,
                                       TfLiteEvalTensor* output) {
   TFLITE_DCHECK(input->type == kTfLiteInt8);
 
-  const RuntimeShape& input_shape = tflite::micro::GetTensorShape(input);
-  const RuntimeShape& output_shape = tflite::micro::GetTensorShape(output);
+  const RuntimeShape& input_shape = tflite_micro::micro::GetTensorShape(input);
+  const RuntimeShape& output_shape = tflite_micro::micro::GetTensorShape(output);
   const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   const int input_height = input_shape.Dims(1);
@@ -169,8 +169,8 @@ TfLiteStatus AverageEvalQuantizedHifi(TfLiteContext* context,
   void* p_scratch = static_cast<void*>(
       context->GetScratchBuffer(context, data->scratch_tensor_index));
 
-  const int8_t* inp_data_ptr = tflite::micro::GetTensorData<int8_t>(input);
-  int8_t* out_data_ptr = tflite::micro::GetTensorData<int8_t>(output);
+  const int8_t* inp_data_ptr = tflite_micro::micro::GetTensorData<int8_t>(input);
+  int8_t* out_data_ptr = tflite_micro::micro::GetTensorData<int8_t>(output);
 
   for (int batch = 0; batch < batches; ++batch) {
     TF_LITE_ENSURE_EQ(
@@ -249,8 +249,8 @@ TfLiteStatus MaxEvalQuantizedHifi(TfLiteContext* context, TfLiteNode* node,
                                   const XtensaOpDataPooling* data,
                                   const TfLiteEvalTensor* input,
                                   TfLiteEvalTensor* output) {
-  const RuntimeShape& input_shape = tflite::micro::GetTensorShape(input);
-  const RuntimeShape& output_shape = tflite::micro::GetTensorShape(output);
+  const RuntimeShape& input_shape = tflite_micro::micro::GetTensorShape(input);
+  const RuntimeShape& output_shape = tflite_micro::micro::GetTensorShape(output);
   const int batches = MatchingDim(input_shape, 0, output_shape, 0);
   const int depth = MatchingDim(input_shape, 3, output_shape, 3);
   const int input_height = input_shape.Dims(1);
@@ -261,8 +261,8 @@ TfLiteStatus MaxEvalQuantizedHifi(TfLiteContext* context, TfLiteNode* node,
   void* p_scratch = static_cast<void*>(
       context->GetScratchBuffer(context, data->scratch_tensor_index));
 
-  const int8_t* inp_data_ptr = tflite::micro::GetTensorData<int8_t>(input);
-  int8_t* out_data_ptr = tflite::micro::GetTensorData<int8_t>(output);
+  const int8_t* inp_data_ptr = tflite_micro::micro::GetTensorData<int8_t>(input);
+  int8_t* out_data_ptr = tflite_micro::micro::GetTensorData<int8_t>(output);
 
   for (int batch = 0; batch < batches; ++batch) {
     TF_LITE_ENSURE_EQ(
@@ -311,28 +311,28 @@ void* XtensaPoolingInit(TfLiteContext* context, const char* buffer,
 
 TFLMRegistration Register_AVERAGE_POOL_2D_INT8() {
 #if defined(HIFI5)
-  return tflite::micro::RegisterOp(XtensaPoolingInit, AveragePrepareHifi,
+  return tflite_micro::micro::RegisterOp(XtensaPoolingInit, AveragePrepareHifi,
                                    AverageEvalInt8);
 #elif defined(VISION_P6)
-  return tflite::micro::RegisterOp(XtensaPoolingInit, AvgPoolingPrepareVision,
+  return tflite_micro::micro::RegisterOp(XtensaPoolingInit, AvgPoolingPrepareVision,
                                    AverageEvalInt8);
 #else
-  return tflite::micro::RegisterOp(XtensaPoolingInit, PoolingPrepare,
+  return tflite_micro::micro::RegisterOp(XtensaPoolingInit, PoolingPrepare,
                                    AverageEvalInt8);
 #endif
 }
 
 TFLMRegistration Register_MAX_POOL_2D_INT8() {
 #if defined(HIFI5)
-  return tflite::micro::RegisterOp(XtensaPoolingInit, MaxPrepareHifi,
+  return tflite_micro::micro::RegisterOp(XtensaPoolingInit, MaxPrepareHifi,
                                    MaxEvalInt8);
 #elif defined(VISION_P6)
-  return tflite::micro::RegisterOp(XtensaPoolingInit, MaxPoolingPrepareVision,
+  return tflite_micro::micro::RegisterOp(XtensaPoolingInit, MaxPoolingPrepareVision,
                                    MaxEvalInt8);
 #else
-  return tflite::micro::RegisterOp(XtensaPoolingInit, PoolingPrepare,
+  return tflite_micro::micro::RegisterOp(XtensaPoolingInit, PoolingPrepare,
                                    MaxEvalInt8);
 #endif
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/xtensa/pooling_vision.cc b/tensorflow/lite/micro/kernels/xtensa/pooling_vision.cc
index b0186f16..b8503246 100644
--- a/tensorflow/lite/micro/kernels/xtensa/pooling_vision.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/pooling_vision.cc
@@ -25,7 +25,7 @@ limitations under the License.
 #define MAX_POOLING 0
 #define AVG_POOLING 1
 
-namespace tflite {
+namespace tflite_micro {
 
 TfLiteStatus PoolingPrepareVision(TfLiteContext* context, TfLiteNode* node,
                                   uint8_t pool_type) {
@@ -108,10 +108,10 @@ TfLiteStatus PoolEvalVision(TfLiteContext* context, TfLiteNode* node,
   const uint32_t output_size = NumElements(output->dims);
 
   xiPool(data.p_context, data.context_size,
-         const_cast<int8_t*>(tflite::micro::GetTensorData<int8_t>(input)),
-         input_size, tflite::micro::GetTensorData<int8_t>(output), output_size);
+         const_cast<int8_t*>(tflite_micro::micro::GetTensorData<int8_t>(input)),
+         input_size, tflite_micro::micro::GetTensorData<int8_t>(output), output_size);
   return kTfLiteOk;
 }
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // VISIONP6
diff --git a/tensorflow/lite/micro/kernels/xtensa/quantize.cc b/tensorflow/lite/micro/kernels/xtensa/quantize.cc
index 15f5243e..15f7b81a 100644
--- a/tensorflow/lite/micro/kernels/xtensa/quantize.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/quantize.cc
@@ -26,7 +26,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_log.h"
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 #if defined(HIFI3) || defined(HIFI4) || defined(HIFI5)
@@ -34,8 +34,8 @@ TfLiteStatus EvalXtensa(TfLiteContext* context, TfLiteNode* node) {
   TFLITE_DCHECK(node->user_data != nullptr);
   auto* op_data = static_cast<OpDataQuantizeReference*>(node->user_data);
 
-  const TfLiteEvalTensor* input = tflite::micro::GetEvalInput(context, node, 0);
-  TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, 0);
+  const TfLiteEvalTensor* input = tflite_micro::micro::GetEvalInput(context, node, 0);
+  TfLiteEvalTensor* output = tflite_micro::micro::GetEvalOutput(context, node, 0);
 
   switch (input->type) {
     case kTfLiteUInt8: {
@@ -43,11 +43,11 @@ TfLiteStatus EvalXtensa(TfLiteContext* context, TfLiteNode* node) {
         case kTfLiteInt8: {
           int size = ElementCount(*input->dims);
           reference_ops::Requantize(
-              tflite::micro::GetTensorData<uint8_t>(input), size,
+              tflite_micro::micro::GetTensorData<uint8_t>(input), size,
               op_data->requantize_output_multiplier,
               op_data->requantize_output_shift, op_data->input_zero_point,
               op_data->quantization_params.zero_point,
-              tflite::micro::GetTensorData<int8_t>(output));
+              tflite_micro::micro::GetTensorData<int8_t>(output));
           break;
         }
 
@@ -65,22 +65,22 @@ TfLiteStatus EvalXtensa(TfLiteContext* context, TfLiteNode* node) {
         case kTfLiteUInt8: {
           int size = ElementCount(*input->dims);
           reference_ops::Requantize(
-              tflite::micro::GetTensorData<int8_t>(input), size,
+              tflite_micro::micro::GetTensorData<int8_t>(input), size,
               op_data->requantize_output_multiplier,
               op_data->requantize_output_shift, op_data->input_zero_point,
               op_data->quantization_params.zero_point,
-              tflite::micro::GetTensorData<uint8_t>(output));
+              tflite_micro::micro::GetTensorData<uint8_t>(output));
           break;
         }
 
         case kTfLiteInt8: {
           int size = ElementCount(*input->dims);
           reference_ops::Requantize(
-              tflite::micro::GetTensorData<int8_t>(input), size,
+              tflite_micro::micro::GetTensorData<int8_t>(input), size,
               op_data->requantize_output_multiplier,
               op_data->requantize_output_shift, op_data->input_zero_point,
               op_data->quantization_params.zero_point,
-              tflite::micro::GetTensorData<int8_t>(output));
+              tflite_micro::micro::GetTensorData<int8_t>(output));
           break;
         }
 
@@ -88,10 +88,10 @@ TfLiteStatus EvalXtensa(TfLiteContext* context, TfLiteNode* node) {
           int size = ElementCount(*input->dims);
           int32_t zero_point = op_data->quantization_params.zero_point;
           reference_ops::Requantize(
-              tflite::micro::GetTensorData<int8_t>(input), size,
+              tflite_micro::micro::GetTensorData<int8_t>(input), size,
               op_data->requantize_output_multiplier,
               op_data->requantize_output_shift, op_data->input_zero_point,
-              zero_point, tflite::micro::GetTensorData<int16_t>(output));
+              zero_point, tflite_micro::micro::GetTensorData<int16_t>(output));
           break;
         }
 
@@ -101,8 +101,8 @@ TfLiteStatus EvalXtensa(TfLiteContext* context, TfLiteNode* node) {
 #if defined(HIFI5)
           const int8_t* input_data_ptr;
           int32_t* output_data_ptr;
-          input_data_ptr = tflite::micro::GetTensorData<int8_t>(input);
-          output_data_ptr = tflite::micro::GetTensorData<int32_t>(output);
+          input_data_ptr = tflite_micro::micro::GetTensorData<int8_t>(input);
+          output_data_ptr = tflite_micro::micro::GetTensorData<int32_t>(output);
 
           TF_LITE_ENSURE_EQ(
               context,
@@ -113,10 +113,10 @@ TfLiteStatus EvalXtensa(TfLiteContext* context, TfLiteNode* node) {
               0);
 #else
           reference_ops::Requantize(
-              tflite::micro::GetTensorData<int8_t>(input), size,
+              tflite_micro::micro::GetTensorData<int8_t>(input), size,
               op_data->requantize_output_multiplier,
               op_data->requantize_output_shift, op_data->input_zero_point,
-              zero_point, tflite::micro::GetTensorData<int32_t>(output));
+              zero_point, tflite_micro::micro::GetTensorData<int32_t>(output));
 #endif  // defined(HIFI5)
           break;
         }
@@ -137,8 +137,8 @@ TfLiteStatus EvalXtensa(TfLiteContext* context, TfLiteNode* node) {
           int size = ElementCount(*input->dims);
           TF_LITE_ENSURE_EQ(context,
                             xa_nn_elm_requantize_asym16s_asym8s(
-                                tflite::micro::GetTensorData<int8_t>(output),
-                                tflite::micro::GetTensorData<int16_t>(input),
+                                tflite_micro::micro::GetTensorData<int8_t>(output),
+                                tflite_micro::micro::GetTensorData<int16_t>(input),
                                 op_data->input_zero_point,
                                 op_data->quantization_params.zero_point,
                                 op_data->requantize_output_shift,
@@ -150,11 +150,11 @@ TfLiteStatus EvalXtensa(TfLiteContext* context, TfLiteNode* node) {
         case kTfLiteInt16: {
           int size = ElementCount(*input->dims);
           reference_ops::Requantize(
-              tflite::micro::GetTensorData<int16_t>(input), size,
+              tflite_micro::micro::GetTensorData<int16_t>(input), size,
               op_data->requantize_output_multiplier,
               op_data->requantize_output_shift, op_data->input_zero_point,
               op_data->quantization_params.zero_point,
-              tflite::micro::GetTensorData<int16_t>(output));
+              tflite_micro::micro::GetTensorData<int16_t>(output));
           break;
         }
 
@@ -163,8 +163,8 @@ TfLiteStatus EvalXtensa(TfLiteContext* context, TfLiteNode* node) {
 #if defined(HIFI5)
           TF_LITE_ENSURE_EQ(context,
                             xa_nn_elm_requantize_asym16s_asym32s(
-                                tflite::micro::GetTensorData<int32_t>(output),
-                                tflite::micro::GetTensorData<int16_t>(input),
+                                tflite_micro::micro::GetTensorData<int32_t>(output),
+                                tflite_micro::micro::GetTensorData<int16_t>(input),
                                 op_data->input_zero_point,
                                 op_data->quantization_params.zero_point,
                                 op_data->requantize_output_shift,
@@ -173,10 +173,10 @@ TfLiteStatus EvalXtensa(TfLiteContext* context, TfLiteNode* node) {
 #else
           int32_t zero_point = op_data->quantization_params.zero_point;
           reference_ops::Requantize(
-              tflite::micro::GetTensorData<int16_t>(input), size,
+              tflite_micro::micro::GetTensorData<int16_t>(input), size,
               op_data->requantize_output_multiplier,
               op_data->requantize_output_shift, op_data->input_zero_point,
-              zero_point, tflite::micro::GetTensorData<int32_t>(output));
+              zero_point, tflite_micro::micro::GetTensorData<int32_t>(output));
 #endif  // defined(HIFI5)
           break;
         }
@@ -196,11 +196,11 @@ TfLiteStatus EvalXtensa(TfLiteContext* context, TfLiteNode* node) {
         case kTfLiteInt8: {
           int size = ElementCount(*input->dims);
           reference_ops::Requantize(
-              tflite::micro::GetTensorData<int32_t>(input), size,
+              tflite_micro::micro::GetTensorData<int32_t>(input), size,
               op_data->requantize_output_multiplier,
               op_data->requantize_output_shift, op_data->input_zero_point,
               op_data->quantization_params.zero_point,
-              tflite::micro::GetTensorData<int8_t>(output));
+              tflite_micro::micro::GetTensorData<int8_t>(output));
           break;
         }
 
@@ -208,10 +208,10 @@ TfLiteStatus EvalXtensa(TfLiteContext* context, TfLiteNode* node) {
           int size = ElementCount(*input->dims);
           int32_t zero_point = op_data->quantization_params.zero_point;
           reference_ops::Requantize(
-              tflite::micro::GetTensorData<int32_t>(input), size,
+              tflite_micro::micro::GetTensorData<int32_t>(input), size,
               op_data->requantize_output_multiplier,
               op_data->requantize_output_shift, op_data->input_zero_point,
-              zero_point, tflite::micro::GetTensorData<int16_t>(output));
+              zero_point, tflite_micro::micro::GetTensorData<int16_t>(output));
           break;
         }
 
@@ -230,20 +230,20 @@ TfLiteStatus EvalXtensa(TfLiteContext* context, TfLiteNode* node) {
         case kTfLiteInt8: {
           reference_ops::AffineQuantize(
               op_data->quantization_params,
-              tflite::micro::GetTensorShape(input),
-              tflite::micro::GetTensorData<float>(input),
-              tflite::micro::GetTensorShape(output),
-              tflite::micro::GetTensorData<int8_t>(output));
+              tflite_micro::micro::GetTensorShape(input),
+              tflite_micro::micro::GetTensorData<float>(input),
+              tflite_micro::micro::GetTensorShape(output),
+              tflite_micro::micro::GetTensorData<int8_t>(output));
           break;
         }
 
         case kTfLiteInt16: {
           reference_ops::AffineQuantize(
               op_data->quantization_params,
-              tflite::micro::GetTensorShape(input),
-              tflite::micro::GetTensorData<float>(input),
-              tflite::micro::GetTensorShape(output),
-              tflite::micro::GetTensorData<int16_t>(output));
+              tflite_micro::micro::GetTensorShape(input),
+              tflite_micro::micro::GetTensorData<float>(input),
+              tflite_micro::micro::GetTensorShape(output),
+              tflite_micro::micro::GetTensorData<int16_t>(output));
           break;
         }
 
@@ -311,7 +311,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_QUANTIZE() {
-  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/xtensa/reduce.cc b/tensorflow/lite/micro/kernels/xtensa/reduce.cc
index c7c507f7..6f7fd09c 100644
--- a/tensorflow/lite/micro/kernels/xtensa/reduce.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/reduce.cc
@@ -27,7 +27,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa_reduce.h"
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 void* XtensaInitReduce(TfLiteContext* context, const char* buffer,
                        size_t length) {
@@ -71,8 +71,8 @@ TfLiteStatus XtensaEvalMax(TfLiteContext* context, TfLiteNode* node) {
   OpDataReduce* op_data = &(op_data_xtensa->reference_op_data);
 
 #if defined(VISION_P6)
-  const TfLiteEvalTensor* input = tflite::micro::GetEvalInput(context, node, 0);
-  TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, 0);
+  const TfLiteEvalTensor* input = tflite_micro::micro::GetEvalInput(context, node, 0);
+  TfLiteEvalTensor* output = tflite_micro::micro::GetEvalOutput(context, node, 0);
   TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);
 
   switch (input->type) {
@@ -101,18 +101,18 @@ TfLiteStatus XtensaEvalSum(TfLiteContext* context, TfLiteNode* node) {
 }
 
 TFLMRegistration Register_MEAN() {
-  return tflite::micro::RegisterOp(XtensaInitReduce, XtensaPrepareMeanOrSum,
+  return tflite_micro::micro::RegisterOp(XtensaInitReduce, XtensaPrepareMeanOrSum,
                                    XtensaEvalMean);
 }
 
 TFLMRegistration Register_REDUCE_MAX() {
-  return tflite::micro::RegisterOp(XtensaInitReduce, XtensaPrepareMax,
+  return tflite_micro::micro::RegisterOp(XtensaInitReduce, XtensaPrepareMax,
                                    XtensaEvalMax);
 }
 
 TFLMRegistration Register_SUM() {
-  return tflite::micro::RegisterOp(XtensaInitReduce, XtensaPrepareMeanOrSum,
+  return tflite_micro::micro::RegisterOp(XtensaInitReduce, XtensaPrepareMeanOrSum,
                                    XtensaEvalSum);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/xtensa/reduce_vision.cc b/tensorflow/lite/micro/kernels/xtensa/reduce_vision.cc
index c76525e2..4c420e56 100644
--- a/tensorflow/lite/micro/kernels/xtensa/reduce_vision.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/reduce_vision.cc
@@ -28,7 +28,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa_reduce.h"
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 inline void OperandDims4D(uint32_t* dims, TfLiteTensor* opnd) {
   for (int i = NumDimensions(opnd) - 1, j = 0; i >= 0; i--, j++) {
@@ -143,10 +143,10 @@ TfLiteStatus ReduceEvalVision(const XtensaReduceOpData& data,
   const uint32_t output_size = NumElements(output->dims);
 
   xiReduce(data.p_context, data.context_size,
-           const_cast<int8_t*>(tflite::micro::GetTensorData<int8_t>(input)),
-           input_size, tflite::micro::GetTensorData<int8_t>(output),
+           const_cast<int8_t*>(tflite_micro::micro::GetTensorData<int8_t>(input)),
+           input_size, tflite_micro::micro::GetTensorData<int8_t>(output),
            output_size);
   return kTfLiteOk;
 }
-}  // namespace tflite
+}  // namespace tflite_micro
 #endif  // defined(VISION_P6)
diff --git a/tensorflow/lite/micro/kernels/xtensa/reshape.cc b/tensorflow/lite/micro/kernels/xtensa/reshape.cc
index 6ce6f5c3..2daa527b 100644
--- a/tensorflow/lite/micro/kernels/xtensa/reshape.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/reshape.cc
@@ -28,7 +28,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/memory_helpers.h"
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 #if defined(VISION_P6)
@@ -62,9 +62,9 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kReshapeInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kReshapeInputTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kReshapeOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kReshapeOutputTensor);
 
   // TODO(b/162522304): storing input bytes in OpData increases some models
   // significantly, possibly due to alignment issues.
@@ -93,10 +93,10 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 
 TFLMRegistration Register_RESHAPE() {
 #if defined(VISION_P6)
-  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, Eval);
 #else
-  return tflite::micro::RegisterOp(nullptr, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, Prepare, Eval);
 #endif
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/xtensa/reshape_vision.cc b/tensorflow/lite/micro/kernels/xtensa/reshape_vision.cc
index 3e855a4a..91979697 100644
--- a/tensorflow/lite/micro/kernels/xtensa/reshape_vision.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/reshape_vision.cc
@@ -28,7 +28,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa.h"
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa_reshape.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 inline void OperandDims4D(uint32_t* dims, TfLiteTensor* opnd) {
   for (int i = NumDimensions(opnd) - 1, j = 0; i >= 0; i--, j++) {
@@ -79,10 +79,10 @@ TfLiteStatus ReshapeEvalVision(const XtensaReshapeData& data,
   const uint32_t output_size = NumElements(output->dims);
 
   xiReshape(data.p_context, data.context_size,
-            const_cast<int8_t*>(tflite::micro::GetTensorData<int8_t>(input)),
-            input_size, tflite::micro::GetTensorData<int8_t>(output),
+            const_cast<int8_t*>(tflite_micro::micro::GetTensorData<int8_t>(input)),
+            input_size, tflite_micro::micro::GetTensorData<int8_t>(output),
             output_size);
   return kTfLiteOk;
 }
-}  // namespace tflite
+}  // namespace tflite_micro
 #endif  // defined(VISION_P6)
diff --git a/tensorflow/lite/micro/kernels/xtensa/softmax.cc b/tensorflow/lite/micro/kernels/xtensa/softmax.cc
index c248fc5a..a3a44f78 100644
--- a/tensorflow/lite/micro/kernels/xtensa/softmax.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/softmax.cc
@@ -28,17 +28,17 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa_softmax.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 #if defined(HIFI3) || defined(HIFI4) || defined(HIFI5)
 TfLiteStatus EvalHifiInt8(const XtensaSoftmaxOpData* op_data,
                           const TfLiteEvalTensor* input,
                           TfLiteEvalTensor* output, TfLiteContext* context) {
-  const RuntimeShape& input_shape = tflite::micro::GetTensorShape(input);
-  const int8_t* input_data = tflite::micro::GetTensorData<int8_t>(input);
-  const RuntimeShape& output_shape = tflite::micro::GetTensorShape(output);
-  int8_t* output_data = tflite::micro::GetTensorData<int8_t>(output);
+  const RuntimeShape& input_shape = tflite_micro::micro::GetTensorShape(input);
+  const int8_t* input_data = tflite_micro::micro::GetTensorData<int8_t>(input);
+  const RuntimeShape& output_shape = tflite_micro::micro::GetTensorShape(output);
+  int8_t* output_data = tflite_micro::micro::GetTensorData<int8_t>(output);
   const int trailing_dim = input_shape.DimensionsCount() - 1;
   const int outer_size =
       MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);
@@ -59,8 +59,8 @@ TfLiteStatus EvalHifiInt8(const XtensaSoftmaxOpData* op_data,
 #endif  // defined(HIFI3) || defined(HIFI4) || defined(HIFI5)
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
-  const TfLiteEvalTensor* input = tflite::micro::GetEvalInput(context, node, 0);
-  TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, 0);
+  const TfLiteEvalTensor* input = tflite_micro::micro::GetEvalInput(context, node, 0);
+  TfLiteEvalTensor* output = tflite_micro::micro::GetEvalOutput(context, node, 0);
 
   if (input->type == kTfLiteInt8 && output->type == kTfLiteInt16) {
     return XtensaEvalSoftmaxInt8Int16(context, node);
@@ -85,29 +85,29 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
         context, node, *(static_cast<XtensaSoftmaxOpData*>(node->user_data)),
         input, output);
 #else
-    tflite::reference_ops::Softmax(
-        params, tflite::micro::GetTensorShape(input),
-        tflite::micro::GetTensorData<int8_t>(input),
-        tflite::micro::GetTensorShape(output),
-        tflite::micro::GetTensorData<int8_t>(output));
+    tflite_micro::reference_ops::Softmax(
+        params, tflite_micro::micro::GetTensorShape(input),
+        tflite_micro::micro::GetTensorData<int8_t>(input),
+        tflite_micro::micro::GetTensorShape(output),
+        tflite_micro::micro::GetTensorData<int8_t>(output));
     return kTfLiteOk;
 #endif  // defined(HIFI3) || defined(HIFI4) || defined(HIFI5)
   }
 
   if (input->type == kTfLiteInt16 && output->type == kTfLiteInt16) {
-    tflite::reference_ops::SoftmaxInt16(
-        params, tflite::micro::GetTensorShape(input),
-        tflite::micro::GetTensorData<int16_t>(input),
-        tflite::micro::GetTensorShape(output),
-        tflite::micro::GetTensorData<int16_t>(output));
+    tflite_micro::reference_ops::SoftmaxInt16(
+        params, tflite_micro::micro::GetTensorShape(input),
+        tflite_micro::micro::GetTensorData<int16_t>(input),
+        tflite_micro::micro::GetTensorShape(output),
+        tflite_micro::micro::GetTensorData<int16_t>(output));
     return kTfLiteOk;
   }
 
   if (input->type == kTfLiteFloat32) {
-    tflite::reference_ops::Softmax(params, tflite::micro::GetTensorShape(input),
-                                   tflite::micro::GetTensorData<float>(input),
-                                   tflite::micro::GetTensorShape(output),
-                                   tflite::micro::GetTensorData<float>(output));
+    tflite_micro::reference_ops::Softmax(params, tflite_micro::micro::GetTensorShape(input),
+                                   tflite_micro::micro::GetTensorData<float>(input),
+                                   tflite_micro::micro::GetTensorShape(output),
+                                   tflite_micro::micro::GetTensorData<float>(output));
     return kTfLiteOk;
   }
 
@@ -119,8 +119,8 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_SOFTMAX() {
-  return tflite::micro::RegisterOp(XtensaInitSoftmax, XtensaPrepareSoftmax,
+  return tflite_micro::micro::RegisterOp(XtensaInitSoftmax, XtensaPrepareSoftmax,
                                    Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/xtensa/softmax_int8_int16.cc b/tensorflow/lite/micro/kernels/xtensa/softmax_int8_int16.cc
index d37a2f54..cf41344a 100644
--- a/tensorflow/lite/micro/kernels/xtensa/softmax_int8_int16.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/softmax_int8_int16.cc
@@ -27,7 +27,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa_softmax.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 #if defined(HIFI3) || defined(HIFI4) || defined(HIFI5)
@@ -64,10 +64,10 @@ TfLiteStatus PrepareHifi(TfLiteContext* context, TfLiteNode* node) {
 TfLiteStatus EvalHifi(const XtensaSoftmaxOpData* op_data,
                       const TfLiteEvalTensor* input, TfLiteEvalTensor* output,
                       TfLiteContext* context) {
-  const RuntimeShape& input_shape = tflite::micro::GetTensorShape(input);
-  const int8_t* input_data = tflite::micro::GetTensorData<int8_t>(input);
-  const RuntimeShape& output_shape = tflite::micro::GetTensorShape(output);
-  int16_t* output_data = tflite::micro::GetTensorData<int16_t>(output);
+  const RuntimeShape& input_shape = tflite_micro::micro::GetTensorShape(input);
+  const int8_t* input_data = tflite_micro::micro::GetTensorData<int8_t>(input);
+  const RuntimeShape& output_shape = tflite_micro::micro::GetTensorShape(output);
+  int16_t* output_data = tflite_micro::micro::GetTensorData<int16_t>(output);
   const int trailing_dim = input_shape.DimensionsCount() - 1;
   const int outer_size =
       MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);
@@ -122,8 +122,8 @@ TfLiteStatus XtensaPrepareSoftmax(TfLiteContext* context, TfLiteNode* node) {
 
 TfLiteStatus XtensaEvalSoftmaxInt8Int16(TfLiteContext* context,
                                         TfLiteNode* node) {
-  const TfLiteEvalTensor* input = tflite::micro::GetEvalInput(context, node, 0);
-  TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, 0);
+  const TfLiteEvalTensor* input = tflite_micro::micro::GetEvalInput(context, node, 0);
+  TfLiteEvalTensor* output = tflite_micro::micro::GetEvalOutput(context, node, 0);
   TFLITE_DCHECK(node->user_data != nullptr);
 
   if (input->type == kTfLiteInt8 && output->type == kTfLiteInt16) {
@@ -132,11 +132,11 @@ TfLiteStatus XtensaEvalSoftmaxInt8Int16(TfLiteContext* context,
                     output, context);
 #else
     SoftmaxParams op_data = *static_cast<SoftmaxParams*>(node->user_data);
-    tflite::reference_ops::Softmax(
-        op_data, tflite::micro::GetTensorShape(input),
-        tflite::micro::GetTensorData<int8_t>(input),
-        tflite::micro::GetTensorShape(output),
-        tflite::micro::GetTensorData<int16_t>(output));
+    tflite_micro::reference_ops::Softmax(
+        op_data, tflite_micro::micro::GetTensorShape(input),
+        tflite_micro::micro::GetTensorData<int8_t>(input),
+        tflite_micro::micro::GetTensorShape(output),
+        tflite_micro::micro::GetTensorData<int16_t>(output));
     return kTfLiteOk;
 #endif  // defined(HIFI3) || defined(HIFI4) || defined(HIFI5)
   } else {
@@ -147,8 +147,8 @@ TfLiteStatus XtensaEvalSoftmaxInt8Int16(TfLiteContext* context,
 }
 
 TFLMRegistration Register_SOFTMAX_INT8_INT16() {
-  return tflite::micro::RegisterOp(XtensaInitSoftmax, XtensaPrepareSoftmax,
+  return tflite_micro::micro::RegisterOp(XtensaInitSoftmax, XtensaPrepareSoftmax,
                                    XtensaEvalSoftmaxInt8Int16);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/xtensa/softmax_vision.cc b/tensorflow/lite/micro/kernels/xtensa/softmax_vision.cc
index 0c783271..a8ebf1fe 100644
--- a/tensorflow/lite/micro/kernels/xtensa/softmax_vision.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/softmax_vision.cc
@@ -27,7 +27,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa.h"
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa_softmax.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 TfLiteStatus SoftmaxPrepareVision(TfLiteContext* context, TfLiteNode* node) {
   TFLITE_DCHECK(node->user_data != nullptr);
@@ -89,12 +89,12 @@ TfLiteStatus SoftmaxEvalVision(TfLiteContext* context, TfLiteNode* node,
   const uint32_t output_size = NumElements(output->dims);
 
   xiSoftmax(data.p_context, data.context_size,
-            const_cast<int8_t*>(tflite::micro::GetTensorData<int8_t>(input)),
-            input_size, tflite::micro::GetTensorData<int8_t>(output),
+            const_cast<int8_t*>(tflite_micro::micro::GetTensorData<int8_t>(input)),
+            input_size, tflite_micro::micro::GetTensorData<int8_t>(output),
             output_size);
 
   return kTfLiteOk;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
 #endif  // defined(VISION_P6)
diff --git a/tensorflow/lite/micro/kernels/xtensa/strided_slice.cc b/tensorflow/lite/micro/kernels/xtensa/strided_slice.cc
index 4b5fe929..c86e6cd3 100644
--- a/tensorflow/lite/micro/kernels/xtensa/strided_slice.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/strided_slice.cc
@@ -27,22 +27,22 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 #if defined(HIFI3) || defined(HIFI4)
-void StridedSlice_int16_hifi4opt(const tflite::StridedSliceParams& op_params,
+void StridedSlice_int16_hifi4opt(const tflite_micro::StridedSliceParams& op_params,
                                  const RuntimeShape& unextended_input_shape,
                                  const int16_t* input_data,
                                  const RuntimeShape& unextended_output_shape,
                                  int16_t* output_data) {
-  using ::tflite::strided_slice::StartForAxis;
-  using ::tflite::strided_slice::StopForAxis;
+  using ::tflite_micro::strided_slice::StartForAxis;
+  using ::tflite_micro::strided_slice::StopForAxis;
 
   ruy::profiler::ScopeLabel label("StridedSlice");
 
   // Note that the output_shape is not used herein.
-  tflite::StridedSliceParams params_copy = op_params;
+  tflite_micro::StridedSliceParams params_copy = op_params;
 
   TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 5);
   TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 5);
@@ -53,7 +53,7 @@ void StridedSlice_int16_hifi4opt(const tflite::StridedSliceParams& op_params,
 
   // Reverse and pad to 5 dimensions because that is what the runtime code
   // requires (ie. all shapes must be 5D and are given backwards).
-  ::tflite::strided_slice::StridedSlicePadIndices(&params_copy, 5);
+  ::tflite_micro::strided_slice::StridedSlicePadIndices(&params_copy, 5);
 
   const int start_0 = StartForAxis(params_copy, input_shape, 0);
   const int stop_0 = StopForAxis(params_copy, input_shape, 0, start_0);
@@ -85,52 +85,52 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
       *(static_cast<const StridedSliceParams*>(node->user_data));
 
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kStridedSliceInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kStridedSliceInputTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kStridedSliceOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kStridedSliceOutputTensor);
   switch (output->type) {
     case kTfLiteFloat32:
       reference_ops::StridedSlice(op_params,
-                                  tflite::micro::GetTensorShape(input),
-                                  tflite::micro::GetTensorData<float>(input),
-                                  tflite::micro::GetTensorShape(output),
-                                  tflite::micro::GetTensorData<float>(output));
+                                  tflite_micro::micro::GetTensorShape(input),
+                                  tflite_micro::micro::GetTensorData<float>(input),
+                                  tflite_micro::micro::GetTensorShape(output),
+                                  tflite_micro::micro::GetTensorData<float>(output));
       break;
     case kTfLiteInt8:
       reference_ops::StridedSlice(op_params,
-                                  tflite::micro::GetTensorShape(input),
-                                  tflite::micro::GetTensorData<int8_t>(input),
-                                  tflite::micro::GetTensorShape(output),
-                                  tflite::micro::GetTensorData<int8_t>(output));
+                                  tflite_micro::micro::GetTensorShape(input),
+                                  tflite_micro::micro::GetTensorData<int8_t>(input),
+                                  tflite_micro::micro::GetTensorShape(output),
+                                  tflite_micro::micro::GetTensorData<int8_t>(output));
       break;
     case kTfLiteInt16:
 #if defined(HIFI3) || defined(HIFI4)
       StridedSlice_int16_hifi4opt(
-          op_params, tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<int16_t>(input),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<int16_t>(output));
+          op_params, tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<int16_t>(input),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<int16_t>(output));
 #else
       reference_ops::StridedSlice(
-          op_params, tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<int16_t>(input),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<int16_t>(output));
+          op_params, tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<int16_t>(input),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<int16_t>(output));
 #endif  // defined(HIFI3) || defined(HIFI4)
       break;
     case kTfLiteInt32:
       reference_ops::StridedSlice(
-          op_params, tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<int32_t>(input),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<int32_t>(output));
+          op_params, tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<int32_t>(input),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<int32_t>(output));
       break;
     case kTfLiteBool:
       reference_ops::StridedSlice(op_params,
-                                  tflite::micro::GetTensorShape(input),
-                                  tflite::micro::GetTensorData<bool>(input),
-                                  tflite::micro::GetTensorShape(output),
-                                  tflite::micro::GetTensorData<bool>(output));
+                                  tflite_micro::micro::GetTensorShape(input),
+                                  tflite_micro::micro::GetTensorData<bool>(input),
+                                  tflite_micro::micro::GetTensorShape(output),
+                                  tflite_micro::micro::GetTensorData<bool>(output));
       break;
     default:
       MicroPrintf("Type %s (%d) not supported.", TfLiteTypeGetName(input->type),
@@ -142,7 +142,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_STRIDED_SLICE() {
-  return tflite::micro::RegisterOp(StridedSliceInit, StridedSlicePrepare, Eval);
+  return tflite_micro::micro::RegisterOp(StridedSliceInit, StridedSlicePrepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/xtensa/sub.cc b/tensorflow/lite/micro/kernels/xtensa/sub.cc
index 3d287d84..bc2e087b 100644
--- a/tensorflow/lite/micro/kernels/xtensa/sub.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/sub.cc
@@ -29,7 +29,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 void* SubInit(TfLiteContext* context, const char* buffer, size_t length) {
   TFLITE_DCHECK(context->AllocatePersistentBuffer != nullptr);
@@ -42,24 +42,24 @@ void EvalSub(TfLiteContext* context, TfLiteNode* node, TfLiteSubParams* params,
   float output_activation_min, output_activation_max;
   CalculateActivationRange(params->activation, &output_activation_min,
                            &output_activation_max);
-  tflite::ArithmeticParams op_params;
+  tflite_micro::ArithmeticParams op_params;
   SetActivationParams(output_activation_min, output_activation_max, &op_params);
   if (data->requires_broadcast) {
-    tflite::reference_ops::BroadcastSubSlow(
-        op_params, tflite::micro::GetTensorShape(input1),
-        tflite::micro::GetTensorData<float>(input1),
-        tflite::micro::GetTensorShape(input2),
-        tflite::micro::GetTensorData<float>(input2),
-        tflite::micro::GetTensorShape(output),
-        tflite::micro::GetTensorData<float>(output));
+    tflite_micro::reference_ops::BroadcastSubSlow(
+        op_params, tflite_micro::micro::GetTensorShape(input1),
+        tflite_micro::micro::GetTensorData<float>(input1),
+        tflite_micro::micro::GetTensorShape(input2),
+        tflite_micro::micro::GetTensorData<float>(input2),
+        tflite_micro::micro::GetTensorShape(output),
+        tflite_micro::micro::GetTensorData<float>(output));
   } else {
-    tflite::reference_ops::SubWithActivation(
-        op_params, tflite::micro::GetTensorShape(input1),
-        tflite::micro::GetTensorData<float>(input1),
-        tflite::micro::GetTensorShape(input2),
-        tflite::micro::GetTensorData<float>(input2),
-        tflite::micro::GetTensorShape(output),
-        tflite::micro::GetTensorData<float>(output));
+    tflite_micro::reference_ops::SubWithActivation(
+        op_params, tflite_micro::micro::GetTensorShape(input1),
+        tflite_micro::micro::GetTensorData<float>(input1),
+        tflite_micro::micro::GetTensorShape(input2),
+        tflite_micro::micro::GetTensorData<float>(input2),
+        tflite_micro::micro::GetTensorShape(output),
+        tflite_micro::micro::GetTensorData<float>(output));
   }
 }
 
@@ -68,7 +68,7 @@ TfLiteStatus EvalSubQuantized(TfLiteContext* context, TfLiteNode* node,
                               const TfLiteEvalTensor* input1,
                               const TfLiteEvalTensor* input2,
                               TfLiteEvalTensor* output) {
-  tflite::ArithmeticParams op_params;
+  tflite_micro::ArithmeticParams op_params;
   op_params.left_shift = data->left_shift;
   op_params.input1_offset = data->input1_offset;
   op_params.input1_multiplier = data->input1_multiplier;
@@ -85,8 +85,8 @@ TfLiteStatus EvalSubQuantized(TfLiteContext* context, TfLiteNode* node,
   // Let's separate them into two different files.
 #if !(defined(HIFI3) || defined(HIFI4))
   bool need_broadcast = reference_ops::ProcessBroadcastShapes(
-      tflite::micro::GetTensorShape(input1),
-      tflite::micro::GetTensorShape(input2), &op_params);
+      tflite_micro::micro::GetTensorShape(input1),
+      tflite_micro::micro::GetTensorShape(input2), &op_params);
 #endif  // !(defined(HIFI3) || defined(HIFI4))
 
   switch (output->type) {
@@ -94,11 +94,11 @@ TfLiteStatus EvalSubQuantized(TfLiteContext* context, TfLiteNode* node,
 #if defined(HIFI3) || defined(HIFI4)
       int err;
       const RuntimeShape extended_input1_shape =
-          RuntimeShape::ExtendedShape(5, tflite::micro::GetTensorShape(input1));
+          RuntimeShape::ExtendedShape(5, tflite_micro::micro::GetTensorShape(input1));
       const RuntimeShape extended_input2_shape =
-          RuntimeShape::ExtendedShape(5, tflite::micro::GetTensorShape(input2));
+          RuntimeShape::ExtendedShape(5, tflite_micro::micro::GetTensorShape(input2));
       const RuntimeShape extended_output_shape =
-          RuntimeShape::ExtendedShape(5, tflite::micro::GetTensorShape(output));
+          RuntimeShape::ExtendedShape(5, tflite_micro::micro::GetTensorShape(output));
       const int* input1_dims = extended_input1_shape.DimsData();
       const int* input2_dims = extended_input2_shape.DimsData();
       const int* output_dims = extended_output_shape.DimsData();
@@ -120,14 +120,14 @@ TfLiteStatus EvalSubQuantized(TfLiteContext* context, TfLiteNode* node,
 
       for (b = 0; b < output_dims[0]; b++) {
         err = xa_nn_elm_sub_broadcast_4D_asym8sxasym8s_asym8s(
-            tflite::micro::GetTensorData<int8_t>(output) + b * out_off,
+            tflite_micro::micro::GetTensorData<int8_t>(output) + b * out_off,
             output_dims + 1, op_params.output_offset, op_params.output_shift,
             op_params.output_multiplier, op_params.quantized_activation_min,
             op_params.quantized_activation_max,
-            tflite::micro::GetTensorData<int8_t>(input1) + b * inp1_off,
+            tflite_micro::micro::GetTensorData<int8_t>(input1) + b * inp1_off,
             input1_dims + 1, op_params.input1_offset, op_params.input1_shift,
             op_params.input1_multiplier,
-            tflite::micro::GetTensorData<int8_t>(input2), input2_dims + 1,
+            tflite_micro::micro::GetTensorData<int8_t>(input2), input2_dims + 1,
             op_params.input2_offset, op_params.input2_shift,
             op_params.input2_multiplier, op_params.left_shift);
 
@@ -135,21 +135,21 @@ TfLiteStatus EvalSubQuantized(TfLiteContext* context, TfLiteNode* node,
       }
 #else   // defined(HIFI3) || defined(HIFI4)
       if (need_broadcast) {
-        tflite::reference_ops::BroadcastQuantSubSlow(
-            op_params, tflite::micro::GetTensorShape(input1),
-            tflite::micro::GetTensorData<int8_t>(input1),
-            tflite::micro::GetTensorShape(input2),
-            tflite::micro::GetTensorData<int8_t>(input2),
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<int8_t>(output));
+        tflite_micro::reference_ops::BroadcastQuantSubSlow(
+            op_params, tflite_micro::micro::GetTensorShape(input1),
+            tflite_micro::micro::GetTensorData<int8_t>(input1),
+            tflite_micro::micro::GetTensorShape(input2),
+            tflite_micro::micro::GetTensorData<int8_t>(input2),
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int8_t>(output));
       } else {
-        tflite::reference_ops::Sub(
-            op_params, tflite::micro::GetTensorShape(input1),
-            tflite::micro::GetTensorData<int8_t>(input1),
-            tflite::micro::GetTensorShape(input2),
-            tflite::micro::GetTensorData<int8_t>(input2),
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<int8_t>(output));
+        tflite_micro::reference_ops::Sub(
+            op_params, tflite_micro::micro::GetTensorShape(input1),
+            tflite_micro::micro::GetTensorData<int8_t>(input1),
+            tflite_micro::micro::GetTensorShape(input2),
+            tflite_micro::micro::GetTensorData<int8_t>(input2),
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int8_t>(output));
       }
 #endif  // defined(HIFI3) || defined(HIFI4)
       break;
@@ -158,11 +158,11 @@ TfLiteStatus EvalSubQuantized(TfLiteContext* context, TfLiteNode* node,
 #if defined(HIFI3) || defined(HIFI4)
       int err;
       const RuntimeShape extended_input1_shape =
-          RuntimeShape::ExtendedShape(5, tflite::micro::GetTensorShape(input1));
+          RuntimeShape::ExtendedShape(5, tflite_micro::micro::GetTensorShape(input1));
       const RuntimeShape extended_input2_shape =
-          RuntimeShape::ExtendedShape(5, tflite::micro::GetTensorShape(input2));
+          RuntimeShape::ExtendedShape(5, tflite_micro::micro::GetTensorShape(input2));
       const RuntimeShape extended_output_shape =
-          RuntimeShape::ExtendedShape(5, tflite::micro::GetTensorShape(output));
+          RuntimeShape::ExtendedShape(5, tflite_micro::micro::GetTensorShape(output));
       const int* input1_dims = extended_input1_shape.DimsData();
       const int* input2_dims = extended_input2_shape.DimsData();
       const int* output_dims = extended_output_shape.DimsData();
@@ -183,14 +183,14 @@ TfLiteStatus EvalSubQuantized(TfLiteContext* context, TfLiteNode* node,
 
       for (b = 0; b < output_dims[0]; b++) {
         err = xa_nn_elm_sub_broadcast_4D_asym16sxasym16s_asym16s(
-            tflite::micro::GetTensorData<int16_t>(output) + b * out_off,
+            tflite_micro::micro::GetTensorData<int16_t>(output) + b * out_off,
             output_dims + 1, op_params.output_offset, op_params.output_shift,
             op_params.output_multiplier, op_params.quantized_activation_min,
             op_params.quantized_activation_max,
-            tflite::micro::GetTensorData<int16_t>(input1) + b * inp1_off,
+            tflite_micro::micro::GetTensorData<int16_t>(input1) + b * inp1_off,
             input1_dims + 1, op_params.input1_offset, op_params.input1_shift,
             op_params.input1_multiplier,
-            tflite::micro::GetTensorData<int16_t>(input2), input2_dims + 1,
+            tflite_micro::micro::GetTensorData<int16_t>(input2), input2_dims + 1,
             op_params.input2_offset, op_params.input2_shift,
             op_params.input2_multiplier, op_params.left_shift);
 
@@ -198,21 +198,21 @@ TfLiteStatus EvalSubQuantized(TfLiteContext* context, TfLiteNode* node,
       }
 #else   // defined(HIFI3) || defined(HIFI4)
       if (need_broadcast) {
-        tflite::reference_ops::BroadcastQuantSubSlow(
-            op_params, tflite::micro::GetTensorShape(input1),
-            tflite::micro::GetTensorData<int16_t>(input1),
-            tflite::micro::GetTensorShape(input2),
-            tflite::micro::GetTensorData<int16_t>(input2),
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<int16_t>(output));
+        tflite_micro::reference_ops::BroadcastQuantSubSlow(
+            op_params, tflite_micro::micro::GetTensorShape(input1),
+            tflite_micro::micro::GetTensorData<int16_t>(input1),
+            tflite_micro::micro::GetTensorShape(input2),
+            tflite_micro::micro::GetTensorData<int16_t>(input2),
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int16_t>(output));
       } else {
-        tflite::reference_ops::Sub(
-            op_params, tflite::micro::GetTensorShape(input1),
-            tflite::micro::GetTensorData<int16_t>(input1),
-            tflite::micro::GetTensorShape(input2),
-            tflite::micro::GetTensorData<int16_t>(input2),
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<int16_t>(output));
+        tflite_micro::reference_ops::Sub(
+            op_params, tflite_micro::micro::GetTensorShape(input1),
+            tflite_micro::micro::GetTensorData<int16_t>(input1),
+            tflite_micro::micro::GetTensorShape(input2),
+            tflite_micro::micro::GetTensorData<int16_t>(input2),
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int16_t>(output));
       }
 #endif  // defined(HIFI3) || defined(HIFI4)
       break;
@@ -229,11 +229,11 @@ TfLiteStatus SubEval(TfLiteContext* context, TfLiteNode* node) {
   auto* params = reinterpret_cast<TfLiteSubParams*>(node->builtin_data);
 
   const TfLiteEvalTensor* input1 =
-      tflite::micro::GetEvalInput(context, node, kSubInputTensor1);
+      tflite_micro::micro::GetEvalInput(context, node, kSubInputTensor1);
   const TfLiteEvalTensor* input2 =
-      tflite::micro::GetEvalInput(context, node, kSubInputTensor2);
+      tflite_micro::micro::GetEvalInput(context, node, kSubInputTensor2);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kSubOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kSubOutputTensor);
 
   TFLITE_DCHECK(node->user_data != nullptr);
   const OpDataSub& data = *(static_cast<const OpDataSub*>(node->user_data));
@@ -253,7 +253,7 @@ TfLiteStatus SubEval(TfLiteContext* context, TfLiteNode* node) {
 }
 
 TFLMRegistration Register_SUB() {
-  return tflite::micro::RegisterOp(SubInit, SubPrepare, SubEval);
+  return tflite_micro::micro::RegisterOp(SubInit, SubPrepare, SubEval);
 }
 
-}  // namespace tflite
\ No newline at end of file
+}  // namespace tflite_micro
\ No newline at end of file
diff --git a/tensorflow/lite/micro/kernels/xtensa/svdf.cc b/tensorflow/lite/micro/kernels/xtensa/svdf.cc
index 3a71f946..de512fb4 100644
--- a/tensorflow/lite/micro/kernels/xtensa/svdf.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/svdf.cc
@@ -30,7 +30,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa_svdf.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 #if defined(HIFI3) || defined(HIFI4) || defined(HIFI5)
@@ -56,7 +56,7 @@ TfLiteStatus EvalIntegerSvdfHifi(TfLiteContext* context, TfLiteNode* node,
 
   // Shift states.
   int16_t* const state_ptr =
-      tflite::micro::GetTensorData<int16_t>(activation_state_tensor);
+      tflite_micro::micro::GetTensorData<int16_t>(activation_state_tensor);
 
   // Left shift the activation_state.
   int num_bytes = sizeof(*state_ptr) * (n_batch * n_filter * n_memory - 1);
@@ -69,9 +69,9 @@ TfLiteStatus EvalIntegerSvdfHifi(TfLiteContext* context, TfLiteNode* node,
   // Note: no need to clear the latest activation, matmul is not accumulative.
 
   // Feature matmul.
-  const int8_t* input = tflite::micro::GetTensorData<int8_t>(input_tensor);
+  const int8_t* input = tflite_micro::micro::GetTensorData<int8_t>(input_tensor);
   const int8_t* weight_feature =
-      tflite::micro::GetTensorData<int8_t>(weights_feature_tensor);
+      tflite_micro::micro::GetTensorData<int8_t>(weights_feature_tensor);
   int16_t* result_in_batch = state_ptr + (n_memory - 1);
 
   for (int b = 0; b < n_batch; b++) {
@@ -87,15 +87,15 @@ TfLiteStatus EvalIntegerSvdfHifi(TfLiteContext* context, TfLiteNode* node,
   // Time weights dot product + activation
   for (int b = 0; b < n_batch; ++b) {
     const int16_t* vector1_ptr =
-        tflite::micro::GetTensorData<int16_t>(weights_time_tensor);
+        tflite_micro::micro::GetTensorData<int16_t>(weights_time_tensor);
     const int16_t* vector2_ptr =
-        tflite::micro::GetTensorData<int16_t>(activation_state_tensor) +
+        tflite_micro::micro::GetTensorData<int16_t>(activation_state_tensor) +
         b * n_memory * n_filter;
     // TODO(#1751): account for optional bias tensor
     const int32_t* bias_ptr =
-        tflite::micro::GetTensorData<int32_t>(bias_tensor);
+        tflite_micro::micro::GetTensorData<int32_t>(bias_tensor);
     int8_t* output_ptr =
-        tflite::micro::GetTensorData<int8_t>(output_tensor) + b * n_unit;
+        tflite_micro::micro::GetTensorData<int8_t>(output_tensor) + b * n_unit;
 
     // TODO(#1751): account for optional bias tensor
     TF_LITE_ENSURE_EQ(
@@ -286,20 +286,20 @@ TfLiteStatus EvalInt8(TfLiteContext* context, TfLiteNode* node) {
   auto* params = static_cast<TfLiteSVDFParams*>(node->builtin_data);
 
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kSvdfInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kSvdfInputTensor);
   const TfLiteEvalTensor* weights_feature =
-      tflite::micro::GetEvalInput(context, node, kSvdfWeightsFeatureTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kSvdfWeightsFeatureTensor);
   const TfLiteEvalTensor* weights_time =
-      tflite::micro::GetEvalInput(context, node, kSvdfWeightsTimeTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kSvdfWeightsTimeTensor);
   // TODO(#1751): account for optional bias tensor
   const TfLiteEvalTensor* bias =
       (NumInputs(node) == 5)
-          ? tflite::micro::GetEvalInput(context, node, kSvdfBiasTensor)
+          ? tflite_micro::micro::GetEvalInput(context, node, kSvdfBiasTensor)
           : nullptr;
-  TfLiteEvalTensor* activation_state = tflite::micro::GetMutableEvalInput(
+  TfLiteEvalTensor* activation_state = tflite_micro::micro::GetMutableEvalInput(
       context, node, kSvdfInputActivationStateTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kSvdfOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kSvdfOutputTensor);
 
   TFLITE_DCHECK(node->user_data != nullptr);
   const OpDataSvdf& data = *(static_cast<const OpDataSvdf*>(node->user_data));
@@ -323,20 +323,20 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   auto* params = static_cast<TfLiteSVDFParams*>(node->builtin_data);
 
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kSvdfInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kSvdfInputTensor);
   const TfLiteEvalTensor* weights_feature =
-      tflite::micro::GetEvalInput(context, node, kSvdfWeightsFeatureTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kSvdfWeightsFeatureTensor);
   const TfLiteEvalTensor* weights_time =
-      tflite::micro::GetEvalInput(context, node, kSvdfWeightsTimeTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kSvdfWeightsTimeTensor);
   // TODO(#1751): account for optional bias tensor
   const TfLiteEvalTensor* bias =
       (NumInputs(node) == 5)
-          ? tflite::micro::GetEvalInput(context, node, kSvdfBiasTensor)
+          ? tflite_micro::micro::GetEvalInput(context, node, kSvdfBiasTensor)
           : nullptr;
-  TfLiteEvalTensor* activation_state = tflite::micro::GetMutableEvalInput(
+  TfLiteEvalTensor* activation_state = tflite_micro::micro::GetMutableEvalInput(
       context, node, kSvdfInputActivationStateTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kSvdfOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kSvdfOutputTensor);
 
   TFLITE_DCHECK(node->user_data != nullptr);
   const OpDataSvdf& data = *(static_cast<const OpDataSvdf*>(node->user_data));
@@ -384,11 +384,11 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_SVDF() {
-  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, Eval);
 }
 
 TFLMRegistration Register_SVDF_INT8() {
-  return tflite::micro::RegisterOp(Init, PrepareInt8, EvalInt8);
+  return tflite_micro::micro::RegisterOp(Init, PrepareInt8, EvalInt8);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/xtensa/transpose_conv.cc b/tensorflow/lite/micro/kernels/xtensa/transpose_conv.cc
index 20b0ab88..d212a48d 100644
--- a/tensorflow/lite/micro/kernels/xtensa/transpose_conv.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/transpose_conv.cc
@@ -27,7 +27,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/xtensa/xtensa.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 // For the TfLite transpose_conv implementation, input tensor 0 corresponds to
@@ -110,7 +110,7 @@ TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node,
     TF_LITE_ENSURE(context, output != nullptr);
     int output_channels = filter->dims->data[kConvQuantizedDimension];
 
-    TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(
+    TF_LITE_ENSURE_STATUS(tflite_micro::PopulateConvolutionQuantizationParams(
         context, input, filter, bias, output, kTfLiteActNone,
         &data->params.output_multiplier, &data->params.output_shift,
         &data->params.quantized_activation_min,
@@ -238,15 +238,15 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   const TfLiteEvalTensor* filter =
-      tflite::micro::GetEvalInput(context, node, kFilterTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kFilterTensor);
   const TfLiteEvalTensor* bias =
       (NumInputs(node) == 4)
-          ? tflite::micro::GetEvalInput(context, node, kBiasTensor)
+          ? tflite_micro::micro::GetEvalInput(context, node, kBiasTensor)
           : nullptr;
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
 
   TFLITE_DCHECK(node->user_data != nullptr);
   const OpData& data = *(static_cast<const OpData*>(node->user_data));
@@ -268,15 +268,15 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
                                &op_params.float_activation_max);
 
       reference_ops::TransposeConv(
-          op_params, tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<float>(input),
-          tflite::micro::GetTensorShape(filter),
-          tflite::micro::GetTensorData<float>(filter),
-          tflite::micro::GetTensorShape(bias),
-          tflite::micro::GetTensorData<float>(bias),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<float>(output),
-          tflite::micro::GetTensorShape(nullptr), nullptr);
+          op_params, tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<float>(input),
+          tflite_micro::micro::GetTensorShape(filter),
+          tflite_micro::micro::GetTensorData<float>(filter),
+          tflite_micro::micro::GetTensorShape(bias),
+          tflite_micro::micro::GetTensorData<float>(bias),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<float>(output),
+          tflite_micro::micro::GetTensorShape(nullptr), nullptr);
       break;
     }
     case kTfLiteInt8: {
@@ -284,15 +284,15 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
           context->GetScratchBuffer(context, data.scratch_buffer_index));
       reference_integer_ops::TransposeConv(
           data.params, data.per_channel_output_multiplier,
-          data.per_channel_output_shift, tflite::micro::GetTensorShape(input),
-          tflite::micro::GetTensorData<int8_t>(input),
-          tflite::micro::GetTensorShape(filter),
-          tflite::micro::GetTensorData<int8_t>(filter),
-          tflite::micro::GetTensorShape(bias),
-          tflite::micro::GetTensorData<int32_t>(bias),
-          tflite::micro::GetTensorShape(output),
-          tflite::micro::GetTensorData<int8_t>(output),
-          tflite::micro::GetTensorShape(nullptr), nullptr, scratch_buffer);
+          data.per_channel_output_shift, tflite_micro::micro::GetTensorShape(input),
+          tflite_micro::micro::GetTensorData<int8_t>(input),
+          tflite_micro::micro::GetTensorShape(filter),
+          tflite_micro::micro::GetTensorData<int8_t>(filter),
+          tflite_micro::micro::GetTensorShape(bias),
+          tflite_micro::micro::GetTensorData<int32_t>(bias),
+          tflite_micro::micro::GetTensorShape(output),
+          tflite_micro::micro::GetTensorData<int8_t>(output),
+          tflite_micro::micro::GetTensorShape(nullptr), nullptr, scratch_buffer);
       break;
     }
     case kTfLiteInt16: {
@@ -304,27 +304,27 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
         std::int64_t* bias_converted_buffer =
             static_cast<int64_t*>(context->GetScratchBuffer(
                 context, data.bias_converted_buffer_index));
-        for (int i = 0; i < tflite::micro::GetTensorShape(bias).FlatSize();
+        for (int i = 0; i < tflite_micro::micro::GetTensorShape(bias).FlatSize();
              i++) {
           bias_converted_buffer[i] = bias->data.i16[i];
         }
         reference_integer_ops::TransposeConv(
             data.params, data.per_channel_output_multiplier,
-            data.per_channel_output_shift, tflite::micro::GetTensorShape(input),
-            tflite::micro::GetTensorData<int16_t>(input),
-            tflite::micro::GetTensorShape(filter),
-            tflite::micro::GetTensorData<int8_t>(filter),
-            tflite::micro::GetTensorShape(bias), bias_converted_buffer,
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<int16_t>(output),
-            tflite::micro::GetTensorShape(nullptr), nullptr, scratch_buffer);
+            data.per_channel_output_shift, tflite_micro::micro::GetTensorShape(input),
+            tflite_micro::micro::GetTensorData<int16_t>(input),
+            tflite_micro::micro::GetTensorShape(filter),
+            tflite_micro::micro::GetTensorData<int8_t>(filter),
+            tflite_micro::micro::GetTensorShape(bias), bias_converted_buffer,
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int16_t>(output),
+            tflite_micro::micro::GetTensorShape(nullptr), nullptr, scratch_buffer);
       } else {
 #if defined(HIFI3) || defined(HIFI4)
-        const RuntimeShape& input_shape = tflite::micro::GetTensorShape(input);
+        const RuntimeShape& input_shape = tflite_micro::micro::GetTensorShape(input);
         const RuntimeShape& filter_shape =
-            tflite::micro::GetTensorShape(filter);
+            tflite_micro::micro::GetTensorShape(filter);
         const RuntimeShape& output_shape =
-            tflite::micro::GetTensorShape(output);
+            tflite_micro::micro::GetTensorShape(output);
         const int stride_width = data.params.stride_width;
         const int stride_height = data.params.stride_height;
         const int pad_width = data.params.padding_values.width;
@@ -341,11 +341,11 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
         const int output_height = output_shape.Dims(1);
         const int output_width = output_shape.Dims(2);
         const int16_t* input_data =
-            tflite::micro::GetTensorData<int16_t>(input);
+            tflite_micro::micro::GetTensorData<int16_t>(input);
         const int8_t* filter_data =
-            tflite::micro::GetTensorData<int8_t>(filter);
-        const int64_t* bias_data = tflite::micro::GetTensorData<int64_t>(bias);
-        int16_t* output_data = tflite::micro::GetTensorData<int16_t>(output);
+            tflite_micro::micro::GetTensorData<int8_t>(filter);
+        const int64_t* bias_data = tflite_micro::micro::GetTensorData<int64_t>(bias);
+        int16_t* output_data = tflite_micro::micro::GetTensorData<int16_t>(output);
 
         const int num_elements = output_shape.FlatSize();
 
@@ -364,15 +364,15 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 #else
         reference_integer_ops::TransposeConv(
             data.params, data.per_channel_output_multiplier,
-            data.per_channel_output_shift, tflite::micro::GetTensorShape(input),
-            tflite::micro::GetTensorData<int16_t>(input),
-            tflite::micro::GetTensorShape(filter),
-            tflite::micro::GetTensorData<int8_t>(filter),
-            tflite::micro::GetTensorShape(bias),
-            tflite::micro::GetTensorData<std::int64_t>(bias),
-            tflite::micro::GetTensorShape(output),
-            tflite::micro::GetTensorData<int16_t>(output),
-            tflite::micro::GetTensorShape(nullptr), nullptr, scratch_buffer);
+            data.per_channel_output_shift, tflite_micro::micro::GetTensorShape(input),
+            tflite_micro::micro::GetTensorData<int16_t>(input),
+            tflite_micro::micro::GetTensorShape(filter),
+            tflite_micro::micro::GetTensorData<int8_t>(filter),
+            tflite_micro::micro::GetTensorShape(bias),
+            tflite_micro::micro::GetTensorData<std::int64_t>(bias),
+            tflite_micro::micro::GetTensorShape(output),
+            tflite_micro::micro::GetTensorData<int16_t>(output),
+            tflite_micro::micro::GetTensorShape(nullptr), nullptr, scratch_buffer);
 #endif  // defined(HIFI3) || defined(HIFI4)
       }
       break;
@@ -388,7 +388,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_TRANSPOSE_CONV() {
-  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(Init, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/xtensa/unidirectional_sequence_lstm.cc b/tensorflow/lite/micro/kernels/xtensa/unidirectional_sequence_lstm.cc
index cbce1e1c..4118edff 100644
--- a/tensorflow/lite/micro/kernels/xtensa/unidirectional_sequence_lstm.cc
+++ b/tensorflow/lite/micro/kernels/xtensa/unidirectional_sequence_lstm.cc
@@ -30,7 +30,7 @@ limitations under the License.
 
 // TODO(b/230666079): Flatten the namespace to match the builtin kernel
 // implementation
-namespace tflite {
+namespace tflite_micro {
 namespace ops {
 namespace micro {
 // namespace unidirectional_sequence_lstm {
@@ -421,27 +421,27 @@ TfLiteStatus CheckInputTensorDimensions(TfLiteContext* context,
   //  > 0 means clipping
   TF_LITE_ENSURE(context, params->cell_clip >= 0);
   TF_LITE_ENSURE(context, params->proj_clip >= 0);
-  const TfLiteEvalTensor* input_to_input_weights = tflite::micro::GetEvalInput(
+  const TfLiteEvalTensor* input_to_input_weights = tflite_micro::micro::GetEvalInput(
       context, node, micro::lstm::full::kInputToInputWeightsTensor);
   if (input_to_input_weights != nullptr) {
     TF_LITE_ENSURE_EQ(context, input_to_input_weights->dims->size, 2);
     TF_LITE_ENSURE_EQ(context, input_to_input_weights->dims->data[0], n_cell);
     TF_LITE_ENSURE_EQ(context, input_to_input_weights->dims->data[1], n_input);
   }
-  const TfLiteEvalTensor* input_to_forget_weights = tflite::micro::GetEvalInput(
+  const TfLiteEvalTensor* input_to_forget_weights = tflite_micro::micro::GetEvalInput(
       context, node, micro::lstm::full::kInputToForgetWeightsTensor);
 
   TF_LITE_ENSURE_EQ(context, input_to_forget_weights->dims->size, 2);
   TF_LITE_ENSURE_EQ(context, input_to_forget_weights->dims->data[0], n_cell);
   TF_LITE_ENSURE_EQ(context, input_to_forget_weights->dims->data[1], n_input);
-  const TfLiteEvalTensor* input_to_cell_weights = tflite::micro::GetEvalInput(
+  const TfLiteEvalTensor* input_to_cell_weights = tflite_micro::micro::GetEvalInput(
       context, node, micro::lstm::full::kInputToCellWeightsTensor);
 
   TF_LITE_ENSURE_EQ(context, input_to_cell_weights->dims->size, 2);
   TF_LITE_ENSURE_EQ(context, input_to_cell_weights->dims->data[0], n_cell);
   TF_LITE_ENSURE_EQ(context, input_to_cell_weights->dims->data[1], n_input);
   const TfLiteEvalTensor* recurrent_to_input_weights =
-      tflite::micro::GetEvalInput(
+      tflite_micro::micro::GetEvalInput(
           context, node, micro::lstm::full::kRecurrentToInputWeightsTensor);
   if (recurrent_to_input_weights != nullptr) {
     TF_LITE_ENSURE_EQ(context, recurrent_to_input_weights->dims->size, 2);
@@ -451,7 +451,7 @@ TfLiteStatus CheckInputTensorDimensions(TfLiteContext* context,
                       n_output);
   }
   const TfLiteEvalTensor* recurrent_to_forget_weights =
-      tflite::micro::GetEvalInput(
+      tflite_micro::micro::GetEvalInput(
           context, node, micro::lstm::full::kRecurrentToForgetWeightsTensor);
 
   TF_LITE_ENSURE_EQ(context, recurrent_to_forget_weights->dims->size, 2);
@@ -460,7 +460,7 @@ TfLiteStatus CheckInputTensorDimensions(TfLiteContext* context,
   TF_LITE_ENSURE_EQ(context, recurrent_to_forget_weights->dims->data[1],
                     n_output);
   const TfLiteEvalTensor* recurrent_to_cell_weights =
-      tflite::micro::GetEvalInput(
+      tflite_micro::micro::GetEvalInput(
           context, node, micro::lstm::full::kRecurrentToCellWeightsTensor);
 
   TF_LITE_ENSURE_EQ(context, recurrent_to_cell_weights->dims->size, 2);
@@ -517,7 +517,7 @@ TfLiteStatus CheckInputTensorDimensions(TfLiteContext* context,
        (cell_to_forget_weights == nullptr) &&
        (cell_to_output_weights == nullptr));
   TF_LITE_ENSURE(context, peephole_weights_all_or_none == true);
-  const TfLiteEvalTensor* input_gate_bias = tflite::micro::GetEvalInput(
+  const TfLiteEvalTensor* input_gate_bias = tflite_micro::micro::GetEvalInput(
       context, node, micro::lstm::full::kInputGateBiasTensor);
 
   if (use_cifg) {
@@ -531,7 +531,7 @@ TfLiteStatus CheckInputTensorDimensions(TfLiteContext* context,
       TF_LITE_ENSURE_TYPES_EQ(context, input_gate_bias->type, kTfLiteFloat32);
     }
   }
-  const TfLiteEvalTensor* forget_gate_bias = tflite::micro::GetEvalInput(
+  const TfLiteEvalTensor* forget_gate_bias = tflite_micro::micro::GetEvalInput(
       context, node, micro::lstm::full::kForgetGateBiasTensor);
 
   TF_LITE_ENSURE_EQ(context, forget_gate_bias->dims->size, 1);
@@ -541,7 +541,7 @@ TfLiteStatus CheckInputTensorDimensions(TfLiteContext* context,
   } else {
     TF_LITE_ENSURE_TYPES_EQ(context, forget_gate_bias->type, kTfLiteFloat32);
   }
-  const TfLiteEvalTensor* cell_gate_bias = tflite::micro::GetEvalInput(
+  const TfLiteEvalTensor* cell_gate_bias = tflite_micro::micro::GetEvalInput(
       context, node, micro::lstm::full::kCellGateBiasTensor);
 
   TF_LITE_ENSURE_EQ(context, cell_gate_bias->dims->size, 1);
@@ -551,7 +551,7 @@ TfLiteStatus CheckInputTensorDimensions(TfLiteContext* context,
   } else {
     TF_LITE_ENSURE_TYPES_EQ(context, cell_gate_bias->type, kTfLiteFloat32);
   }
-  const TfLiteEvalTensor* output_gate_bias = tflite::micro::GetEvalInput(
+  const TfLiteEvalTensor* output_gate_bias = tflite_micro::micro::GetEvalInput(
       context, node, micro::lstm::full::kOutputGateBiasTensor);
   TF_LITE_ENSURE_EQ(context, output_gate_bias->dims->size, 1);
   TF_LITE_ENSURE_EQ(context, output_gate_bias->dims->data[0], n_cell);
@@ -591,7 +591,7 @@ TfLiteStatus CheckInputTensorDimensions(TfLiteContext* context,
 
   if (use_layer_norm) {
     const TfLiteEvalTensor* input_layer_norm_coefficients =
-        tflite::micro::GetEvalInput(
+        tflite_micro::micro::GetEvalInput(
             context, node,
             micro::lstm::full::kInputLayerNormCoefficientsTensor);
     if (use_cifg) {
@@ -610,7 +610,7 @@ TfLiteStatus CheckInputTensorDimensions(TfLiteContext* context,
       }
     }
     const TfLiteEvalTensor* forget_layer_norm_coefficients =
-        tflite::micro::GetEvalInput(
+        tflite_micro::micro::GetEvalInput(
             context, node,
             micro::lstm::full::kForgetLayerNormCoefficientsTensor);
     TF_LITE_ENSURE_EQ(context, forget_layer_norm_coefficients->dims->size, 1);
@@ -624,7 +624,7 @@ TfLiteStatus CheckInputTensorDimensions(TfLiteContext* context,
                               kTfLiteFloat32);
     }
     const TfLiteEvalTensor* cell_layer_norm_coefficients =
-        tflite::micro::GetEvalInput(
+        tflite_micro::micro::GetEvalInput(
             context, node, micro::lstm::full::kCellLayerNormCoefficientsTensor);
     TF_LITE_ENSURE_EQ(context, cell_layer_norm_coefficients->dims->size, 1);
     TF_LITE_ENSURE_EQ(context, cell_layer_norm_coefficients->dims->data[0],
@@ -637,7 +637,7 @@ TfLiteStatus CheckInputTensorDimensions(TfLiteContext* context,
                               kTfLiteFloat32);
     }
     const TfLiteEvalTensor* output_layer_norm_coefficients =
-        tflite::micro::GetEvalInput(
+        tflite_micro::micro::GetEvalInput(
             context, node,
             micro::lstm::full::kOutputLayerNormCoefficientsTensor);
 
@@ -857,7 +857,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
 
   // Inferring batch size, number of outputs and sequence length and
   // number of cells from the input tensors.
-  const TfLiteEvalTensor* input = tflite::micro::GetEvalInput(
+  const TfLiteEvalTensor* input = tflite_micro::micro::GetEvalInput(
       context, node, micro::lstm::full::kInputTensor);
   const bool is_integer = input->type == kTfLiteInt8;
   TF_LITE_ENSURE(context, input->dims->size > 1);
@@ -867,13 +867,13 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   const bool time_major = params->time_major;
   const int n_batch = time_major ? input->dims->data[1] : input->dims->data[0];
   const int n_input = input->dims->data[2];
-  const TfLiteEvalTensor* input_to_output_weights = tflite::micro::GetEvalInput(
+  const TfLiteEvalTensor* input_to_output_weights = tflite_micro::micro::GetEvalInput(
       context, node, micro::lstm::full::kInputToOutputWeightsTensor);
   const int n_cell = input_to_output_weights->dims->data[0];
   TF_LITE_ENSURE_EQ(context, input_to_output_weights->dims->size, 2);
   TF_LITE_ENSURE_EQ(context, input_to_output_weights->dims->data[1], n_input);
   const TfLiteEvalTensor* recurrent_to_output_weights =
-      tflite::micro::GetEvalInput(
+      tflite_micro::micro::GetEvalInput(
           context, node, micro::lstm::full::kRecurrentToOutputWeightsTensor);
 
   TF_LITE_ENSURE_EQ(context, recurrent_to_output_weights->dims->size, 2);
@@ -887,12 +887,12 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
                                           n_cell, use_layer_norm, is_integer));
   // Get the pointer to output, output_state and cell_state buffer tensors.
   //  TfLiteEvalTensor* output =
-  //      tflite::micro::GetEvalOutput(context, node,
+  //      tflite_micro::micro::GetEvalOutput(context, node,
   //      micro::lstm::full::kOutputTensor);
-  TfLiteEvalTensor* output_state = tflite::micro::GetMutableEvalInput(
+  TfLiteEvalTensor* output_state = tflite_micro::micro::GetMutableEvalInput(
       context, node, micro::lstm::full::kOutputStateTensor);
   TFLITE_DCHECK(output_state != nullptr);
-  TfLiteEvalTensor* cell_state = tflite::micro::GetMutableEvalInput(
+  TfLiteEvalTensor* cell_state = tflite_micro::micro::GetMutableEvalInput(
       context, node, micro::lstm::full::kCellStateTensor);
   TFLITE_DCHECK(cell_state != nullptr);
   // Check the shape of input state tensors.
@@ -950,27 +950,27 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   //  const bool use_layer_norm = op_data->use_layer_norm;
   //  const bool time_major = params->time_major;
 
-  const TfLiteEvalTensor* input = tflite::micro::GetEvalInput(
+  const TfLiteEvalTensor* input = tflite_micro::micro::GetEvalInput(
       context, node, micro::lstm::full::kInputTensor);
-  const TfLiteEvalTensor* input_to_input_weights = tflite::micro::GetEvalInput(
+  const TfLiteEvalTensor* input_to_input_weights = tflite_micro::micro::GetEvalInput(
       context, node, micro::lstm::full::kInputToInputWeightsTensor);
-  const TfLiteEvalTensor* input_to_forget_weights = tflite::micro::GetEvalInput(
+  const TfLiteEvalTensor* input_to_forget_weights = tflite_micro::micro::GetEvalInput(
       context, node, micro::lstm::full::kInputToForgetWeightsTensor);
-  const TfLiteEvalTensor* input_to_cell_weights = tflite::micro::GetEvalInput(
+  const TfLiteEvalTensor* input_to_cell_weights = tflite_micro::micro::GetEvalInput(
       context, node, micro::lstm::full::kInputToCellWeightsTensor);
-  const TfLiteEvalTensor* input_to_output_weights = tflite::micro::GetEvalInput(
+  const TfLiteEvalTensor* input_to_output_weights = tflite_micro::micro::GetEvalInput(
       context, node, micro::lstm::full::kInputToOutputWeightsTensor);
   const TfLiteEvalTensor* recurrent_to_input_weights =
-      tflite::micro::GetEvalInput(
+      tflite_micro::micro::GetEvalInput(
           context, node, micro::lstm::full::kRecurrentToInputWeightsTensor);
   const TfLiteEvalTensor* recurrent_to_forget_weights =
-      tflite::micro::GetEvalInput(
+      tflite_micro::micro::GetEvalInput(
           context, node, micro::lstm::full::kRecurrentToForgetWeightsTensor);
   const TfLiteEvalTensor* recurrent_to_cell_weights =
-      tflite::micro::GetEvalInput(
+      tflite_micro::micro::GetEvalInput(
           context, node, micro::lstm::full::kRecurrentToCellWeightsTensor);
   const TfLiteEvalTensor* recurrent_to_output_weights =
-      tflite::micro::GetEvalInput(
+      tflite_micro::micro::GetEvalInput(
           context, node, micro::lstm::full::kRecurrentToOutputWeightsTensor);
   const TfLiteEvalTensor* cell_to_input_weights = context->GetEvalTensor(
       context,
@@ -1023,7 +1023,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
           node->inputs
               ->data[micro::lstm::full::kOutputLayerNormCoefficientsTensor]);
 
-  TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(
+  TfLiteEvalTensor* output = tflite_micro::micro::GetEvalOutput(
       context, node, micro::lstm::full::kOutputTensor);
 
   // Copy out the LSTM specific params so they can be passed in the function.
@@ -1115,7 +1115,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace ops
 
 TFLMRegistration Register_UNIDIRECTIONAL_SEQUENCE_LSTM() {
-  return tflite::micro::RegisterOp(ops::micro::Init, ops::micro::Prepare,
+  return tflite_micro::micro::RegisterOp(ops::micro::Init, ops::micro::Prepare,
                                    ops::micro::Eval);
 }
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/xtensa/xtensa_add.h b/tensorflow/lite/micro/kernels/xtensa/xtensa_add.h
index a2213393..20142fa5 100644
--- a/tensorflow/lite/micro/kernels/xtensa/xtensa_add.h
+++ b/tensorflow/lite/micro/kernels/xtensa/xtensa_add.h
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/c/builtin_op_data.h"
 #include "tensorflow/lite/c/common.h"
 #include "tensorflow/lite/micro/kernels/add.h"
-namespace tflite {
+namespace tflite_micro {
 
 struct XtensaAddOpData {
   OpDataAdd reference_op_data;
@@ -43,6 +43,6 @@ TfLiteStatus AddEvalQuantizedVision(TfLiteContext* context, TfLiteNode* node,
 
 #endif  // VISION_P6
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_XTENSA_XTENSA_ADD_H_
diff --git a/tensorflow/lite/micro/kernels/xtensa/xtensa_conv.h b/tensorflow/lite/micro/kernels/xtensa/xtensa_conv.h
index 16fbc9fd..a7ba3450 100644
--- a/tensorflow/lite/micro/kernels/xtensa/xtensa_conv.h
+++ b/tensorflow/lite/micro/kernels/xtensa/xtensa_conv.h
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/types.h"
 #include "tensorflow/lite/micro/kernels/conv.h"
 
-namespace tflite {
+namespace tflite_micro {
 struct XtensaConvOpData {
   OpDataConv reference_op_data;
 
@@ -84,6 +84,6 @@ TfLiteStatus ConvReferenceEvalInt16(TfLiteContext* context, TfLiteNode* node);
 void* ConvInitXtensa(TfLiteContext* context, const char* buffer, size_t length);
 TfLiteStatus ConvPrepareXtensa(TfLiteContext* context, TfLiteNode* node);
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_XTENSA_XTENSA_CONV_H_
diff --git a/tensorflow/lite/micro/kernels/xtensa/xtensa_depthwise_conv.h b/tensorflow/lite/micro/kernels/xtensa/xtensa_depthwise_conv.h
index 7d0d765c..8aca9de3 100644
--- a/tensorflow/lite/micro/kernels/xtensa/xtensa_depthwise_conv.h
+++ b/tensorflow/lite/micro/kernels/xtensa/xtensa_depthwise_conv.h
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/types.h"
 #include "tensorflow/lite/micro/kernels/depthwise_conv.h"
 
-namespace tflite {
+namespace tflite_micro {
 struct XtensaDepthwiseConvOpData {
   OpDataConv reference_op_data;
 
@@ -69,6 +69,6 @@ TfLiteStatus DepthwiseConvEvalVision(TfLiteContext* context, TfLiteNode* node,
 
 #endif  // VISION_P6
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_XTENSA_XTENSA_DEPTHWISE_CONV_H_
diff --git a/tensorflow/lite/micro/kernels/xtensa/xtensa_fully_connected.h b/tensorflow/lite/micro/kernels/xtensa/xtensa_fully_connected.h
index e030f0b3..fcedf829 100644
--- a/tensorflow/lite/micro/kernels/xtensa/xtensa_fully_connected.h
+++ b/tensorflow/lite/micro/kernels/xtensa/xtensa_fully_connected.h
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/fully_connected.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 struct XtensaFullyConnectedOpData {
   OpDataFullyConnected reference_op_data;
 
@@ -73,6 +73,6 @@ TfLiteStatus XtensaCalculateOpDataFullyConnected(
 TfLiteStatus XtensaPrepareFullyConnected(TfLiteContext* context,
                                          TfLiteNode* node);
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_XTENSA_XTENSA_FULLY_CONNECTED_H_
diff --git a/tensorflow/lite/micro/kernels/xtensa/xtensa_pad.h b/tensorflow/lite/micro/kernels/xtensa/xtensa_pad.h
index 12e386d1..101fc403 100644
--- a/tensorflow/lite/micro/kernels/xtensa/xtensa_pad.h
+++ b/tensorflow/lite/micro/kernels/xtensa/xtensa_pad.h
@@ -19,7 +19,7 @@ limitations under the License.
 
 #include "tensorflow/lite/c/common.h"
 #include "tensorflow/lite/kernels/internal/types.h"
-namespace tflite {
+namespace tflite_micro {
 
 struct OpDataPad {
   PadParams params;
@@ -44,6 +44,6 @@ TfLiteStatus PadEvalVision(const XtensaPadData& data,
                            TfLiteEvalTensor* output);
 #endif  // VISION_P6
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_XTENSA_XTENSA_PAD_H_
diff --git a/tensorflow/lite/micro/kernels/xtensa/xtensa_pooling.h b/tensorflow/lite/micro/kernels/xtensa/xtensa_pooling.h
index a2346e3a..1086f716 100644
--- a/tensorflow/lite/micro/kernels/xtensa/xtensa_pooling.h
+++ b/tensorflow/lite/micro/kernels/xtensa/xtensa_pooling.h
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/c/builtin_op_data.h"
 #include "tensorflow/lite/c/common.h"
 #include "tensorflow/lite/micro/kernels/pooling.h"
-namespace tflite {
+namespace tflite_micro {
 
 struct XtensaOpDataPooling {
   OpDataPooling reference_op_data;
@@ -71,6 +71,6 @@ TfLiteStatus MaxEvalQuantizedHifi(TfLiteContext* context, TfLiteNode* node,
 void* XtensaPoolingInit(TfLiteContext* context, const char* buffer,
                         size_t length);
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_XTENSA_XTENSA_POOLING_H_
diff --git a/tensorflow/lite/micro/kernels/xtensa/xtensa_reduce.h b/tensorflow/lite/micro/kernels/xtensa/xtensa_reduce.h
index 6f5f65a3..a1db4fe2 100644
--- a/tensorflow/lite/micro/kernels/xtensa/xtensa_reduce.h
+++ b/tensorflow/lite/micro/kernels/xtensa/xtensa_reduce.h
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/types.h"
 #include "tensorflow/lite/micro/kernels/reduce.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 struct XtensaReduceOpData {
   OpDataReduce reference_op_data;
@@ -42,6 +42,6 @@ TfLiteStatus ReduceEvalVision(const XtensaReduceOpData& data,
 
 #endif  // VISION_P6
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_XTENSA_XTENSA_REDUCE_H_
diff --git a/tensorflow/lite/micro/kernels/xtensa/xtensa_reshape.h b/tensorflow/lite/micro/kernels/xtensa/xtensa_reshape.h
index e472ef11..9dce2f61 100644
--- a/tensorflow/lite/micro/kernels/xtensa/xtensa_reshape.h
+++ b/tensorflow/lite/micro/kernels/xtensa/xtensa_reshape.h
@@ -20,7 +20,7 @@ limitations under the License.
 #include "tensorflow/lite/c/common.h"
 #include "tensorflow/lite/kernels/internal/types.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 #if defined(VISION_P6)
 
@@ -39,6 +39,6 @@ TfLiteStatus ReshapeEvalVision(const XtensaReshapeData& data,
                                TfLiteEvalTensor* output);
 #endif  // VISION_P6
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_XTENSA_XTENSA_RESHAPE_H_
diff --git a/tensorflow/lite/micro/kernels/xtensa/xtensa_softmax.h b/tensorflow/lite/micro/kernels/xtensa/xtensa_softmax.h
index d7e6a149..1e8e393a 100644
--- a/tensorflow/lite/micro/kernels/xtensa/xtensa_softmax.h
+++ b/tensorflow/lite/micro/kernels/xtensa/xtensa_softmax.h
@@ -20,7 +20,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/types.h"
 #include "tensorflow/lite/micro/kernels/softmax.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 #if defined(HIFI3) || defined(HIFI4) || defined(HIFI5)
 struct XtensaSoftmaxOpData {
@@ -53,6 +53,6 @@ TfLiteStatus SoftmaxEvalVision(TfLiteContext* context, TfLiteNode* node,
                                TfLiteEvalTensor* output);
 #endif  // defined(VISION_P6)
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_XTENSA_XTENSA_SOFTMAX_H_
diff --git a/tensorflow/lite/micro/kernels/xtensa/xtensa_svdf.h b/tensorflow/lite/micro/kernels/xtensa/xtensa_svdf.h
index 3c257ce4..002f1496 100644
--- a/tensorflow/lite/micro/kernels/xtensa/xtensa_svdf.h
+++ b/tensorflow/lite/micro/kernels/xtensa/xtensa_svdf.h
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/kernels/internal/types.h"
 #include "tensorflow/lite/micro/kernels/svdf.h"
 
-namespace tflite {
+namespace tflite_micro {
 #if defined(HIFIMINI)
 TfLiteStatus EvalIntegerSvdfHifimini(
     TfLiteContext* context, TfLiteNode* node,
@@ -34,6 +34,6 @@ TfLiteStatus EvalIntegerSvdfHifimini(
     OpDataSvdf data);
 #endif  // HIFIMINI
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_KERNELS_XTENSA_XTENSA_SVDF_H_
diff --git a/tensorflow/lite/micro/kernels/zeros_like.cc b/tensorflow/lite/micro/kernels/zeros_like.cc
index 597e50e3..ff688790 100644
--- a/tensorflow/lite/micro/kernels/zeros_like.cc
+++ b/tensorflow/lite/micro/kernels/zeros_like.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/kernels/kernel_util.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 constexpr int kInputTensor = 0;
@@ -52,23 +52,23 @@ void resetZeros(T* out, const int num_elements) {
 
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   TfLiteEvalTensor* output =
-      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
-  int flat_size = MatchingFlatSize(tflite::micro::GetTensorShape(input),
-                                   tflite::micro::GetTensorShape(output));
+      tflite_micro::micro::GetEvalOutput(context, node, kOutputTensor);
+  int flat_size = MatchingFlatSize(tflite_micro::micro::GetTensorShape(input),
+                                   tflite_micro::micro::GetTensorShape(output));
   switch (input->type) {
     case kTfLiteInt64:
-      resetZeros(tflite::micro::GetTensorData<int64_t>(output), flat_size);
+      resetZeros(tflite_micro::micro::GetTensorData<int64_t>(output), flat_size);
       break;
     case kTfLiteInt32:
-      resetZeros(tflite::micro::GetTensorData<int32_t>(output), flat_size);
+      resetZeros(tflite_micro::micro::GetTensorData<int32_t>(output), flat_size);
       break;
     case kTfLiteInt8:
-      resetZeros(tflite::micro::GetTensorData<int8_t>(output), flat_size);
+      resetZeros(tflite_micro::micro::GetTensorData<int8_t>(output), flat_size);
       break;
     case kTfLiteFloat32:
-      resetZeros(tflite::micro::GetTensorData<float>(output), flat_size);
+      resetZeros(tflite_micro::micro::GetTensorData<float>(output), flat_size);
       break;
     default:
       MicroPrintf(
@@ -82,7 +82,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
 }  // namespace
 
 TFLMRegistration Register_ZEROS_LIKE() {
-  return tflite::micro::RegisterOp(nullptr, Prepare, Eval);
+  return tflite_micro::micro::RegisterOp(nullptr, Prepare, Eval);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/kernels/zeros_like_test.cc b/tensorflow/lite/micro/kernels/zeros_like_test.cc
index 9a28cf40..52d82722 100644
--- a/tensorflow/lite/micro/kernels/zeros_like_test.cc
+++ b/tensorflow/lite/micro/kernels/zeros_like_test.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -57,7 +57,7 @@ void TestZerosLike(int* input_dims_data, const T* input_data,
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
@@ -66,7 +66,7 @@ TF_LITE_MICRO_TEST(TestZerosLikeFloat) {
   int input_dims[] = {2, 2, 3};
   const float input_values[] = {-2.0, -1.0, 0.0, 1.0, 2.0, 3.0};
   const float golden[] = {0.0, 0.0, 0.0, 0.0, 0.0, 0.0};
-  tflite::testing::TestZerosLike<float>(input_dims, input_values, golden,
+  tflite_micro::testing::TestZerosLike<float>(input_dims, input_values, golden,
                                         output_data);
 }
 
@@ -75,7 +75,7 @@ TF_LITE_MICRO_TEST(TestZerosLikeInt8) {
   int input_dims[] = {3, 1, 2, 3};
   const int8_t input_values[] = {-2, -1, 0, 1, 2, 3};
   const int8_t golden[] = {0, 0, 0, 0, 0, 0};
-  tflite::testing::TestZerosLike<int8_t>(input_dims, input_values, golden,
+  tflite_micro::testing::TestZerosLike<int8_t>(input_dims, input_values, golden,
                                          output_data);
 }
 
@@ -84,7 +84,7 @@ TF_LITE_MICRO_TEST(TestZerosLikeInt32) {
   int input_dims[] = {4, 1, 2, 2, 1};
   const int32_t input_values[] = {-2, -1, 0, 3};
   const int32_t golden[] = {0, 0, 0, 0};
-  tflite::testing::TestZerosLike<int32_t>(input_dims, input_values, golden,
+  tflite_micro::testing::TestZerosLike<int32_t>(input_dims, input_values, golden,
                                           output_data);
 }
 
@@ -93,7 +93,7 @@ TF_LITE_MICRO_TEST(TestZerosLikeInt64) {
   int input_dims[] = {4, 1, 2, 2, 1};
   const int64_t input_values[] = {-2, -1, 0, 3};
   const int64_t golden[] = {0, 0, 0, 0};
-  tflite::testing::TestZerosLike<int64_t>(input_dims, input_values, golden,
+  tflite_micro::testing::TestZerosLike<int64_t>(input_dims, input_values, golden,
                                           output_data);
 }
 
diff --git a/tensorflow/lite/micro/memory_arena_threshold_test.cc b/tensorflow/lite/micro/memory_arena_threshold_test.cc
index 3017f56b..9561f126 100644
--- a/tensorflow/lite/micro/memory_arena_threshold_test.cc
+++ b/tensorflow/lite/micro/memory_arena_threshold_test.cc
@@ -148,10 +148,10 @@ void EnsureAllocatedSizeThreshold(const char* allocation_type, size_t actual,
 }
 
 void ValidateModelAllocationThresholds(
-    const tflite::RecordingMicroAllocator& allocator,
+    const tflite_micro::RecordingMicroAllocator& allocator,
     const ModelAllocationThresholds& thresholds) {
   MicroPrintf("Overhead from RecordingMicroAllocator is %d",
-              tflite::RecordingMicroAllocator::GetDefaultTailUsage());
+              tflite_micro::RecordingMicroAllocator::GetDefaultTailUsage());
   allocator.PrintAllocations();
 
   EnsureAllocatedSizeThreshold(
@@ -167,27 +167,27 @@ void ValidateModelAllocationThresholds(
       "TfLiteEvalTensor",
       allocator
           .GetRecordedAllocation(
-              tflite::RecordedAllocationType::kTfLiteEvalTensorData)
+              tflite_micro::RecordedAllocationType::kTfLiteEvalTensorData)
           .used_bytes,
       sizeof(TfLiteEvalTensor) * thresholds.tensor_count);
   EnsureAllocatedSizeThreshold(
       "VariableBufferData",
       allocator
           .GetRecordedAllocation(
-              tflite::RecordedAllocationType::kTfLiteTensorVariableBufferData)
+              tflite_micro::RecordedAllocationType::kTfLiteTensorVariableBufferData)
           .used_bytes,
       thresholds.tensor_variable_buffer_data_size);
   EnsureAllocatedSizeThreshold(
       "PersistentTfLiteTensor",
       allocator
           .GetRecordedAllocation(
-              tflite::RecordedAllocationType::kPersistentTfLiteTensorData)
+              tflite_micro::RecordedAllocationType::kPersistentTfLiteTensorData)
           .used_bytes,
       thresholds.persistent_tflite_tensor_data_size);
   EnsureAllocatedSizeThreshold(
       "PersistentTfliteTensorQuantizationData",
       allocator
-          .GetRecordedAllocation(tflite::RecordedAllocationType::
+          .GetRecordedAllocation(tflite_micro::RecordedAllocationType::
                                      kPersistentTfLiteTensorQuantizationData)
           .used_bytes,
       thresholds.persistent_tflite_tensor_quantization_data_size);
@@ -195,22 +195,22 @@ void ValidateModelAllocationThresholds(
       "PersistentBufferData",
       allocator
           .GetRecordedAllocation(
-              tflite::RecordedAllocationType::kPersistentBufferData)
+              tflite_micro::RecordedAllocationType::kPersistentBufferData)
           .used_bytes,
       thresholds.persistent_buffer_data);
   EnsureAllocatedSizeThreshold(
       "NodeAndRegistration",
       allocator
           .GetRecordedAllocation(
-              tflite::RecordedAllocationType::kNodeAndRegistrationArray)
+              tflite_micro::RecordedAllocationType::kNodeAndRegistrationArray)
           .used_bytes,
-      sizeof(tflite::NodeAndRegistration) *
+      sizeof(tflite_micro::NodeAndRegistration) *
           thresholds.node_and_registration_count);
 
   // Ensure tail allocation recording is not missing any large chunks:
   size_t tail_est_length = sizeof(TfLiteEvalTensor) * thresholds.tensor_count +
                            thresholds.tensor_variable_buffer_data_size +
-                           sizeof(tflite::NodeAndRegistration) *
+                           sizeof(tflite_micro::NodeAndRegistration) *
                                thresholds.node_and_registration_count +
                            thresholds.op_runtime_data_size;
   TF_LITE_MICRO_EXPECT_LE(thresholds.tail_alloc_size - tail_est_length,
@@ -222,17 +222,17 @@ void ValidateModelAllocationThresholds(
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(TestKeywordModelMemoryThreshold) {
-  tflite::MicroMutableOpResolver<4> op_resolver;
+  tflite_micro::MicroMutableOpResolver<4> op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(
-      op_resolver.AddFullyConnected(tflite::Register_FULLY_CONNECTED_INT8()),
+      op_resolver.AddFullyConnected(tflite_micro::Register_FULLY_CONNECTED_INT8()),
       kTfLiteOk);
   TF_LITE_MICRO_EXPECT_EQ(op_resolver.AddQuantize(), kTfLiteOk);
   TF_LITE_MICRO_EXPECT_EQ(
-      op_resolver.AddSoftmax(tflite::Register_SOFTMAX_INT8_INT16()), kTfLiteOk);
-  TF_LITE_MICRO_EXPECT_EQ(op_resolver.AddSvdf(tflite::Register_SVDF_INT8()),
+      op_resolver.AddSoftmax(tflite_micro::Register_SOFTMAX_INT8_INT16()), kTfLiteOk);
+  TF_LITE_MICRO_EXPECT_EQ(op_resolver.AddSvdf(tflite_micro::Register_SVDF_INT8()),
                           kTfLiteOk);
-  tflite::RecordingMicroInterpreter interpreter(
-      tflite::GetModel(g_keyword_scrambled_model_data), op_resolver,
+  tflite_micro::RecordingMicroInterpreter interpreter(
+      tflite_micro::GetModel(g_keyword_scrambled_model_data), op_resolver,
       keyword_model_tensor_arena, kKeywordModelTensorArenaSize);
 
   interpreter.AllocateTensors();
@@ -243,11 +243,11 @@ TF_LITE_MICRO_TEST(TestKeywordModelMemoryThreshold) {
       kKeywordModelNodeAndRegistrationCount;
   thresholds.total_alloc_size =
       kKeywordModelOnlyTotalSize +
-      tflite::RecordingMicroAllocator::GetDefaultTailUsage();
+      tflite_micro::RecordingMicroAllocator::GetDefaultTailUsage();
   thresholds.head_alloc_size = kKeywordModelHeadSize;
   thresholds.tail_alloc_size =
       kKeywordModelOnlyTailSize +
-      tflite::RecordingMicroAllocator::GetDefaultTailUsage();
+      tflite_micro::RecordingMicroAllocator::GetDefaultTailUsage();
   thresholds.tensor_variable_buffer_data_size =
       kKeywordModelTfLiteTensorVariableBufferDataSize;
   thresholds.op_runtime_data_size = kKeywordModelOpRuntimeDataSize;
@@ -262,7 +262,7 @@ TF_LITE_MICRO_TEST(TestKeywordModelMemoryThreshold) {
 }
 
 TF_LITE_MICRO_TEST(TestConvModelMemoryThreshold) {
-  tflite::MicroMutableOpResolver<6> op_resolver;
+  tflite_micro::MicroMutableOpResolver<6> op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(op_resolver.AddConv2D(), kTfLiteOk);
   TF_LITE_MICRO_EXPECT_EQ(op_resolver.AddQuantize(), kTfLiteOk);
   TF_LITE_MICRO_EXPECT_EQ(op_resolver.AddMaxPool2D(), kTfLiteOk);
@@ -270,8 +270,8 @@ TF_LITE_MICRO_TEST(TestConvModelMemoryThreshold) {
   TF_LITE_MICRO_EXPECT_EQ(op_resolver.AddFullyConnected(), kTfLiteOk);
   TF_LITE_MICRO_EXPECT_EQ(op_resolver.AddDequantize(), kTfLiteOk);
 
-  tflite::RecordingMicroInterpreter interpreter(
-      tflite::GetModel(kTestConvModelData), op_resolver, test_conv_tensor_arena,
+  tflite_micro::RecordingMicroInterpreter interpreter(
+      tflite_micro::GetModel(kTestConvModelData), op_resolver, test_conv_tensor_arena,
       kTestConvModelArenaSize);
 
   interpreter.AllocateTensors();
@@ -282,11 +282,11 @@ TF_LITE_MICRO_TEST(TestConvModelMemoryThreshold) {
       kTestConvModelNodeAndRegistrationCount;
   thresholds.total_alloc_size =
       kTestConvModelOnlyTotalSize +
-      tflite::RecordingMicroAllocator::GetDefaultTailUsage();
+      tflite_micro::RecordingMicroAllocator::GetDefaultTailUsage();
   thresholds.head_alloc_size = kTestConvModelHeadSize;
   thresholds.tail_alloc_size =
       kTestConvModelOnlyTailSize +
-      tflite::RecordingMicroAllocator::GetDefaultTailUsage();
+      tflite_micro::RecordingMicroAllocator::GetDefaultTailUsage();
   thresholds.op_runtime_data_size = kTestConvModelOpRuntimeDataSize;
   thresholds.persistent_buffer_data = kTestConvModelPersistentBufferDataSize;
   thresholds.persistent_tflite_tensor_data_size =
diff --git a/tensorflow/lite/micro/memory_helpers.cc b/tensorflow/lite/micro/memory_helpers.cc
index 685f04b2..761270b9 100644
--- a/tensorflow/lite/micro/memory_helpers.cc
+++ b/tensorflow/lite/micro/memory_helpers.cc
@@ -24,7 +24,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/tflite_bridge/flatbuffer_conversions_bridge.h"
 #include "tensorflow/lite/schema/schema_generated.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 uint8_t* AlignPointerUp(uint8_t* data, size_t alignment) {
   std::uintptr_t data_as_uintptr_t = reinterpret_cast<std::uintptr_t>(data);
@@ -98,7 +98,7 @@ TfLiteStatus TfLiteTypeSizeOf(TfLiteType type, size_t* size) {
   return kTfLiteOk;
 }
 
-TfLiteStatus BytesRequiredForTensor(const tflite::Tensor& flatbuffer_tensor,
+TfLiteStatus BytesRequiredForTensor(const tflite_micro::Tensor& flatbuffer_tensor,
                                     size_t* bytes, size_t* type_size) {
   int element_count = 1;
   // If flatbuffer_tensor.shape == nullptr, then flatbuffer_tensor is a scalar
@@ -149,7 +149,7 @@ TfLiteStatus AllocateOutputDimensionsFromInput(TfLiteContext* context,
 
   size_t size = 0;
   TfLiteTypeSizeOf(input->type, &size);
-  const int dimensions_count = tflite::GetTensorShape(input).DimensionsCount();
+  const int dimensions_count = tflite_micro::GetTensorShape(input).DimensionsCount();
   for (int i = 0; i < dimensions_count; i++) {
     size *= input->dims->data[i];
   }
@@ -168,4 +168,4 @@ TfLiteStatus AllocateOutputDimensionsFromInput(TfLiteContext* context,
   return kTfLiteOk;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/memory_helpers.h b/tensorflow/lite/micro/memory_helpers.h
index f3392e47..51186064 100644
--- a/tensorflow/lite/micro/memory_helpers.h
+++ b/tensorflow/lite/micro/memory_helpers.h
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/c/common.h"
 #include "tensorflow/lite/schema/schema_generated.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // Returns the next pointer address aligned to the given alignment.
 uint8_t* AlignPointerUp(uint8_t* data, size_t alignment);
@@ -43,7 +43,7 @@ size_t AlignSizeUp(size_t count = 1) {
 TfLiteStatus TfLiteTypeSizeOf(TfLiteType type, size_t* size);
 
 // How many bytes are needed to hold a tensor's contents.
-TfLiteStatus BytesRequiredForTensor(const tflite::Tensor& flatbuffer_tensor,
+TfLiteStatus BytesRequiredForTensor(const tflite_micro::Tensor& flatbuffer_tensor,
                                     size_t* bytes, size_t* type_size);
 
 // How many bytes are used in a TfLiteEvalTensor instance. The byte length is
@@ -59,6 +59,6 @@ TfLiteStatus AllocateOutputDimensionsFromInput(TfLiteContext* context,
                                                const TfLiteTensor* input2,
                                                TfLiteTensor* output);
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_MEMORY_HELPERS_H_
diff --git a/tensorflow/lite/micro/memory_helpers_test.cc b/tensorflow/lite/micro/memory_helpers_test.cc
index e44c5866..c88a585f 100644
--- a/tensorflow/lite/micro/memory_helpers_test.cc
+++ b/tensorflow/lite/micro/memory_helpers_test.cc
@@ -38,32 +38,32 @@ TF_LITE_MICRO_TESTS_BEGIN
 TF_LITE_MICRO_TEST(TestAlignPointerUp) {
   uint8_t* input0 = reinterpret_cast<uint8_t*>(0);
 
-  uint8_t* input0_aligned1 = tflite::AlignPointerUp(input0, 1);
+  uint8_t* input0_aligned1 = tflite_micro::AlignPointerUp(input0, 1);
   TF_LITE_MICRO_EXPECT(input0 == input0_aligned1);
 
-  uint8_t* input0_aligned2 = tflite::AlignPointerUp(input0, 2);
+  uint8_t* input0_aligned2 = tflite_micro::AlignPointerUp(input0, 2);
   TF_LITE_MICRO_EXPECT(input0 == input0_aligned2);
 
-  uint8_t* input0_aligned3 = tflite::AlignPointerUp(input0, 3);
+  uint8_t* input0_aligned3 = tflite_micro::AlignPointerUp(input0, 3);
   TF_LITE_MICRO_EXPECT(input0 == input0_aligned3);
 
-  uint8_t* input0_aligned16 = tflite::AlignPointerUp(input0, 16);
+  uint8_t* input0_aligned16 = tflite_micro::AlignPointerUp(input0, 16);
   TF_LITE_MICRO_EXPECT(input0 == input0_aligned16);
 
   uint8_t* input23 = reinterpret_cast<uint8_t*>(23);
 
-  uint8_t* input23_aligned1 = tflite::AlignPointerUp(input23, 1);
+  uint8_t* input23_aligned1 = tflite_micro::AlignPointerUp(input23, 1);
   TF_LITE_MICRO_EXPECT(input23 == input23_aligned1);
 
-  uint8_t* input23_aligned2 = tflite::AlignPointerUp(input23, 2);
+  uint8_t* input23_aligned2 = tflite_micro::AlignPointerUp(input23, 2);
   uint8_t* expected23_aligned2 = reinterpret_cast<uint8_t*>(24);
   TF_LITE_MICRO_EXPECT(expected23_aligned2 == input23_aligned2);
 
-  uint8_t* input23_aligned3 = tflite::AlignPointerUp(input23, 3);
+  uint8_t* input23_aligned3 = tflite_micro::AlignPointerUp(input23, 3);
   uint8_t* expected23_aligned3 = reinterpret_cast<uint8_t*>(24);
   TF_LITE_MICRO_EXPECT(expected23_aligned3 == input23_aligned3);
 
-  uint8_t* input23_aligned16 = tflite::AlignPointerUp(input23, 16);
+  uint8_t* input23_aligned16 = tflite_micro::AlignPointerUp(input23, 16);
   uint8_t* expected23_aligned16 = reinterpret_cast<uint8_t*>(32);
   TF_LITE_MICRO_EXPECT(expected23_aligned16 == input23_aligned16);
 }
@@ -71,46 +71,46 @@ TF_LITE_MICRO_TEST(TestAlignPointerUp) {
 TF_LITE_MICRO_TEST(TestAlignPointerDown) {
   uint8_t* input0 = reinterpret_cast<uint8_t*>(0);
 
-  uint8_t* input0_aligned1 = tflite::AlignPointerDown(input0, 1);
+  uint8_t* input0_aligned1 = tflite_micro::AlignPointerDown(input0, 1);
   TF_LITE_MICRO_EXPECT(input0 == input0_aligned1);
 
-  uint8_t* input0_aligned2 = tflite::AlignPointerDown(input0, 2);
+  uint8_t* input0_aligned2 = tflite_micro::AlignPointerDown(input0, 2);
   TF_LITE_MICRO_EXPECT(input0 == input0_aligned2);
 
-  uint8_t* input0_aligned3 = tflite::AlignPointerDown(input0, 3);
+  uint8_t* input0_aligned3 = tflite_micro::AlignPointerDown(input0, 3);
   TF_LITE_MICRO_EXPECT(input0 == input0_aligned3);
 
-  uint8_t* input0_aligned16 = tflite::AlignPointerDown(input0, 16);
+  uint8_t* input0_aligned16 = tflite_micro::AlignPointerDown(input0, 16);
   TF_LITE_MICRO_EXPECT(input0 == input0_aligned16);
 
   uint8_t* input23 = reinterpret_cast<uint8_t*>(23);
 
-  uint8_t* input23_aligned1 = tflite::AlignPointerDown(input23, 1);
+  uint8_t* input23_aligned1 = tflite_micro::AlignPointerDown(input23, 1);
   TF_LITE_MICRO_EXPECT(input23 == input23_aligned1);
 
-  uint8_t* input23_aligned2 = tflite::AlignPointerDown(input23, 2);
+  uint8_t* input23_aligned2 = tflite_micro::AlignPointerDown(input23, 2);
   uint8_t* expected23_aligned2 = reinterpret_cast<uint8_t*>(22);
   TF_LITE_MICRO_EXPECT(expected23_aligned2 == input23_aligned2);
 
-  uint8_t* input23_aligned3 = tflite::AlignPointerDown(input23, 3);
+  uint8_t* input23_aligned3 = tflite_micro::AlignPointerDown(input23, 3);
   uint8_t* expected23_aligned3 = reinterpret_cast<uint8_t*>(21);
   TF_LITE_MICRO_EXPECT(expected23_aligned3 == input23_aligned3);
 
-  uint8_t* input23_aligned16 = tflite::AlignPointerDown(input23, 16);
+  uint8_t* input23_aligned16 = tflite_micro::AlignPointerDown(input23, 16);
   uint8_t* expected23_aligned16 = reinterpret_cast<uint8_t*>(16);
   TF_LITE_MICRO_EXPECT(expected23_aligned16 == input23_aligned16);
 }
 
 TF_LITE_MICRO_TEST(TestAlignSizeUp) {
-  TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(1), tflite::AlignSizeUp(1, 1));
-  TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(2), tflite::AlignSizeUp(1, 2));
-  TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(3), tflite::AlignSizeUp(1, 3));
-  TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(16), tflite::AlignSizeUp(1, 16));
-
-  TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(23), tflite::AlignSizeUp(23, 1));
-  TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(24), tflite::AlignSizeUp(23, 2));
-  TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(24), tflite::AlignSizeUp(23, 3));
-  TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(32), tflite::AlignSizeUp(23, 16));
+  TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(1), tflite_micro::AlignSizeUp(1, 1));
+  TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(2), tflite_micro::AlignSizeUp(1, 2));
+  TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(3), tflite_micro::AlignSizeUp(1, 3));
+  TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(16), tflite_micro::AlignSizeUp(1, 16));
+
+  TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(23), tflite_micro::AlignSizeUp(23, 1));
+  TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(24), tflite_micro::AlignSizeUp(23, 2));
+  TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(24), tflite_micro::AlignSizeUp(23, 3));
+  TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(32), tflite_micro::AlignSizeUp(23, 16));
 }
 
 TF_LITE_MICRO_TEST(TestTemplatedAlignSizeUp) {
@@ -121,82 +121,82 @@ TF_LITE_MICRO_TEST(TestTemplatedAlignSizeUp) {
   };
 
   TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(32),
-                          tflite::AlignSizeUp<TestAlignSizeUp>());
+                          tflite_micro::AlignSizeUp<TestAlignSizeUp>());
   TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(64),
-                          tflite::AlignSizeUp<TestAlignSizeUp>(2));
+                          tflite_micro::AlignSizeUp<TestAlignSizeUp>(2));
 }
 
 TF_LITE_MICRO_TEST(TestTypeSizeOf) {
   size_t size;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::TfLiteTypeSizeOf(kTfLiteFloat16, &size));
+                          tflite_micro::TfLiteTypeSizeOf(kTfLiteFloat16, &size));
   TF_LITE_MICRO_EXPECT_EQ(sizeof(int16_t), size);
 
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::TfLiteTypeSizeOf(kTfLiteFloat32, &size));
+                          tflite_micro::TfLiteTypeSizeOf(kTfLiteFloat32, &size));
   TF_LITE_MICRO_EXPECT_EQ(sizeof(float), size);
 
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::TfLiteTypeSizeOf(kTfLiteFloat64, &size));
+                          tflite_micro::TfLiteTypeSizeOf(kTfLiteFloat64, &size));
   TF_LITE_MICRO_EXPECT_EQ(sizeof(double), size);
 
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::TfLiteTypeSizeOf(kTfLiteInt16, &size));
+                          tflite_micro::TfLiteTypeSizeOf(kTfLiteInt16, &size));
   TF_LITE_MICRO_EXPECT_EQ(sizeof(int16_t), size);
 
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::TfLiteTypeSizeOf(kTfLiteInt32, &size));
+                          tflite_micro::TfLiteTypeSizeOf(kTfLiteInt32, &size));
   TF_LITE_MICRO_EXPECT_EQ(sizeof(int32_t), size);
 
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::TfLiteTypeSizeOf(kTfLiteUInt32, &size));
+                          tflite_micro::TfLiteTypeSizeOf(kTfLiteUInt32, &size));
   TF_LITE_MICRO_EXPECT_EQ(sizeof(uint32_t), size);
 
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::TfLiteTypeSizeOf(kTfLiteUInt8, &size));
+                          tflite_micro::TfLiteTypeSizeOf(kTfLiteUInt8, &size));
   TF_LITE_MICRO_EXPECT_EQ(sizeof(uint8_t), size);
 
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::TfLiteTypeSizeOf(kTfLiteInt8, &size));
+                          tflite_micro::TfLiteTypeSizeOf(kTfLiteInt8, &size));
   TF_LITE_MICRO_EXPECT_EQ(sizeof(int8_t), size);
 
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::TfLiteTypeSizeOf(kTfLiteInt64, &size));
+                          tflite_micro::TfLiteTypeSizeOf(kTfLiteInt64, &size));
   TF_LITE_MICRO_EXPECT_EQ(sizeof(int64_t), size);
 
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::TfLiteTypeSizeOf(kTfLiteUInt64, &size));
+                          tflite_micro::TfLiteTypeSizeOf(kTfLiteUInt64, &size));
   TF_LITE_MICRO_EXPECT_EQ(sizeof(uint64_t), size);
 
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::TfLiteTypeSizeOf(kTfLiteBool, &size));
+                          tflite_micro::TfLiteTypeSizeOf(kTfLiteBool, &size));
   TF_LITE_MICRO_EXPECT_EQ(sizeof(bool), size);
 
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::TfLiteTypeSizeOf(kTfLiteComplex64, &size));
+                          tflite_micro::TfLiteTypeSizeOf(kTfLiteComplex64, &size));
   TF_LITE_MICRO_EXPECT_EQ(sizeof(float) * 2, size);
 
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::TfLiteTypeSizeOf(kTfLiteComplex128, &size));
+                          tflite_micro::TfLiteTypeSizeOf(kTfLiteComplex128, &size));
   TF_LITE_MICRO_EXPECT_EQ(sizeof(double) * 2, size);
 
   TF_LITE_MICRO_EXPECT_NE(
-      kTfLiteOk, tflite::TfLiteTypeSizeOf(static_cast<TfLiteType>(-1), &size));
+      kTfLiteOk, tflite_micro::TfLiteTypeSizeOf(static_cast<TfLiteType>(-1), &size));
 }
 
 TF_LITE_MICRO_TEST(TestBytesRequiredForTensor) {
-  const tflite::Tensor* tensor100 =
-      tflite::testing::Create1dFlatbufferTensor(100);
+  const tflite_micro::Tensor* tensor100 =
+      tflite_micro::testing::Create1dFlatbufferTensor(100);
   size_t bytes;
   size_t type_size;
-  TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, tflite::BytesRequiredForTensor(
+  TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, tflite_micro::BytesRequiredForTensor(
                                          *tensor100, &bytes, &type_size));
   TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(400), bytes);
   TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(4), type_size);
 
-  const tflite::Tensor* tensor200 =
-      tflite::testing::Create1dFlatbufferTensor(200);
-  TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, tflite::BytesRequiredForTensor(
+  const tflite_micro::Tensor* tensor200 =
+      tflite_micro::testing::Create1dFlatbufferTensor(200);
+  TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, tflite_micro::BytesRequiredForTensor(
                                          *tensor200, &bytes, &type_size));
   TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(800), bytes);
   TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(4), type_size);
@@ -207,19 +207,19 @@ TF_LITE_MICRO_TEST(TestAllocateOutputDimensionsFromInput) {
   int input1_dims[] = {1, 1};
   int input2_dims[] = {kDimsLen, 5, 5, 5, 5};
   int output_dims[] = {0, 0, 0, 0, 0};
-  TfLiteTensor input_tensor1 = tflite::testing::CreateTensor<int32_t>(
-      nullptr, tflite::testing::IntArrayFromInts(input1_dims));
-  TfLiteTensor input_tensor2 = tflite::testing::CreateTensor<int32_t>(
-      nullptr, tflite::testing::IntArrayFromInts(input2_dims));
-  TfLiteTensor output_tensor = tflite::testing::CreateTensor<int32_t>(
-      nullptr, tflite::testing::IntArrayFromInts(output_dims));
+  TfLiteTensor input_tensor1 = tflite_micro::testing::CreateTensor<int32_t>(
+      nullptr, tflite_micro::testing::IntArrayFromInts(input1_dims));
+  TfLiteTensor input_tensor2 = tflite_micro::testing::CreateTensor<int32_t>(
+      nullptr, tflite_micro::testing::IntArrayFromInts(input2_dims));
+  TfLiteTensor output_tensor = tflite_micro::testing::CreateTensor<int32_t>(
+      nullptr, tflite_micro::testing::IntArrayFromInts(output_dims));
   TfLiteContext context;
   // Only need to allocate space for output_tensor.dims.  Use a simple
   // fake allocator.
   context.AllocatePersistentBuffer = FakeAllocatePersistentBuffer;
 
   TF_LITE_MICRO_EXPECT_EQ(
-      kTfLiteOk, tflite::AllocateOutputDimensionsFromInput(
+      kTfLiteOk, tflite_micro::AllocateOutputDimensionsFromInput(
                      &context, &input_tensor1, &input_tensor2, &output_tensor));
 
   TF_LITE_MICRO_EXPECT_EQ(output_tensor.bytes, input_tensor2.bytes);
@@ -232,7 +232,7 @@ TF_LITE_MICRO_TEST(TestAllocateOutputDimensionsFromInput) {
   // Output tensor size must be 0 to allocate output dimensions from input.
   output_tensor.dims->size = 0;
   TF_LITE_MICRO_EXPECT_EQ(
-      kTfLiteOk, tflite::AllocateOutputDimensionsFromInput(
+      kTfLiteOk, tflite_micro::AllocateOutputDimensionsFromInput(
                      &context, &input_tensor2, &input_tensor1, &output_tensor));
   for (int i = 0; i < kDimsLen; i++) {
     TF_LITE_MICRO_EXPECT_EQ(input_tensor2.dims->data[i],
diff --git a/tensorflow/lite/micro/memory_planner/greedy_memory_planner.cc b/tensorflow/lite/micro/memory_planner/greedy_memory_planner.cc
index a087b236..0a45d270 100644
--- a/tensorflow/lite/micro/memory_planner/greedy_memory_planner.cc
+++ b/tensorflow/lite/micro/memory_planner/greedy_memory_planner.cc
@@ -17,7 +17,7 @@ limitations under the License.
 
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -444,4 +444,4 @@ bool GreedyMemoryPlanner::DoAnyBuffersOverlap() {
   return were_overlaps_found;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/memory_planner/greedy_memory_planner.h b/tensorflow/lite/micro/memory_planner/greedy_memory_planner.h
index b2cdb617..f789fbcb 100644
--- a/tensorflow/lite/micro/memory_planner/greedy_memory_planner.h
+++ b/tensorflow/lite/micro/memory_planner/greedy_memory_planner.h
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/compatibility.h"
 #include "tensorflow/lite/micro/memory_planner/micro_memory_planner.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 constexpr int kOnlinePlannedBuffer = -1;
 
@@ -112,6 +112,8 @@ class GreedyMemoryPlanner : public MicroMemoryPlanner {
   // that aren't being used during a phase of invocation are overwritten.
   bool preserves_all_tensors() const override { return false; }
 
+  TF_LITE_REMOVE_VIRTUAL_DELETE
+
  private:
   // Whether a buffer is active in a given time range.
   bool DoesEntryOverlapInTime(const ListEntry* entry, const int first_time_used,
@@ -161,10 +163,8 @@ class GreedyMemoryPlanner : public MicroMemoryPlanner {
 
   // Whether buffers have been added since the last plan was calculated.
   bool need_to_calculate_offsets_;
-
-  TF_LITE_REMOVE_VIRTUAL_DELETE
 };
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_MEMORY_PLANNER_GREEDY_MEMORY_PLANNER_H_
diff --git a/tensorflow/lite/micro/memory_planner/greedy_memory_planner_test.cc b/tensorflow/lite/micro/memory_planner/greedy_memory_planner_test.cc
index 1be1ca44..5490a5d6 100644
--- a/tensorflow/lite/micro/memory_planner/greedy_memory_planner_test.cc
+++ b/tensorflow/lite/micro/memory_planner/greedy_memory_planner_test.cc
@@ -17,11 +17,11 @@ limitations under the License.
 
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 // We don't declare this in the header since it's not a public interface, but we
 // need to call it to test it, so declare it here instead.
 void ReverseSortInPlace(int* values, int* ids, int size);
-}  // namespace tflite
+}  // namespace tflite_micro
 
 namespace {
 constexpr int kScratchBufferSize = 4096;
@@ -36,7 +36,7 @@ TF_LITE_MICRO_TEST(TestReverseSortInPlace) {
   int a_ids[a_size] = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9};
   const int a_expected_values[a_size] = {10, 9, 8, 7, 6, 5, 4, 3, 2, 1};
   const int a_expected_ids[a_size] = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9};
-  tflite::ReverseSortInPlace(a_values, a_ids, a_size);
+  tflite_micro::ReverseSortInPlace(a_values, a_ids, a_size);
   for (int i = 0; i < a_size; ++i) {
     TF_LITE_MICRO_EXPECT_EQ(a_expected_values[i], a_values[i]);
     TF_LITE_MICRO_EXPECT_EQ(a_expected_ids[i], a_ids[i]);
@@ -47,7 +47,7 @@ TF_LITE_MICRO_TEST(TestReverseSortInPlace) {
   int b_ids[b_size] = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9};
   const int b_expected_values[b_size] = {10, 9, 8, 7, 6, 5, 4, 3, 2, 1};
   const int b_expected_ids[b_size] = {9, 8, 7, 6, 5, 4, 3, 2, 1, 0};
-  tflite::ReverseSortInPlace(b_values, b_ids, b_size);
+  tflite_micro::ReverseSortInPlace(b_values, b_ids, b_size);
   for (int i = 0; i < b_size; ++i) {
     TF_LITE_MICRO_EXPECT_EQ(b_expected_values[i], b_values[i]);
     TF_LITE_MICRO_EXPECT_EQ(b_expected_ids[i], b_ids[i]);
@@ -80,7 +80,7 @@ TF_LITE_MICRO_TEST(TestReverseSortInPlace) {
       14, 24, 34, 44, 54, 64, 74, 84, 94, 3,  13, 23, 33, 43, 53, 63, 73,
       83, 93, 2,  12, 22, 32, 42, 52, 62, 72, 82, 92, 1,  11, 21, 31, 41,
       51, 61, 71, 81, 91, 0,  10, 20, 30, 40, 50, 60, 70, 80, 90};
-  tflite::ReverseSortInPlace(c_values, c_ids, c_size);
+  tflite_micro::ReverseSortInPlace(c_values, c_ids, c_size);
   for (int i = 0; i < c_size; ++i) {
     TF_LITE_MICRO_EXPECT_EQ(c_expected_values[i], c_values[i]);
     TF_LITE_MICRO_EXPECT_EQ(c_expected_ids[i], c_ids[i]);
@@ -88,7 +88,7 @@ TF_LITE_MICRO_TEST(TestReverseSortInPlace) {
 }
 
 TF_LITE_MICRO_TEST(TestGreedyBasics) {
-  tflite::GreedyMemoryPlanner planner;
+  tflite_micro::GreedyMemoryPlanner planner;
   planner.Init(g_scratch_buffer, kScratchBufferSize);
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, planner.AddBuffer(10, 0, 1));
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, planner.AddBuffer(20, 2, 3));
@@ -107,7 +107,7 @@ TF_LITE_MICRO_TEST(TestGreedyBasics) {
 }
 
 TF_LITE_MICRO_TEST(TestGreedyMedium) {
-  tflite::GreedyMemoryPlanner planner;
+  tflite_micro::GreedyMemoryPlanner planner;
   planner.Init(g_scratch_buffer, kScratchBufferSize);
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, planner.AddBuffer(10, 0, 1));
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, planner.AddBuffer(20, 1, 2));
@@ -140,7 +140,7 @@ TF_LITE_MICRO_TEST(TestGreedyMedium) {
 }
 
 TF_LITE_MICRO_TEST(TestPersonDetectionModel) {
-  tflite::GreedyMemoryPlanner planner;
+  tflite_micro::GreedyMemoryPlanner planner;
   planner.Init(g_scratch_buffer, kScratchBufferSize);
   // These buffer sizes and time ranges are taken from the 250KB MobileNet model
   // used in the person detection example.
@@ -186,7 +186,7 @@ TF_LITE_MICRO_TEST(TestPersonDetectionModel) {
 }
 
 TF_LITE_MICRO_TEST(TestOverlapCase) {
-  tflite::GreedyMemoryPlanner planner;
+  tflite_micro::GreedyMemoryPlanner planner;
   planner.Init(g_scratch_buffer, kScratchBufferSize);
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, planner.AddBuffer(100, 0, 1));
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, planner.AddBuffer(50, 2, 3));
@@ -203,7 +203,7 @@ TF_LITE_MICRO_TEST(TestOverlapCase) {
 TF_LITE_MICRO_TEST(TestSmallScratch) {
   constexpr int scratch_buffer_size = 40;
   unsigned char scratch_buffer[scratch_buffer_size];
-  tflite::GreedyMemoryPlanner planner;
+  tflite_micro::GreedyMemoryPlanner planner;
   planner.Init(scratch_buffer, scratch_buffer_size);
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, planner.AddBuffer(100, 0, 1));
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteError, planner.AddBuffer(50, 2, 3));
diff --git a/tensorflow/lite/micro/memory_planner/linear_memory_planner.cc b/tensorflow/lite/micro/memory_planner/linear_memory_planner.cc
index 5c6afb54..5d161d53 100644
--- a/tensorflow/lite/micro/memory_planner/linear_memory_planner.cc
+++ b/tensorflow/lite/micro/memory_planner/linear_memory_planner.cc
@@ -17,7 +17,7 @@ limitations under the License.
 
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 LinearMemoryPlanner::LinearMemoryPlanner()
     : current_buffer_count_(0), next_free_offset_(0) {}
@@ -50,4 +50,4 @@ TfLiteStatus LinearMemoryPlanner::GetOffsetForBuffer(int buffer_index,
   return kTfLiteOk;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/memory_planner/linear_memory_planner.h b/tensorflow/lite/micro/memory_planner/linear_memory_planner.h
index 9850569f..c701ceef 100644
--- a/tensorflow/lite/micro/memory_planner/linear_memory_planner.h
+++ b/tensorflow/lite/micro/memory_planner/linear_memory_planner.h
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/compatibility.h"
 #include "tensorflow/lite/micro/memory_planner/micro_memory_planner.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // The simplest possible memory planner that just lays out all buffers at
 // increasing offsets without trying to reuse memory.
@@ -48,6 +48,6 @@ class LinearMemoryPlanner : public MicroMemoryPlanner {
   TF_LITE_REMOVE_VIRTUAL_DELETE
 };
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_MEMORY_PLANNER_LINEAR_MEMORY_PLANNER_H_
diff --git a/tensorflow/lite/micro/memory_planner/linear_memory_planner_test.cc b/tensorflow/lite/micro/memory_planner/linear_memory_planner_test.cc
index cf4c4381..dc5eda0f 100644
--- a/tensorflow/lite/micro/memory_planner/linear_memory_planner_test.cc
+++ b/tensorflow/lite/micro/memory_planner/linear_memory_planner_test.cc
@@ -20,7 +20,7 @@ limitations under the License.
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(TestBasics) {
-  tflite::LinearMemoryPlanner planner;
+  tflite_micro::LinearMemoryPlanner planner;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, planner.AddBuffer(10, 0, 1));
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, planner.AddBuffer(20, 1, 2));
   TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(30),
@@ -35,7 +35,7 @@ TF_LITE_MICRO_TEST(TestBasics) {
 }
 
 TF_LITE_MICRO_TEST(TestErrorHandling) {
-  tflite::LinearMemoryPlanner planner;
+  tflite_micro::LinearMemoryPlanner planner;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, planner.AddBuffer(10, 0, 1));
 
   int offset = -1;
@@ -43,7 +43,7 @@ TF_LITE_MICRO_TEST(TestErrorHandling) {
 }
 
 TF_LITE_MICRO_TEST(TestPersonDetectionModel) {
-  tflite::LinearMemoryPlanner planner;
+  tflite_micro::LinearMemoryPlanner planner;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, planner.AddBuffer(9216, 0, 29));
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, planner.AddBuffer(3, 28, 29));
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, planner.AddBuffer(256, 27, 28));
diff --git a/tensorflow/lite/micro/memory_planner/memory_plan_struct.h b/tensorflow/lite/micro/memory_planner/memory_plan_struct.h
index c8c431cc..3ca7ab9f 100644
--- a/tensorflow/lite/micro/memory_planner/memory_plan_struct.h
+++ b/tensorflow/lite/micro/memory_planner/memory_plan_struct.h
@@ -21,7 +21,7 @@ limitations under the License.
 
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // This is an experimental feature and subjected to change.
 // More description is available at
@@ -68,6 +68,6 @@ constexpr size_t SizeOfBufferPlan(int32_t buffer_count) {
          sizeof(BufferDescriptor) * Max(buffer_count - 1, 0);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_MEMORY_PLANNER_MEMORY_PLAN_STRUCT_H_
diff --git a/tensorflow/lite/micro/memory_planner/micro_memory_planner.h b/tensorflow/lite/micro/memory_planner/micro_memory_planner.h
index 035f4673..f3e84f6a 100644
--- a/tensorflow/lite/micro/memory_planner/micro_memory_planner.h
+++ b/tensorflow/lite/micro/memory_planner/micro_memory_planner.h
@@ -18,7 +18,7 @@ limitations under the License.
 
 #include "tensorflow/lite/c/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // Interface class for planning the layout of memory buffers during the
 // execution of a graph.
@@ -90,6 +90,6 @@ class MicroMemoryPlanner {
   }
 };
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_MICRO_MEMORY_PLANNER_MEMORY_PLANNER_H_
diff --git a/tensorflow/lite/micro/memory_planner/non_persistent_buffer_planner_shim.cc b/tensorflow/lite/micro/memory_planner/non_persistent_buffer_planner_shim.cc
index 9bcb80c1..1302cf24 100644
--- a/tensorflow/lite/micro/memory_planner/non_persistent_buffer_planner_shim.cc
+++ b/tensorflow/lite/micro/memory_planner/non_persistent_buffer_planner_shim.cc
@@ -17,7 +17,7 @@ limitations under the License.
 
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 NonPersistentMemoryPlannerShim::NonPersistentMemoryPlannerShim(
     const BufferPlan* buffer_plan)
@@ -63,4 +63,4 @@ TfLiteStatus NonPersistentMemoryPlannerShim::GetOffsetForBuffer(
   return kTfLiteOk;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/memory_planner/non_persistent_buffer_planner_shim.h b/tensorflow/lite/micro/memory_planner/non_persistent_buffer_planner_shim.h
index 13a3fad8..4d4eb31a 100644
--- a/tensorflow/lite/micro/memory_planner/non_persistent_buffer_planner_shim.h
+++ b/tensorflow/lite/micro/memory_planner/non_persistent_buffer_planner_shim.h
@@ -20,7 +20,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/memory_planner/memory_plan_struct.h"
 #include "tensorflow/lite/micro/memory_planner/micro_memory_planner.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 /*   This is an experimental feature and subjected to change.
  *
@@ -128,6 +128,6 @@ class NonPersistentMemoryPlannerShim : public MicroMemoryPlanner {
   TF_LITE_REMOVE_VIRTUAL_DELETE
 };
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_MEMORY_PLANNER_NON_PERSISTENT_MEMORY_PLANNER_SHIM_H__
diff --git a/tensorflow/lite/micro/memory_planner/non_persistent_buffer_planner_shim_test.cc b/tensorflow/lite/micro/memory_planner/non_persistent_buffer_planner_shim_test.cc
index 5ccd469f..4f44e2db 100644
--- a/tensorflow/lite/micro/memory_planner/non_persistent_buffer_planner_shim_test.cc
+++ b/tensorflow/lite/micro/memory_planner/non_persistent_buffer_planner_shim_test.cc
@@ -30,13 +30,13 @@ constexpr int32_t kBuffer1Offset = 10;
 //       [1] = { .offset = 10}
 //   }
 // };
-tflite::BufferPlan* CreateBufferPlan() {
+tflite_micro::BufferPlan* CreateBufferPlan() {
   // Some targets do not support dynamic memory (i.e., no malloc or new), thus,
   // the test need to place non-transitent memories in static variables. This is
   // safe because tests are guarateed to run serially.
-  static int8_t buffer_plan_buffer[tflite::SizeOfBufferPlan(kBufferCnt)];
-  tflite::BufferPlan* buffer_plan_ptr =
-      reinterpret_cast<tflite::BufferPlan*>(buffer_plan_buffer);
+  static int8_t buffer_plan_buffer[tflite_micro::SizeOfBufferPlan(kBufferCnt)];
+  tflite_micro::BufferPlan* buffer_plan_ptr =
+      reinterpret_cast<tflite_micro::BufferPlan*>(buffer_plan_buffer);
   buffer_plan_ptr->buffer_count = kBufferCnt;
   buffer_plan_ptr->buffer_plan_entries[0].offset = kBuffer0Offset;
   buffer_plan_ptr->buffer_plan_entries[1].offset = kBuffer1Offset;
@@ -48,7 +48,7 @@ tflite::BufferPlan* CreateBufferPlan() {
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(TestGetOffsetForBuffer) {
-  tflite::NonPersistentMemoryPlannerShim planner(CreateBufferPlan());
+  tflite_micro::NonPersistentMemoryPlannerShim planner(CreateBufferPlan());
 
   int offset0 = -1;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, planner.GetOffsetForBuffer(0, &offset0));
@@ -60,7 +60,7 @@ TF_LITE_MICRO_TEST(TestGetOffsetForBuffer) {
 }
 
 TF_LITE_MICRO_TEST(TestErrorGetOffsetForBuffer) {
-  tflite::NonPersistentMemoryPlannerShim planner(CreateBufferPlan());
+  tflite_micro::NonPersistentMemoryPlannerShim planner(CreateBufferPlan());
 
   int offset = -1;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteError,
@@ -68,7 +68,7 @@ TF_LITE_MICRO_TEST(TestErrorGetOffsetForBuffer) {
 }
 
 TF_LITE_MICRO_TEST(TestAddBufferSuccess) {
-  tflite::NonPersistentMemoryPlannerShim planner(CreateBufferPlan());
+  tflite_micro::NonPersistentMemoryPlannerShim planner(CreateBufferPlan());
 
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, planner.AddBuffer(/*size=*/10,
                                                        /*first_time_used=*/0,
@@ -79,7 +79,7 @@ TF_LITE_MICRO_TEST(TestAddBufferSuccess) {
 }
 
 TF_LITE_MICRO_TEST(TestAddBufferFailWhenExceedRange) {
-  tflite::NonPersistentMemoryPlannerShim planner(CreateBufferPlan());
+  tflite_micro::NonPersistentMemoryPlannerShim planner(CreateBufferPlan());
 
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, planner.AddBuffer(/*size=*/10,
                                                        /*first_time_used=*/0,
@@ -94,7 +94,7 @@ TF_LITE_MICRO_TEST(TestAddBufferFailWhenExceedRange) {
 }
 
 TF_LITE_MICRO_TEST(TestBasics) {
-  tflite::NonPersistentMemoryPlannerShim planner(CreateBufferPlan());
+  tflite_micro::NonPersistentMemoryPlannerShim planner(CreateBufferPlan());
 
   TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(0),
                           planner.GetMaximumMemorySize());
diff --git a/tensorflow/lite/micro/micro_allocation_info.cc b/tensorflow/lite/micro/micro_allocation_info.cc
index a89a5e6c..fccc50ca 100644
--- a/tensorflow/lite/micro/micro_allocation_info.cc
+++ b/tensorflow/lite/micro/micro_allocation_info.cc
@@ -24,7 +24,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/memory_planner/greedy_memory_planner.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 constexpr char kOfflineMemAllocMetadata[] = "OfflineMemoryAllocation";
@@ -372,4 +372,4 @@ TfLiteStatus AllocationInfoBuilder::GetOfflinePlannedOffsets(
   return kTfLiteOk;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/micro_allocation_info.h b/tensorflow/lite/micro/micro_allocation_info.h
index 688d04e7..2cb7c5fa 100644
--- a/tensorflow/lite/micro/micro_allocation_info.h
+++ b/tensorflow/lite/micro/micro_allocation_info.h
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_allocator.h"
 #include "tensorflow/lite/schema/schema_generated.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // Used to hold information used during allocation calculations.
 struct AllocationInfo {
@@ -127,12 +127,12 @@ class AllocationInfoBuilder {
   TfLiteStatus ValidateSubgraph(const SubGraph* subgraph,
                                 TfLiteEvalTensor* eval_tensors);
 
-  const tflite::Model* model_ = nullptr;
+  const tflite_micro::Model* model_ = nullptr;
   INonPersistentBufferAllocator* non_persistent_allocator_ = nullptr;
   GraphAllocationInfo info_;
   int allocation_scope_count_ = 0;
 };
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_MICRO_ALLOCATION_INFO_H_
diff --git a/tensorflow/lite/micro/micro_allocation_info_test.cc b/tensorflow/lite/micro/micro_allocation_info_test.cc
index 5dca2b6a..40565806 100644
--- a/tensorflow/lite/micro/micro_allocation_info_test.cc
+++ b/tensorflow/lite/micro/micro_allocation_info_test.cc
@@ -24,18 +24,18 @@ TF_LITE_MICRO_TESTS_BEGIN
 TF_LITE_MICRO_TEST(TestSingleSubgraph) {
   constexpr int kArenaSize = 1024;
   uint8_t arena[kArenaSize];
-  const tflite::Model* model = tflite::testing::GetSimpleMockModel();
-  tflite::SingleArenaBufferAllocator allocator(arena, kArenaSize);
-  tflite::AllocationInfoBuilder builder(model, &allocator);
+  const tflite_micro::Model* model = tflite_micro::testing::GetSimpleMockModel();
+  tflite_micro::SingleArenaBufferAllocator allocator(arena, kArenaSize);
+  tflite_micro::AllocationInfoBuilder builder(model, &allocator);
   builder.CreateAllocationInfo(0);
-  tflite::MicroAllocator* micro_allocator =
-      tflite::MicroAllocator::Create(arena, kArenaSize);
-  tflite::SubgraphAllocations* subgraph_allocations =
+  tflite_micro::MicroAllocator* micro_allocator =
+      tflite_micro::MicroAllocator::Create(arena, kArenaSize);
+  tflite_micro::SubgraphAllocations* subgraph_allocations =
       micro_allocator->StartModelAllocation(model);
   builder.InitializeAllocationInfo(nullptr, subgraph_allocations);
   builder.MarkAllocationLifetimes(0, nullptr, nullptr, subgraph_allocations);
   TF_LITE_MICRO_EXPECT_EQ(builder.AllocationCount(), 4);
-  tflite::AllocationInfo* allocation_info = builder.Finish();
+  tflite_micro::AllocationInfo* allocation_info = builder.Finish();
   TF_LITE_MICRO_EXPECT_EQ(allocation_info[0].first_created, 0);
   TF_LITE_MICRO_EXPECT_EQ(allocation_info[0].last_used, 2);
   TF_LITE_MICRO_EXPECT_EQ(allocation_info[1].first_created, -1);
@@ -49,18 +49,18 @@ TF_LITE_MICRO_TEST(TestSingleSubgraph) {
 TF_LITE_MICRO_TEST(TestSingleSubgraphWithIntermediates) {
   constexpr int kArenaSize = 1024;
   uint8_t arena[kArenaSize];
-  const tflite::Model* model = tflite::testing::GetSimpleStatefulModel();
-  tflite::SingleArenaBufferAllocator allocator(arena, kArenaSize);
-  tflite::AllocationInfoBuilder builder(model, &allocator);
+  const tflite_micro::Model* model = tflite_micro::testing::GetSimpleStatefulModel();
+  tflite_micro::SingleArenaBufferAllocator allocator(arena, kArenaSize);
+  tflite_micro::AllocationInfoBuilder builder(model, &allocator);
   builder.CreateAllocationInfo(0);
-  tflite::MicroAllocator* micro_allocator =
-      tflite::MicroAllocator::Create(arena, kArenaSize);
-  tflite::SubgraphAllocations* subgraph_allocations =
+  tflite_micro::MicroAllocator* micro_allocator =
+      tflite_micro::MicroAllocator::Create(arena, kArenaSize);
+  tflite_micro::SubgraphAllocations* subgraph_allocations =
       micro_allocator->StartModelAllocation(model);
   builder.InitializeAllocationInfo(nullptr, subgraph_allocations);
   builder.MarkAllocationLifetimes(0, nullptr, nullptr, subgraph_allocations);
   TF_LITE_MICRO_EXPECT_EQ(builder.AllocationCount(), 4);
-  tflite::AllocationInfo* allocation_info = builder.Finish();
+  tflite_micro::AllocationInfo* allocation_info = builder.Finish();
   TF_LITE_MICRO_EXPECT_EQ(allocation_info[0].first_created, 0);
   TF_LITE_MICRO_EXPECT_EQ(allocation_info[0].last_used, 1);
   TF_LITE_MICRO_EXPECT_EQ(allocation_info[0].needs_allocating, true);
@@ -78,19 +78,19 @@ TF_LITE_MICRO_TEST(TestSingleSubgraphWithIntermediates) {
 TF_LITE_MICRO_TEST(TestMultiSubgraphWithIf) {
   constexpr int kArenaSize = 1024;
   uint8_t arena[kArenaSize];
-  const tflite::Model* model =
-      tflite::testing::GetSimpleModelWithSubgraphsAndIf();
-  tflite::SingleArenaBufferAllocator allocator(arena, kArenaSize);
-  tflite::AllocationInfoBuilder builder(model, &allocator);
+  const tflite_micro::Model* model =
+      tflite_micro::testing::GetSimpleModelWithSubgraphsAndIf();
+  tflite_micro::SingleArenaBufferAllocator allocator(arena, kArenaSize);
+  tflite_micro::AllocationInfoBuilder builder(model, &allocator);
   builder.CreateAllocationInfo(0);
-  tflite::MicroAllocator* micro_allocator =
-      tflite::MicroAllocator::Create(arena, kArenaSize);
-  tflite::SubgraphAllocations* subgraph_allocations =
+  tflite_micro::MicroAllocator* micro_allocator =
+      tflite_micro::MicroAllocator::Create(arena, kArenaSize);
+  tflite_micro::SubgraphAllocations* subgraph_allocations =
       micro_allocator->StartModelAllocation(model);
   builder.InitializeAllocationInfo(nullptr, subgraph_allocations);
   builder.MarkAllocationLifetimes(0, nullptr, nullptr, subgraph_allocations);
   TF_LITE_MICRO_EXPECT_EQ(builder.AllocationCount(), 10);
-  tflite::AllocationInfo* allocation_info = builder.Finish();
+  tflite_micro::AllocationInfo* allocation_info = builder.Finish();
   TF_LITE_MICRO_EXPECT_EQ(allocation_info[0].first_created, 0);
   TF_LITE_MICRO_EXPECT_EQ(allocation_info[0].last_used, 5);
   TF_LITE_MICRO_EXPECT_EQ(allocation_info[1].first_created, 0);
@@ -116,19 +116,19 @@ TF_LITE_MICRO_TEST(TestMultiSubgraphWithIf) {
 TF_LITE_MICRO_TEST(TestMultiSubgraphWithIfAndEmptySubgraph) {
   constexpr int kArenaSize = 1024;
   uint8_t arena[kArenaSize];
-  const tflite::Model* model =
-      tflite::testing::GetSimpleModelWithIfAndEmptySubgraph();
-  tflite::SingleArenaBufferAllocator allocator(arena, kArenaSize);
-  tflite::AllocationInfoBuilder builder(model, &allocator);
+  const tflite_micro::Model* model =
+      tflite_micro::testing::GetSimpleModelWithIfAndEmptySubgraph();
+  tflite_micro::SingleArenaBufferAllocator allocator(arena, kArenaSize);
+  tflite_micro::AllocationInfoBuilder builder(model, &allocator);
   builder.CreateAllocationInfo(0);
-  tflite::MicroAllocator* micro_allocator =
-      tflite::MicroAllocator::Create(arena, kArenaSize);
-  tflite::SubgraphAllocations* subgraph_allocations =
+  tflite_micro::MicroAllocator* micro_allocator =
+      tflite_micro::MicroAllocator::Create(arena, kArenaSize);
+  tflite_micro::SubgraphAllocations* subgraph_allocations =
       micro_allocator->StartModelAllocation(model);
   builder.InitializeAllocationInfo(nullptr, subgraph_allocations);
   builder.MarkAllocationLifetimes(0, nullptr, nullptr, subgraph_allocations);
   TF_LITE_MICRO_EXPECT_EQ(builder.AllocationCount(), 10);
-  tflite::AllocationInfo* allocation_info = builder.Finish();
+  tflite_micro::AllocationInfo* allocation_info = builder.Finish();
   TF_LITE_MICRO_EXPECT_EQ(allocation_info[0].first_created, 0);
   TF_LITE_MICRO_EXPECT_EQ(allocation_info[0].last_used, 4);
   TF_LITE_MICRO_EXPECT_EQ(allocation_info[1].first_created, 0);
@@ -154,19 +154,19 @@ TF_LITE_MICRO_TEST(TestMultiSubgraphWithIfAndEmptySubgraph) {
 TF_LITE_MICRO_TEST(TestMultiSubgraphWithIfAndInputSubgraphOverlap) {
   constexpr int kArenaSize = 2048;
   uint8_t arena[kArenaSize];
-  const tflite::Model* model =
-      tflite::testing::GetModelWithIfAndSubgraphInputTensorOverlap();
-  tflite::SingleArenaBufferAllocator allocator(arena, kArenaSize);
-  tflite::AllocationInfoBuilder builder(model, &allocator);
+  const tflite_micro::Model* model =
+      tflite_micro::testing::GetModelWithIfAndSubgraphInputTensorOverlap();
+  tflite_micro::SingleArenaBufferAllocator allocator(arena, kArenaSize);
+  tflite_micro::AllocationInfoBuilder builder(model, &allocator);
   builder.CreateAllocationInfo(0);
-  tflite::MicroAllocator* micro_allocator =
-      tflite::MicroAllocator::Create(arena, kArenaSize);
-  tflite::SubgraphAllocations* subgraph_allocations =
+  tflite_micro::MicroAllocator* micro_allocator =
+      tflite_micro::MicroAllocator::Create(arena, kArenaSize);
+  tflite_micro::SubgraphAllocations* subgraph_allocations =
       micro_allocator->StartModelAllocation(model);
   builder.InitializeAllocationInfo(nullptr, subgraph_allocations);
   builder.MarkAllocationLifetimes(0, nullptr, nullptr, subgraph_allocations);
   TF_LITE_MICRO_EXPECT_EQ(builder.AllocationCount(), 11);
-  tflite::AllocationInfo* allocation_info = builder.Finish();
+  tflite_micro::AllocationInfo* allocation_info = builder.Finish();
   TF_LITE_MICRO_EXPECT_EQ(allocation_info[0].first_created, 0);
   TF_LITE_MICRO_EXPECT_EQ(allocation_info[0].last_used, 5);
   TF_LITE_MICRO_EXPECT_EQ(allocation_info[1].first_created, 0);
diff --git a/tensorflow/lite/micro/micro_allocator.cc b/tensorflow/lite/micro/micro_allocator.cc
index 930da754..17153db2 100644
--- a/tensorflow/lite/micro/micro_allocator.cc
+++ b/tensorflow/lite/micro/micro_allocator.cc
@@ -36,7 +36,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/tflite_bridge/flatbuffer_conversions_bridge.h"
 #include "tensorflow/lite/schema/schema_generated.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
@@ -184,7 +184,7 @@ namespace internal {
 // Returns a pointer to any buffer associated with the flatbuffer tensor. Can
 // return nullptr if no buffer is found.
 void* GetFlatbufferTensorBuffer(
-    const tflite::Tensor& flatbuffer_tensor,
+    const tflite_micro::Tensor& flatbuffer_tensor,
     const flatbuffers::Vector<flatbuffers::Offset<Buffer>>* buffers) {
   // We need to figure out where the actual contents of this tensor are stored
   // in memory. We'll check to see if there's a serialized buffer (pretty much
@@ -216,7 +216,7 @@ void* GetFlatbufferTensorBuffer(
 TfLiteStatus InitializeTfLiteTensorFromFlatbuffer(
     IPersistentBufferAllocator* persistent_buffer_allocator,
     INonPersistentBufferAllocator* non_persistent_buffer_allocator,
-    bool allocate_temp, const tflite::Tensor& flatbuffer_tensor,
+    bool allocate_temp, const tflite_micro::Tensor& flatbuffer_tensor,
     const flatbuffers::Vector<flatbuffers::Offset<Buffer>>* buffers,
     TfLiteTensor* result) {
   TFLITE_DCHECK(result != nullptr);
@@ -225,7 +225,7 @@ TfLiteStatus InitializeTfLiteTensorFromFlatbuffer(
   // Make sure the serialized type is one we know how to deal with, and convert
   // it from a flatbuffer enum into a constant used by the kernel C API.
   TF_LITE_ENSURE_STATUS(
-      tflite::ConvertTensorType(flatbuffer_tensor.type(), &result->type));
+      tflite_micro::ConvertTensorType(flatbuffer_tensor.type(), &result->type));
   // Make sure we remember if the serialized tensor is designated as a variable.
   result->is_variable = flatbuffer_tensor.is_variable();
 
@@ -334,14 +334,14 @@ TfLiteStatus InitializeTfLiteTensorFromFlatbuffer(
 }
 
 TfLiteStatus InitializeTfLiteEvalTensorFromFlatbuffer(
-    const tflite::Tensor& flatbuffer_tensor,
+    const tflite_micro::Tensor& flatbuffer_tensor,
     const flatbuffers::Vector<flatbuffers::Offset<Buffer>>* buffers,
     TfLiteEvalTensor* result) {
   *result = {};
   // Make sure the serialized type is one we know how to deal with, and convert
   // it from a flatbuffer enum into a constant used by the kernel C API.
   TF_LITE_ENSURE_STATUS(
-      tflite::ConvertTensorType(flatbuffer_tensor.type(), &result->type));
+      tflite_micro::ConvertTensorType(flatbuffer_tensor.type(), &result->type));
 
   result->data.data = GetFlatbufferTensorBuffer(flatbuffer_tensor, buffers);
 
@@ -981,4 +981,4 @@ TfLiteBridgeBuiltinDataAllocator* MicroAllocator::GetBuiltinDataAllocator() {
   return builtin_data_allocator_;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/micro_allocator.h b/tensorflow/lite/micro/micro_allocator.h
index 4eff167d..9f57f580 100644
--- a/tensorflow/lite/micro/micro_allocator.h
+++ b/tensorflow/lite/micro/micro_allocator.h
@@ -26,7 +26,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/tflite_bridge/flatbuffer_conversions_bridge.h"
 #include "tensorflow/lite/schema/schema_generated.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // TODO(b/199402574): rename to tflite_internal or just remove internal
 // namespace.
@@ -39,7 +39,7 @@ namespace internal {
 TfLiteStatus InitializeTfLiteTensorFromFlatbuffer(
     IPersistentBufferAllocator* persistent_buffer_allocator,
     INonPersistentBufferAllocator* non_persistent_buffer_allocator,
-    bool allocate_temp, const tflite::Tensor& flatbuffer_tensor,
+    bool allocate_temp, const tflite_micro::Tensor& flatbuffer_tensor,
     const flatbuffers::Vector<flatbuffers::Offset<Buffer>>* buffers,
     TfLiteTensor* result);
 
@@ -250,6 +250,14 @@ class MicroAllocator {
 
   TfLiteBridgeBuiltinDataAllocator* GetBuiltinDataAllocator();
 
+  MicroMemoryPlanner* memory_planner() { return memory_planner_;}
+
+  // Returns the pointer for the array of ScratchBufferRequest allocations in
+  // the head section.
+  internal::ScratchBufferRequest* GetScratchBufferRequests();
+
+  size_t GetScratchBufferRequestCount() { return scratch_buffer_request_count_;}
+
  protected:
   MicroAllocator(SingleArenaBufferAllocator* memory_allocator,
                  MicroMemoryPlanner* memory_planner);
@@ -313,10 +321,6 @@ class MicroAllocator {
   // preparing.
   TfLiteStatus InitScratchBufferData();
 
-  // Returns the pointer for the array of ScratchBufferRequest allocations in
-  // the head section.
-  internal::ScratchBufferRequest* GetScratchBufferRequests();
-
   // A simple memory allocator that always allocate from the arena tail or head.
   INonPersistentBufferAllocator* non_persistent_buffer_allocator_;
   IPersistentBufferAllocator* persistent_buffer_allocator_;
@@ -343,5 +347,5 @@ class MicroAllocator {
   TF_LITE_REMOVE_VIRTUAL_DELETE
 };
 
-}  // namespace tflite
+}  // namespace tflite_micro
 #endif  // TENSORFLOW_LITE_MICRO_MICRO_ALLOCATOR_H_
diff --git a/tensorflow/lite/micro/micro_allocator_test.cc b/tensorflow/lite/micro/micro_allocator_test.cc
index 9e19e271..bf0f13b7 100644
--- a/tensorflow/lite/micro/micro_allocator_test.cc
+++ b/tensorflow/lite/micro/micro_allocator_test.cc
@@ -29,7 +29,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/testing/micro_test.h"
 #include "tensorflow/lite/micro/testing/test_conv_model.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -67,7 +67,7 @@ void VerifyMockTfLiteEvalTensor(TfLiteEvalTensor* tensor) {
   TF_LITE_MICRO_EXPECT_EQ(1, tensor->dims->data[0]);
   size_t buffer_size;
   TF_LITE_MICRO_EXPECT_EQ(
-      kTfLiteOk, tflite::TfLiteEvalTensorByteLength(tensor, &buffer_size));
+      kTfLiteOk, tflite_micro::TfLiteEvalTensorByteLength(tensor, &buffer_size));
   TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(4), buffer_size);
   TF_LITE_MICRO_EXPECT(nullptr != tensor->data.raw);
   TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(0),
@@ -81,7 +81,7 @@ void VerifyMockWeightTfLiteEvalTensor(TfLiteEvalTensor* tensor) {
   TF_LITE_MICRO_EXPECT_EQ(1, tensor->dims->data[0]);
   size_t buffer_size;
   TF_LITE_MICRO_EXPECT_EQ(
-      kTfLiteOk, tflite::TfLiteEvalTensorByteLength(tensor, &buffer_size));
+      kTfLiteOk, tflite_micro::TfLiteEvalTensorByteLength(tensor, &buffer_size));
   TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(1), buffer_size);
   TF_LITE_MICRO_EXPECT(nullptr != tensor->data.raw);
 }
@@ -159,24 +159,24 @@ size_t GetArenaUsedBytesBySimpleMockModel(bool is_memory_planner_injected) {
 
 }  // namespace
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(TestInitializeRuntimeTensor) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::SingleArenaBufferAllocator* simple_allocator =
-      tflite::SingleArenaBufferAllocator::Create(arena, arena_size);
+  tflite_micro::SingleArenaBufferAllocator* simple_allocator =
+      tflite_micro::SingleArenaBufferAllocator::Create(arena, arena_size);
 
-  const tflite::Tensor* tensor = tflite::testing::Create1dFlatbufferTensor(100);
-  const flatbuffers::Vector<flatbuffers::Offset<tflite::Buffer>>* buffers =
-      tflite::testing::CreateFlatbufferBuffers();
+  const tflite_micro::Tensor* tensor = tflite_micro::testing::Create1dFlatbufferTensor(100);
+  const flatbuffers::Vector<flatbuffers::Offset<tflite_micro::Buffer>>* buffers =
+      tflite_micro::testing::CreateFlatbufferBuffers();
 
   TfLiteTensor allocated_tensor;
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteOk,
-      tflite::internal::InitializeTfLiteTensorFromFlatbuffer(
+      tflite_micro::internal::InitializeTfLiteTensorFromFlatbuffer(
           simple_allocator, simple_allocator, /*allocate_temp=*/false, *tensor,
           buffers, &allocated_tensor));
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteInt32, allocated_tensor.type);
@@ -195,16 +195,16 @@ TF_LITE_MICRO_TEST(TestInitializeRuntimeTensor) {
 TF_LITE_MICRO_TEST(TestInitializeTempRuntimeTensor) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::SingleArenaBufferAllocator* simple_allocator =
-      tflite::SingleArenaBufferAllocator::Create(arena, arena_size);
+  tflite_micro::SingleArenaBufferAllocator* simple_allocator =
+      tflite_micro::SingleArenaBufferAllocator::Create(arena, arena_size);
 
-  const tflite::Tensor* tensor = tflite::testing::Create1dFlatbufferTensor(100);
-  const flatbuffers::Vector<flatbuffers::Offset<tflite::Buffer>>* buffers =
-      tflite::testing::CreateFlatbufferBuffers();
+  const tflite_micro::Tensor* tensor = tflite_micro::testing::Create1dFlatbufferTensor(100);
+  const flatbuffers::Vector<flatbuffers::Offset<tflite_micro::Buffer>>* buffers =
+      tflite_micro::testing::CreateFlatbufferBuffers();
 
   TfLiteTensor allocated_temp_tensor;
   TF_LITE_MICRO_EXPECT_EQ(
-      kTfLiteOk, tflite::internal::InitializeTfLiteTensorFromFlatbuffer(
+      kTfLiteOk, tflite_micro::internal::InitializeTfLiteTensorFromFlatbuffer(
                      simple_allocator, simple_allocator, /*allocate_temp=*/true,
                      *tensor, buffers, &allocated_temp_tensor));
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteInt32, allocated_temp_tensor.type);
@@ -222,18 +222,18 @@ TF_LITE_MICRO_TEST(TestInitializeTempRuntimeTensor) {
 TF_LITE_MICRO_TEST(TestInitializeQuantizedTensor) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::SingleArenaBufferAllocator* simple_allocator =
-      tflite::SingleArenaBufferAllocator::Create(arena, arena_size);
+  tflite_micro::SingleArenaBufferAllocator* simple_allocator =
+      tflite_micro::SingleArenaBufferAllocator::Create(arena, arena_size);
 
-  const tflite::Tensor* tensor =
-      tflite::testing::CreateQuantizedFlatbufferTensor(100);
-  const flatbuffers::Vector<flatbuffers::Offset<tflite::Buffer>>* buffers =
-      tflite::testing::CreateFlatbufferBuffers();
+  const tflite_micro::Tensor* tensor =
+      tflite_micro::testing::CreateQuantizedFlatbufferTensor(100);
+  const flatbuffers::Vector<flatbuffers::Offset<tflite_micro::Buffer>>* buffers =
+      tflite_micro::testing::CreateFlatbufferBuffers();
 
   TfLiteTensor allocated_tensor;
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteOk,
-      tflite::internal::InitializeTfLiteTensorFromFlatbuffer(
+      tflite_micro::internal::InitializeTfLiteTensorFromFlatbuffer(
           simple_allocator, simple_allocator, /*allocate_temp=*/false, *tensor,
           buffers, &allocated_tensor));
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteInt32, allocated_tensor.type);
@@ -249,18 +249,18 @@ TF_LITE_MICRO_TEST(TestInitializeQuantizedTensor) {
 TF_LITE_MICRO_TEST(TestMissingQuantization) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::SingleArenaBufferAllocator* simple_allocator =
-      tflite::SingleArenaBufferAllocator::Create(arena, arena_size);
+  tflite_micro::SingleArenaBufferAllocator* simple_allocator =
+      tflite_micro::SingleArenaBufferAllocator::Create(arena, arena_size);
 
-  const tflite::Tensor* tensor =
-      tflite::testing::CreateMissingQuantizationFlatbufferTensor(100);
-  const flatbuffers::Vector<flatbuffers::Offset<tflite::Buffer>>* buffers =
-      tflite::testing::CreateFlatbufferBuffers();
+  const tflite_micro::Tensor* tensor =
+      tflite_micro::testing::CreateMissingQuantizationFlatbufferTensor(100);
+  const flatbuffers::Vector<flatbuffers::Offset<tflite_micro::Buffer>>* buffers =
+      tflite_micro::testing::CreateFlatbufferBuffers();
 
   TfLiteTensor allocated_tensor;
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteOk,
-      tflite::internal::InitializeTfLiteTensorFromFlatbuffer(
+      tflite_micro::internal::InitializeTfLiteTensorFromFlatbuffer(
           simple_allocator, simple_allocator, /*allocate_temp=*/false, *tensor,
           buffers, &allocated_tensor));
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteInt32, allocated_tensor.type);
@@ -271,30 +271,30 @@ TF_LITE_MICRO_TEST(TestMissingQuantization) {
 }
 
 TF_LITE_MICRO_TEST(TestFailsWhenModelStartsTwice) {
-  const tflite::Model* model = tflite::testing::GetSimpleMockModel();
-  tflite::testing::TestingOpResolver op_resolver;
+  const tflite_micro::Model* model = tflite_micro::testing::GetSimpleMockModel();
+  tflite_micro::testing::TestingOpResolver op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::testing::GetTestingOpResolver(op_resolver));
+                          tflite_micro::testing::GetTestingOpResolver(op_resolver));
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::MicroAllocator* allocator =
-      tflite::MicroAllocator::Create(arena, arena_size);
+  tflite_micro::MicroAllocator* allocator =
+      tflite_micro::MicroAllocator::Create(arena, arena_size);
   TF_LITE_MICRO_EXPECT(nullptr != allocator);
   TF_LITE_MICRO_EXPECT(nullptr != allocator->StartModelAllocation(model));
   TF_LITE_MICRO_EXPECT(nullptr == allocator->StartModelAllocation(model));
 }
 
 TF_LITE_MICRO_TEST(TestFailsWithWrongSequence) {
-  const tflite::Model* model = tflite::testing::GetSimpleMockModel();
-  tflite::ScratchBufferHandle* scratch_buffer_handles = nullptr;
-  tflite::testing::TestingOpResolver op_resolver;
+  const tflite_micro::Model* model = tflite_micro::testing::GetSimpleMockModel();
+  tflite_micro::ScratchBufferHandle* scratch_buffer_handles = nullptr;
+  tflite_micro::testing::TestingOpResolver op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::testing::GetTestingOpResolver(op_resolver));
-  tflite::SubgraphAllocations* subgraph_allocations = nullptr;
+                          tflite_micro::testing::GetTestingOpResolver(op_resolver));
+  tflite_micro::SubgraphAllocations* subgraph_allocations = nullptr;
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::MicroAllocator* allocator =
-      tflite::MicroAllocator::Create(arena, arena_size);
+  tflite_micro::MicroAllocator* allocator =
+      tflite_micro::MicroAllocator::Create(arena, arena_size);
   TF_LITE_MICRO_EXPECT(nullptr != allocator);
 
   // We can't finish allocation before it ever got started.
@@ -308,37 +308,37 @@ TF_LITE_MICRO_TEST(TestFailsWithWrongSequence) {
 }
 
 TF_LITE_MICRO_TEST(TestMockModelAllocation) {
-  const tflite::Model* model = tflite::testing::GetSimpleMockModel();
-  tflite::ScratchBufferHandle* scratch_buffer_handles = nullptr;
-  tflite::testing::TestingOpResolver op_resolver;
+  const tflite_micro::Model* model = tflite_micro::testing::GetSimpleMockModel();
+  tflite_micro::ScratchBufferHandle* scratch_buffer_handles = nullptr;
+  tflite_micro::testing::TestingOpResolver op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::testing::GetTestingOpResolver(op_resolver));
+                          tflite_micro::testing::GetTestingOpResolver(op_resolver));
   constexpr size_t arena_size = 1024 + 16;
   uint8_t arena[arena_size];
-  tflite::MicroAllocator* allocator =
-      tflite::MicroAllocator::Create(arena, arena_size);
+  tflite_micro::MicroAllocator* allocator =
+      tflite_micro::MicroAllocator::Create(arena, arena_size);
   TF_LITE_MICRO_EXPECT(nullptr != allocator);
-  tflite::SubgraphAllocations* subgraph_allocations =
+  tflite_micro::SubgraphAllocations* subgraph_allocations =
       allocator->StartModelAllocation(model);
   TF_LITE_MICRO_EXPECT(nullptr != subgraph_allocations);
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteOk, allocator->FinishModelAllocation(model, subgraph_allocations,
                                                   &scratch_buffer_handles));
   size_t expected_arena_used_bytes =
-      tflite::testing::GetArenaUsedBytesBySimpleMockModel(
+      tflite_micro::testing::GetArenaUsedBytesBySimpleMockModel(
           /*is_memory_planner_injected=*/false);
   TF_LITE_MICRO_EXPECT_EQ(allocator->used_bytes(), expected_arena_used_bytes);
 
-  size_t model_tensor_size = tflite::testing::GetModelTensorCount(model);
+  size_t model_tensor_size = tflite_micro::testing::GetModelTensorCount(model);
   TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(4), model_tensor_size);
 
-  tflite::testing::AllocateAndVerifyMockTensor(model, allocator,
+  tflite_micro::testing::AllocateAndVerifyMockTensor(model, allocator,
                                                subgraph_allocations, 0);
-  tflite::testing::AllocateAndVerifyMockWeightTensor(model, allocator,
+  tflite_micro::testing::AllocateAndVerifyMockWeightTensor(model, allocator,
                                                      subgraph_allocations, 1);
-  tflite::testing::AllocateAndVerifyMockTensor(model, allocator,
+  tflite_micro::testing::AllocateAndVerifyMockTensor(model, allocator,
                                                subgraph_allocations, 2);
-  tflite::testing::AllocateAndVerifyMockTensor(model, allocator,
+  tflite_micro::testing::AllocateAndVerifyMockTensor(model, allocator,
                                                subgraph_allocations, 3);
 
   TfLiteEvalTensor* eval_tensors = subgraph_allocations[0].tensors;
@@ -350,41 +350,41 @@ TF_LITE_MICRO_TEST(TestMockModelAllocation) {
   TF_LITE_MICRO_EXPECT_NE(eval_tensors[3].data.raw, eval_tensors[2].data.raw);
 
   // SimpleMockModel has 2 operators:
-  tflite::testing::VerifyRegistrationAndNodeAllocation(subgraph_allocations,
+  tflite_micro::testing::VerifyRegistrationAndNodeAllocation(subgraph_allocations,
                                                        /*count=*/2,
                                                        /*num_subgraphs=*/1);
 }
 
 TF_LITE_MICRO_TEST(TestMockModelAllocationInTwoSeparateArenas) {
-  const tflite::Model* model = tflite::testing::GetSimpleMockModel();
-  tflite::ScratchBufferHandle* scratch_buffer_handles = nullptr;
-  tflite::testing::TestingOpResolver op_resolver;
+  const tflite_micro::Model* model = tflite_micro::testing::GetSimpleMockModel();
+  tflite_micro::ScratchBufferHandle* scratch_buffer_handles = nullptr;
+  tflite_micro::testing::TestingOpResolver op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::testing::GetTestingOpResolver(op_resolver));
+                          tflite_micro::testing::GetTestingOpResolver(op_resolver));
   constexpr size_t arena_size = 1024;
   uint8_t persistent_arena[arena_size];
   uint8_t non_persistent_arena[arena_size];
 
-  tflite::MicroAllocator* allocator = tflite::MicroAllocator::Create(
+  tflite_micro::MicroAllocator* allocator = tflite_micro::MicroAllocator::Create(
       persistent_arena, arena_size, non_persistent_arena, arena_size);
   TF_LITE_MICRO_EXPECT(nullptr != allocator);
-  tflite::SubgraphAllocations* subgraph_allocations =
+  tflite_micro::SubgraphAllocations* subgraph_allocations =
       allocator->StartModelAllocation(model);
   TF_LITE_MICRO_EXPECT(nullptr != subgraph_allocations);
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteOk, allocator->FinishModelAllocation(model, subgraph_allocations,
                                                   &scratch_buffer_handles));
 
-  size_t model_tensor_size = tflite::testing::GetModelTensorCount(model);
+  size_t model_tensor_size = tflite_micro::testing::GetModelTensorCount(model);
   TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(4), model_tensor_size);
 
-  tflite::testing::AllocateAndVerifyMockTensor(model, allocator,
+  tflite_micro::testing::AllocateAndVerifyMockTensor(model, allocator,
                                                subgraph_allocations, 0);
-  tflite::testing::AllocateAndVerifyMockWeightTensor(model, allocator,
+  tflite_micro::testing::AllocateAndVerifyMockWeightTensor(model, allocator,
                                                      subgraph_allocations, 1);
-  tflite::testing::AllocateAndVerifyMockTensor(model, allocator,
+  tflite_micro::testing::AllocateAndVerifyMockTensor(model, allocator,
                                                subgraph_allocations, 2);
-  tflite::testing::AllocateAndVerifyMockTensor(model, allocator,
+  tflite_micro::testing::AllocateAndVerifyMockTensor(model, allocator,
                                                subgraph_allocations, 3);
 
   TfLiteEvalTensor* eval_tensors = subgraph_allocations[0].tensors;
@@ -396,44 +396,44 @@ TF_LITE_MICRO_TEST(TestMockModelAllocationInTwoSeparateArenas) {
   TF_LITE_MICRO_EXPECT_NE(eval_tensors[3].data.raw, eval_tensors[2].data.raw);
 
   // SimpleMockModel has 2 operators:
-  tflite::testing::VerifyRegistrationAndNodeAllocation(subgraph_allocations,
+  tflite_micro::testing::VerifyRegistrationAndNodeAllocation(subgraph_allocations,
                                                        /*count=*/2,
                                                        /*num_subgraphs=*/1);
 }
 
 TF_LITE_MICRO_TEST(TestMockModelAllocationWithGivenMemoryPlanner) {
-  const tflite::Model* model = tflite::testing::GetSimpleMockModel();
-  tflite::ScratchBufferHandle* scratch_buffer_handles = nullptr;
-  tflite::testing::TestingOpResolver op_resolver;
+  const tflite_micro::Model* model = tflite_micro::testing::GetSimpleMockModel();
+  tflite_micro::ScratchBufferHandle* scratch_buffer_handles = nullptr;
+  tflite_micro::testing::TestingOpResolver op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::testing::GetTestingOpResolver(op_resolver));
+                          tflite_micro::testing::GetTestingOpResolver(op_resolver));
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::GreedyMemoryPlanner memory_planner;
-  tflite::MicroAllocator* allocator =
-      tflite::MicroAllocator::Create(arena, arena_size, &memory_planner);
+  tflite_micro::GreedyMemoryPlanner memory_planner;
+  tflite_micro::MicroAllocator* allocator =
+      tflite_micro::MicroAllocator::Create(arena, arena_size, &memory_planner);
   TF_LITE_MICRO_EXPECT(nullptr != allocator);
-  tflite::SubgraphAllocations* subgraph_allocations =
+  tflite_micro::SubgraphAllocations* subgraph_allocations =
       allocator->StartModelAllocation(model);
   TF_LITE_MICRO_EXPECT(nullptr != subgraph_allocations);
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteOk, allocator->FinishModelAllocation(model, subgraph_allocations,
                                                   &scratch_buffer_handles));
   size_t expected_arena_used_bytes =
-      tflite::testing::GetArenaUsedBytesBySimpleMockModel(
+      tflite_micro::testing::GetArenaUsedBytesBySimpleMockModel(
           /*is_memory_planner_injected=*/true);
   TF_LITE_MICRO_EXPECT_EQ(allocator->used_bytes(), expected_arena_used_bytes);
 
-  size_t model_tensor_size = tflite::testing::GetModelTensorCount(model);
+  size_t model_tensor_size = tflite_micro::testing::GetModelTensorCount(model);
   TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(4), model_tensor_size);
 
-  tflite::testing::AllocateAndVerifyMockTensor(model, allocator,
+  tflite_micro::testing::AllocateAndVerifyMockTensor(model, allocator,
                                                subgraph_allocations, 0);
-  tflite::testing::AllocateAndVerifyMockWeightTensor(model, allocator,
+  tflite_micro::testing::AllocateAndVerifyMockWeightTensor(model, allocator,
                                                      subgraph_allocations, 1);
-  tflite::testing::AllocateAndVerifyMockTensor(model, allocator,
+  tflite_micro::testing::AllocateAndVerifyMockTensor(model, allocator,
                                                subgraph_allocations, 2);
-  tflite::testing::AllocateAndVerifyMockTensor(model, allocator,
+  tflite_micro::testing::AllocateAndVerifyMockTensor(model, allocator,
                                                subgraph_allocations, 3);
 
   TfLiteEvalTensor* eval_tensors = subgraph_allocations[0].tensors;
@@ -445,7 +445,7 @@ TF_LITE_MICRO_TEST(TestMockModelAllocationWithGivenMemoryPlanner) {
   TF_LITE_MICRO_EXPECT_NE(eval_tensors[3].data.raw, eval_tensors[2].data.raw);
 
   // SimpleMockModel has 2 operators:
-  tflite::testing::VerifyRegistrationAndNodeAllocation(subgraph_allocations,
+  tflite_micro::testing::VerifyRegistrationAndNodeAllocation(subgraph_allocations,
                                                        /*count=*/2,
                                                        /*num_subgraphs=*/1);
 }
@@ -453,23 +453,23 @@ TF_LITE_MICRO_TEST(TestMockModelAllocationWithGivenMemoryPlanner) {
 TF_LITE_MICRO_TEST(TestMultiTenantAllocation) {
   // The `OpResolver` is shared among different models in this test for
   // simplicity but in practice you could have different `OpResolver`.
-  tflite::testing::TestingOpResolver op_resolver;
+  tflite_micro::testing::TestingOpResolver op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::testing::GetTestingOpResolver(op_resolver));
+                          tflite_micro::testing::GetTestingOpResolver(op_resolver));
 
   // Create a shared allocator.
   constexpr size_t arena_size = 4096;
   uint8_t arena[arena_size];
-  tflite::MicroAllocator* allocator =
-      tflite::MicroAllocator::Create(arena, arena_size);
+  tflite_micro::MicroAllocator* allocator =
+      tflite_micro::MicroAllocator::Create(arena, arena_size);
   TF_LITE_MICRO_EXPECT(nullptr != allocator);
 
-  tflite::ScratchBufferHandle* scratch_buffer_handles = nullptr;
+  tflite_micro::ScratchBufferHandle* scratch_buffer_handles = nullptr;
 
   // Allocate for model 1. We use ComplexMockModel here to cover the code path
   // allocatig variables.
-  const tflite::Model* model1 = tflite::testing::GetComplexMockModel();
-  tflite::SubgraphAllocations* subgraph_allocations1 =
+  const tflite_micro::Model* model1 = tflite_micro::testing::GetComplexMockModel();
+  tflite_micro::SubgraphAllocations* subgraph_allocations1 =
       allocator->StartModelAllocation(model1);
   TF_LITE_MICRO_EXPECT(nullptr != subgraph_allocations1);
   TF_LITE_MICRO_EXPECT_EQ(
@@ -478,8 +478,8 @@ TF_LITE_MICRO_TEST(TestMultiTenantAllocation) {
   const size_t single_model_used_bytes = allocator->used_bytes();
 
   // Allocate for model 2.
-  const tflite::Model* model2 = tflite::testing::GetComplexMockModel();
-  tflite::SubgraphAllocations* subgraph_allocations2 =
+  const tflite_micro::Model* model2 = tflite_micro::testing::GetComplexMockModel();
+  tflite_micro::SubgraphAllocations* subgraph_allocations2 =
       allocator->StartModelAllocation(model2);
   TF_LITE_MICRO_EXPECT(nullptr != subgraph_allocations2);
   TF_LITE_MICRO_EXPECT_EQ(
@@ -494,24 +494,24 @@ TF_LITE_MICRO_TEST(TestMultiTenantAllocation) {
 TF_LITE_MICRO_TEST(TestMultiTenantAllocationInTwoSeparateArenas) {
   // The `OpResolver` is shared among different models in this test for
   // simplicity but in practice you could have different `OpResolver`.
-  tflite::testing::TestingOpResolver op_resolver;
+  tflite_micro::testing::TestingOpResolver op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::testing::GetTestingOpResolver(op_resolver));
+                          tflite_micro::testing::GetTestingOpResolver(op_resolver));
 
   // Create a shared allocator.
   constexpr size_t arena_size = 4096;
   uint8_t persistent_arena[arena_size];
   uint8_t non_persistent_arena[arena_size];
 
-  tflite::MicroAllocator* allocator = tflite::MicroAllocator::Create(
+  tflite_micro::MicroAllocator* allocator = tflite_micro::MicroAllocator::Create(
       persistent_arena, arena_size, non_persistent_arena, arena_size);
   TF_LITE_MICRO_EXPECT(nullptr != allocator);
-  tflite::ScratchBufferHandle* scratch_buffer_handles = nullptr;
+  tflite_micro::ScratchBufferHandle* scratch_buffer_handles = nullptr;
 
   // Allocate for model 1. We use ComplexMockModel here to cover the code path
   // allocatig variables.
-  const tflite::Model* model1 = tflite::testing::GetComplexMockModel();
-  tflite::SubgraphAllocations* subgraph_allocations1 =
+  const tflite_micro::Model* model1 = tflite_micro::testing::GetComplexMockModel();
+  tflite_micro::SubgraphAllocations* subgraph_allocations1 =
       allocator->StartModelAllocation(model1);
   TF_LITE_MICRO_EXPECT(nullptr != subgraph_allocations1);
   TF_LITE_MICRO_EXPECT_EQ(
@@ -520,8 +520,8 @@ TF_LITE_MICRO_TEST(TestMultiTenantAllocationInTwoSeparateArenas) {
   const size_t single_model_used_bytes = allocator->used_bytes();
 
   // Allocate for model 2.
-  const tflite::Model* model2 = tflite::testing::GetComplexMockModel();
-  tflite::SubgraphAllocations* subgraph_allocations2 =
+  const tflite_micro::Model* model2 = tflite_micro::testing::GetComplexMockModel();
+  tflite_micro::SubgraphAllocations* subgraph_allocations2 =
       allocator->StartModelAllocation(model2);
   TF_LITE_MICRO_EXPECT(nullptr != subgraph_allocations2);
   TF_LITE_MICRO_EXPECT_EQ(
@@ -534,17 +534,17 @@ TF_LITE_MICRO_TEST(TestMultiTenantAllocationInTwoSeparateArenas) {
 }
 
 TF_LITE_MICRO_TEST(TestAllocationForModelsWithBranches) {
-  const tflite::Model* model = tflite::testing::GetSimpleModelWithBranch();
-  tflite::ScratchBufferHandle* scratch_buffer_handles = nullptr;
-  tflite::testing::TestingOpResolver op_resolver;
+  const tflite_micro::Model* model = tflite_micro::testing::GetSimpleModelWithBranch();
+  tflite_micro::ScratchBufferHandle* scratch_buffer_handles = nullptr;
+  tflite_micro::testing::TestingOpResolver op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::testing::GetTestingOpResolver(op_resolver));
+                          tflite_micro::testing::GetTestingOpResolver(op_resolver));
   constexpr size_t arena_size = 4096;
   uint8_t arena[arena_size];
-  tflite::MicroAllocator* allocator =
-      tflite::MicroAllocator::Create(arena, arena_size);
+  tflite_micro::MicroAllocator* allocator =
+      tflite_micro::MicroAllocator::Create(arena, arena_size);
   TF_LITE_MICRO_EXPECT(nullptr != allocator);
-  tflite::SubgraphAllocations* subgraph_allocations =
+  tflite_micro::SubgraphAllocations* subgraph_allocations =
       allocator->StartModelAllocation(model);
   TF_LITE_MICRO_EXPECT(nullptr != subgraph_allocations);
   TF_LITE_MICRO_EXPECT_EQ(
@@ -552,97 +552,97 @@ TF_LITE_MICRO_TEST(TestAllocationForModelsWithBranches) {
                                                   &scratch_buffer_handles));
 
   int8_t* start =
-      tflite::micro::GetTensorData<int8_t>(&subgraph_allocations[0].tensors[0]);
+      tflite_micro::micro::GetTensorData<int8_t>(&subgraph_allocations[0].tensors[0]);
   // Check test_helpers.cc BuildSimpleModelWithBranch for model structure.
   // t0 is the first tensor, so place it in offset 0.
-  TF_LITE_MICRO_EXPECT_EQ(0, tflite::micro::GetTensorData<int8_t>(
+  TF_LITE_MICRO_EXPECT_EQ(0, tflite_micro::micro::GetTensorData<int8_t>(
                                  &subgraph_allocations[0].tensors[0]) -
                                  start);
   // bytes = 2 * 2 * 3 * sizeof(float32) = 48, same for other tensors.
   size_t buffer_size;
   TF_LITE_MICRO_EXPECT_EQ(
-      kTfLiteOk, tflite::TfLiteEvalTensorByteLength(
+      kTfLiteOk, tflite_micro::TfLiteEvalTensorByteLength(
                      &subgraph_allocations[0].tensors[0], &buffer_size));
   TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(48), buffer_size);
   // t1 can't reuse any memory, as n0 requires both t0 and t1.
-  TF_LITE_MICRO_EXPECT_EQ(96, tflite::micro::GetTensorData<int8_t>(
+  TF_LITE_MICRO_EXPECT_EQ(96, tflite_micro::micro::GetTensorData<int8_t>(
                                   &subgraph_allocations[0].tensors[1]) -
                                   start);
   // t2 can't reuse any memory, as n1 requires both t0 and t2. Also n2 requires
   // both t1 and t2.
-  TF_LITE_MICRO_EXPECT_EQ(48, tflite::micro::GetTensorData<int8_t>(
+  TF_LITE_MICRO_EXPECT_EQ(48, tflite_micro::micro::GetTensorData<int8_t>(
                                   &subgraph_allocations[0].tensors[2]) -
                                   start);
   // t3 reuses the same memory from t0 as t0 is not an input to any node.
-  TF_LITE_MICRO_EXPECT_EQ(0, tflite::micro::GetTensorData<int8_t>(
+  TF_LITE_MICRO_EXPECT_EQ(0, tflite_micro::micro::GetTensorData<int8_t>(
                                  &subgraph_allocations[0].tensors[3]) -
                                  start);
 
   // SimpleModelWithBranch has 3 operators:
-  tflite::testing::VerifyRegistrationAndNodeAllocation(subgraph_allocations,
+  tflite_micro::testing::VerifyRegistrationAndNodeAllocation(subgraph_allocations,
                                                        /*count=*/3,
                                                        /*num_subgraphs=*/1);
 }
 
 TF_LITE_MICRO_TEST(TestAllocationForComplexModelAllocation) {
-  const tflite::Model* model = tflite::testing::GetComplexMockModel();
-  tflite::ScratchBufferHandle* scratch_buffer_handles = nullptr;
-  tflite::testing::TestingOpResolver op_resolver;
+  const tflite_micro::Model* model = tflite_micro::testing::GetComplexMockModel();
+  tflite_micro::ScratchBufferHandle* scratch_buffer_handles = nullptr;
+  tflite_micro::testing::TestingOpResolver op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::testing::GetTestingOpResolver(op_resolver));
+                          tflite_micro::testing::GetTestingOpResolver(op_resolver));
   constexpr size_t arena_size = 2048;
   uint8_t arena[arena_size];
-  tflite::MicroAllocator* allocator =
-      tflite::MicroAllocator::Create(arena, arena_size);
+  tflite_micro::MicroAllocator* allocator =
+      tflite_micro::MicroAllocator::Create(arena, arena_size);
   TF_LITE_MICRO_EXPECT(nullptr != allocator);
-  tflite::SubgraphAllocations* subgraph_allocations =
+  tflite_micro::SubgraphAllocations* subgraph_allocations =
       allocator->StartModelAllocation(model);
   TF_LITE_MICRO_EXPECT(nullptr != subgraph_allocations);
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteOk, allocator->FinishModelAllocation(model, subgraph_allocations,
                                                   &scratch_buffer_handles));
 
-  size_t model_tensor_size = tflite::testing::GetModelTensorCount(model);
+  size_t model_tensor_size = tflite_micro::testing::GetModelTensorCount(model);
   TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(10), model_tensor_size);
 
   // NOTE: Tensor indexes match the values in GetComplexMockModel().
-  tflite::testing::AllocateAndVerifyMockTensor(model, allocator,
+  tflite_micro::testing::AllocateAndVerifyMockTensor(model, allocator,
                                                subgraph_allocations, 0);
-  tflite::testing::AllocateAndVerifyMockTensor(model, allocator,
+  tflite_micro::testing::AllocateAndVerifyMockTensor(model, allocator,
                                                subgraph_allocations, 1,
                                                /*is_variable=*/
                                                true);
-  tflite::testing::AllocateAndVerifyMockWeightTensor(model, allocator,
+  tflite_micro::testing::AllocateAndVerifyMockWeightTensor(model, allocator,
                                                      subgraph_allocations, 2);
-  tflite::testing::AllocateAndVerifyMockTensor(model, allocator,
+  tflite_micro::testing::AllocateAndVerifyMockTensor(model, allocator,
                                                subgraph_allocations, 3);
-  tflite::testing::AllocateAndVerifyMockTensor(model, allocator,
+  tflite_micro::testing::AllocateAndVerifyMockTensor(model, allocator,
                                                subgraph_allocations, 4,
                                                /*is_variable=*/
                                                true);
-  tflite::testing::AllocateAndVerifyMockWeightTensor(model, allocator,
+  tflite_micro::testing::AllocateAndVerifyMockWeightTensor(model, allocator,
                                                      subgraph_allocations, 5);
-  tflite::testing::AllocateAndVerifyMockTensor(model, allocator,
+  tflite_micro::testing::AllocateAndVerifyMockTensor(model, allocator,
                                                subgraph_allocations, 6);
-  tflite::testing::AllocateAndVerifyMockTensor(model, allocator,
+  tflite_micro::testing::AllocateAndVerifyMockTensor(model, allocator,
                                                subgraph_allocations, 7,
                                                /*is_variable=*/
                                                true);
-  tflite::testing::AllocateAndVerifyMockWeightTensor(model, allocator,
+  tflite_micro::testing::AllocateAndVerifyMockWeightTensor(model, allocator,
                                                      subgraph_allocations, 8);
-  tflite::testing::AllocateAndVerifyMockTensor(model, allocator,
+  tflite_micro::testing::AllocateAndVerifyMockTensor(model, allocator,
                                                subgraph_allocations, 9);
 
   // // Ensure that variable tensors have unique address
-  tflite::testing::EnsureUniqueVariableTensorBuffer(
+  tflite_micro::testing::EnsureUniqueVariableTensorBuffer(
       model, subgraph_allocations[0].tensors, 1);
-  tflite::testing::EnsureUniqueVariableTensorBuffer(
+  tflite_micro::testing::EnsureUniqueVariableTensorBuffer(
       model, subgraph_allocations[0].tensors, 4);
-  tflite::testing::EnsureUniqueVariableTensorBuffer(
+  tflite_micro::testing::EnsureUniqueVariableTensorBuffer(
       model, subgraph_allocations[0].tensors, 7);
 
   // ComplexMockModel has 3 operators:
-  tflite::testing::VerifyRegistrationAndNodeAllocation(subgraph_allocations,
+  tflite_micro::testing::VerifyRegistrationAndNodeAllocation(subgraph_allocations,
                                                        /*count=*/3,
                                                        /*num_subgraphs=*/1);
 }
@@ -651,10 +651,10 @@ TF_LITE_MICRO_TEST(OfflinePlannerBranchesAllOnline) {
   int version = 1;
   int subgraph = 0;
   constexpr int number_tensors = 4;
-  tflite::testing::TestingOpResolver op_resolver;
+  tflite_micro::testing::TestingOpResolver op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::testing::GetTestingOpResolver(op_resolver));
-  const int32_t metadata_buffer[tflite::testing::kOfflinePlannerHeaderSize +
+                          tflite_micro::testing::GetTestingOpResolver(op_resolver));
+  const int32_t metadata_buffer[tflite_micro::testing::kOfflinePlannerHeaderSize +
                                 number_tensors] = {version, subgraph,
                                                    number_tensors,  // header
                                                    // memory offsets:
@@ -663,7 +663,7 @@ TF_LITE_MICRO_TEST(OfflinePlannerBranchesAllOnline) {
   // The structure is identical to the one in
   // TestAllocationForModelsWithBranches
   int number_connections = 3;
-  tflite::testing::NodeConnection node_list[3] = {{
+  tflite_micro::testing::NodeConnection node_list[3] = {{
                                                       {0},  // input
                                                       {1}   // output
                                                   },
@@ -676,17 +676,17 @@ TF_LITE_MICRO_TEST(OfflinePlannerBranchesAllOnline) {
                                                       {3}      // output
                                                   }};
 
-  const tflite::Model* model = tflite::testing::GetModelWithOfflinePlanning(
+  const tflite_micro::Model* model = tflite_micro::testing::GetModelWithOfflinePlanning(
       number_tensors, metadata_buffer, node_list, number_connections);
 
-  tflite::ScratchBufferHandle* scratch_buffer_handles = nullptr;
+  tflite_micro::ScratchBufferHandle* scratch_buffer_handles = nullptr;
 
   constexpr size_t arena_size = 4096;
   uint8_t arena[arena_size];
-  tflite::MicroAllocator* allocator =
-      tflite::MicroAllocator::Create(arena, arena_size);
+  tflite_micro::MicroAllocator* allocator =
+      tflite_micro::MicroAllocator::Create(arena, arena_size);
 
-  tflite::SubgraphAllocations* subgraph_allocations =
+  tflite_micro::SubgraphAllocations* subgraph_allocations =
       allocator->StartModelAllocation(model);
   TF_LITE_MICRO_EXPECT(nullptr != subgraph_allocations);
   TF_LITE_MICRO_EXPECT_EQ(
@@ -697,57 +697,57 @@ TF_LITE_MICRO_TEST(OfflinePlannerBranchesAllOnline) {
   // identical to that in TestAllocationForModelsWithBranches,
   // the offsets be should identical to that test.
   int8_t* start =
-      tflite::micro::GetTensorData<int8_t>(&subgraph_allocations[0].tensors[0]);
-  TF_LITE_MICRO_EXPECT_EQ(0, tflite::micro::GetTensorData<int8_t>(
+      tflite_micro::micro::GetTensorData<int8_t>(&subgraph_allocations[0].tensors[0]);
+  TF_LITE_MICRO_EXPECT_EQ(0, tflite_micro::micro::GetTensorData<int8_t>(
                                  &subgraph_allocations[0].tensors[0]) -
                                  start);
 
   size_t buffer_size;
   TF_LITE_MICRO_EXPECT_EQ(
-      kTfLiteOk, tflite::TfLiteEvalTensorByteLength(
+      kTfLiteOk, tflite_micro::TfLiteEvalTensorByteLength(
                      &subgraph_allocations[0].tensors[0], &buffer_size));
   TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(48), buffer_size);
-  TF_LITE_MICRO_EXPECT_EQ(96, tflite::micro::GetTensorData<int8_t>(
+  TF_LITE_MICRO_EXPECT_EQ(96, tflite_micro::micro::GetTensorData<int8_t>(
                                   &subgraph_allocations[0].tensors[1]) -
                                   start);
-  TF_LITE_MICRO_EXPECT_EQ(48, tflite::micro::GetTensorData<int8_t>(
+  TF_LITE_MICRO_EXPECT_EQ(48, tflite_micro::micro::GetTensorData<int8_t>(
                                   &subgraph_allocations[0].tensors[2]) -
                                   start);
-  TF_LITE_MICRO_EXPECT_EQ(0, tflite::micro::GetTensorData<int8_t>(
+  TF_LITE_MICRO_EXPECT_EQ(0, tflite_micro::micro::GetTensorData<int8_t>(
                                  &subgraph_allocations[0].tensors[3]) -
                                  start);
 }
 
 TF_LITE_MICRO_TEST(OfflinePlannerBasic) {
   constexpr int number_tensors = 4;
-  tflite::testing::TestingOpResolver op_resolver;
+  tflite_micro::testing::TestingOpResolver op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::testing::GetTestingOpResolver(op_resolver));
-  const int32_t metadata_buffer[tflite::testing::kOfflinePlannerHeaderSize +
+                          tflite_micro::testing::GetTestingOpResolver(op_resolver));
+  const int32_t metadata_buffer[tflite_micro::testing::kOfflinePlannerHeaderSize +
                                 number_tensors] = {1,         0, number_tensors,
                                                    /*t0=*/0,
                                                    /*t1=*/48,
                                                    /*t2=*/0,
                                                    /*t3=*/48};
   constexpr int number_connections = 3;
-  tflite::testing::NodeConnection node_list[number_connections] = {
-      {/*input=*/{tflite::testing::t0},
-       /*output=*/{tflite::testing::t1}},
-      {/*input=*/{tflite::testing::t1},
-       /*output=*/{tflite::testing::t2}},
-      {/*input=*/{tflite::testing::t2},
-       /*output=*/{tflite::testing::t3}}};
-
-  const tflite::Model* model = tflite::testing::GetModelWithOfflinePlanning(
+  tflite_micro::testing::NodeConnection node_list[number_connections] = {
+      {/*input=*/{tflite_micro::testing::t0},
+       /*output=*/{tflite_micro::testing::t1}},
+      {/*input=*/{tflite_micro::testing::t1},
+       /*output=*/{tflite_micro::testing::t2}},
+      {/*input=*/{tflite_micro::testing::t2},
+       /*output=*/{tflite_micro::testing::t3}}};
+
+  const tflite_micro::Model* model = tflite_micro::testing::GetModelWithOfflinePlanning(
       number_tensors, metadata_buffer, node_list, number_connections);
 
-  tflite::ScratchBufferHandle* scratch_buffer_handles = nullptr;
+  tflite_micro::ScratchBufferHandle* scratch_buffer_handles = nullptr;
   constexpr size_t arena_size = 4096;
   uint8_t arena[arena_size];
-  tflite::MicroAllocator* allocator =
-      tflite::MicroAllocator::Create(arena, arena_size);
+  tflite_micro::MicroAllocator* allocator =
+      tflite_micro::MicroAllocator::Create(arena, arena_size);
 
-  tflite::SubgraphAllocations* subgraph_allocations =
+  tflite_micro::SubgraphAllocations* subgraph_allocations =
       allocator->StartModelAllocation(model);
   TF_LITE_MICRO_EXPECT(nullptr != subgraph_allocations);
   TF_LITE_MICRO_EXPECT_EQ(
@@ -755,27 +755,27 @@ TF_LITE_MICRO_TEST(OfflinePlannerBasic) {
                                                   &scratch_buffer_handles));
 
   int8_t* start =
-      tflite::micro::GetTensorData<int8_t>(&subgraph_allocations[0].tensors[0]);
-  TF_LITE_MICRO_EXPECT_EQ(0, tflite::micro::GetTensorData<int8_t>(
+      tflite_micro::micro::GetTensorData<int8_t>(&subgraph_allocations[0].tensors[0]);
+  TF_LITE_MICRO_EXPECT_EQ(0, tflite_micro::micro::GetTensorData<int8_t>(
                                  &subgraph_allocations[0].tensors[0]) -
                                  start);
-  TF_LITE_MICRO_EXPECT_EQ(48, tflite::micro::GetTensorData<int8_t>(
+  TF_LITE_MICRO_EXPECT_EQ(48, tflite_micro::micro::GetTensorData<int8_t>(
                                   &subgraph_allocations[0].tensors[1]) -
                                   start);
-  TF_LITE_MICRO_EXPECT_EQ(0, tflite::micro::GetTensorData<int8_t>(
+  TF_LITE_MICRO_EXPECT_EQ(0, tflite_micro::micro::GetTensorData<int8_t>(
                                  &subgraph_allocations[0].tensors[2]) -
                                  start);
-  TF_LITE_MICRO_EXPECT_EQ(48, tflite::micro::GetTensorData<int8_t>(
+  TF_LITE_MICRO_EXPECT_EQ(48, tflite_micro::micro::GetTensorData<int8_t>(
                                   &subgraph_allocations[0].tensors[3]) -
                                   start);
 }
 
 TF_LITE_MICRO_TEST(OfflinePlannerOverlappingAllocation) {
   constexpr int number_tensors = 4;
-  tflite::testing::TestingOpResolver op_resolver;
+  tflite_micro::testing::TestingOpResolver op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::testing::GetTestingOpResolver(op_resolver));
-  const int32_t metadata_buffer[tflite::testing::kOfflinePlannerHeaderSize +
+                          tflite_micro::testing::GetTestingOpResolver(op_resolver));
+  const int32_t metadata_buffer[tflite_micro::testing::kOfflinePlannerHeaderSize +
                                 number_tensors] = {/*version=*/1,
                                                    /*subgraph=*/0,
                                                    number_tensors,
@@ -785,23 +785,23 @@ TF_LITE_MICRO_TEST(OfflinePlannerOverlappingAllocation) {
                                                    /*t3=*/-1};
 
   int number_connections = 2;
-  tflite::testing::NodeConnection node_list[2] = {
-      {/*input, scratch=*/{tflite::testing::t0, tflite::testing::t1},
-       /*output=*/{tflite::testing::t2}},
-      {/*input=*/{tflite::testing::t2},
-       /*output=*/{tflite::testing::t3}},
+  tflite_micro::testing::NodeConnection node_list[2] = {
+      {/*input, scratch=*/{tflite_micro::testing::t0, tflite_micro::testing::t1},
+       /*output=*/{tflite_micro::testing::t2}},
+      {/*input=*/{tflite_micro::testing::t2},
+       /*output=*/{tflite_micro::testing::t3}},
   };
 
-  const tflite::Model* model = tflite::testing::GetModelWithOfflinePlanning(
+  const tflite_micro::Model* model = tflite_micro::testing::GetModelWithOfflinePlanning(
       number_tensors, metadata_buffer, node_list, number_connections);
 
-  tflite::ScratchBufferHandle* scratch_buffer_handles = nullptr;
+  tflite_micro::ScratchBufferHandle* scratch_buffer_handles = nullptr;
   constexpr size_t arena_size = 4096;
   uint8_t arena[arena_size];
-  tflite::MicroAllocator* allocator =
-      tflite::MicroAllocator::Create(arena, arena_size);
+  tflite_micro::MicroAllocator* allocator =
+      tflite_micro::MicroAllocator::Create(arena, arena_size);
 
-  tflite::SubgraphAllocations* subgraph_allocations =
+  tflite_micro::SubgraphAllocations* subgraph_allocations =
       allocator->StartModelAllocation(model);
   TF_LITE_MICRO_EXPECT(nullptr != subgraph_allocations);
   TF_LITE_MICRO_EXPECT_EQ(
@@ -809,17 +809,17 @@ TF_LITE_MICRO_TEST(OfflinePlannerOverlappingAllocation) {
                                                   &scratch_buffer_handles));
 
   int8_t* start =
-      tflite::micro::GetTensorData<int8_t>(&subgraph_allocations[0].tensors[0]);
-  TF_LITE_MICRO_EXPECT_EQ(0, tflite::micro::GetTensorData<int8_t>(
+      tflite_micro::micro::GetTensorData<int8_t>(&subgraph_allocations[0].tensors[0]);
+  TF_LITE_MICRO_EXPECT_EQ(0, tflite_micro::micro::GetTensorData<int8_t>(
                                  &subgraph_allocations[0].tensors[0]) -
                                  start);
-  TF_LITE_MICRO_EXPECT_EQ(0, tflite::micro::GetTensorData<int8_t>(
+  TF_LITE_MICRO_EXPECT_EQ(0, tflite_micro::micro::GetTensorData<int8_t>(
                                  &subgraph_allocations[0].tensors[1]) -
                                  start);
-  TF_LITE_MICRO_EXPECT_EQ(48, tflite::micro::GetTensorData<int8_t>(
+  TF_LITE_MICRO_EXPECT_EQ(48, tflite_micro::micro::GetTensorData<int8_t>(
                                   &subgraph_allocations[0].tensors[2]) -
                                   start);
-  TF_LITE_MICRO_EXPECT_EQ(0, tflite::micro::GetTensorData<int8_t>(
+  TF_LITE_MICRO_EXPECT_EQ(0, tflite_micro::micro::GetTensorData<int8_t>(
                                  &subgraph_allocations[0].tensors[3]) -
                                  start);
   // TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(48), context.tensors[0].bytes);
@@ -827,10 +827,10 @@ TF_LITE_MICRO_TEST(OfflinePlannerOverlappingAllocation) {
 
 TF_LITE_MICRO_TEST(OfflinePlannerOfflineOnline) {
   constexpr int number_tensors = 5;
-  tflite::testing::TestingOpResolver op_resolver;
+  tflite_micro::testing::TestingOpResolver op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::testing::GetTestingOpResolver(op_resolver));
-  const int32_t metadata_buffer[tflite::testing::kOfflinePlannerHeaderSize +
+                          tflite_micro::testing::GetTestingOpResolver(op_resolver));
+  const int32_t metadata_buffer[tflite_micro::testing::kOfflinePlannerHeaderSize +
                                 number_tensors] = {/*version=*/1,
                                                    /*subgraph=*/0,
                                                    number_tensors,
@@ -841,27 +841,27 @@ TF_LITE_MICRO_TEST(OfflinePlannerOfflineOnline) {
                                                    /*t4=*/-1};
 
   constexpr int number_connections = 2;
-  tflite::testing::NodeConnection node_list[number_connections] = {
+  tflite_micro::testing::NodeConnection node_list[number_connections] = {
       {
-          /*input, scratch=*/{tflite::testing::t0, tflite::testing::t1},
-          /*output=*/{tflite::testing::t2},
+          /*input, scratch=*/{tflite_micro::testing::t0, tflite_micro::testing::t1},
+          /*output=*/{tflite_micro::testing::t2},
       },
       {
-          /*input=*/{tflite::testing::t2},
-          /*output1, output2=*/{tflite::testing::t3, tflite::testing::t4},
+          /*input=*/{tflite_micro::testing::t2},
+          /*output1, output2=*/{tflite_micro::testing::t3, tflite_micro::testing::t4},
       },
   };
 
-  const tflite::Model* model = tflite::testing::GetModelWithOfflinePlanning(
+  const tflite_micro::Model* model = tflite_micro::testing::GetModelWithOfflinePlanning(
       number_tensors, metadata_buffer, node_list, number_connections);
 
-  tflite::ScratchBufferHandle* scratch_buffer_handles = nullptr;
+  tflite_micro::ScratchBufferHandle* scratch_buffer_handles = nullptr;
   constexpr size_t arena_size = 4096;
   uint8_t arena[arena_size];
-  tflite::MicroAllocator* allocator =
-      tflite::MicroAllocator::Create(arena, arena_size);
+  tflite_micro::MicroAllocator* allocator =
+      tflite_micro::MicroAllocator::Create(arena, arena_size);
 
-  tflite::SubgraphAllocations* subgraph_allocations =
+  tflite_micro::SubgraphAllocations* subgraph_allocations =
       allocator->StartModelAllocation(model);
   TF_LITE_MICRO_EXPECT(nullptr != subgraph_allocations);
   TF_LITE_MICRO_EXPECT_EQ(
@@ -869,30 +869,30 @@ TF_LITE_MICRO_TEST(OfflinePlannerOfflineOnline) {
                                                   &scratch_buffer_handles));
 
   int8_t* start =
-      tflite::micro::GetTensorData<int8_t>(&subgraph_allocations[0].tensors[0]);
-  TF_LITE_MICRO_EXPECT_EQ(0, tflite::micro::GetTensorData<int8_t>(
+      tflite_micro::micro::GetTensorData<int8_t>(&subgraph_allocations[0].tensors[0]);
+  TF_LITE_MICRO_EXPECT_EQ(0, tflite_micro::micro::GetTensorData<int8_t>(
                                  &subgraph_allocations[0].tensors[0]) -
                                  start);
-  TF_LITE_MICRO_EXPECT_EQ(48, tflite::micro::GetTensorData<int8_t>(
+  TF_LITE_MICRO_EXPECT_EQ(48, tflite_micro::micro::GetTensorData<int8_t>(
                                   &subgraph_allocations[0].tensors[1]) -
                                   start);
-  TF_LITE_MICRO_EXPECT_EQ(96, tflite::micro::GetTensorData<int8_t>(
+  TF_LITE_MICRO_EXPECT_EQ(96, tflite_micro::micro::GetTensorData<int8_t>(
                                   &subgraph_allocations[0].tensors[2]) -
                                   start);
-  TF_LITE_MICRO_EXPECT_EQ(48, tflite::micro::GetTensorData<int8_t>(
+  TF_LITE_MICRO_EXPECT_EQ(48, tflite_micro::micro::GetTensorData<int8_t>(
                                   &subgraph_allocations[0].tensors[4]) -
                                   start);
-  TF_LITE_MICRO_EXPECT_EQ(0, tflite::micro::GetTensorData<int8_t>(
+  TF_LITE_MICRO_EXPECT_EQ(0, tflite_micro::micro::GetTensorData<int8_t>(
                                  &subgraph_allocations[0].tensors[3]) -
                                  start);
 }
 
 TF_LITE_MICRO_TEST(TestAllocatePersistentTfLiteTensor) {
-  const tflite::Model* model = tflite::GetModel(kTestConvModelData);
+  const tflite_micro::Model* model = tflite_micro::GetModel(kTestConvModelData);
   constexpr size_t arena_size = 1024 * 12;
   uint8_t arena[arena_size];
-  tflite::MicroAllocator* allocator =
-      tflite::MicroAllocator::Create(arena, arena_size);
+  tflite_micro::MicroAllocator* allocator =
+      tflite_micro::MicroAllocator::Create(arena, arena_size);
   TF_LITE_MICRO_EXPECT(allocator != nullptr);
 
   TfLiteTensor* tensor1 = allocator->AllocatePersistentTfLiteTensor(
@@ -915,21 +915,21 @@ TF_LITE_MICRO_TEST(TestAllocatePersistentTfLiteTensor) {
 }
 
 TF_LITE_MICRO_TEST(TestFailAllocatePersistentTfLiteTensor) {
-  const tflite::Model* model = tflite::testing::GetSimpleMockModel();
+  const tflite_micro::Model* model = tflite_micro::testing::GetSimpleMockModel();
   // MicroAllocator::Create always allocates GreedyMemoryPlanner,
   // SingleArenaBufferAllocator and MicroAllocator objects.
   // Memory available should be <= the sum of the alignments which
   // is < sizeof(TfLiteTensor).
-  constexpr size_t kArenaSize = sizeof(tflite::GreedyMemoryPlanner) +
-                                alignof(tflite::GreedyMemoryPlanner) +
-                                sizeof(tflite::MicroAllocator) +
-                                alignof(tflite::MicroAllocator) +
-                                sizeof(tflite::SingleArenaBufferAllocator) +
-                                alignof(tflite::SingleArenaBufferAllocator) +
-                                tflite::MicroArenaBufferAlignment();
+  constexpr size_t kArenaSize = sizeof(tflite_micro::GreedyMemoryPlanner) +
+                                alignof(tflite_micro::GreedyMemoryPlanner) +
+                                sizeof(tflite_micro::MicroAllocator) +
+                                alignof(tflite_micro::MicroAllocator) +
+                                sizeof(tflite_micro::SingleArenaBufferAllocator) +
+                                alignof(tflite_micro::SingleArenaBufferAllocator) +
+                                tflite_micro::MicroArenaBufferAlignment();
   uint8_t arena[kArenaSize];
-  tflite::MicroAllocator* allocator =
-      tflite::MicroAllocator::Create(arena, sizeof(arena));
+  tflite_micro::MicroAllocator* allocator =
+      tflite_micro::MicroAllocator::Create(arena, sizeof(arena));
   TF_LITE_MICRO_EXPECT(allocator != nullptr);
 
   TfLiteTensor* tensor1 = allocator->AllocatePersistentTfLiteTensor(
@@ -939,11 +939,11 @@ TF_LITE_MICRO_TEST(TestFailAllocatePersistentTfLiteTensor) {
 }
 
 TF_LITE_MICRO_TEST(TestAllocateSingleTempTfLiteTensor) {
-  const tflite::Model* model = tflite::testing::GetSimpleMockModel();
+  const tflite_micro::Model* model = tflite_micro::testing::GetSimpleMockModel();
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::MicroAllocator* allocator =
-      tflite::MicroAllocator::Create(arena, arena_size);
+  tflite_micro::MicroAllocator* allocator =
+      tflite_micro::MicroAllocator::Create(arena, arena_size);
   TF_LITE_MICRO_EXPECT(allocator != nullptr);
 
   TfLiteTensor* tensor1 = allocator->AllocateTempTfLiteTensor(
@@ -953,11 +953,11 @@ TF_LITE_MICRO_TEST(TestAllocateSingleTempTfLiteTensor) {
 }
 
 TF_LITE_MICRO_TEST(TestAllocateChainOfTfLiteTensor) {
-  const tflite::Model* model = tflite::testing::GetSimpleMockModel();
+  const tflite_micro::Model* model = tflite_micro::testing::GetSimpleMockModel();
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::MicroAllocator* allocator =
-      tflite::MicroAllocator::Create(arena, arena_size);
+  tflite_micro::MicroAllocator* allocator =
+      tflite_micro::MicroAllocator::Create(arena, arena_size);
   TF_LITE_MICRO_EXPECT(allocator != nullptr);
 
   TfLiteTensor* tensor1 = allocator->AllocateTempTfLiteTensor(
@@ -976,11 +976,11 @@ TF_LITE_MICRO_TEST(TestAllocateChainOfTfLiteTensor) {
 }
 
 TF_LITE_MICRO_TEST(TestAllocateAndDeallocateChainOfTfLiteTensor) {
-  const tflite::Model* model = tflite::testing::GetSimpleMockModel();
+  const tflite_micro::Model* model = tflite_micro::testing::GetSimpleMockModel();
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::MicroAllocator* allocator =
-      tflite::MicroAllocator::Create(arena, arena_size);
+  tflite_micro::MicroAllocator* allocator =
+      tflite_micro::MicroAllocator::Create(arena, arena_size);
   TF_LITE_MICRO_EXPECT(allocator != nullptr);
 
   TfLiteTensor* tensor1 = allocator->AllocateTempTfLiteTensor(
@@ -1009,12 +1009,12 @@ TF_LITE_MICRO_TEST(TestAllocateAndDeallocateChainOfTfLiteTensor) {
 TF_LITE_MICRO_TEST(TestAllocateAndDeallocateTempBuffer) {
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::MicroAllocator* allocator =
-      tflite::MicroAllocator::Create(arena, arena_size);
+  tflite_micro::MicroAllocator* allocator =
+      tflite_micro::MicroAllocator::Create(arena, arena_size);
   TF_LITE_MICRO_EXPECT(allocator != nullptr);
   TF_LITE_MICRO_EXPECT_TRUE(allocator->IsAllTempDeallocated());
   uint8_t* buffer1 =
-      allocator->AllocateTempBuffer(10, tflite::MicroArenaBufferAlignment());
+      allocator->AllocateTempBuffer(10, tflite_micro::MicroArenaBufferAlignment());
   TF_LITE_MICRO_EXPECT(buffer1 != nullptr);
   TF_LITE_MICRO_EXPECT_FALSE(allocator->IsAllTempDeallocated());
   allocator->DeallocateTempBuffer(buffer1);
@@ -1022,11 +1022,11 @@ TF_LITE_MICRO_TEST(TestAllocateAndDeallocateTempBuffer) {
 }
 
 TF_LITE_MICRO_TEST(TestAllocateTfLiteTensorWithReset) {
-  const tflite::Model* model = tflite::testing::GetSimpleMockModel();
+  const tflite_micro::Model* model = tflite_micro::testing::GetSimpleMockModel();
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::MicroAllocator* allocator =
-      tflite::MicroAllocator::Create(arena, arena_size);
+  tflite_micro::MicroAllocator* allocator =
+      tflite_micro::MicroAllocator::Create(arena, arena_size);
   TF_LITE_MICRO_EXPECT(allocator != nullptr);
 
   TfLiteTensor* tensor1 = allocator->AllocateTempTfLiteTensor(
@@ -1050,10 +1050,10 @@ TF_LITE_MICRO_TEST(TestAllocateTfLiteTensorWithReset) {
 
 TF_LITE_MICRO_TEST(TestOperatorInputsNotInSubgraphInputs) {
   constexpr int number_tensors = 5;
-  tflite::testing::TestingOpResolver op_resolver;
+  tflite_micro::testing::TestingOpResolver op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::testing::GetTestingOpResolver(op_resolver));
-  const int32_t metadata_buffer[tflite::testing::kOfflinePlannerHeaderSize +
+                          tflite_micro::testing::GetTestingOpResolver(op_resolver));
+  const int32_t metadata_buffer[tflite_micro::testing::kOfflinePlannerHeaderSize +
                                 number_tensors] = {/*version=*/1,
                                                    /*subgraph=*/0,
                                                    number_tensors,
@@ -1064,28 +1064,28 @@ TF_LITE_MICRO_TEST(TestOperatorInputsNotInSubgraphInputs) {
                                                    /*t4=*/-1};
 
   constexpr int number_connections = 2;
-  tflite::testing::NodeConnection node_list[number_connections] = {
+  tflite_micro::testing::NodeConnection node_list[number_connections] = {
       {// t0: input (actual input part of subgraph inputs as
        // well as operator inputs)
        // t1: scratch1 (only in operator inputs)
        // t2: scratch2 (only in operator inputs)
-       {tflite::testing::t0, tflite::testing::t1, tflite::testing::t2},
-       /*t3: output=*/{tflite::testing::t3}},
-      {/*t3: input=*/{tflite::testing::t3},
-       /*t4: output=*/{tflite::testing::t4}},
+       {tflite_micro::testing::t0, tflite_micro::testing::t1, tflite_micro::testing::t2},
+       /*t3: output=*/{tflite_micro::testing::t3}},
+      {/*t3: input=*/{tflite_micro::testing::t3},
+       /*t4: output=*/{tflite_micro::testing::t4}},
   };
 
-  const tflite::Model* model = tflite::testing::GetModelWithOfflinePlanning(
+  const tflite_micro::Model* model = tflite_micro::testing::GetModelWithOfflinePlanning(
       number_tensors, metadata_buffer, node_list, number_connections,
       /*Only first tensor (t0) is in subgraph input list=*/1);
 
-  tflite::ScratchBufferHandle* scratch_buffer_handles = nullptr;
+  tflite_micro::ScratchBufferHandle* scratch_buffer_handles = nullptr;
   constexpr size_t arena_size = 4096;
   uint8_t arena[arena_size];
-  tflite::MicroAllocator* allocator =
-      tflite::MicroAllocator::Create(arena, arena_size);
+  tflite_micro::MicroAllocator* allocator =
+      tflite_micro::MicroAllocator::Create(arena, arena_size);
 
-  tflite::SubgraphAllocations* subgraph_allocations =
+  tflite_micro::SubgraphAllocations* subgraph_allocations =
       allocator->StartModelAllocation(model);
   TF_LITE_MICRO_EXPECT(nullptr != subgraph_allocations);
   TF_LITE_MICRO_EXPECT_EQ(
@@ -1093,30 +1093,30 @@ TF_LITE_MICRO_TEST(TestOperatorInputsNotInSubgraphInputs) {
                                                   &scratch_buffer_handles));
 
   int8_t* start =
-      tflite::micro::GetTensorData<int8_t>(&subgraph_allocations[0].tensors[0]);
-  TF_LITE_MICRO_EXPECT_EQ(0, tflite::micro::GetTensorData<int8_t>(
+      tflite_micro::micro::GetTensorData<int8_t>(&subgraph_allocations[0].tensors[0]);
+  TF_LITE_MICRO_EXPECT_EQ(0, tflite_micro::micro::GetTensorData<int8_t>(
                                  &subgraph_allocations[0].tensors[0]) -
                                  start);
-  TF_LITE_MICRO_EXPECT_EQ(0, tflite::micro::GetTensorData<int8_t>(
+  TF_LITE_MICRO_EXPECT_EQ(0, tflite_micro::micro::GetTensorData<int8_t>(
                                  &subgraph_allocations[0].tensors[1]) -
                                  start);
-  TF_LITE_MICRO_EXPECT_EQ(0, tflite::micro::GetTensorData<int8_t>(
+  TF_LITE_MICRO_EXPECT_EQ(0, tflite_micro::micro::GetTensorData<int8_t>(
                                  &subgraph_allocations[0].tensors[2]) -
                                  start);
-  TF_LITE_MICRO_EXPECT_EQ(48, tflite::micro::GetTensorData<int8_t>(
+  TF_LITE_MICRO_EXPECT_EQ(48, tflite_micro::micro::GetTensorData<int8_t>(
                                   &subgraph_allocations[0].tensors[3]) -
                                   start);
-  TF_LITE_MICRO_EXPECT_EQ(0, tflite::micro::GetTensorData<int8_t>(
+  TF_LITE_MICRO_EXPECT_EQ(0, tflite_micro::micro::GetTensorData<int8_t>(
                                  &subgraph_allocations[0].tensors[4]) -
                                  start);
 }
 
 TF_LITE_MICRO_TEST(TestTypicalFirstOpAndSecondOpWithScratchTensors) {
   constexpr int number_tensors = 6;
-  tflite::testing::TestingOpResolver op_resolver;
+  tflite_micro::testing::TestingOpResolver op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::testing::GetTestingOpResolver(op_resolver));
-  const int32_t metadata_buffer[tflite::testing::kOfflinePlannerHeaderSize +
+                          tflite_micro::testing::GetTestingOpResolver(op_resolver));
+  const int32_t metadata_buffer[tflite_micro::testing::kOfflinePlannerHeaderSize +
                                 number_tensors] = {/*version=*/1,
                                                    /*subgraph=*/0,
                                                    number_tensors,
@@ -1128,30 +1128,30 @@ TF_LITE_MICRO_TEST(TestTypicalFirstOpAndSecondOpWithScratchTensors) {
                                                    /*t5=*/-1};
 
   constexpr int number_connections = 3;
-  tflite::testing::NodeConnection node_list[number_connections] = {
-      {/*t0: input (subgraph and operator input)=*/{tflite::testing::t0},
-       /*t1: output=*/{tflite::testing::t1}},
+  tflite_micro::testing::NodeConnection node_list[number_connections] = {
+      {/*t0: input (subgraph and operator input)=*/{tflite_micro::testing::t0},
+       /*t1: output=*/{tflite_micro::testing::t1}},
       {// t1: input
        // t2: scratch1 (only in operator inputs)
        // t3: scratch2 (only in operator inputs)
-       {tflite::testing::t1, tflite::testing::t2, tflite::testing::t3},
+       {tflite_micro::testing::t1, tflite_micro::testing::t2, tflite_micro::testing::t3},
 
-       /*t4: output=*/{tflite::testing::t4}},
-      {/*t4: input=*/{tflite::testing::t4},
-       /*t5: output=*/{tflite::testing::t5}},
+       /*t4: output=*/{tflite_micro::testing::t4}},
+      {/*t4: input=*/{tflite_micro::testing::t4},
+       /*t5: output=*/{tflite_micro::testing::t5}},
   };
 
-  const tflite::Model* model = tflite::testing::GetModelWithOfflinePlanning(
+  const tflite_micro::Model* model = tflite_micro::testing::GetModelWithOfflinePlanning(
       number_tensors, metadata_buffer, node_list, number_connections,
       /*Only first tensor (t0) is in subgraph input list=*/1);
 
-  tflite::ScratchBufferHandle* scratch_buffer_handles = nullptr;
+  tflite_micro::ScratchBufferHandle* scratch_buffer_handles = nullptr;
   constexpr size_t arena_size = 4096;
   uint8_t arena[arena_size];
-  tflite::MicroAllocator* allocator =
-      tflite::MicroAllocator::Create(arena, arena_size);
+  tflite_micro::MicroAllocator* allocator =
+      tflite_micro::MicroAllocator::Create(arena, arena_size);
 
-  tflite::SubgraphAllocations* subgraph_allocations =
+  tflite_micro::SubgraphAllocations* subgraph_allocations =
       allocator->StartModelAllocation(model);
   TF_LITE_MICRO_EXPECT(nullptr != subgraph_allocations);
   TF_LITE_MICRO_EXPECT_EQ(
@@ -1159,41 +1159,41 @@ TF_LITE_MICRO_TEST(TestTypicalFirstOpAndSecondOpWithScratchTensors) {
                                                   &scratch_buffer_handles));
 
   int8_t* start =
-      tflite::micro::GetTensorData<int8_t>(&subgraph_allocations[0].tensors[0]);
-  TF_LITE_MICRO_EXPECT_EQ(0, tflite::micro::GetTensorData<int8_t>(
+      tflite_micro::micro::GetTensorData<int8_t>(&subgraph_allocations[0].tensors[0]);
+  TF_LITE_MICRO_EXPECT_EQ(0, tflite_micro::micro::GetTensorData<int8_t>(
                                  &subgraph_allocations[0].tensors[0]) -
                                  start);
-  TF_LITE_MICRO_EXPECT_EQ(0, tflite::micro::GetTensorData<int8_t>(
+  TF_LITE_MICRO_EXPECT_EQ(0, tflite_micro::micro::GetTensorData<int8_t>(
                                  &subgraph_allocations[0].tensors[1]) -
                                  start);
-  TF_LITE_MICRO_EXPECT_EQ(0, tflite::micro::GetTensorData<int8_t>(
+  TF_LITE_MICRO_EXPECT_EQ(0, tflite_micro::micro::GetTensorData<int8_t>(
                                  &subgraph_allocations[0].tensors[2]) -
                                  start);
-  TF_LITE_MICRO_EXPECT_EQ(0, tflite::micro::GetTensorData<int8_t>(
+  TF_LITE_MICRO_EXPECT_EQ(0, tflite_micro::micro::GetTensorData<int8_t>(
                                  &subgraph_allocations[0].tensors[3]) -
                                  start);
-  TF_LITE_MICRO_EXPECT_EQ(48, tflite::micro::GetTensorData<int8_t>(
+  TF_LITE_MICRO_EXPECT_EQ(48, tflite_micro::micro::GetTensorData<int8_t>(
                                   &subgraph_allocations[0].tensors[4]) -
                                   start);
-  TF_LITE_MICRO_EXPECT_EQ(0, tflite::micro::GetTensorData<int8_t>(
+  TF_LITE_MICRO_EXPECT_EQ(0, tflite_micro::micro::GetTensorData<int8_t>(
                                  &subgraph_allocations[0].tensors[5]) -
                                  start);
 }
 
 TF_LITE_MICRO_TEST(TestModelWithUnusedTensors) {
-  tflite::testing::TestingOpResolver op_resolver;
+  tflite_micro::testing::TestingOpResolver op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::testing::GetTestingOpResolver(op_resolver));
+                          tflite_micro::testing::GetTestingOpResolver(op_resolver));
 
-  const tflite::Model* model = tflite::testing::GetModelWithUnusedInputs();
+  const tflite_micro::Model* model = tflite_micro::testing::GetModelWithUnusedInputs();
 
-  tflite::ScratchBufferHandle* scratch_buffer_handles = nullptr;
+  tflite_micro::ScratchBufferHandle* scratch_buffer_handles = nullptr;
   constexpr size_t arena_size = 4096;
   uint8_t arena[arena_size];
-  tflite::MicroAllocator* allocator =
-      tflite::MicroAllocator::Create(arena, arena_size);
+  tflite_micro::MicroAllocator* allocator =
+      tflite_micro::MicroAllocator::Create(arena, arena_size);
 
-  tflite::SubgraphAllocations* subgraph_allocations =
+  tflite_micro::SubgraphAllocations* subgraph_allocations =
       allocator->StartModelAllocation(model);
   TF_LITE_MICRO_EXPECT(nullptr != subgraph_allocations);
   TF_LITE_MICRO_EXPECT_EQ(
@@ -1202,37 +1202,37 @@ TF_LITE_MICRO_TEST(TestModelWithUnusedTensors) {
 
   // Unused input tensor should not occupy any space.
   int8_t* start =
-      tflite::micro::GetTensorData<int8_t>(&subgraph_allocations[0].tensors[2]);
-  TF_LITE_MICRO_EXPECT_EQ(64, tflite::micro::GetTensorData<int8_t>(
+      tflite_micro::micro::GetTensorData<int8_t>(&subgraph_allocations[0].tensors[2]);
+  TF_LITE_MICRO_EXPECT_EQ(64, tflite_micro::micro::GetTensorData<int8_t>(
                                   &subgraph_allocations[0].tensors[0]) -
                                   start);
-  TF_LITE_MICRO_EXPECT_EQ(0, tflite::micro::GetTensorData<int8_t>(
+  TF_LITE_MICRO_EXPECT_EQ(0, tflite_micro::micro::GetTensorData<int8_t>(
                                  &subgraph_allocations[0].tensors[1]) -
                                  start);
-  TF_LITE_MICRO_EXPECT_EQ(0, tflite::micro::GetTensorData<int8_t>(
+  TF_LITE_MICRO_EXPECT_EQ(0, tflite_micro::micro::GetTensorData<int8_t>(
                                  &subgraph_allocations[0].tensors[2]) -
                                  start);
   // Unused tensor should not occupy any space.
-  TF_LITE_MICRO_EXPECT_EQ(0, tflite::micro::GetTensorData<int8_t>(
+  TF_LITE_MICRO_EXPECT_EQ(0, tflite_micro::micro::GetTensorData<int8_t>(
                                  &subgraph_allocations[0].tensors[3]) -
                                  start);
 }
 
 TF_LITE_MICRO_TEST(TestModelWithUnusedOperatorOutputs) {
-  tflite::testing::TestingOpResolver op_resolver;
+  tflite_micro::testing::TestingOpResolver op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::testing::GetTestingOpResolver(op_resolver));
+                          tflite_micro::testing::GetTestingOpResolver(op_resolver));
 
-  const tflite::Model* model =
-      tflite::testing::GetModelWithUnusedOperatorOutputs();
+  const tflite_micro::Model* model =
+      tflite_micro::testing::GetModelWithUnusedOperatorOutputs();
 
-  tflite::ScratchBufferHandle* scratch_buffer_handles = nullptr;
+  tflite_micro::ScratchBufferHandle* scratch_buffer_handles = nullptr;
   constexpr size_t arena_size = 4096;
   uint8_t arena[arena_size];
-  tflite::MicroAllocator* allocator =
-      tflite::MicroAllocator::Create(arena, arena_size);
+  tflite_micro::MicroAllocator* allocator =
+      tflite_micro::MicroAllocator::Create(arena, arena_size);
 
-  tflite::SubgraphAllocations* subgraph_allocations =
+  tflite_micro::SubgraphAllocations* subgraph_allocations =
       allocator->StartModelAllocation(model);
   TF_LITE_MICRO_EXPECT(nullptr != subgraph_allocations);
   TF_LITE_MICRO_EXPECT_EQ(
@@ -1241,11 +1241,11 @@ TF_LITE_MICRO_TEST(TestModelWithUnusedOperatorOutputs) {
 
   // Unused output tensor should have its own allocation.
   int8_t* start =
-      tflite::micro::GetTensorData<int8_t>(&subgraph_allocations[0].tensors[1]);
-  TF_LITE_MICRO_EXPECT_EQ(64, tflite::micro::GetTensorData<int8_t>(
+      tflite_micro::micro::GetTensorData<int8_t>(&subgraph_allocations[0].tensors[1]);
+  TF_LITE_MICRO_EXPECT_EQ(64, tflite_micro::micro::GetTensorData<int8_t>(
                                   &subgraph_allocations[0].tensors[0]) -
                                   start);
-  TF_LITE_MICRO_EXPECT_EQ(0, tflite::micro::GetTensorData<int8_t>(
+  TF_LITE_MICRO_EXPECT_EQ(0, tflite_micro::micro::GetTensorData<int8_t>(
                                  &subgraph_allocations[0].tensors[1]) -
                                  start);
 }
@@ -1254,14 +1254,14 @@ TF_LITE_MICRO_TEST(TestModelWithUnusedOperatorOutputs) {
 // interpreter and confirm that the eval tensors' offsets are exactly what was
 // specified in the offline plan.
 TF_LITE_MICRO_TEST(TestMockModelAllocationByNonPersistentMemoryPlannerShim) {
-  const tflite::Model* model = tflite::testing::GetSimpleMockModel();
+  const tflite_micro::Model* model = tflite_micro::testing::GetSimpleMockModel();
 
   // The simple model has three activation tensors 0, 2, 3, which corresponding
   // to buffer request 0, 1, 2.
   constexpr size_t kBufferEntriesCount = 3;
   constexpr size_t kBufferPlanSize =
-      sizeof(tflite::BufferPlan) +
-      (kBufferEntriesCount) * sizeof(tflite::BufferDescriptor);
+      sizeof(tflite_micro::BufferPlan) +
+      (kBufferEntriesCount) * sizeof(tflite_micro::BufferDescriptor);
   // The offsets of buffers are chosen to be very different from what the
   // default greedy memory planner would select to reflect that buffers does NOT
   // need to start at offset 0 and in contiguous order. The offsets in a given
@@ -1274,34 +1274,34 @@ TF_LITE_MICRO_TEST(TestMockModelAllocationByNonPersistentMemoryPlannerShim) {
   // Allocate a memory plan buffer first b/c the struct BufferPlan has a
   // flexible member array.
   uint8_t buffer_plan_arena[kBufferPlanSize];
-  tflite::BufferPlan* non_persistent_buffer_plan =
-      reinterpret_cast<tflite::BufferPlan*>(buffer_plan_arena);
+  tflite_micro::BufferPlan* non_persistent_buffer_plan =
+      reinterpret_cast<tflite_micro::BufferPlan*>(buffer_plan_arena);
   non_persistent_buffer_plan->buffer_count = kBufferEntriesCount;
   non_persistent_buffer_plan->buffer_plan_entries[0].offset = kOffset0;
   non_persistent_buffer_plan->buffer_plan_entries[1].offset = kOffset1;
   non_persistent_buffer_plan->buffer_plan_entries[2].offset = kOffset2;
 
-  tflite::NonPersistentMemoryPlannerShim planner(non_persistent_buffer_plan);
+  tflite_micro::NonPersistentMemoryPlannerShim planner(non_persistent_buffer_plan);
 
-  tflite::ScratchBufferHandle* scratch_buffer_handles = nullptr;
-  tflite::testing::TestingOpResolver op_resolver;
+  tflite_micro::ScratchBufferHandle* scratch_buffer_handles = nullptr;
+  tflite_micro::testing::TestingOpResolver op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::testing::GetTestingOpResolver(op_resolver));
+                          tflite_micro::testing::GetTestingOpResolver(op_resolver));
   constexpr size_t arena_size = 1024;
   uint8_t arena[arena_size];
-  tflite::MicroAllocator* allocator =
-      tflite::MicroAllocator::Create(arena, arena_size, &planner);
+  tflite_micro::MicroAllocator* allocator =
+      tflite_micro::MicroAllocator::Create(arena, arena_size, &planner);
   TF_LITE_MICRO_EXPECT(allocator != nullptr);
-  tflite::SubgraphAllocations* subgraph_allocations =
+  tflite_micro::SubgraphAllocations* subgraph_allocations =
       allocator->StartModelAllocation(model);
   TF_LITE_MICRO_EXPECT(nullptr != subgraph_allocations);
   TF_LITE_MICRO_EXPECT_EQ(
       kTfLiteOk, allocator->FinishModelAllocation(model, subgraph_allocations,
                                                   &scratch_buffer_handles));
 
-  size_t model_tensor_size = tflite::testing::GetModelTensorCount(model);
+  size_t model_tensor_size = tflite_micro::testing::GetModelTensorCount(model);
   TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(4), model_tensor_size);
-  tflite::testing::AllocateAndVerifyMockWeightTensor(model, allocator,
+  tflite_micro::testing::AllocateAndVerifyMockWeightTensor(model, allocator,
                                                      subgraph_allocations, 1);
 
   TfLiteEvalTensor* eval_tensors = subgraph_allocations[0].tensors;
@@ -1309,7 +1309,7 @@ TF_LITE_MICRO_TEST(TestMockModelAllocationByNonPersistentMemoryPlannerShim) {
   // Offset is relative to the arena after the buffer alignment adjustment which
   // happens when MicroAllocator is created.
   uint8_t* aligned_arena =
-      tflite::AlignPointerUp(arena, tflite::MicroArenaBufferAlignment());
+      tflite_micro::AlignPointerUp(arena, tflite_micro::MicroArenaBufferAlignment());
 
   TF_LITE_MICRO_EXPECT_TRUE(static_cast<uint8_t*>(eval_tensors[0].data.data) ==
                             (aligned_arena + kOffset0));
@@ -1319,25 +1319,25 @@ TF_LITE_MICRO_TEST(TestMockModelAllocationByNonPersistentMemoryPlannerShim) {
                             (aligned_arena + kOffset2));
 
   // SimpleMockModel has 2 operators:
-  tflite::testing::VerifyRegistrationAndNodeAllocation(subgraph_allocations,
+  tflite_micro::testing::VerifyRegistrationAndNodeAllocation(subgraph_allocations,
                                                        /*count=*/2,
                                                        /*num_subgraphs=*/1);
 }
 
 TF_LITE_MICRO_TEST(TestMultiSubgraphNumScratchAllocations) {
   // Any test model with multiple subgraphs will suffice
-  const tflite::Model* model =
-      tflite::testing::GetSimpleModelWithNullInputsAndOutputs();
+  const tflite_micro::Model* model =
+      tflite_micro::testing::GetSimpleModelWithNullInputsAndOutputs();
 
   constexpr size_t arena_size = 2048 * 2;
   uint8_t arena[arena_size];
 
-  tflite::MicroAllocator* allocator = nullptr;
-  tflite::SubgraphAllocations* subgraph_allocations = nullptr;
-  tflite::ScratchBufferHandle* scratch_buffer_handles = nullptr;
+  tflite_micro::MicroAllocator* allocator = nullptr;
+  tflite_micro::SubgraphAllocations* subgraph_allocations = nullptr;
+  tflite_micro::ScratchBufferHandle* scratch_buffer_handles = nullptr;
 
   // First iteration: no scratch buffers
-  allocator = tflite::MicroAllocator::Create(arena, arena_size);
+  allocator = tflite_micro::MicroAllocator::Create(arena, arena_size);
   TF_LITE_MICRO_EXPECT(nullptr != allocator);
 
   subgraph_allocations = allocator->StartModelAllocation(model);
@@ -1350,7 +1350,7 @@ TF_LITE_MICRO_TEST(TestMultiSubgraphNumScratchAllocations) {
   size_t used_bytes = allocator->used_bytes();
 
   // Second iteration: the same but request two scratch buffers
-  tflite::MicroAllocator::Create(arena, arena_size);
+  tflite_micro::MicroAllocator::Create(arena, arena_size);
   TF_LITE_MICRO_EXPECT(nullptr != allocator);
 
   subgraph_allocations = allocator->StartModelAllocation(model);
@@ -1377,11 +1377,11 @@ TF_LITE_MICRO_TEST(TestMultiSubgraphNumScratchAllocations) {
                                                   &scratch_buffer_handles));
 
   // Check that AllocateScratchBufferHandles was only called once, i.e. only two
-  // tflite::ScratchBufferHandle should have been allocated.
+  // tflite_micro::ScratchBufferHandle should have been allocated.
   size_t used_bytes_with_scratch = allocator->used_bytes();
 
   TF_LITE_MICRO_EXPECT_EQ(used_bytes_with_scratch,
-                          used_bytes + sizeof(tflite::ScratchBufferHandle) * 2);
+                          used_bytes + sizeof(tflite_micro::ScratchBufferHandle) * 2);
 }
 
 TF_LITE_MICRO_TESTS_END
diff --git a/tensorflow/lite/micro/micro_arena_constants.h b/tensorflow/lite/micro/micro_arena_constants.h
index 82828176..96c109e8 100644
--- a/tensorflow/lite/micro/micro_arena_constants.h
+++ b/tensorflow/lite/micro/micro_arena_constants.h
@@ -16,13 +16,13 @@ limitations under the License.
 #ifndef TENSORFLOW_LITE_MICRO_MICRO_ARENA_CONSTANTS_H_
 #define TENSORFLOW_LITE_MICRO_MICRO_ARENA_CONSTANTS_H_
 
-namespace tflite {
+namespace tflite_micro {
 
 // The default buffer alignment requirement.
 // We align tensor buffers to 16-byte boundaries, since this is a common
 // requirement for SIMD extensions.
 constexpr int MicroArenaBufferAlignment() { return 16; }
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_MICRO_ARENA_CONSTANTS_H_
diff --git a/tensorflow/lite/micro/micro_context.cc b/tensorflow/lite/micro/micro_context.cc
index 295b3c34..41dee5cc 100644
--- a/tensorflow/lite/micro/micro_context.cc
+++ b/tensorflow/lite/micro/micro_context.cc
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_common.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 int GetTensorIndex(int index, int max_size, const int* tensor_indices) {
@@ -74,4 +74,4 @@ void MicroContextReportOpError(struct TfLiteContext* context,
   va_end(args);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/micro_context.h b/tensorflow/lite/micro/micro_context.h
index 2dd3233a..40490ac0 100644
--- a/tensorflow/lite/micro/micro_context.h
+++ b/tensorflow/lite/micro/micro_context.h
@@ -19,7 +19,28 @@ limitations under the License.
 #include "tensorflow/lite/c/common.h"
 #include "tensorflow/lite/micro/micro_graph.h"
 
-namespace tflite {
+#define XCORE_TFLITE_MICRO_PATCHED
+
+#ifdef NO_INTERPRETER
+
+namespace tflite_micro {
+  const TfLiteStatus kTfLiteAbort = static_cast<TfLiteStatus>(15);
+
+  struct MicroContext{
+      TfLiteTensor* (*AllocateTempInputTensor)(const TfLiteNode* node, int index);
+      TfLiteTensor* (*AllocateTempOutputTensor)(const TfLiteNode* node, int index);
+      void (*DeallocateTempTfLiteTensor)(TfLiteTensor* tensor);
+      void* (*external_context)();
+      MicroGraph& (*graph)();
+  };
+  static inline MicroContext* GetMicroContext(const struct TfLiteContext* context){
+      return reinterpret_cast<MicroContext*>(context->impl_);
+  }
+}
+
+#else
+
+namespace tflite_micro {
 // TODO(b/149795762): kTfLiteAbort cannot be part of the tflite TfLiteStatus.
 const TfLiteStatus kTfLiteAbort = static_cast<TfLiteStatus>(15);
 
@@ -52,6 +73,7 @@ class MicroContext {
 
   // Returns a temporary TfLiteTensor struct for a given index.
   virtual TfLiteTensor* AllocateTempTfLiteTensor(int tensor_idx) = 0;
+  virtual TfLiteTensor* AllocateTempTfLiteTensor(int tensor_idx, int sg){return nullptr;}
 
   // Returns a temporary TfLiteTensor struct for the specified input tensor of a
   // given mode. This is the recommended API over the deprecated
@@ -85,6 +107,7 @@ class MicroContext {
 
   // Returns a TfLiteEvalTensor struct for a given index.
   virtual TfLiteEvalTensor* GetEvalTensor(int tensor_idx) = 0;
+  virtual TfLiteEvalTensor* GetEvalTensor(int tensor_idx, int sg){return nullptr;}
 
   // Does not take ownership of the pointer and the pointer must refer to valid
   // an object that outlive this class instance.
@@ -124,10 +147,18 @@ inline TfLiteTensor* MicroContextGetTensor(const struct TfLiteContext* context,
                                            int tensor_idx) {
   return GetMicroContext(context)->AllocateTempTfLiteTensor(tensor_idx);
 }
+inline TfLiteTensor* MicroContextGetTensor(const struct TfLiteContext* context,
+                                           int tensor_idx, int sg) {
+  return GetMicroContext(context)->AllocateTempTfLiteTensor(tensor_idx, sg);
+}
 inline TfLiteEvalTensor* MicroContextGetEvalTensor(
     const struct TfLiteContext* context, int tensor_idx) {
   return GetMicroContext(context)->GetEvalTensor(tensor_idx);
 }
+inline TfLiteEvalTensor* MicroContextGetEvalTensor(
+    const struct TfLiteContext* context, int tensor_idx, int sg) {
+  return GetMicroContext(context)->GetEvalTensor(tensor_idx, sg);
+}
 inline TfLiteExternalContext* MicroContextGetExternalContext(
     TfLiteContext* context, TfLiteExternalContextType unused) {
   return reinterpret_cast<TfLiteExternalContext*>(
@@ -138,6 +169,8 @@ inline TfLiteExternalContext* MicroContextGetExternalContext(
 void MicroContextReportOpError(struct TfLiteContext* context,
                                const char* format, ...);
 
-}  // namespace tflite
+}  // namespace tflite_micro
+
+#endif  // NO_INTERPRETER
 
 #endif  // TENSORFLOW_LITE_MICRO_MICRO_CONTEXT_H_
diff --git a/tensorflow/lite/micro/micro_graph.h b/tensorflow/lite/micro/micro_graph.h
index 79b36496..3bd12792 100644
--- a/tensorflow/lite/micro/micro_graph.h
+++ b/tensorflow/lite/micro/micro_graph.h
@@ -20,11 +20,26 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_common.h"
 #include "tensorflow/lite/micro/micro_resource_variable.h"
 
-namespace tflite {
+#ifdef NO_INTERPRETER
+
+namespace tflite_micro {
+  struct MicroGraph{
+      int (*NumSubgraphs)();
+      size_t (*NumSubgraphInputs)(int subgraph_idx);
+      size_t (*NumSubgraphOutputs)(int subgraph_idx);
+      TfLiteEvalTensor* (*GetSubgraphInput)(int subgraph_idx, int i);
+      TfLiteEvalTensor* (*GetSubgraphOutput)(int subgraph_idx, int i);
+      TfLiteStatus (*InvokeSubgraph)(int subgraph_idx);
+  };
+}
+
+#else
+
+namespace tflite_micro {
 
 // Abstracts the details of interacting with the graph from the kernels
 //
-// Provides methods to invoke any subgraph in the tflite::Graph.
+// Provides methods to invoke any subgraph in the tflite_micro::Graph.
 class MicroGraph {
  public:
   virtual ~MicroGraph() = default;
@@ -57,6 +72,8 @@ class MicroGraph {
   TF_LITE_REMOVE_VIRTUAL_DELETE
 };
 
-}  // namespace tflite
+}  // namespace tflite_micro
+
+#endif  // NO_INTERPRETER
 
 #endif  // TENSORFLOW_LITE_MICRO_MICRO_GRAPH_H_
diff --git a/tensorflow/lite/micro/micro_interpreter.cc b/tensorflow/lite/micro/micro_interpreter.cc
index 7f4565e6..bef65f5a 100644
--- a/tensorflow/lite/micro/micro_interpreter.cc
+++ b/tensorflow/lite/micro/micro_interpreter.cc
@@ -32,7 +32,7 @@ limitations under the License.
 #include "tensorflow/lite/schema/schema_generated.h"
 #include "tensorflow/lite/schema/schema_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 MemoryPlannerType FlagToMemoryPlannerType(bool preserve_all_tensors) {
   if (preserve_all_tensors) {
@@ -334,4 +334,4 @@ TfLiteStatus MicroInterpreter::SetMicroExternalContext(
   return micro_context_.set_external_context(external_context_payload);
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/micro_interpreter.h b/tensorflow/lite/micro/micro_interpreter.h
index 1c419962..abe0f83f 100644
--- a/tensorflow/lite/micro/micro_interpreter.h
+++ b/tensorflow/lite/micro/micro_interpreter.h
@@ -35,7 +35,7 @@ limitations under the License.
 // tensorflow/core.
 #define TFLITE_SCHEMA_VERSION (3)
 
-namespace tflite {
+namespace tflite_micro {
 
 class MicroInterpreter {
  public:
@@ -146,6 +146,13 @@ class MicroInterpreter {
     return allocator_.preserves_all_tensor();
   }
 
+  size_t operators_size(int sg) const { return model_->subgraphs()->Get(sg)->operators()->size(); }
+
+  // For debugging only.
+  const NodeAndRegistration node_and_registration(int node_index, int sg)  {
+    return graph_.GetAllocations()[sg].node_and_registrations[node_index];
+  }
+
  protected:
   const MicroAllocator& allocator() const { return allocator_; }
   const TfLiteContext& context() const { return context_; }
@@ -177,6 +184,6 @@ class MicroInterpreter {
   MicroInterpreterContext micro_context_;
 };
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_MICRO_INTERPRETER_H_
diff --git a/tensorflow/lite/micro/micro_interpreter_context.cc b/tensorflow/lite/micro/micro_interpreter_context.cc
index 098df15d..adff9014 100644
--- a/tensorflow/lite/micro/micro_interpreter_context.cc
+++ b/tensorflow/lite/micro/micro_interpreter_context.cc
@@ -19,7 +19,7 @@ limitations under the License.
 
 #include "tensorflow/lite/kernels/internal/compatibility.h"
 
-namespace tflite {
+namespace tflite_micro {
 MicroInterpreterContext::MicroInterpreterContext(MicroAllocator* allocator,
                                                  const Model* model,
                                                  MicroInterpreterGraph* graph)
@@ -56,6 +56,12 @@ TfLiteTensor* MicroInterpreterContext::AllocateTempTfLiteTensor(
                                              graph_.GetCurrentSubgraphIndex());
 }
 
+TfLiteTensor* MicroInterpreterContext::AllocateTempTfLiteTensor(int tensor_idx, int sg) {
+  return allocator_.AllocateTempTfLiteTensor(model_, graph_.GetAllocations(),
+                                             tensor_idx,
+                                             sg);
+}
+
 void MicroInterpreterContext::DeallocateTempTfLiteTensor(TfLiteTensor* tensor) {
   return allocator_.DeallocateTempTfLiteTensor(tensor);
 }
@@ -76,6 +82,11 @@ TfLiteEvalTensor* MicroInterpreterContext::GetEvalTensor(int tensor_idx) {
               .tensors[tensor_idx];
 }
 
+TfLiteEvalTensor* MicroInterpreterContext::GetEvalTensor(int tensor_idx, int sg) {
+  return &graph_.GetAllocations()[sg]
+              .tensors[tensor_idx];
+}
+
 void MicroInterpreterContext::SetScratchBufferHandles(
     ScratchBufferHandle* scratch_buffer_handles) {
   scratch_buffer_handles_ = scratch_buffer_handles;
@@ -83,7 +94,8 @@ void MicroInterpreterContext::SetScratchBufferHandles(
 
 TfLiteStatus MicroInterpreterContext::set_external_context(
     void* external_context_payload) {
-  TFLITE_DCHECK(state_ == InterpreterState::kPrepare ||
+  TFLITE_DCHECK(state_ == InterpreterState::kInit ||
+                state_ == InterpreterState::kPrepare ||
                 state_ == InterpreterState::kInvoke);
   if (external_context_payload == nullptr ||
       external_context_payload_ != nullptr) {
@@ -106,4 +118,4 @@ MicroInterpreterContext::GetInterpreterState() const {
   return state_;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/micro_interpreter_context.h b/tensorflow/lite/micro/micro_interpreter_context.h
index 5986dc37..0681bba6 100644
--- a/tensorflow/lite/micro/micro_interpreter_context.h
+++ b/tensorflow/lite/micro/micro_interpreter_context.h
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_interpreter_graph.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // A full implementation of the MicroContext, to be used by the
 // MicroInterpreter. Kernels should not depend on this directly. Instead they
@@ -67,6 +67,7 @@ class MicroInterpreterContext : public MicroContext {
   // Returns a temporary TfLiteTensor struct for a given index.
   // Virtual so that it can be faked for kernel tests.
   virtual TfLiteTensor* AllocateTempTfLiteTensor(int tensor_idx) override;
+  virtual TfLiteTensor* AllocateTempTfLiteTensor(int tensor_idx, int sg) override;
 
   // Deallocates a temp TfLiteTensor.
   // Virtual so that it can be faked for kernel tests.
@@ -85,6 +86,7 @@ class MicroInterpreterContext : public MicroContext {
   // Returns a TfLiteEvalTensor struct for a given index.
   // Virtual so that it can be faked for kernel tests.
   virtual TfLiteEvalTensor* GetEvalTensor(int tensor_idx) override;
+  virtual TfLiteEvalTensor* GetEvalTensor(int tensor_idx, int sg) override;
 
   // Sets the State of MemoryPlanning MicroInterpreterContext
   void SetInterpreterState(InterpreterState state);
@@ -118,6 +120,6 @@ class MicroInterpreterContext : public MicroContext {
   TF_LITE_REMOVE_VIRTUAL_DELETE
 };
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_MICRO_INTERPRETER_CONTEXT_H_
diff --git a/tensorflow/lite/micro/micro_interpreter_context_test.cc b/tensorflow/lite/micro/micro_interpreter_context_test.cc
index 3af123f5..f2fd0bfd 100644
--- a/tensorflow/lite/micro/micro_interpreter_context_test.cc
+++ b/tensorflow/lite/micro/micro_interpreter_context_test.cc
@@ -22,24 +22,24 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-using ::tflite::testing::IntArrayFromInts;
+using ::tflite_micro::testing::IntArrayFromInts;
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
-tflite::MicroInterpreterContext CreateMicroInterpreterContext() {
+tflite_micro::MicroInterpreterContext CreateMicroInterpreterContext() {
   // Some targets do not support dynamic memory (i.e., no malloc or new), thus,
   // the test need to place non-transient memories in static variables. This is
   // safe because tests are guaranteed to run serially.
   constexpr size_t kArenaSize = 1024;
   static uint8_t tensor_arena[kArenaSize];
 
-  const tflite::Model* model = tflite::testing::GetSimpleMockModel();
+  const tflite_micro::Model* model = tflite_micro::testing::GetSimpleMockModel();
   MicroAllocator* micro_allocator =
       MicroAllocator::Create(tensor_arena, kArenaSize);
   static MicroInterpreterGraph micro_graph(nullptr, nullptr, nullptr, nullptr);
 
-  tflite::MicroInterpreterContext micro_context(micro_allocator, model,
+  tflite_micro::MicroInterpreterContext micro_context(micro_allocator, model,
                                                 &micro_graph);
   return micro_context;
 }
@@ -50,23 +50,23 @@ struct TestExternalContextPayloadData {
   alignas(4) uint8_t blob_data[128];
 };
 }  // namespace
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
 // Ensures that a regular set and get pair works ok.
 TF_LITE_MICRO_TEST(TestSetGetExternalContextSuccess) {
-  tflite::MicroInterpreterContext micro_context =
-      tflite::CreateMicroInterpreterContext();
+  tflite_micro::MicroInterpreterContext micro_context =
+      tflite_micro::CreateMicroInterpreterContext();
   micro_context.SetInterpreterState(
-      tflite::MicroInterpreterContext::InterpreterState::kInvoke);
+      tflite_micro::MicroInterpreterContext::InterpreterState::kInvoke);
 
-  tflite::TestExternalContextPayloadData payload;
+  tflite_micro::TestExternalContextPayloadData payload;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
                           micro_context.set_external_context(&payload));
 
-  tflite::TestExternalContextPayloadData* returned_external_context =
-      reinterpret_cast<tflite::TestExternalContextPayloadData*>(
+  tflite_micro::TestExternalContextPayloadData* returned_external_context =
+      reinterpret_cast<tflite_micro::TestExternalContextPayloadData*>(
           micro_context.external_context());
 
   // What is returned should be the same as what is set.
@@ -74,11 +74,11 @@ TF_LITE_MICRO_TEST(TestSetGetExternalContextSuccess) {
 }
 
 TF_LITE_MICRO_TEST(TestGetExternalContextWithoutSetShouldReturnNull) {
-  tflite::MicroInterpreterContext micro_context =
-      tflite::CreateMicroInterpreterContext();
+  tflite_micro::MicroInterpreterContext micro_context =
+      tflite_micro::CreateMicroInterpreterContext();
 
-  tflite::TestExternalContextPayloadData* returned_external_context =
-      reinterpret_cast<tflite::TestExternalContextPayloadData*>(
+  tflite_micro::TestExternalContextPayloadData* returned_external_context =
+      reinterpret_cast<tflite_micro::TestExternalContextPayloadData*>(
           micro_context.external_context());
 
   // Return a null if nothing is set before.
@@ -86,11 +86,11 @@ TF_LITE_MICRO_TEST(TestGetExternalContextWithoutSetShouldReturnNull) {
 }
 
 TF_LITE_MICRO_TEST(TestSetExternalContextCanOnlyBeCalledOnce) {
-  tflite::MicroInterpreterContext micro_context =
-      tflite::CreateMicroInterpreterContext();
+  tflite_micro::MicroInterpreterContext micro_context =
+      tflite_micro::CreateMicroInterpreterContext();
   micro_context.SetInterpreterState(
-      tflite::MicroInterpreterContext::InterpreterState::kPrepare);
-  tflite::TestExternalContextPayloadData payload;
+      tflite_micro::MicroInterpreterContext::InterpreterState::kPrepare);
+  tflite_micro::TestExternalContextPayloadData payload;
 
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
                           micro_context.set_external_context(&payload));
@@ -101,17 +101,17 @@ TF_LITE_MICRO_TEST(TestSetExternalContextCanOnlyBeCalledOnce) {
 }
 
 TF_LITE_MICRO_TEST(TestSetExternalContextToNullShouldFail) {
-  tflite::MicroInterpreterContext micro_context =
-      tflite::CreateMicroInterpreterContext();
+  tflite_micro::MicroInterpreterContext micro_context =
+      tflite_micro::CreateMicroInterpreterContext();
   micro_context.SetInterpreterState(
-      tflite::MicroInterpreterContext::InterpreterState::kPrepare);
+      tflite_micro::MicroInterpreterContext::InterpreterState::kPrepare);
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteError,
                           micro_context.set_external_context(nullptr));
 }
 
 TF_LITE_MICRO_TEST(TestGetTempInputTensor) {
-  tflite::MicroInterpreterContext micro_context =
-      tflite::CreateMicroInterpreterContext();
+  tflite_micro::MicroInterpreterContext micro_context =
+      tflite_micro::CreateMicroInterpreterContext();
 
   TfLiteNode node;
   int input_data[] = {2, 0, 1};
@@ -130,8 +130,8 @@ TF_LITE_MICRO_TEST(TestGetTempInputTensor) {
 }
 
 TF_LITE_MICRO_TEST(TestGetTempOutputTensor) {
-  tflite::MicroInterpreterContext micro_context =
-      tflite::CreateMicroInterpreterContext();
+  tflite_micro::MicroInterpreterContext micro_context =
+      tflite_micro::CreateMicroInterpreterContext();
 
   TfLiteNode node;
   int output_data[] = {1, 0};
@@ -147,18 +147,18 @@ TF_LITE_MICRO_TEST(TestGetTempOutputTensor) {
 }
 
 TF_LITE_MICRO_TEST(TestAllocateTempBuffer) {
-  tflite::MicroInterpreterContext micro_context =
-      tflite::CreateMicroInterpreterContext();
+  tflite_micro::MicroInterpreterContext micro_context =
+      tflite_micro::CreateMicroInterpreterContext();
   micro_context.SetInterpreterState(
-      tflite::MicroInterpreterContext::InterpreterState::kPrepare);
+      tflite_micro::MicroInterpreterContext::InterpreterState::kPrepare);
   uint8_t* buffer1 =
-      micro_context.AllocateTempBuffer(10, tflite::MicroArenaBufferAlignment());
+      micro_context.AllocateTempBuffer(10, tflite_micro::MicroArenaBufferAlignment());
   TF_LITE_MICRO_EXPECT(buffer1 != nullptr);
 }
 
 TF_LITE_MICRO_TEST(TestGetTempIntermediateTensor) {
-  tflite::MicroInterpreterContext micro_context =
-      tflite::CreateMicroInterpreterContext();
+  tflite_micro::MicroInterpreterContext micro_context =
+      tflite_micro::CreateMicroInterpreterContext();
 
   TfLiteNode node;
   int intermediate_data[] = {1, 0};
diff --git a/tensorflow/lite/micro/micro_interpreter_graph.cc b/tensorflow/lite/micro/micro_interpreter_graph.cc
index dbd7072a..b69ab242 100644
--- a/tensorflow/lite/micro/micro_interpreter_graph.cc
+++ b/tensorflow/lite/micro/micro_interpreter_graph.cc
@@ -24,7 +24,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_profiler.h"
 #include "tensorflow/lite/schema/schema_generated.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 const char* OpNameFromRegistration(const TFLMRegistration* registration) {
@@ -223,7 +223,7 @@ TfLiteStatus MicroInterpreterGraph::ResetVariableTensors() {
             &subgraph_allocations_[subgraph_idx].tensors[i], &buffer_size));
 
         int value = 0;
-        if (tensor->type() == tflite::TensorType_INT8) {
+        if (tensor->type() == tflite_micro::TensorType_INT8) {
           value = tensor->quantization()->zero_point()->Get(0);
         }
         memset(subgraph_allocations_[subgraph_idx].tensors[i].data.raw, value,
@@ -269,4 +269,4 @@ TfLiteEvalTensor* MicroInterpreterGraph::GetSubgraphOutput(int subgraph_idx,
   return &subgraph_allocations_[subgraph_idx].tensors[tensor_idx];
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/micro_interpreter_graph.h b/tensorflow/lite/micro/micro_interpreter_graph.h
index 5c2121aa..b3de4c97 100644
--- a/tensorflow/lite/micro/micro_interpreter_graph.h
+++ b/tensorflow/lite/micro/micro_interpreter_graph.h
@@ -22,12 +22,12 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_resource_variable.h"
 #include "tensorflow/lite/schema/schema_generated.h"
 
-namespace tflite {
+namespace tflite_micro {
 
-// Abstracts the details of interacting with the tflite::Model.
+// Abstracts the details of interacting with the tflite_micro::Model.
 //
 // Provides methods to access, initialize, prepare, invoke and free any
-// subgraph in the tflite::Graph.
+// subgraph in the tflite_micro::Graph.
 class MicroInterpreterGraph : public MicroGraph {
  public:
   // The lifetime of the context, model, allocator and resource_variables must
@@ -105,6 +105,6 @@ class MicroInterpreterGraph : public MicroGraph {
   TF_LITE_REMOVE_VIRTUAL_DELETE
 };
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_MICRO_INTERPRETER_GRAPH_H_
diff --git a/tensorflow/lite/micro/micro_interpreter_test.cc b/tensorflow/lite/micro/micro_interpreter_test.cc
index 0ba31c4a..f332099c 100644
--- a/tensorflow/lite/micro/micro_interpreter_test.cc
+++ b/tensorflow/lite/micro/micro_interpreter_test.cc
@@ -25,7 +25,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 constexpr size_t buffer_arena_size = 256 * 1024;
@@ -52,23 +52,23 @@ class MockProfiler : public MicroProfilerInterface {
 };
 
 }  // namespace
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(TestInterpreter) {
-  const tflite::Model* model = tflite::testing::GetSimpleMockModel();
+  const tflite_micro::Model* model = tflite_micro::testing::GetSimpleMockModel();
   TF_LITE_MICRO_EXPECT(nullptr != model);
-  tflite::testing::TestingOpResolver op_resolver;
+  tflite_micro::testing::TestingOpResolver op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::testing::GetTestingOpResolver(op_resolver));
+                          tflite_micro::testing::GetTestingOpResolver(op_resolver));
 
   constexpr size_t allocator_buffer_size = 2000;
   uint8_t allocator_buffer[allocator_buffer_size];
 
   // Create a new scope so that we can test the destructor.
   {
-    tflite::MicroInterpreter interpreter(model, op_resolver, allocator_buffer,
+    tflite_micro::MicroInterpreter interpreter(model, op_resolver, allocator_buffer,
                                          allocator_buffer_size);
     TF_LITE_MICRO_EXPECT_EQ(interpreter.AllocateTensors(), kTfLiteOk);
     TF_LITE_MICRO_EXPECT_LE(interpreter.arena_used_bytes(), 928 + 100);
@@ -105,13 +105,13 @@ TF_LITE_MICRO_TEST(TestInterpreter) {
     TF_LITE_MICRO_EXPECT_EQ(42, output->data.i32[0]);
   }
 
-  TF_LITE_MICRO_EXPECT_EQ(tflite::testing::MockCustom::freed_, true);
+  TF_LITE_MICRO_EXPECT_EQ(tflite_micro::testing::MockCustom::freed_, true);
 }
 
 TF_LITE_MICRO_TEST(TestMultiTenantInterpreter) {
-  tflite::testing::TestingOpResolver op_resolver;
+  tflite_micro::testing::TestingOpResolver op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::testing::GetTestingOpResolver(op_resolver));
+                          tflite_micro::testing::GetTestingOpResolver(op_resolver));
   constexpr size_t arena_size = 8192;
   uint8_t arena[arena_size];
 
@@ -119,10 +119,10 @@ TF_LITE_MICRO_TEST(TestMultiTenantInterpreter) {
 
   // Get simple_model_head_usage.
   {
-    tflite::RecordingMicroAllocator* allocator =
-        tflite::RecordingMicroAllocator::Create(arena, arena_size);
-    const tflite::Model* model0 = tflite::testing::GetSimpleMockModel();
-    tflite::MicroInterpreter interpreter0(model0, op_resolver, allocator);
+    tflite_micro::RecordingMicroAllocator* allocator =
+        tflite_micro::RecordingMicroAllocator::Create(arena, arena_size);
+    const tflite_micro::Model* model0 = tflite_micro::testing::GetSimpleMockModel();
+    tflite_micro::MicroInterpreter interpreter0(model0, op_resolver, allocator);
     TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, interpreter0.AllocateTensors());
     simple_model_head_usage =
         allocator->GetSimpleMemoryAllocator()->GetNonPersistentUsedBytes();
@@ -135,13 +135,13 @@ TF_LITE_MICRO_TEST(TestMultiTenantInterpreter) {
   }
 
   // Shared allocator for various models.
-  tflite::RecordingMicroAllocator* allocator =
-      tflite::RecordingMicroAllocator::Create(arena, arena_size);
+  tflite_micro::RecordingMicroAllocator* allocator =
+      tflite_micro::RecordingMicroAllocator::Create(arena, arena_size);
 
   // Get complex_model_head_usage. No head space reuse since it's the first
   // model allocated in the `allocator`.
-  const tflite::Model* model1 = tflite::testing::GetComplexMockModel();
-  tflite::MicroInterpreter interpreter1(model1, op_resolver, allocator);
+  const tflite_micro::Model* model1 = tflite_micro::testing::GetComplexMockModel();
+  tflite_micro::MicroInterpreter interpreter1(model1, op_resolver, allocator);
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, interpreter1.AllocateTensors());
   TfLiteTensor* input1 = interpreter1.input(0);
   TfLiteTensor* output1 = interpreter1.output(0);
@@ -151,8 +151,8 @@ TF_LITE_MICRO_TEST(TestMultiTenantInterpreter) {
   // Allocate simple model from the same `allocator`. Some head space will
   // be reused thanks to multi-tenant TFLM support. Also makes sure that
   // the output is correct.
-  const tflite::Model* model2 = tflite::testing::GetSimpleMockModel();
-  tflite::MicroInterpreter interpreter2(model2, op_resolver, allocator);
+  const tflite_micro::Model* model2 = tflite_micro::testing::GetSimpleMockModel();
+  tflite_micro::MicroInterpreter interpreter2(model2, op_resolver, allocator);
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, interpreter2.AllocateTensors());
   TfLiteTensor* input2 = interpreter2.input(0);
   TfLiteTensor* output2 = interpreter2.output(0);
@@ -180,8 +180,8 @@ TF_LITE_MICRO_TEST(TestMultiTenantInterpreter) {
 
   // Allocate another complex model from the `allocator` will not increase
   // head space usage.
-  const tflite::Model* model3 = tflite::testing::GetComplexMockModel();
-  tflite::MicroInterpreter interpreter3(model3, op_resolver, allocator);
+  const tflite_micro::Model* model3 = tflite_micro::testing::GetComplexMockModel();
+  tflite_micro::MicroInterpreter interpreter3(model3, op_resolver, allocator);
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, interpreter3.AllocateTensors());
   TfLiteTensor* input3 = interpreter3.input(0);
   TfLiteTensor* output3 = interpreter3.output(0);
@@ -198,23 +198,23 @@ TF_LITE_MICRO_TEST(TestMultiTenantInterpreter) {
 }
 
 TF_LITE_MICRO_TEST(TestKernelMemoryPlanning) {
-  const tflite::Model* model = tflite::testing::GetSimpleStatefulModel();
+  const tflite_micro::Model* model = tflite_micro::testing::GetSimpleStatefulModel();
   TF_LITE_MICRO_EXPECT(nullptr != model);
 
-  tflite::testing::TestingOpResolver op_resolver;
+  tflite_micro::testing::TestingOpResolver op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::testing::GetTestingOpResolver(op_resolver));
+                          tflite_micro::testing::GetTestingOpResolver(op_resolver));
 
   constexpr size_t allocator_buffer_size = 4096 + 1024;
   uint8_t allocator_buffer[allocator_buffer_size];
 
-  tflite::RecordingMicroAllocator* allocator =
-      tflite::RecordingMicroAllocator::Create(allocator_buffer,
+  tflite_micro::RecordingMicroAllocator* allocator =
+      tflite_micro::RecordingMicroAllocator::Create(allocator_buffer,
                                               allocator_buffer_size);
 
   // Make sure kernel memory planning works in multi-tenant context.
   for (int i = 0; i < 3; i++) {
-    tflite::MicroInterpreter interpreter(model, op_resolver, allocator);
+    tflite_micro::MicroInterpreter interpreter(model, op_resolver, allocator);
     TF_LITE_MICRO_EXPECT_EQ(interpreter.AllocateTensors(), kTfLiteOk);
     TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(1), interpreter.inputs_size());
     TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(2), interpreter.outputs_size());
@@ -251,34 +251,34 @@ TF_LITE_MICRO_TEST(TestKernelMemoryPlanning) {
 // b/147830765 has one example of a change that caused trouble for this simple
 // case.
 TF_LITE_MICRO_TEST(TestIncompleteInitialization) {
-  const tflite::Model* model = tflite::testing::GetComplexMockModel();
+  const tflite_micro::Model* model = tflite_micro::testing::GetComplexMockModel();
   TF_LITE_MICRO_EXPECT(nullptr != model);
 
-  tflite::testing::TestingOpResolver op_resolver;
+  tflite_micro::testing::TestingOpResolver op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::testing::GetTestingOpResolver(op_resolver));
+                          tflite_micro::testing::GetTestingOpResolver(op_resolver));
 
   constexpr size_t allocator_buffer_size = 2048;
   uint8_t allocator_buffer[allocator_buffer_size];
 
-  tflite::MicroInterpreter interpreter(model, op_resolver, allocator_buffer,
+  tflite_micro::MicroInterpreter interpreter(model, op_resolver, allocator_buffer,
                                        allocator_buffer_size);
 }
 
 // Test that an interpreter with a supplied profiler correctly calls the
 // profiler each time an operator is invoked.
 TF_LITE_MICRO_TEST(InterpreterWithProfilerShouldProfileOps) {
-  const tflite::Model* model = tflite::testing::GetComplexMockModel();
+  const tflite_micro::Model* model = tflite_micro::testing::GetComplexMockModel();
   TF_LITE_MICRO_EXPECT(nullptr != model);
 
-  tflite::testing::TestingOpResolver op_resolver;
+  tflite_micro::testing::TestingOpResolver op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::testing::GetTestingOpResolver(op_resolver));
+                          tflite_micro::testing::GetTestingOpResolver(op_resolver));
 
   constexpr size_t allocator_buffer_size = 2048;
   uint8_t allocator_buffer[allocator_buffer_size];
-  tflite::MockProfiler profiler;
-  tflite::MicroInterpreter interpreter(model, op_resolver, allocator_buffer,
+  tflite_micro::MockProfiler profiler;
+  tflite_micro::MicroInterpreter interpreter(model, op_resolver, allocator_buffer,
                                        allocator_buffer_size, nullptr,
                                        &profiler);
 
@@ -296,12 +296,12 @@ TF_LITE_MICRO_TEST(InterpreterWithProfilerShouldProfileOps) {
 }
 
 TF_LITE_MICRO_TEST(TestIncompleteInitializationAllocationsWithSmallArena) {
-  const tflite::Model* model = tflite::testing::GetComplexMockModel();
+  const tflite_micro::Model* model = tflite_micro::testing::GetComplexMockModel();
   TF_LITE_MICRO_EXPECT(nullptr != model);
 
-  tflite::testing::TestingOpResolver op_resolver;
+  tflite_micro::testing::TestingOpResolver op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::testing::GetTestingOpResolver(op_resolver));
+                          tflite_micro::testing::GetTestingOpResolver(op_resolver));
 
   // This test is designed to create the following classes/buffers successfully
   // on the arena:
@@ -318,17 +318,17 @@ TF_LITE_MICRO_TEST(TestIncompleteInitializationAllocationsWithSmallArena) {
   constexpr size_t max_scratch_buffer_request_size = 192;
   constexpr size_t max_micro_builtin_data_allocator_size = 16;
   constexpr size_t allocator_buffer_size =
-      sizeof(tflite::RecordingSingleArenaBufferAllocator) +
-      sizeof(tflite::RecordingMicroAllocator) +
+      sizeof(tflite_micro::RecordingSingleArenaBufferAllocator) +
+      sizeof(tflite_micro::RecordingMicroAllocator) +
       max_micro_builtin_data_allocator_size + max_scratch_buffer_request_size;
   uint8_t allocator_buffer[allocator_buffer_size];
 
-  tflite::RecordingMicroAllocator* allocator =
-      tflite::RecordingMicroAllocator::Create(allocator_buffer,
+  tflite_micro::RecordingMicroAllocator* allocator =
+      tflite_micro::RecordingMicroAllocator::Create(allocator_buffer,
                                               allocator_buffer_size);
   TF_LITE_MICRO_EXPECT(nullptr != allocator);
 
-  tflite::MicroInterpreter interpreter(model, op_resolver, allocator);
+  tflite_micro::MicroInterpreter interpreter(model, op_resolver, allocator);
 
   // Interpreter fails because arena is too small:
   TF_LITE_MICRO_EXPECT_EQ(interpreter.Invoke(), kTfLiteError);
@@ -344,37 +344,37 @@ TF_LITE_MICRO_TEST(TestIncompleteInitializationAllocationsWithSmallArena) {
       static_cast<size_t>(0),
       allocator
           ->GetRecordedAllocation(
-              tflite::RecordedAllocationType::kTfLiteEvalTensorData)
+              tflite_micro::RecordedAllocationType::kTfLiteEvalTensorData)
           .used_bytes);
   TF_LITE_MICRO_EXPECT_EQ(
       static_cast<size_t>(0),
       allocator
           ->GetRecordedAllocation(
-              tflite::RecordedAllocationType::kTfLiteTensorVariableBufferData)
+              tflite_micro::RecordedAllocationType::kTfLiteTensorVariableBufferData)
           .used_bytes);
   TF_LITE_MICRO_EXPECT_EQ(
       static_cast<size_t>(0),
-      allocator->GetRecordedAllocation(tflite::RecordedAllocationType::kOpData)
+      allocator->GetRecordedAllocation(tflite_micro::RecordedAllocationType::kOpData)
           .used_bytes);
 }
 
 TF_LITE_MICRO_TEST(TestInterpreterDoesNotAllocateUntilInvoke) {
-  const tflite::Model* model = tflite::testing::GetComplexMockModel();
+  const tflite_micro::Model* model = tflite_micro::testing::GetComplexMockModel();
   TF_LITE_MICRO_EXPECT(nullptr != model);
 
-  tflite::testing::TestingOpResolver op_resolver;
+  tflite_micro::testing::TestingOpResolver op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::testing::GetTestingOpResolver(op_resolver));
+                          tflite_micro::testing::GetTestingOpResolver(op_resolver));
 
   constexpr size_t allocator_buffer_size = 1024 * 10;
   uint8_t allocator_buffer[allocator_buffer_size];
 
-  tflite::RecordingMicroAllocator* allocator =
-      tflite::RecordingMicroAllocator::Create(allocator_buffer,
+  tflite_micro::RecordingMicroAllocator* allocator =
+      tflite_micro::RecordingMicroAllocator::Create(allocator_buffer,
                                               allocator_buffer_size);
   TF_LITE_MICRO_EXPECT(nullptr != allocator);
 
-  tflite::MicroInterpreter interpreter(model, op_resolver, allocator);
+  tflite_micro::MicroInterpreter interpreter(model, op_resolver, allocator);
 
   // Ensure allocations are zero (ignore tail since some internal structs are
   // initialized with this space):
@@ -385,17 +385,17 @@ TF_LITE_MICRO_TEST(TestInterpreterDoesNotAllocateUntilInvoke) {
       static_cast<size_t>(0),
       allocator
           ->GetRecordedAllocation(
-              tflite::RecordedAllocationType::kTfLiteTensorVariableBufferData)
+              tflite_micro::RecordedAllocationType::kTfLiteTensorVariableBufferData)
           .used_bytes);
   TF_LITE_MICRO_EXPECT_EQ(
       static_cast<size_t>(0),
       allocator
           ->GetRecordedAllocation(
-              tflite::RecordedAllocationType::kTfLiteEvalTensorData)
+              tflite_micro::RecordedAllocationType::kTfLiteEvalTensorData)
           .used_bytes);
   TF_LITE_MICRO_EXPECT_EQ(
       static_cast<size_t>(0),
-      allocator->GetRecordedAllocation(tflite::RecordedAllocationType::kOpData)
+      allocator->GetRecordedAllocation(tflite_micro::RecordedAllocationType::kOpData)
           .used_bytes);
 
   TF_LITE_MICRO_EXPECT_EQ(interpreter.Invoke(), kTfLiteOk);
@@ -409,14 +409,14 @@ TF_LITE_MICRO_TEST(TestInterpreterDoesNotAllocateUntilInvoke) {
   TF_LITE_MICRO_EXPECT_GT(
       allocator
           ->GetRecordedAllocation(
-              tflite::RecordedAllocationType::kTfLiteEvalTensorData)
+              tflite_micro::RecordedAllocationType::kTfLiteEvalTensorData)
           .used_bytes,
       0);
 
   TF_LITE_MICRO_EXPECT_GT(
       allocator
           ->GetRecordedAllocation(
-              tflite::RecordedAllocationType::kTfLiteTensorVariableBufferData)
+              tflite_micro::RecordedAllocationType::kTfLiteTensorVariableBufferData)
           .used_bytes,
       static_cast<size_t>(0));
 
@@ -424,25 +424,25 @@ TF_LITE_MICRO_TEST(TestInterpreterDoesNotAllocateUntilInvoke) {
   // operator creation in our mock models is inconsistent.  Revisit what
   // this check should be once the mock models are properly created.
   TF_LITE_MICRO_EXPECT_EQ(
-      allocator->GetRecordedAllocation(tflite::RecordedAllocationType::kOpData)
+      allocator->GetRecordedAllocation(tflite_micro::RecordedAllocationType::kOpData)
           .used_bytes,
       static_cast<size_t>(0));
 }
 
 TF_LITE_MICRO_TEST(TestInterpreterMultipleInputs) {
-  const tflite::Model* model = tflite::testing::GetSimpleMultipleInputsModel();
+  const tflite_micro::Model* model = tflite_micro::testing::GetSimpleMultipleInputsModel();
   TF_LITE_MICRO_EXPECT(nullptr != model);
 
-  tflite::testing::TestingOpResolver op_resolver;
+  tflite_micro::testing::TestingOpResolver op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::testing::GetTestingOpResolver(op_resolver));
+                          tflite_micro::testing::GetTestingOpResolver(op_resolver));
 
   constexpr size_t allocator_buffer_size = 2000;
   uint8_t allocator_buffer[allocator_buffer_size];
 
   // Create a new scope so that we can test the destructor.
   {
-    tflite::MicroInterpreter interpreter(model, op_resolver, allocator_buffer,
+    tflite_micro::MicroInterpreter interpreter(model, op_resolver, allocator_buffer,
                                          allocator_buffer_size);
 
     TF_LITE_MICRO_EXPECT_EQ(interpreter.AllocateTensors(), kTfLiteOk);
@@ -490,23 +490,23 @@ TF_LITE_MICRO_TEST(TestInterpreterMultipleInputs) {
     TF_LITE_MICRO_EXPECT_EQ(66, output->data.i32[0]);
   }
 
-  TF_LITE_MICRO_EXPECT_EQ(tflite::testing::MultipleInputs::freed_, true);
+  TF_LITE_MICRO_EXPECT_EQ(tflite_micro::testing::MultipleInputs::freed_, true);
 }
 
 TF_LITE_MICRO_TEST(TestInterpreterNullInputsAndOutputs) {
-  const tflite::Model* model =
-      tflite::testing::GetSimpleModelWithNullInputsAndOutputs();
+  const tflite_micro::Model* model =
+      tflite_micro::testing::GetSimpleModelWithNullInputsAndOutputs();
   TF_LITE_MICRO_EXPECT(nullptr != model);
 
-  tflite::testing::TestingOpResolver op_resolver;
+  tflite_micro::testing::TestingOpResolver op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::testing::GetTestingOpResolver(op_resolver));
+                          tflite_micro::testing::GetTestingOpResolver(op_resolver));
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, op_resolver.AddCallOnce());
 
   constexpr size_t allocator_buffer_size = 2000;
   uint8_t allocator_buffer[allocator_buffer_size];
 
-  tflite::MicroInterpreter interpreter(model, op_resolver, allocator_buffer,
+  tflite_micro::MicroInterpreter interpreter(model, op_resolver, allocator_buffer,
                                        allocator_buffer_size);
 
   TF_LITE_MICRO_EXPECT_EQ(interpreter.AllocateTensors(), kTfLiteOk);
@@ -520,14 +520,14 @@ TF_LITE_MICRO_TEST(TestInterpreterNullInputsAndOutputs) {
 // This test is disabled from Bluepill platform because it requires more SRAM
 // than what our Bluepill simulation platform specifies.
 TF_LITE_MICRO_TEST(TestArenaUsedBytes) {
-  const tflite::Model* model = tflite::testing::GetModelWith256x256Tensor();
+  const tflite_micro::Model* model = tflite_micro::testing::GetModelWith256x256Tensor();
   TF_LITE_MICRO_EXPECT(nullptr != model);
 
-  tflite::testing::TestingOpResolver op_resolver;
+  tflite_micro::testing::TestingOpResolver op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
-                          tflite::testing::GetTestingOpResolver(op_resolver));
-  tflite::MicroInterpreter interpreter(model, op_resolver, tflite::arena_buffer,
-                                       tflite::buffer_arena_size);
+                          tflite_micro::testing::GetTestingOpResolver(op_resolver));
+  tflite_micro::MicroInterpreter interpreter(model, op_resolver, tflite_micro::arena_buffer,
+                                       tflite_micro::buffer_arena_size);
   TF_LITE_MICRO_EXPECT_EQ(interpreter.AllocateTensors(), kTfLiteOk);
 
   // Store the required arena size before Invoke() because this is what this
@@ -540,9 +540,9 @@ TF_LITE_MICRO_TEST(TestArenaUsedBytes) {
   // model to run. Plus alignment padding is because SingleArenaBufferAllocator
   // is given the arena after the alignment.
   size_t required_arena_size =
-      used_arena_size + tflite::MicroArenaBufferAlignment();
-  tflite::MicroInterpreter interpreter2(
-      model, op_resolver, tflite::arena_buffer, required_arena_size);
+      used_arena_size + tflite_micro::MicroArenaBufferAlignment();
+  tflite_micro::MicroInterpreter interpreter2(
+      model, op_resolver, tflite_micro::arena_buffer, required_arena_size);
   TF_LITE_MICRO_EXPECT_EQ(interpreter2.AllocateTensors(), kTfLiteOk);
 
   TF_LITE_MICRO_EXPECT_EQ(interpreter2.Invoke(), kTfLiteOk);
diff --git a/tensorflow/lite/micro/micro_log.h b/tensorflow/lite/micro/micro_log.h
index 669e9e66..c9db1e44 100644
--- a/tensorflow/lite/micro/micro_log.h
+++ b/tensorflow/lite/micro/micro_log.h
@@ -24,11 +24,11 @@ void VMicroPrintf(const char* format, va_list args);
 #else
 // We use a #define to ensure that the strings are completely stripped, to
 // prevent an unnecessary increase in the binary size.
-#define MicroPrintf(...) tflite::Unused(__VA_ARGS__)
-#define VMicroPrintf(...) tflite::Unused(__VA_ARGS__)
+#define MicroPrintf(...) tflite_micro::Unused(__VA_ARGS__)
+#define VMicroPrintf(...) tflite_micro::Unused(__VA_ARGS__)
 #endif
 
-namespace tflite {
+namespace tflite_micro {
 
 // From
 // https://stackoverflow.com/questions/23235910/variadic-unused-function-macro
@@ -37,6 +37,6 @@ void Unused(Args&&... args) {
   (void)(sizeof...(args));
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_MICRO_LOG_H_
diff --git a/tensorflow/lite/micro/micro_log_test.cc b/tensorflow/lite/micro/micro_log_test.cc
index 97ac8be1..1d46802a 100644
--- a/tensorflow/lite/micro/micro_log_test.cc
+++ b/tensorflow/lite/micro/micro_log_test.cc
@@ -17,12 +17,12 @@ limitations under the License.
 
 #include "tensorflow/lite/micro/system_setup.h"
 
-namespace tflite {
+namespace tflite_micro {
 inline void InitializeTest() { InitializeTarget(); }
-}  // namespace tflite
+}  // namespace tflite_micro
 
 int main(int argc, char** argv) {
-  tflite::InitializeTest();
+  tflite_micro::InitializeTest();
 #ifndef TF_LITE_STRIP_ERROR_STRINGS
   MicroPrintf("Number: %d", 42);
   MicroPrintf("Badly-formed format string %");
diff --git a/tensorflow/lite/micro/micro_mutable_op_resolver.h b/tensorflow/lite/micro/micro_mutable_op_resolver.h
index b2563a93..2790bba6 100644
--- a/tensorflow/lite/micro/micro_mutable_op_resolver.h
+++ b/tensorflow/lite/micro/micro_mutable_op_resolver.h
@@ -36,7 +36,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_op_resolver.h"
 #include "tensorflow/lite/schema/schema_generated.h"
 
-namespace tflite {
+namespace tflite_micro {
 TFLMRegistration* Register_DETECTION_POSTPROCESS();
 
 template <unsigned int tOpCount>
@@ -46,7 +46,7 @@ class MicroMutableOpResolver : public MicroOpResolver {
 
   explicit MicroMutableOpResolver() {}
 
-  const TFLMRegistration* FindOp(tflite::BuiltinOperator op) const override {
+  const TFLMRegistration* FindOp(tflite_micro::BuiltinOperator op) const override {
     if (op == BuiltinOperator_CUSTOM) return nullptr;
 
     for (unsigned int i = 0; i < registrations_len_; ++i) {
@@ -121,7 +121,7 @@ class MicroMutableOpResolver : public MicroOpResolver {
   }
 
   TfLiteStatus AddAddN() {
-    return AddBuiltin(BuiltinOperator_ADD_N, tflite::Register_ADD_N(),
+    return AddBuiltin(BuiltinOperator_ADD_N, tflite_micro::Register_ADD_N(),
                       ParseAddN);
   }
 
@@ -135,7 +135,7 @@ class MicroMutableOpResolver : public MicroOpResolver {
 
   TfLiteStatus AddAssignVariable() {
     return AddBuiltin(BuiltinOperator_ASSIGN_VARIABLE,
-                      tflite::Register_ASSIGN_VARIABLE(), ParseAssignVariable);
+                      tflite_micro::Register_ASSIGN_VARIABLE(), ParseAssignVariable);
   }
 
   TfLiteStatus AddAveragePool2D(
@@ -145,7 +145,7 @@ class MicroMutableOpResolver : public MicroOpResolver {
 
   TfLiteStatus AddBatchMatMul() {
     return AddBuiltin(BuiltinOperator_BATCH_MATMUL,
-                      tflite::Register_BATCH_MATMUL(), ParseBatchMatMul);
+                      tflite_micro::Register_BATCH_MATMUL(), ParseBatchMatMul);
   }
 
   TfLiteStatus AddBatchToSpaceNd() {
@@ -177,7 +177,7 @@ class MicroMutableOpResolver : public MicroOpResolver {
   }
 
   TfLiteStatus AddCircularBuffer() {
-    return AddCustom("CIRCULAR_BUFFER", tflite::Register_CIRCULAR_BUFFER());
+    return AddCustom("CIRCULAR_BUFFER", tflite_micro::Register_CIRCULAR_BUFFER());
   }
 
   TfLiteStatus AddConcatenation() {
@@ -191,22 +191,22 @@ class MicroMutableOpResolver : public MicroOpResolver {
   }
 
   TfLiteStatus AddCos() {
-    return AddBuiltin(BuiltinOperator_COS, tflite::Register_COS(), ParseCos);
+    return AddBuiltin(BuiltinOperator_COS, tflite_micro::Register_COS(), ParseCos);
   }
 
   TfLiteStatus AddCumSum() {
-    return AddBuiltin(BuiltinOperator_CUMSUM, tflite::Register_CUMSUM(),
+    return AddBuiltin(BuiltinOperator_CUMSUM, tflite_micro::Register_CUMSUM(),
                       ParseCumsum);
   }
 
   TfLiteStatus AddDelay() {
     // TODO(b/286250473): change back name to "Delay" and remove namespace
-    return AddCustom("SignalDelay", tflite::tflm_signal::Register_DELAY());
+    return AddCustom("SignalDelay", tflite_micro::tflm_signal::Register_DELAY());
   }
 
   TfLiteStatus AddDepthToSpace() {
     return AddBuiltin(BuiltinOperator_DEPTH_TO_SPACE,
-                      tflite::Register_DEPTH_TO_SPACE(), ParseDepthToSpace);
+                      tflite_micro::Register_DEPTH_TO_SPACE(), ParseDepthToSpace);
   }
 
   TfLiteStatus AddDepthwiseConv2D(
@@ -216,17 +216,17 @@ class MicroMutableOpResolver : public MicroOpResolver {
   }
 
   TfLiteStatus AddDequantize() {
-    return AddBuiltin(BuiltinOperator_DEQUANTIZE, tflite::Register_DEQUANTIZE(),
+    return AddBuiltin(BuiltinOperator_DEQUANTIZE, tflite_micro::Register_DEQUANTIZE(),
                       ParseDequantize);
   }
 
   TfLiteStatus AddDetectionPostprocess() {
     return AddCustom("TFLite_Detection_PostProcess",
-                     tflite::Register_DETECTION_POSTPROCESS());
+                     tflite_micro::Register_DETECTION_POSTPROCESS());
   }
 
   TfLiteStatus AddDiv() {
-    return AddBuiltin(BuiltinOperator_DIV, tflite::Register_DIV(), ParseDiv);
+    return AddBuiltin(BuiltinOperator_DIV, tflite_micro::Register_DIV(), ParseDiv);
   }
 
   TfLiteStatus AddEmbeddingLookup() {
@@ -236,11 +236,11 @@ class MicroMutableOpResolver : public MicroOpResolver {
 
   TfLiteStatus AddEnergy() {
     // TODO(b/286250473): change back name to "Energy" and remove namespace
-    return AddCustom("SignalEnergy", tflite::tflm_signal::Register_ENERGY());
+    return AddCustom("SignalEnergy", tflite_micro::tflm_signal::Register_ENERGY());
   }
 
   TfLiteStatus AddElu() {
-    return AddBuiltin(BuiltinOperator_ELU, tflite::Register_ELU(), ParseElu);
+    return AddBuiltin(BuiltinOperator_ELU, tflite_micro::Register_ELU(), ParseElu);
   }
 
   TfLiteStatus AddEqual() {
@@ -248,9 +248,9 @@ class MicroMutableOpResolver : public MicroOpResolver {
   }
 
   TfLiteStatus AddEthosU() {
-    TFLMRegistration* registration = tflite::Register_ETHOSU();
+    TFLMRegistration* registration = tflite_micro::Register_ETHOSU();
     if (registration) {
-      return AddCustom(tflite::GetString_ETHOSU(), registration);
+      return AddCustom(tflite_micro::GetString_ETHOSU(), registration);
     }
     return kTfLiteOk;
   }
@@ -267,36 +267,36 @@ class MicroMutableOpResolver : public MicroOpResolver {
   TfLiteStatus AddFftAutoScale() {
     // TODO(b/286250473): change back name and remove namespace
     return AddCustom("SignalFftAutoScale",
-                     tflite::tflm_signal::Register_FFT_AUTO_SCALE());
+                     tflite_micro::tflm_signal::Register_FFT_AUTO_SCALE());
   }
 
   TfLiteStatus AddFill() {
-    return AddBuiltin(BuiltinOperator_FILL, tflite::Register_FILL(), ParseFill);
+    return AddBuiltin(BuiltinOperator_FILL, tflite_micro::Register_FILL(), ParseFill);
   }
 
   TfLiteStatus AddFilterBank() {
     // TODO(b/286250473): change back name to "FilterBank" and remove namespace
     return AddCustom("SignalFilterBank",
-                     tflite::tflm_signal::Register_FILTER_BANK());
+                     tflite_micro::tflm_signal::Register_FILTER_BANK());
   }
   TfLiteStatus AddFilterBankLog() {
     // TODO(b/286250473): change back name to "FilterBankLog" and remove
     // namespace
     return AddCustom("SignalFilterBankLog",
-                     tflite::tflm_signal::Register_FILTER_BANK_LOG());
+                     tflite_micro::tflm_signal::Register_FILTER_BANK_LOG());
   }
   TfLiteStatus AddFilterBankSquareRoot() {
     // TODO(b/286250473): change back name to "FilterBankSquareRoot" and remove
     // namespace
     return AddCustom("SignalFilterBankSquareRoot",
-                     tflite::tflm_signal::Register_FILTER_BANK_SQUARE_ROOT());
+                     tflite_micro::tflm_signal::Register_FILTER_BANK_SQUARE_ROOT());
   }
   TfLiteStatus AddFilterBankSpectralSubtraction() {
     // TODO(b/286250473): change back name to "FilterBankSpectralSubtraction"
     // and remove namespace
     return AddCustom(
         "SignalFilterBankSpectralSubtraction",
-        tflite::tflm_signal::Register_FILTER_BANK_SPECTRAL_SUBTRACTION());
+        tflite_micro::tflm_signal::Register_FILTER_BANK_SPECTRAL_SUBTRACTION());
   }
 
   TfLiteStatus AddFloor() {
@@ -304,18 +304,18 @@ class MicroMutableOpResolver : public MicroOpResolver {
   }
 
   TfLiteStatus AddFloorDiv() {
-    return AddBuiltin(BuiltinOperator_FLOOR_DIV, tflite::Register_FLOOR_DIV(),
+    return AddBuiltin(BuiltinOperator_FLOOR_DIV, tflite_micro::Register_FLOOR_DIV(),
                       ParseFloorDiv);
   }
 
   TfLiteStatus AddFloorMod() {
-    return AddBuiltin(BuiltinOperator_FLOOR_MOD, tflite::Register_FLOOR_MOD(),
+    return AddBuiltin(BuiltinOperator_FLOOR_MOD, tflite_micro::Register_FLOOR_MOD(),
                       ParseFloorMod);
   }
 
   TfLiteStatus AddFramer() {
     // TODO(b/286250473): change back name to "Framer" and remove namespace
-    return AddCustom("SignalFramer", tflite::tflm_signal::Register_FRAMER());
+    return AddCustom("SignalFramer", tflite_micro::tflm_signal::Register_FRAMER());
   }
 
   TfLiteStatus AddFullyConnected(
@@ -325,12 +325,12 @@ class MicroMutableOpResolver : public MicroOpResolver {
   }
 
   TfLiteStatus AddGather() {
-    return AddBuiltin(BuiltinOperator_GATHER, tflite::Register_GATHER(),
+    return AddBuiltin(BuiltinOperator_GATHER, tflite_micro::Register_GATHER(),
                       ParseGather);
   }
 
   TfLiteStatus AddGatherNd() {
-    return AddBuiltin(BuiltinOperator_GATHER_ND, tflite::Register_GATHER_ND(),
+    return AddBuiltin(BuiltinOperator_GATHER_ND, tflite_micro::Register_GATHER_ND(),
                       ParseGatherNd);
   }
 
@@ -345,12 +345,12 @@ class MicroMutableOpResolver : public MicroOpResolver {
   }
 
   TfLiteStatus AddHardSwish() {
-    return AddBuiltin(BuiltinOperator_HARD_SWISH, tflite::Register_HARD_SWISH(),
+    return AddBuiltin(BuiltinOperator_HARD_SWISH, tflite_micro::Register_HARD_SWISH(),
                       ParseHardSwish);
   }
 
   TfLiteStatus AddIf() {
-    return AddBuiltin(BuiltinOperator_IF, tflite::Register_IF(), ParseIf);
+    return AddBuiltin(BuiltinOperator_IF, tflite_micro::Register_IF(), ParseIf);
   }
 
   TfLiteStatus AddIrfft(const TFLMRegistration* registration =
@@ -365,12 +365,12 @@ class MicroMutableOpResolver : public MicroOpResolver {
   }
 
   TfLiteStatus AddL2Pool2D() {
-    return AddBuiltin(BuiltinOperator_L2_POOL_2D, tflite::Register_L2_POOL_2D(),
+    return AddBuiltin(BuiltinOperator_L2_POOL_2D, tflite_micro::Register_L2_POOL_2D(),
                       ParsePool);
   }
 
   TfLiteStatus AddLeakyRelu() {
-    return AddBuiltin(BuiltinOperator_LEAKY_RELU, tflite::Register_LEAKY_RELU(),
+    return AddBuiltin(BuiltinOperator_LEAKY_RELU, tflite_micro::Register_LEAKY_RELU(),
                       ParseLeakyRelu);
   }
 
@@ -389,7 +389,7 @@ class MicroMutableOpResolver : public MicroOpResolver {
 
   TfLiteStatus AddLogicalAnd() {
     return AddBuiltin(BuiltinOperator_LOGICAL_AND,
-                      tflite::Register_LOGICAL_AND(), ParseLogicalAnd);
+                      tflite_micro::Register_LOGICAL_AND(), ParseLogicalAnd);
   }
 
   TfLiteStatus AddLogicalNot() {
@@ -398,18 +398,18 @@ class MicroMutableOpResolver : public MicroOpResolver {
   }
 
   TfLiteStatus AddLogicalOr() {
-    return AddBuiltin(BuiltinOperator_LOGICAL_OR, tflite::Register_LOGICAL_OR(),
+    return AddBuiltin(BuiltinOperator_LOGICAL_OR, tflite_micro::Register_LOGICAL_OR(),
                       ParseLogicalOr);
   }
 
   TfLiteStatus AddLogistic() {
-    return AddBuiltin(BuiltinOperator_LOGISTIC, tflite::Register_LOGISTIC(),
+    return AddBuiltin(BuiltinOperator_LOGISTIC, tflite_micro::Register_LOGISTIC(),
                       ParseLogistic);
   }
 
   TfLiteStatus AddLogSoftmax() {
     return AddBuiltin(BuiltinOperator_LOG_SOFTMAX,
-                      tflite::Register_LOG_SOFTMAX(), ParseLogSoftmax);
+                      tflite_micro::Register_LOG_SOFTMAX(), ParseLogSoftmax);
   }
 
   TfLiteStatus AddMaximum() {
@@ -423,7 +423,7 @@ class MicroMutableOpResolver : public MicroOpResolver {
   }
 
   TfLiteStatus AddMirrorPad() {
-    return AddBuiltin(BuiltinOperator_MIRROR_PAD, tflite::Register_MIRROR_PAD(),
+    return AddBuiltin(BuiltinOperator_MIRROR_PAD, tflite_micro::Register_MIRROR_PAD(),
                       ParseMirrorPad);
   }
 
@@ -452,7 +452,7 @@ class MicroMutableOpResolver : public MicroOpResolver {
   TfLiteStatus AddOverlapAdd() {
     // TODO(b/286250473): change back name to "OverlapAdd" and remove namespace
     return AddCustom("SignalOverlapAdd",
-                     tflite::tflm_signal::Register_OVERLAP_ADD());
+                     tflite_micro::tflm_signal::Register_OVERLAP_ADD());
   }
 
   TfLiteStatus AddPack() {
@@ -469,11 +469,11 @@ class MicroMutableOpResolver : public MicroOpResolver {
 
   TfLiteStatus AddPCAN() {
     // TODO(b/286250473): change back name to "PCAN" and remove namespace
-    return AddCustom("SignalPCAN", tflite::tflm_signal::Register_PCAN());
+    return AddCustom("SignalPCAN", tflite_micro::tflm_signal::Register_PCAN());
   }
 
   TfLiteStatus AddPrelu() {
-    return AddBuiltin(BuiltinOperator_PRELU, tflite::Register_PRELU(),
+    return AddBuiltin(BuiltinOperator_PRELU, tflite_micro::Register_PRELU(),
                       ParsePrelu);
   }
 
@@ -484,7 +484,7 @@ class MicroMutableOpResolver : public MicroOpResolver {
 
   TfLiteStatus AddReadVariable() {
     return AddBuiltin(BuiltinOperator_READ_VARIABLE,
-                      tflite::Register_READ_VARIABLE(), ParseReadVariable);
+                      tflite_micro::Register_READ_VARIABLE(), ParseReadVariable);
   }
 
   TfLiteStatus AddReduceMax() {
@@ -493,11 +493,11 @@ class MicroMutableOpResolver : public MicroOpResolver {
   }
 
   TfLiteStatus AddRelu() {
-    return AddBuiltin(BuiltinOperator_RELU, tflite::Register_RELU(), ParseRelu);
+    return AddBuiltin(BuiltinOperator_RELU, tflite_micro::Register_RELU(), ParseRelu);
   }
 
   TfLiteStatus AddRelu6() {
-    return AddBuiltin(BuiltinOperator_RELU6, tflite::Register_RELU6(),
+    return AddBuiltin(BuiltinOperator_RELU6, tflite_micro::Register_RELU6(),
                       ParseRelu6);
   }
 
@@ -586,7 +586,7 @@ class MicroMutableOpResolver : public MicroOpResolver {
 
   TfLiteStatus AddSquaredDifference() {
     return AddBuiltin(BuiltinOperator_SQUARED_DIFFERENCE,
-                      tflite::Register_SQUARED_DIFFERENCE(),
+                      tflite_micro::Register_SQUARED_DIFFERENCE(),
                       ParseSquaredDifference);
   }
 
@@ -597,11 +597,11 @@ class MicroMutableOpResolver : public MicroOpResolver {
 
   TfLiteStatus AddStacker() {
     // TODO(b/286250473): change back name to "Stacker" and remove namespace
-    return AddCustom("SignalStacker", tflite::tflm_signal::Register_STACKER());
+    return AddCustom("SignalStacker", tflite_micro::tflm_signal::Register_STACKER());
   }
 
   TfLiteStatus AddSub() {
-    return AddBuiltin(BuiltinOperator_SUB, tflite::Register_SUB(), ParseSub);
+    return AddBuiltin(BuiltinOperator_SUB, tflite_micro::Register_SUB(), ParseSub);
   }
 
   TfLiteStatus AddSum() {
@@ -618,7 +618,7 @@ class MicroMutableOpResolver : public MicroOpResolver {
 
   TfLiteStatus AddTransposeConv() {
     return AddBuiltin(BuiltinOperator_TRANSPOSE_CONV,
-                      tflite::Register_TRANSPOSE_CONV(), ParseTransposeConv);
+                      tflite_micro::Register_TRANSPOSE_CONV(), ParseTransposeConv);
   }
 
   TfLiteStatus AddTranspose() {
@@ -648,7 +648,7 @@ class MicroMutableOpResolver : public MicroOpResolver {
 
   TfLiteStatus AddWindow() {
     // TODO(b/286250473): change back name to "Window" and remove namespace
-    return AddCustom("SignalWindow", tflite::tflm_signal::Register_WINDOW());
+    return AddCustom("SignalWindow", tflite_micro::tflm_signal::Register_WINDOW());
   }
 
   TfLiteStatus AddZerosLike() {
@@ -659,7 +659,7 @@ class MicroMutableOpResolver : public MicroOpResolver {
   unsigned int GetRegistrationLength() { return registrations_len_; }
 
  private:
-  TfLiteStatus AddBuiltin(tflite::BuiltinOperator op,
+  TfLiteStatus AddBuiltin(tflite_micro::BuiltinOperator op,
                           const TFLMRegistration& registration,
                           TfLiteBridgeBuiltinParseFunction parser) {
     if (op == BuiltinOperator_CUSTOM) {
@@ -703,6 +703,6 @@ class MicroMutableOpResolver : public MicroOpResolver {
   unsigned int num_buitin_ops_ = 0;
 };
 
-};  // namespace tflite
+};  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_MICRO_MUTABLE_OP_RESOLVER_H_
diff --git a/tensorflow/lite/micro/micro_mutable_op_resolver_test.cc b/tensorflow/lite/micro/micro_mutable_op_resolver_test.cc
index dbbb8720..1c9f0730 100644
--- a/tensorflow/lite/micro/micro_mutable_op_resolver_test.cc
+++ b/tensorflow/lite/micro/micro_mutable_op_resolver_test.cc
@@ -18,7 +18,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_op_resolver.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 void* MockInit(TfLiteContext* context, const char* buffer, size_t length) {
   // Do nothing.
@@ -55,20 +55,20 @@ class MockErrorReporter : public ErrorReporter {
 };
 
 }  // namespace
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(TestOperations) {
-  using tflite::BuiltinOperator_CONV_2D;
-  using tflite::BuiltinOperator_RELU;
-  using tflite::MicroMutableOpResolver;
+  using tflite_micro::BuiltinOperator_CONV_2D;
+  using tflite_micro::BuiltinOperator_RELU;
+  using tflite_micro::MicroMutableOpResolver;
 
   static TFLMRegistration r = {};
-  r.init = tflite::MockInit;
-  r.free = tflite::MockFree;
-  r.prepare = tflite::MockPrepare;
-  r.invoke = tflite::MockInvoke;
+  r.init = tflite_micro::MockInit;
+  r.free = tflite_micro::MockFree;
+  r.prepare = tflite_micro::MockPrepare;
+  r.invoke = tflite_micro::MockInvoke;
 
   MicroMutableOpResolver<1> micro_op_resolver;
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk,
@@ -78,7 +78,7 @@ TF_LITE_MICRO_TEST(TestOperations) {
   TF_LITE_MICRO_EXPECT_EQ(kTfLiteError,
                           micro_op_resolver.AddCustom("mock_custom", &r));
 
-  tflite::MicroOpResolver* resolver = &micro_op_resolver;
+  tflite_micro::MicroOpResolver* resolver = &micro_op_resolver;
 
   TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(1),
                           micro_op_resolver.GetRegistrationLength());
diff --git a/tensorflow/lite/micro/micro_op_resolver.cc b/tensorflow/lite/micro/micro_op_resolver.cc
index 8d9603c0..ad13e413 100644
--- a/tensorflow/lite/micro/micro_op_resolver.cc
+++ b/tensorflow/lite/micro/micro_op_resolver.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_log.h"
 #include "tensorflow/lite/schema/schema_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 TfLiteStatus GetRegistrationFromOpCode(const OperatorCode* opcode,
                                        const MicroOpResolver& op_resolver,
@@ -52,4 +52,4 @@ TfLiteStatus GetRegistrationFromOpCode(const OperatorCode* opcode,
   }
   return status;
 }
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/micro_op_resolver.h b/tensorflow/lite/micro/micro_op_resolver.h
index e9aac380..af4e4d26 100644
--- a/tensorflow/lite/micro/micro_op_resolver.h
+++ b/tensorflow/lite/micro/micro_op_resolver.h
@@ -20,7 +20,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/tflite_bridge/flatbuffer_conversions_bridge.h"
 #include "tensorflow/lite/schema/schema_generated.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // This is an interface for the OpResolver for TFLiteMicro. The differences from
 // the TFLite OpResolver base class are to:
@@ -57,6 +57,6 @@ TfLiteStatus GetRegistrationFromOpCode(const OperatorCode* opcode,
                                        const MicroOpResolver& op_resolver,
                                        const TFLMRegistration** registration);
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_MICRO_OP_RESOLVER_H_
diff --git a/tensorflow/lite/micro/micro_profiler.cc b/tensorflow/lite/micro/micro_profiler.cc
index e7743aae..0cf43b16 100644
--- a/tensorflow/lite/micro/micro_profiler.cc
+++ b/tensorflow/lite/micro/micro_profiler.cc
@@ -22,7 +22,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_log.h"
 #include "tensorflow/lite/micro/micro_time.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 uint32_t MicroProfiler::BeginEvent(const char* tag) {
   if (num_events_ == kMaxEvents) {
@@ -120,4 +120,4 @@ int MicroProfiler::FindExistingOrNextPosition(const char* tag_name) {
   }
   return pos < num_events_ ? pos : -1;
 }
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/micro_profiler.h b/tensorflow/lite/micro/micro_profiler.h
index 234948dd..fe8e586e 100644
--- a/tensorflow/lite/micro/micro_profiler.h
+++ b/tensorflow/lite/micro/micro_profiler.h
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/compatibility.h"
 #include "tensorflow/lite/micro/micro_profiler_interface.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // MicroProfiler creates a common way to gain fine-grained insight into runtime
 // performance. Bottleck operators can be identified along with slow code
@@ -135,6 +135,6 @@ class ScopedMicroProfiler {
 };
 #endif  // !defined(TF_LITE_STRIP_ERROR_STRINGS)
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_MICRO_PROFILER_H_
diff --git a/tensorflow/lite/micro/micro_profiler_interface.h b/tensorflow/lite/micro/micro_profiler_interface.h
index f839a74a..a879501b 100644
--- a/tensorflow/lite/micro/micro_profiler_interface.h
+++ b/tensorflow/lite/micro/micro_profiler_interface.h
@@ -18,7 +18,7 @@ limitations under the License.
 
 #include <cstdint>
 
-namespace tflite {
+namespace tflite_micro {
 
 // Interface class that the TFLM framework relies on for profiling.
 class MicroProfilerInterface {
@@ -33,6 +33,6 @@ class MicroProfilerInterface {
   virtual void EndEvent(uint32_t event_handle) = 0;
 };
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_MICRO_PROFILER_INTERFACE_H_
diff --git a/tensorflow/lite/micro/micro_resource_variable.cc b/tensorflow/lite/micro/micro_resource_variable.cc
index 767e7d17..5ba26b3e 100644
--- a/tensorflow/lite/micro/micro_resource_variable.cc
+++ b/tensorflow/lite/micro/micro_resource_variable.cc
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_log.h"
 #include "tensorflow/lite/micro/micro_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {}  // namespace
 
@@ -155,4 +155,4 @@ int MicroResourceVariables::FindId(const char* container,
   return -1;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/micro_resource_variable.h b/tensorflow/lite/micro/micro_resource_variable.h
index fb9917d4..44bf92d0 100644
--- a/tensorflow/lite/micro/micro_resource_variable.h
+++ b/tensorflow/lite/micro/micro_resource_variable.h
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/c/common.h"
 #include "tensorflow/lite/micro/micro_allocator.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 class MicroResourceVariables {
  public:
@@ -84,6 +84,6 @@ class MicroResourceVariables {
   int num_resource_variables_;
 };
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TFLITE_MICRO_TENSORFLOW_LITE_MICRO_MICRO_RESOURCE_H_
diff --git a/tensorflow/lite/micro/micro_resource_variable_test.cc b/tensorflow/lite/micro/micro_resource_variable_test.cc
index 13868bb4..0d38a4a8 100644
--- a/tensorflow/lite/micro/micro_resource_variable_test.cc
+++ b/tensorflow/lite/micro/micro_resource_variable_test.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/test_helpers.h"
 #include "tensorflow/lite/micro/testing/micro_test.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace {
 
 constexpr int kMaxBufferSize = 1024;
@@ -38,15 +38,15 @@ TfLiteContext* GetMockContext() {
 }
 
 }  // namespace
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(CreateVariables) {
-  tflite::MicroResourceVariables* resource_variables =
-      tflite::MicroResourceVariables::Create(
-          tflite::MicroAllocator::Create(tflite::buffer_,
-                                         tflite::kMaxBufferSize),
+  tflite_micro::MicroResourceVariables* resource_variables =
+      tflite_micro::MicroResourceVariables::Create(
+          tflite_micro::MicroAllocator::Create(tflite_micro::buffer_,
+                                         tflite_micro::kMaxBufferSize),
           4);
   int id1 = resource_variables->CreateIdIfNoneFound("", "var1");
   TF_LITE_MICRO_EXPECT_GE(id1, 0);
@@ -74,10 +74,10 @@ TF_LITE_MICRO_TEST(CreateVariables) {
 }
 
 TF_LITE_MICRO_TEST(AllocateResourceBuffers) {
-  tflite::MicroResourceVariables* resource_variables =
-      tflite::MicroResourceVariables::Create(
-          tflite::MicroAllocator::Create(tflite::buffer_,
-                                         tflite::kMaxBufferSize),
+  tflite_micro::MicroResourceVariables* resource_variables =
+      tflite_micro::MicroResourceVariables::Create(
+          tflite_micro::MicroAllocator::Create(tflite_micro::buffer_,
+                                         tflite_micro::kMaxBufferSize),
           2);
   int id1 = resource_variables->CreateIdIfNoneFound("", "var1");
   TF_LITE_MICRO_EXPECT_GE(id1, 0);
@@ -87,19 +87,19 @@ TF_LITE_MICRO_TEST(AllocateResourceBuffers) {
 
   TfLiteTensor tensor = {};
   tensor.bytes = 42;
-  resource_variables->Allocate(id1, tflite::GetMockContext(), &tensor);
-  TF_LITE_MICRO_EXPECT_EQ(42, tflite::last_allocation_size_);
+  resource_variables->Allocate(id1, tflite_micro::GetMockContext(), &tensor);
+  TF_LITE_MICRO_EXPECT_EQ(42, tflite_micro::last_allocation_size_);
 
   tensor.bytes = 100;
-  resource_variables->Allocate(id2, tflite::GetMockContext(), &tensor);
-  TF_LITE_MICRO_EXPECT_EQ(100, tflite::last_allocation_size_);
+  resource_variables->Allocate(id2, tflite_micro::GetMockContext(), &tensor);
+  TF_LITE_MICRO_EXPECT_EQ(100, tflite_micro::last_allocation_size_);
 }
 
 TF_LITE_MICRO_TEST(VerifyAssignAndReadResourceBuffer) {
-  tflite::MicroResourceVariables* resource_variables =
-      tflite::MicroResourceVariables::Create(
-          tflite::MicroAllocator::Create(tflite::buffer_,
-                                         tflite::kMaxBufferSize),
+  tflite_micro::MicroResourceVariables* resource_variables =
+      tflite_micro::MicroResourceVariables::Create(
+          tflite_micro::MicroAllocator::Create(tflite_micro::buffer_,
+                                         tflite_micro::kMaxBufferSize),
           1);
   int id = resource_variables->CreateIdIfNoneFound("", "var1");
   TF_LITE_MICRO_EXPECT_GE(id, 0);
@@ -107,8 +107,8 @@ TF_LITE_MICRO_TEST(VerifyAssignAndReadResourceBuffer) {
   TfLiteTensor tensor = {};
   const int bytes = 32 * sizeof(int32_t);
   tensor.bytes = bytes;
-  resource_variables->Allocate(id, tflite::GetMockContext(), &tensor);
-  TF_LITE_MICRO_EXPECT_EQ(bytes, tflite::last_allocation_size_);
+  resource_variables->Allocate(id, tflite_micro::GetMockContext(), &tensor);
+  TF_LITE_MICRO_EXPECT_EQ(bytes, tflite_micro::last_allocation_size_);
 
   int32_t golden[32] = {1,  2,  3,  4,  5,  6,  7,  8,  9,  10, 11,
                         12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,
@@ -116,7 +116,7 @@ TF_LITE_MICRO_TEST(VerifyAssignAndReadResourceBuffer) {
   int dims[] = {1, 32};
   TfLiteEvalTensor assign_tensor = {
       .data = {golden},
-      .dims = tflite::testing::IntArrayFromInts(dims),
+      .dims = tflite_micro::testing::IntArrayFromInts(dims),
 
       .type = kTfLiteFloat32,
   };
@@ -125,7 +125,7 @@ TF_LITE_MICRO_TEST(VerifyAssignAndReadResourceBuffer) {
   int32_t buffer[32];
   TfLiteEvalTensor read_tensor = {
       .data = {buffer},
-      .dims = tflite::testing::IntArrayFromInts(dims),
+      .dims = tflite_micro::testing::IntArrayFromInts(dims),
       .type = kTfLiteInt32,
   };
   resource_variables->Read(id, &read_tensor);
@@ -135,10 +135,10 @@ TF_LITE_MICRO_TEST(VerifyAssignAndReadResourceBuffer) {
 }
 
 TF_LITE_MICRO_TEST(CreateVariablesNullContainer) {
-  tflite::MicroResourceVariables* resource_variables =
-      tflite::MicroResourceVariables::Create(
-          tflite::MicroAllocator::Create(tflite::buffer_,
-                                         tflite::kMaxBufferSize),
+  tflite_micro::MicroResourceVariables* resource_variables =
+      tflite_micro::MicroResourceVariables::Create(
+          tflite_micro::MicroAllocator::Create(tflite_micro::buffer_,
+                                         tflite_micro::kMaxBufferSize),
           4);
   int id1 = resource_variables->CreateIdIfNoneFound(nullptr, "var1");
   TF_LITE_MICRO_EXPECT_GE(id1, 0);
diff --git a/tensorflow/lite/micro/micro_time.cc b/tensorflow/lite/micro/micro_time.cc
index 2d74fdba..2bad1644 100644
--- a/tensorflow/lite/micro/micro_time.cc
+++ b/tensorflow/lite/micro/micro_time.cc
@@ -30,7 +30,7 @@ limitations under the License.
 #include <ctime>
 #endif
 
-namespace tflite {
+namespace tflite_micro {
 
 #if !defined(TF_LITE_USE_CTIME)
 
@@ -55,4 +55,4 @@ uint32_t ticks_per_second() { return CLOCKS_PER_SEC; }
 uint32_t GetCurrentTimeTicks() { return clock(); }
 #endif
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/micro_time.h b/tensorflow/lite/micro/micro_time.h
index 7a8ab455..27e7292d 100644
--- a/tensorflow/lite/micro/micro_time.h
+++ b/tensorflow/lite/micro/micro_time.h
@@ -17,7 +17,7 @@ limitations under the License.
 
 #include <cstdint>
 
-namespace tflite {
+namespace tflite_micro {
 
 // These functions should be implemented by each target platform, and provide an
 // accurate tick count along with how many ticks there are per second.
@@ -31,6 +31,6 @@ inline uint32_t TicksToMs(int32_t ticks) {
                                static_cast<float>(ticks_per_second()));
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_MICRO_TIME_H_
diff --git a/tensorflow/lite/micro/micro_time_test.cc b/tensorflow/lite/micro/micro_time_test.cc
index 7e3a02d5..004b309a 100644
--- a/tensorflow/lite/micro/micro_time_test.cc
+++ b/tensorflow/lite/micro/micro_time_test.cc
@@ -20,17 +20,17 @@ limitations under the License.
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(TestBasicTimerFunctionality) {
-  uint32_t ticks_per_second = tflite::ticks_per_second();
+  uint32_t ticks_per_second = tflite_micro::ticks_per_second();
 
   // Retry enough times to guarantee a tick advance, while not taking too long
   // to complete.  With 1e6 retries, assuming each loop takes tens of cycles,
   // this will retry for less than 10 seconds on a 10MHz platform.
   constexpr int kMaxRetries = 1e6;
-  unsigned int start_time = tflite::GetCurrentTimeTicks();
+  unsigned int start_time = tflite_micro::GetCurrentTimeTicks();
 
   if (ticks_per_second != 0) {
     for (int i = 0; i < kMaxRetries; i++) {
-      if (tflite::GetCurrentTimeTicks() - start_time > 0) {
+      if (tflite_micro::GetCurrentTimeTicks() - start_time > 0) {
         break;
       }
     }
@@ -42,7 +42,7 @@ TF_LITE_MICRO_TEST(TestBasicTimerFunctionality) {
   // GetTicksPerSecond() == 0 means the timer is not implemented on this
   // platform.
   TF_LITE_MICRO_EXPECT(ticks_per_second == 0 ||
-                       tflite::GetCurrentTimeTicks() - start_time > 0);
+                       tflite_micro::GetCurrentTimeTicks() - start_time > 0);
 }
 
 TF_LITE_MICRO_TESTS_END
diff --git a/tensorflow/lite/micro/micro_utils.cc b/tensorflow/lite/micro/micro_utils.cc
index 7b0c9cf6..534bf7f4 100644
--- a/tensorflow/lite/micro/micro_utils.cc
+++ b/tensorflow/lite/micro/micro_utils.cc
@@ -25,7 +25,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/memory_helpers.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 int ElementCount(const TfLiteIntArray& dims) {
   int result = 1;
@@ -87,4 +87,4 @@ void SignedSymmetricPerChannelQuantize(
   }
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/micro_utils.h b/tensorflow/lite/micro/micro_utils.h
index 98ef81dc..ad53ffdb 100644
--- a/tensorflow/lite/micro/micro_utils.h
+++ b/tensorflow/lite/micro/micro_utils.h
@@ -23,7 +23,7 @@ limitations under the License.
 
 #include "tensorflow/lite/c/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // Returns number of elements in the shape array.
 
@@ -157,6 +157,6 @@ inline int QMaxFromTfLiteType(TfLiteType type) {
   }
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_MICRO_UTILS_H_
diff --git a/tensorflow/lite/micro/micro_utils_test.cc b/tensorflow/lite/micro/micro_utils_test.cc
index 0df9f2a8..771d7c1a 100644
--- a/tensorflow/lite/micro/micro_utils_test.cc
+++ b/tensorflow/lite/micro/micro_utils_test.cc
@@ -20,7 +20,7 @@ limitations under the License.
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(FloatToAsymmetricQuantizedUInt8Test) {
-  using tflite::FloatToQuantizedType;
+  using tflite_micro::FloatToQuantizedType;
   // [0, 127.5] -> zero_point=0, scale=0.5
   TF_LITE_MICRO_EXPECT_EQ(0, FloatToQuantizedType<uint8_t>(0, 0.5, 0));
   TF_LITE_MICRO_EXPECT_EQ(254, FloatToQuantizedType<uint8_t>(127, 0.5, 0));
@@ -35,7 +35,7 @@ TF_LITE_MICRO_TEST(FloatToAsymmetricQuantizedUInt8Test) {
 }
 
 TF_LITE_MICRO_TEST(FloatToAsymmetricQuantizedInt8Test) {
-  using tflite::FloatToQuantizedType;
+  using tflite_micro::FloatToQuantizedType;
   // [-64, 63.5] -> zero_point=0, scale=0.5
   TF_LITE_MICRO_EXPECT_EQ(2, FloatToQuantizedType<int8_t>(1, 0.5, 0));
   TF_LITE_MICRO_EXPECT_EQ(4, FloatToQuantizedType<int8_t>(2, 0.5, 0));
@@ -53,7 +53,7 @@ TF_LITE_MICRO_TEST(FloatToAsymmetricQuantizedInt8Test) {
 }
 
 TF_LITE_MICRO_TEST(FloatToSymmetricQuantizedInt8Test) {
-  using tflite::FloatToSymmetricQuantizedType;
+  using tflite_micro::FloatToSymmetricQuantizedType;
   // [-64, 63.5] -> zero_point=0, scale=0.5
   TF_LITE_MICRO_EXPECT_EQ(2, FloatToSymmetricQuantizedType<int8_t>(1, 0.5));
   TF_LITE_MICRO_EXPECT_EQ(4, FloatToSymmetricQuantizedType<int8_t>(2, 0.5));
@@ -75,7 +75,7 @@ TF_LITE_MICRO_TEST(FloatToSymmetricQuantizedInt8Test) {
 }
 
 TF_LITE_MICRO_TEST(FloatToAsymmetricQuantizedInt32Test) {
-  using tflite::FloatToSymmetricQuantizedType;
+  using tflite_micro::FloatToSymmetricQuantizedType;
   TF_LITE_MICRO_EXPECT_EQ(0, FloatToSymmetricQuantizedType<int32_t>(0, 0.5));
   TF_LITE_MICRO_EXPECT_EQ(2, FloatToSymmetricQuantizedType<int32_t>(1, 0.5));
   TF_LITE_MICRO_EXPECT_EQ(-2, FloatToSymmetricQuantizedType<int32_t>(-1, 0.5));
@@ -89,7 +89,7 @@ TF_LITE_MICRO_TEST(AsymmetricQuantizeInt8) {
   int8_t goldens[] = {-20, -5, -3, -3, -1, 1, 3, 5, 7, 9};
   constexpr int length = sizeof(values) / sizeof(float);
   int8_t quantized[length];
-  tflite::Quantize(values, quantized, length, 0.5, 1);
+  tflite_micro::Quantize(values, quantized, length, 0.5, 1);
   for (int i = 0; i < length; i++) {
     TF_LITE_MICRO_EXPECT_EQ(quantized[i], goldens[i]);
   }
@@ -100,7 +100,7 @@ TF_LITE_MICRO_TEST(AsymmetricQuantizeUInt8) {
   uint8_t goldens[] = {106, 121, 123, 123, 125, 127, 129, 131, 133, 135};
   constexpr int length = sizeof(values) / sizeof(float);
   uint8_t quantized[length];
-  tflite::Quantize(values, quantized, length, 0.5, 127);
+  tflite_micro::Quantize(values, quantized, length, 0.5, 127);
   for (int i = 0; i < length; i++) {
     TF_LITE_MICRO_EXPECT_EQ(quantized[i], goldens[i]);
   }
@@ -111,7 +111,7 @@ TF_LITE_MICRO_TEST(SymmetricQuantizeInt32) {
   int32_t goldens[] = {-21, -6, -4, -4, -2, 0, 2, 4, 6, 8};
   constexpr int length = sizeof(values) / sizeof(float);
   int32_t quantized[length];
-  tflite::SymmetricQuantize(values, quantized, length, 0.5);
+  tflite_micro::SymmetricQuantize(values, quantized, length, 0.5);
   for (int i = 0; i < length; i++) {
     TF_LITE_MICRO_EXPECT_EQ(quantized[i], goldens[i]);
   }
@@ -119,7 +119,7 @@ TF_LITE_MICRO_TEST(SymmetricQuantizeInt32) {
 
 // Verify Max function works as expected.
 TF_LITE_MICRO_TEST(Max) {
-  using tflite::Max;
+  using tflite_micro::Max;
   // [0, 127.5] -> zero_point=0, scale=0.5
   TF_LITE_MICRO_EXPECT_EQ(3, Max(2, 3));
   TF_LITE_MICRO_EXPECT_EQ(2, Max(2, 2));
diff --git a/tensorflow/lite/micro/mock_micro_graph.cc b/tensorflow/lite/micro/mock_micro_graph.cc
index 9c652fb2..8064abf7 100644
--- a/tensorflow/lite/micro/mock_micro_graph.cc
+++ b/tensorflow/lite/micro/mock_micro_graph.cc
@@ -17,7 +17,7 @@ limitations under the License.
 
 #include "tensorflow/lite/micro/test_helpers.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 MockMicroGraph::MockMicroGraph(SingleArenaBufferAllocator* allocator)
     : allocator_(allocator), init_count_(0), prepare_count_(0), free_count_(0) {
@@ -61,4 +61,4 @@ MicroResourceVariables* MockMicroGraph::GetResourceVariables() {
   return nullptr;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/mock_micro_graph.h b/tensorflow/lite/micro/mock_micro_graph.h
index 745e8f0c..219b6b4b 100644
--- a/tensorflow/lite/micro/mock_micro_graph.h
+++ b/tensorflow/lite/micro/mock_micro_graph.h
@@ -21,7 +21,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_graph.h"
 #include "tensorflow/lite/schema/schema_generated.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // MockMicroGraph stubs out all MicroGraph methods used during invoke. A count
 // of the number of calls to invoke for each subgraph is maintained for
@@ -55,6 +55,6 @@ class MockMicroGraph : public MicroGraph {
   TF_LITE_REMOVE_VIRTUAL_DELETE
 };
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_MOCK_MICRO_GRAPH_H_
diff --git a/tensorflow/lite/micro/python/tflite_size/src/flatbuffer_size.cc b/tensorflow/lite/micro/python/tflite_size/src/flatbuffer_size.cc
index 5c06dfc1..0f4d15a2 100644
--- a/tensorflow/lite/micro/python/tflite_size/src/flatbuffer_size.cc
+++ b/tensorflow/lite/micro/python/tflite_size/src/flatbuffer_size.cc
@@ -567,7 +567,7 @@ struct ToJsonWithSizeInfoVisitor : public IterationVisitor {
 
 }  // namespace
 
-namespace tflite {
+namespace tflite_micro {
 std::string FlatBufferSizeToJsonString(
     const uint8_t* buffer, const flatbuffers::TypeTable* type_table) {
   ToJsonWithSizeInfoVisitor tostring_visitor;
@@ -575,4 +575,4 @@ std::string FlatBufferSizeToJsonString(
   return tostring_visitor.s;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/python/tflite_size/src/flatbuffer_size.h b/tensorflow/lite/micro/python/tflite_size/src/flatbuffer_size.h
index 6e4243ad..f65d8bb6 100644
--- a/tensorflow/lite/micro/python/tflite_size/src/flatbuffer_size.h
+++ b/tensorflow/lite/micro/python/tflite_size/src/flatbuffer_size.h
@@ -20,11 +20,11 @@ limitations under the License.
 #include "flatbuffers/flatbuffers.h"
 #include "flatbuffers/util.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 std::string FlatBufferSizeToJsonString(
     const uint8_t* buffer, const flatbuffers::TypeTable* type_table);
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_PYTHON_TFLITE_SIZE_SRC_FLATBUFFERS_SIZE_H_
diff --git a/tensorflow/lite/micro/python/tflite_size/src/flatbuffer_size_wrapper.cc b/tensorflow/lite/micro/python/tflite_size/src/flatbuffer_size_wrapper.cc
index 816445e4..b9d1c713 100644
--- a/tensorflow/lite/micro/python/tflite_size/src/flatbuffer_size_wrapper.cc
+++ b/tensorflow/lite/micro/python/tflite_size/src/flatbuffer_size_wrapper.cc
@@ -20,7 +20,7 @@ limitations under the License.
 #include "flatbuffer_size.h"
 #include "tensorflow/lite/schema/reflection/schema_generated.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 FlatbufferSizeWrapper::~FlatbufferSizeWrapper() {}
 
@@ -28,11 +28,11 @@ FlatbufferSizeWrapper::FlatbufferSizeWrapper() {}
 
 std::string FlatbufferSizeWrapper::ConvertToJsonString(
     const char* in_flatbuffer) {
-  std::string output = tflite::FlatBufferSizeToJsonString(
+  std::string output = tflite_micro::FlatBufferSizeToJsonString(
       reinterpret_cast<const uint8_t*>(in_flatbuffer),
-      tflite::ModelTypeTable());
+      tflite_micro::ModelTypeTable());
 
   return output;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/python/tflite_size/src/flatbuffer_size_wrapper.h b/tensorflow/lite/micro/python/tflite_size/src/flatbuffer_size_wrapper.h
index fc48f868..96df3291 100644
--- a/tensorflow/lite/micro/python/tflite_size/src/flatbuffer_size_wrapper.h
+++ b/tensorflow/lite/micro/python/tflite_size/src/flatbuffer_size_wrapper.h
@@ -19,7 +19,7 @@ limitations under the License.
 
 #include <string>
 
-namespace tflite {
+namespace tflite_micro {
 
 class FlatbufferSizeWrapper {
  public:
@@ -29,5 +29,5 @@ class FlatbufferSizeWrapper {
   std::string ConvertToJsonString(const char* in_flatbuffer);
 };
 
-}  // namespace tflite
+}  // namespace tflite_micro
 #endif  // TENSORFLOW_LITE_MICRO_PYTHON_TFLITE_SIZE_SRC_FLATBUFFERS_SIZE_WRAPPER_H_
diff --git a/tensorflow/lite/micro/python/tflite_size/src/flatbuffer_size_wrapper_pybind.cc b/tensorflow/lite/micro/python/tflite_size/src/flatbuffer_size_wrapper_pybind.cc
index e8786c1d..60870168 100644
--- a/tensorflow/lite/micro/python/tflite_size/src/flatbuffer_size_wrapper_pybind.cc
+++ b/tensorflow/lite/micro/python/tflite_size/src/flatbuffer_size_wrapper_pybind.cc
@@ -22,11 +22,11 @@ namespace py = pybind11;
 PYBIND11_MODULE(flatbuffer_size_wrapper_pybind, m) {
   m.doc() = "FlatbufferSize";
 
-  py::class_<tflite::FlatbufferSizeWrapper>(m, "FlatbufferSize")
+  py::class_<tflite_micro::FlatbufferSizeWrapper>(m, "FlatbufferSize")
       .def(py::init([]() {
-        return std::unique_ptr<tflite::FlatbufferSizeWrapper>(
-            new tflite::FlatbufferSizeWrapper());
+        return std::unique_ptr<tflite_micro::FlatbufferSizeWrapper>(
+            new tflite_micro::FlatbufferSizeWrapper());
       }))
       .def("convertToJsonString",
-           &tflite::FlatbufferSizeWrapper::ConvertToJsonString);
+           &tflite_micro::FlatbufferSizeWrapper::ConvertToJsonString);
 }
diff --git a/tensorflow/lite/micro/recording_micro_allocator.cc b/tensorflow/lite/micro/recording_micro_allocator.cc
index f41dba61..e2ae0bb0 100644
--- a/tensorflow/lite/micro/recording_micro_allocator.cc
+++ b/tensorflow/lite/micro/recording_micro_allocator.cc
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_allocator.h"
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 size_t RecordingMicroAllocator::GetDefaultTailUsage() {
   // RecordingMicroAllocator inherits from MicroAllocator and its tail usage is
@@ -248,4 +248,4 @@ void RecordingMicroAllocator::RecordAllocationUsage(
       snapshotted_allocation.count;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/recording_micro_allocator.h b/tensorflow/lite/micro/recording_micro_allocator.h
index b6f69264..e585f0e8 100644
--- a/tensorflow/lite/micro/recording_micro_allocator.h
+++ b/tensorflow/lite/micro/recording_micro_allocator.h
@@ -20,7 +20,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/compatibility.h"
 #include "tensorflow/lite/micro/micro_allocator.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // List of buckets currently recorded by this class. Each type keeps a list of
 // allocated information during model initialization.
@@ -120,6 +120,6 @@ class RecordingMicroAllocator : public MicroAllocator {
   TF_LITE_REMOVE_VIRTUAL_DELETE
 };
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_RECORDING_MICRO_ALLOCATOR_H_
diff --git a/tensorflow/lite/micro/recording_micro_allocator_test.cc b/tensorflow/lite/micro/recording_micro_allocator_test.cc
index 9d3a5965..becf6673 100644
--- a/tensorflow/lite/micro/recording_micro_allocator_test.cc
+++ b/tensorflow/lite/micro/recording_micro_allocator_test.cc
@@ -23,7 +23,7 @@ limitations under the License.
 #define TF_LITE_TENSOR_STRUCT_SIZE sizeof(TfLiteTensor)
 #define TF_LITE_EVAL_TENSOR_STRUCT_SIZE sizeof(TfLiteEvalTensor)
 #define TF_LITE_AFFINE_QUANTIZATION_SIZE sizeof(TfLiteAffineQuantization)
-#define NODE_AND_REGISTRATION_STRUCT_SIZE sizeof(tflite::NodeAndRegistration)
+#define NODE_AND_REGISTRATION_STRUCT_SIZE sizeof(tflite_micro::NodeAndRegistration)
 
 // TODO(b/158303868): Move tests into anonymous namespace.
 namespace {
@@ -35,19 +35,19 @@ constexpr int kTestConvArenaSize = 1024 * 12;
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(TestRecordsTfLiteEvalTensorArrayData) {
-  tflite::ScratchBufferHandle* scratch_buffer_handles = nullptr;
-  tflite::testing::TestingOpResolver ops_resolver;
-  const tflite::Model* model = tflite::GetModel(kTestConvModelData);
+  tflite_micro::ScratchBufferHandle* scratch_buffer_handles = nullptr;
+  tflite_micro::testing::TestingOpResolver ops_resolver;
+  const tflite_micro::Model* model = tflite_micro::GetModel(kTestConvModelData);
   uint8_t arena[kTestConvArenaSize];
 
-  tflite::RecordingMicroAllocator* micro_allocator =
-      tflite::RecordingMicroAllocator::Create(arena, kTestConvArenaSize);
+  tflite_micro::RecordingMicroAllocator* micro_allocator =
+      tflite_micro::RecordingMicroAllocator::Create(arena, kTestConvArenaSize);
   // TODO(b/158102673): ugly workaround for not having fatal assertions. Same
   // throughout this file.
   TF_LITE_MICRO_EXPECT(micro_allocator != nullptr);
   if (micro_allocator == nullptr) return 1;
 
-  tflite::SubgraphAllocations* subgraph_allocations =
+  tflite_micro::SubgraphAllocations* subgraph_allocations =
       micro_allocator->StartModelAllocation(model);
   TF_LITE_MICRO_EXPECT(nullptr != subgraph_allocations);
   if (subgraph_allocations == nullptr) return 1;
@@ -59,13 +59,13 @@ TF_LITE_MICRO_TEST(TestRecordsTfLiteEvalTensorArrayData) {
 
   micro_allocator->PrintAllocations();
 
-  tflite::RecordedAllocation recorded_allocation =
+  tflite_micro::RecordedAllocation recorded_allocation =
       micro_allocator->GetRecordedAllocation(
-          tflite::RecordedAllocationType::kTfLiteEvalTensorData);
+          tflite_micro::RecordedAllocationType::kTfLiteEvalTensorData);
 
   micro_allocator->PrintAllocations();
 
-  size_t tensors_count = tflite::testing::GetModelTensorCount(model);
+  size_t tensors_count = tflite_micro::testing::GetModelTensorCount(model);
 
   TF_LITE_MICRO_EXPECT_EQ(recorded_allocation.count, tensors_count);
   TF_LITE_MICRO_EXPECT_EQ(recorded_allocation.requested_bytes,
@@ -75,17 +75,17 @@ TF_LITE_MICRO_TEST(TestRecordsTfLiteEvalTensorArrayData) {
 }
 
 TF_LITE_MICRO_TEST(TestRecordsNodeAndRegistrationArrayData) {
-  tflite::ScratchBufferHandle* scratch_buffer_handles = nullptr;
-  tflite::testing::TestingOpResolver ops_resolver;
-  const tflite::Model* model = tflite::GetModel(kTestConvModelData);
+  tflite_micro::ScratchBufferHandle* scratch_buffer_handles = nullptr;
+  tflite_micro::testing::TestingOpResolver ops_resolver;
+  const tflite_micro::Model* model = tflite_micro::GetModel(kTestConvModelData);
   uint8_t arena[kTestConvArenaSize];
 
-  tflite::RecordingMicroAllocator* micro_allocator =
-      tflite::RecordingMicroAllocator::Create(arena, kTestConvArenaSize);
+  tflite_micro::RecordingMicroAllocator* micro_allocator =
+      tflite_micro::RecordingMicroAllocator::Create(arena, kTestConvArenaSize);
   TF_LITE_MICRO_EXPECT(micro_allocator != nullptr);
   if (micro_allocator == nullptr) return 1;
 
-  tflite::SubgraphAllocations* subgraph_allocations =
+  tflite_micro::SubgraphAllocations* subgraph_allocations =
       micro_allocator->StartModelAllocation(model);
   TF_LITE_MICRO_EXPECT(nullptr != subgraph_allocations);
   if (subgraph_allocations == nullptr) return 1;
@@ -96,9 +96,9 @@ TF_LITE_MICRO_TEST(TestRecordsNodeAndRegistrationArrayData) {
   if (status != kTfLiteOk) return 1;
 
   size_t num_ops = model->subgraphs()->Get(0)->operators()->size();
-  tflite::RecordedAllocation recorded_allocation =
+  tflite_micro::RecordedAllocation recorded_allocation =
       micro_allocator->GetRecordedAllocation(
-          tflite::RecordedAllocationType::kNodeAndRegistrationArray);
+          tflite_micro::RecordedAllocationType::kNodeAndRegistrationArray);
 
   TF_LITE_MICRO_EXPECT_EQ(recorded_allocation.count, num_ops);
   TF_LITE_MICRO_EXPECT_EQ(recorded_allocation.requested_bytes,
@@ -108,22 +108,22 @@ TF_LITE_MICRO_TEST(TestRecordsNodeAndRegistrationArrayData) {
 }
 
 TF_LITE_MICRO_TEST(TestRecordsMultiTenantAllocations) {
-  tflite::ScratchBufferHandle* scratch_buffer_handles = nullptr;
-  tflite::testing::TestingOpResolver ops_resolver;
-  const tflite::Model* model = tflite::GetModel(kTestConvModelData);
+  tflite_micro::ScratchBufferHandle* scratch_buffer_handles = nullptr;
+  tflite_micro::testing::TestingOpResolver ops_resolver;
+  const tflite_micro::Model* model = tflite_micro::GetModel(kTestConvModelData);
 
   // Double the arena size to allocate two models inside of it:
   uint8_t arena[kTestConvArenaSize * 2];
 
   TfLiteStatus status;
 
-  tflite::RecordingMicroAllocator* micro_allocator =
-      tflite::RecordingMicroAllocator::Create(arena, kTestConvArenaSize * 2);
+  tflite_micro::RecordingMicroAllocator* micro_allocator =
+      tflite_micro::RecordingMicroAllocator::Create(arena, kTestConvArenaSize * 2);
   TF_LITE_MICRO_EXPECT(micro_allocator != nullptr);
   if (micro_allocator == nullptr) return 1;
 
   // First allocation with the model in the arena:
-  tflite::SubgraphAllocations* subgraph_allocations =
+  tflite_micro::SubgraphAllocations* subgraph_allocations =
       micro_allocator->StartModelAllocation(model);
   TF_LITE_MICRO_EXPECT(nullptr != subgraph_allocations);
   if (subgraph_allocations == nullptr) return 1;
@@ -143,11 +143,11 @@ TF_LITE_MICRO_TEST(TestRecordsMultiTenantAllocations) {
   TF_LITE_MICRO_EXPECT_EQ(status, kTfLiteOk);
   if (status != kTfLiteOk) return 1;
 
-  size_t tensors_count = tflite::testing::GetModelTensorCount(model);
+  size_t tensors_count = tflite_micro::testing::GetModelTensorCount(model);
 
-  tflite::RecordedAllocation recorded_allocation =
+  tflite_micro::RecordedAllocation recorded_allocation =
       micro_allocator->GetRecordedAllocation(
-          tflite::RecordedAllocationType::kTfLiteEvalTensorData);
+          tflite_micro::RecordedAllocationType::kTfLiteEvalTensorData);
 
   // Node and tensor arrays must be allocated as well as each node and tensor.
   TF_LITE_MICRO_EXPECT_EQ(recorded_allocation.count, tensors_count * 2);
@@ -158,11 +158,11 @@ TF_LITE_MICRO_TEST(TestRecordsMultiTenantAllocations) {
 }
 
 TF_LITE_MICRO_TEST(TestRecordsPersistentTfLiteTensorData) {
-  const tflite::Model* model = tflite::GetModel(kTestConvModelData);
+  const tflite_micro::Model* model = tflite_micro::GetModel(kTestConvModelData);
   uint8_t arena[kTestConvArenaSize];
 
-  tflite::RecordingMicroAllocator* micro_allocator =
-      tflite::RecordingMicroAllocator::Create(arena, kTestConvArenaSize);
+  tflite_micro::RecordingMicroAllocator* micro_allocator =
+      tflite_micro::RecordingMicroAllocator::Create(arena, kTestConvArenaSize);
   TF_LITE_MICRO_EXPECT(micro_allocator != nullptr);
   if (micro_allocator == nullptr) return 1;
 
@@ -171,9 +171,9 @@ TF_LITE_MICRO_TEST(TestRecordsPersistentTfLiteTensorData) {
   TF_LITE_MICRO_EXPECT(tensor != nullptr);
   if (tensor == nullptr) return 1;
 
-  tflite::RecordedAllocation recorded_allocation =
+  tflite_micro::RecordedAllocation recorded_allocation =
       micro_allocator->GetRecordedAllocation(
-          tflite::RecordedAllocationType::kPersistentTfLiteTensorData);
+          tflite_micro::RecordedAllocationType::kPersistentTfLiteTensorData);
 
   TF_LITE_MICRO_EXPECT_EQ(recorded_allocation.count, static_cast<size_t>(1));
   TF_LITE_MICRO_EXPECT_EQ(recorded_allocation.requested_bytes,
@@ -183,11 +183,11 @@ TF_LITE_MICRO_TEST(TestRecordsPersistentTfLiteTensorData) {
 }
 
 TF_LITE_MICRO_TEST(TestRecordsPersistentTfLiteTensorQuantizationData) {
-  const tflite::Model* model = tflite::GetModel(kTestConvModelData);
+  const tflite_micro::Model* model = tflite_micro::GetModel(kTestConvModelData);
   uint8_t arena[kTestConvArenaSize];
 
-  tflite::RecordingMicroAllocator* micro_allocator =
-      tflite::RecordingMicroAllocator::Create(arena, kTestConvArenaSize);
+  tflite_micro::RecordingMicroAllocator* micro_allocator =
+      tflite_micro::RecordingMicroAllocator::Create(arena, kTestConvArenaSize);
   TF_LITE_MICRO_EXPECT(micro_allocator != nullptr);
   if (micro_allocator == nullptr) return 1;
 
@@ -199,9 +199,9 @@ TF_LITE_MICRO_TEST(TestRecordsPersistentTfLiteTensorQuantizationData) {
   // Walk the model subgraph to find all tensors with quantization params and
   // keep a tally.
   size_t quantized_channel_bytes = 0;
-  const tflite::Tensor* cur_tensor =
+  const tflite_micro::Tensor* cur_tensor =
       model->subgraphs()->Get(0)->tensors()->Get(0);
-  const tflite::QuantizationParameters* quantization_params =
+  const tflite_micro::QuantizationParameters* quantization_params =
       cur_tensor->quantization();
   if (quantization_params && quantization_params->scale() &&
       quantization_params->scale()->size() > 0 &&
@@ -215,9 +215,9 @@ TF_LITE_MICRO_TEST(TestRecordsPersistentTfLiteTensorQuantizationData) {
   size_t expected_requested_bytes =
       TF_LITE_AFFINE_QUANTIZATION_SIZE + quantized_channel_bytes;
 
-  tflite::RecordedAllocation recorded_allocation =
+  tflite_micro::RecordedAllocation recorded_allocation =
       micro_allocator->GetRecordedAllocation(
-          tflite::RecordedAllocationType::
+          tflite_micro::RecordedAllocationType::
               kPersistentTfLiteTensorQuantizationData);
 
   // Each quantized tensors has 2 mallocs (quant struct, zero point dimensions):
@@ -231,8 +231,8 @@ TF_LITE_MICRO_TEST(TestRecordsPersistentTfLiteTensorQuantizationData) {
 TF_LITE_MICRO_TEST(TestRecordsPersistentBufferData) {
   uint8_t arena[kTestConvArenaSize];
 
-  tflite::RecordingMicroAllocator* micro_allocator =
-      tflite::RecordingMicroAllocator::Create(arena, kTestConvArenaSize);
+  tflite_micro::RecordingMicroAllocator* micro_allocator =
+      tflite_micro::RecordingMicroAllocator::Create(arena, kTestConvArenaSize);
   TF_LITE_MICRO_EXPECT(micro_allocator != nullptr);
   if (micro_allocator == nullptr) return 1;
 
@@ -240,9 +240,9 @@ TF_LITE_MICRO_TEST(TestRecordsPersistentBufferData) {
   TF_LITE_MICRO_EXPECT(buffer != nullptr);
   if (buffer == nullptr) return 1;
 
-  tflite::RecordedAllocation recorded_allocation =
+  tflite_micro::RecordedAllocation recorded_allocation =
       micro_allocator->GetRecordedAllocation(
-          tflite::RecordedAllocationType::kPersistentBufferData);
+          tflite_micro::RecordedAllocationType::kPersistentBufferData);
 
   TF_LITE_MICRO_EXPECT_EQ(recorded_allocation.count, static_cast<size_t>(1));
   TF_LITE_MICRO_EXPECT_EQ(recorded_allocation.requested_bytes,
@@ -255,7 +255,7 @@ TF_LITE_MICRO_TEST(TestRecordsPersistentBufferData) {
   if (buffer == nullptr) return 1;
 
   recorded_allocation = micro_allocator->GetRecordedAllocation(
-      tflite::RecordedAllocationType::kPersistentBufferData);
+      tflite_micro::RecordedAllocationType::kPersistentBufferData);
 
   TF_LITE_MICRO_EXPECT_EQ(recorded_allocation.count, static_cast<size_t>(2));
   TF_LITE_MICRO_EXPECT_EQ(recorded_allocation.requested_bytes,
@@ -265,20 +265,20 @@ TF_LITE_MICRO_TEST(TestRecordsPersistentBufferData) {
 }
 
 TF_LITE_MICRO_TEST(TestMultiSubgraphModel) {
-  tflite::ScratchBufferHandle* scratch_buffer_handles = nullptr;
-  tflite::testing::TestingOpResolver ops_resolver;
-  const tflite::Model* model =
-      tflite::testing::GetSimpleModelWithNullInputsAndOutputs();
+  tflite_micro::ScratchBufferHandle* scratch_buffer_handles = nullptr;
+  tflite_micro::testing::TestingOpResolver ops_resolver;
+  const tflite_micro::Model* model =
+      tflite_micro::testing::GetSimpleModelWithNullInputsAndOutputs();
   const int arena_size = 2048;
 
   uint8_t arena[arena_size];
 
-  tflite::RecordingMicroAllocator* micro_allocator =
-      tflite::RecordingMicroAllocator::Create(arena, arena_size);
+  tflite_micro::RecordingMicroAllocator* micro_allocator =
+      tflite_micro::RecordingMicroAllocator::Create(arena, arena_size);
   TF_LITE_MICRO_EXPECT(micro_allocator != nullptr);
   if (micro_allocator == nullptr) return 1;
 
-  tflite::SubgraphAllocations* subgraph_allocations =
+  tflite_micro::SubgraphAllocations* subgraph_allocations =
       micro_allocator->StartModelAllocation(model);
   TF_LITE_MICRO_EXPECT(nullptr != subgraph_allocations);
   if (subgraph_allocations == nullptr) return 1;
@@ -292,14 +292,14 @@ TF_LITE_MICRO_TEST(TestMultiSubgraphModel) {
   size_t num_tensors = 0;
   for (size_t subgraph_idx = 0; subgraph_idx < model->subgraphs()->size();
        subgraph_idx++) {
-    const tflite::SubGraph* subgraph = model->subgraphs()->Get(subgraph_idx);
+    const tflite_micro::SubGraph* subgraph = model->subgraphs()->Get(subgraph_idx);
     num_ops += subgraph->operators()->size();
     num_tensors += subgraph->tensors()->size();
   }
 
-  tflite::RecordedAllocation recorded_allocation =
+  tflite_micro::RecordedAllocation recorded_allocation =
       micro_allocator->GetRecordedAllocation(
-          tflite::RecordedAllocationType::kNodeAndRegistrationArray);
+          tflite_micro::RecordedAllocationType::kNodeAndRegistrationArray);
 
   TF_LITE_MICRO_EXPECT_EQ(recorded_allocation.count, num_ops);
   TF_LITE_MICRO_EXPECT_EQ(recorded_allocation.requested_bytes,
@@ -308,7 +308,7 @@ TF_LITE_MICRO_TEST(TestMultiSubgraphModel) {
                           num_ops * NODE_AND_REGISTRATION_STRUCT_SIZE);
 
   recorded_allocation = micro_allocator->GetRecordedAllocation(
-      tflite::RecordedAllocationType::kTfLiteEvalTensorData);
+      tflite_micro::RecordedAllocationType::kTfLiteEvalTensorData);
 
   TF_LITE_MICRO_EXPECT_EQ(recorded_allocation.count, num_tensors);
   TF_LITE_MICRO_EXPECT_EQ(recorded_allocation.requested_bytes,
diff --git a/tensorflow/lite/micro/recording_micro_interpreter.h b/tensorflow/lite/micro/recording_micro_interpreter.h
index 24987c2e..f53d0335 100644
--- a/tensorflow/lite/micro/recording_micro_interpreter.h
+++ b/tensorflow/lite/micro/recording_micro_interpreter.h
@@ -20,7 +20,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_profiler_interface.h"
 #include "tensorflow/lite/micro/recording_micro_allocator.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // Utility subclass that enables internal recordings of the MicroInterpreter.
 // This class should be used to audit and analyze memory arena usage for a given
@@ -64,6 +64,6 @@ class RecordingMicroInterpreter : public MicroInterpreter {
   const RecordingMicroAllocator& recording_micro_allocator_;
 };
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_RECORDING_MICRO_INTERPRETER_H_
diff --git a/tensorflow/lite/micro/system_setup.cc b/tensorflow/lite/micro/system_setup.cc
index db4a1007..d12507b6 100644
--- a/tensorflow/lite/micro/system_setup.cc
+++ b/tensorflow/lite/micro/system_setup.cc
@@ -15,11 +15,11 @@ limitations under the License.
 
 #include "tensorflow/lite/micro/system_setup.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // To add an equivalent function for your own platform, create your own
 // implementation file, and place it in a subfolder named after the target. See
 // tensorflow/lite/micro/debug_log.cc for a similar example.
 void InitializeTarget() {}
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/system_setup.h b/tensorflow/lite/micro/system_setup.h
index 71ab13a8..f01d27e9 100644
--- a/tensorflow/lite/micro/system_setup.h
+++ b/tensorflow/lite/micro/system_setup.h
@@ -15,13 +15,13 @@ limitations under the License.
 #ifndef TENSORFLOW_LITE_MICRO_SYSTEM_SETUP_H_
 #define TENSORFLOW_LITE_MICRO_SYSTEM_SETUP_H_
 
-namespace tflite {
+namespace tflite_micro {
 
 // This should called during initialization of TFLM binaries and tests. It can
 // be specialized if there is a need for custom target-specific intialization.
 // For more information, see tensorflow/lite/micro/system_setup.cc.
 void InitializeTarget();
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_SYSTEM_SETUP_H_
diff --git a/tensorflow/lite/micro/test_helper_custom_ops.cc b/tensorflow/lite/micro/test_helper_custom_ops.cc
index 374aabcc..5eabc26e 100644
--- a/tensorflow/lite/micro/test_helper_custom_ops.cc
+++ b/tensorflow/lite/micro/test_helper_custom_ops.cc
@@ -32,7 +32,7 @@ limitations under the License.
 
 // TODO(b/170464050): Use TFLM test only version of schema_utils.
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 
 const TFLMRegistration* PackerOp::getRegistration() {
@@ -63,20 +63,20 @@ TfLiteStatus PackerOp::Prepare(TfLiteContext* context, TfLiteNode* node) {
 
 TfLiteStatus PackerOp::Invoke(TfLiteContext* context, TfLiteNode* node) {
   const TfLiteEvalTensor* input1 =
-      tflite::micro::GetEvalInput(context, node, 0);
+      tflite_micro::micro::GetEvalInput(context, node, 0);
   TF_LITE_ENSURE(context, input1 != nullptr);
   const int32_t* input1_data = input1->data.i32;
   TF_LITE_ENSURE_EQ(context, input1->dims->size, 1);
   const int32_t input1_len = input1->dims->data[0];
 
   const TfLiteEvalTensor* input2 =
-      tflite::micro::GetEvalInput(context, node, 1);
+      tflite_micro::micro::GetEvalInput(context, node, 1);
   TF_LITE_ENSURE(context, input2 != nullptr);
   const int32_t* input2_data = input2->data.i32;
   TF_LITE_ENSURE_EQ(context, input2->dims->size, 1);
   const int32_t input2_len = input2->dims->data[0];
 
-  TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, 0);
+  TfLiteEvalTensor* output = tflite_micro::micro::GetEvalOutput(context, node, 0);
   TF_LITE_ENSURE(context, output != nullptr);
   int32_t* output_data = output->data.i32;
   int32_t output_len = output->dims->data[0];
@@ -108,4 +108,4 @@ TfLiteStatus PackerOp::Invoke(TfLiteContext* context, TfLiteNode* node) {
 bool PackerOp::freed_ = false;
 
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/test_helper_custom_ops.h b/tensorflow/lite/micro/test_helper_custom_ops.h
index d28bb403..aab4aca1 100644
--- a/tensorflow/lite/micro/test_helper_custom_ops.h
+++ b/tensorflow/lite/micro/test_helper_custom_ops.h
@@ -27,7 +27,7 @@ limitations under the License.
 #include "tensorflow/lite/portable_type_to_tflitetype.h"
 #include "tensorflow/lite/schema/schema_generated.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 
 class PackerOp {
@@ -44,6 +44,6 @@ class PackerOp {
 };
 
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_TEST_HELPER_CUSTOM_OPS_H_
diff --git a/tensorflow/lite/micro/test_helpers.cc b/tensorflow/lite/micro/test_helpers.cc
index 3f0f5ec0..8fffd9ac 100644
--- a/tensorflow/lite/micro/test_helpers.cc
+++ b/tensorflow/lite/micro/test_helpers.cc
@@ -35,7 +35,7 @@ limitations under the License.
 
 // TODO(b/170464050): Use TFLM test only version of schema_utils.
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 namespace {
 
@@ -131,15 +131,15 @@ class ModelBuilder {
   flatbuffers::FlatBufferBuilder* builder_;
 
   static constexpr int kMaxOperatorCodes = 10;
-  flatbuffers::Offset<tflite::OperatorCode> operator_codes_[kMaxOperatorCodes];
+  flatbuffers::Offset<tflite_micro::OperatorCode> operator_codes_[kMaxOperatorCodes];
   int next_operator_code_id_ = 0;
 
   static constexpr int kMaxOperators = 50;
-  flatbuffers::Offset<tflite::Operator> operators_[kMaxOperators];
+  flatbuffers::Offset<tflite_micro::Operator> operators_[kMaxOperators];
   int next_operator_id_ = 0;
 
   static constexpr int kMaxTensors = 50;
-  flatbuffers::Offset<tflite::Tensor> tensors_[kMaxTensors];
+  flatbuffers::Offset<tflite_micro::Tensor> tensors_[kMaxTensors];
 
   static constexpr int kMaxMetadataBuffers = 10;
 
@@ -156,7 +156,7 @@ class ModelBuilder {
 ModelBuilder::Operator ModelBuilder::RegisterOp(BuiltinOperator op,
                                                 const char* custom_code) {
   TFLITE_DCHECK(next_operator_code_id_ <= kMaxOperatorCodes);
-  operator_codes_[next_operator_code_id_] = tflite::CreateOperatorCodeDirect(
+  operator_codes_[next_operator_code_id_] = tflite_micro::CreateOperatorCodeDirect(
       *builder_, /*deprecated_builtin_code=*/0, custom_code, /*version=*/0, op);
   next_operator_code_id_++;
   return next_operator_code_id_ - 1;
@@ -168,12 +168,12 @@ ModelBuilder::Node ModelBuilder::AddNode(
     std::initializer_list<ModelBuilder::Tensor> outputs,
     std::initializer_list<ModelBuilder::Tensor> intermediates) {
   TFLITE_DCHECK(next_operator_id_ <= kMaxOperators);
-  operators_[next_operator_id_] = tflite::CreateOperator(
+  operators_[next_operator_id_] = tflite_micro::CreateOperator(
       *builder_, op, builder_->CreateVector(inputs.begin(), inputs.size()),
       builder_->CreateVector(outputs.begin(), outputs.size()),
       BuiltinOptions_NONE,
       /*builtin_options=*/0,
-      /*custom_options=*/0, tflite::CustomOptionsFormat_FLEXBUFFERS,
+      /*custom_options=*/0, tflite_micro::CustomOptionsFormat_FLEXBUFFERS,
       /*mutating_variable_inputs =*/0,
       builder_->CreateVector(intermediates.begin(), intermediates.size()));
   next_operator_id_++;
@@ -187,7 +187,7 @@ void ModelBuilder::AddMetadata(const char* description_string,
       CreateMetadata(*builder_, builder_->CreateString(description_string),
                      1 + ModelBuilder::nbr_of_metadata_buffers_);
 
-  metadata_buffers_[nbr_of_metadata_buffers_] = tflite::CreateBuffer(
+  metadata_buffers_[nbr_of_metadata_buffers_] = tflite_micro::CreateBuffer(
       *builder_, builder_->CreateVector((uint8_t*)metadata_buffer_data,
                                         sizeof(uint32_t) * num_elements));
 
@@ -201,7 +201,7 @@ const Model* ModelBuilder::BuildModel(
   // Model schema requires an empty buffer at idx 0.
   size_t buffer_size = 1 + ModelBuilder::nbr_of_metadata_buffers_;
   flatbuffers::Offset<Buffer> buffers[kMaxMetadataBuffers];
-  buffers[0] = tflite::CreateBuffer(*builder_);
+  buffers[0] = tflite_micro::CreateBuffer(*builder_);
 
   // Place the metadata buffers first in the buffer since the indices for them
   // have already been set in AddMetadata()
@@ -223,7 +223,7 @@ const Model* ModelBuilder::BuildModel(
   }
 
   const flatbuffers::Offset<SubGraph> subgraphs[subgraphs_size] = {
-      tflite::CreateSubGraph(
+      tflite_micro::CreateSubGraph(
           *builder_, builder_->CreateVector(tensors_, next_tensor_id_),
           builder_->CreateVector(inputs.begin(), num_subgraph_inputs),
           builder_->CreateVector(outputs.begin(), outputs.size()),
@@ -232,7 +232,7 @@ const Model* ModelBuilder::BuildModel(
 
   flatbuffers::Offset<Model> model_offset;
   if (ModelBuilder::nbr_of_metadata_buffers_ > 0) {
-    model_offset = tflite::CreateModel(
+    model_offset = tflite_micro::CreateModel(
         *builder_, 0,
         builder_->CreateVector(operator_codes_, next_operator_code_id_),
         builder_->CreateVector(subgraphs, subgraphs_size),
@@ -241,7 +241,7 @@ const Model* ModelBuilder::BuildModel(
         builder_->CreateVector(metadata_,
                                ModelBuilder::nbr_of_metadata_buffers_));
   } else {
-    model_offset = tflite::CreateModel(
+    model_offset = tflite_micro::CreateModel(
         *builder_, 0,
         builder_->CreateVector(operator_codes_, next_operator_code_id_),
         builder_->CreateVector(subgraphs, subgraphs_size),
@@ -249,7 +249,7 @@ const Model* ModelBuilder::BuildModel(
         builder_->CreateVector(buffers, buffer_size));
   }
 
-  tflite::FinishModelBuffer(*builder_, model_offset);
+  tflite_micro::FinishModelBuffer(*builder_, model_offset);
   void* model_pointer = builder_->GetBufferPointer();
   const Model* model = flatbuffers::GetRoot<Model>(model_pointer);
   return model;
@@ -258,7 +258,7 @@ const Model* ModelBuilder::BuildModel(
 ModelBuilder::Tensor ModelBuilder::AddTensorImpl(
     TensorType type, bool is_variable, std::initializer_list<int32_t> shape) {
   TFLITE_DCHECK(next_tensor_id_ <= kMaxTensors);
-  tensors_[next_tensor_id_] = tflite::CreateTensor(
+  tensors_[next_tensor_id_] = tflite_micro::CreateTensor(
       *builder_, builder_->CreateVector(shape.begin(), shape.size()), type,
       /* buffer */ 0, /* name */ 0, /* quantization */ 0,
       /* is_variable */ is_variable,
@@ -352,7 +352,7 @@ const Model* BuildModelWithOfflinePlanning(int number_of_tensors,
 
   model_builder.AddMetadata(
       "OfflineMemoryAllocation", metadata_buffer,
-      number_of_tensors + tflite::testing::kOfflinePlannerHeaderSize);
+      number_of_tensors + tflite_micro::testing::kOfflinePlannerHeaderSize);
 
   return model_builder.BuildModel(
       node_conn[0].input, node_conn[num_conns - 1].output, num_subgraph_inputs);
@@ -442,11 +442,19 @@ const Model* BuildModelWithUnusedOperatorOutputs() {
           *builder, builder->CreateVector(tensor_shape, tensor_shape_size),
           TensorType_INT8, 0,
           builder->CreateString("test_unused_output_tensor"), 0, false)};
+#ifdef _MSC_VER
+  constexpr size_t inputs_size = 1;
+#else
   constexpr size_t inputs_size = 0;
+#endif
   const int32_t inputs[inputs_size] = {};
   constexpr size_t outputs_size = 1;
   const int32_t outputs[outputs_size] = {0};
-  constexpr size_t operator_inputs_size = 0;
+#ifdef _MSC_VER
+  constexpr size_t operator_inputs_size = 1;
+#else
+   constexpr size_t operator_inputs_size = 0;
+#endif
   const int32_t operator_inputs[operator_inputs_size] = {};
   constexpr size_t operator_outputs_size = 2;
   const int32_t operator_outputs[operator_outputs_size] = {0, 1};
@@ -1216,7 +1224,7 @@ const Model* BuildModelWithIfAndSubgraphInputTensorOverlap() {
 
   constexpr TensorType kTensorType = TensorType_INT32;
   constexpr int kBlockSize =
-      tflite::MicroArenaBufferAlignment() / sizeof(int32_t);
+      tflite_micro::MicroArenaBufferAlignment() / sizeof(int32_t);
   constexpr size_t kBuffersCount = 1;
   const Offset<Buffer> buffers[kBuffersCount] = {
       CreateBuffer(*builder),
@@ -1482,7 +1490,7 @@ TfLiteStatus SimpleStatefulOp::Invoke(TfLiteContext* context,
   *data->invoke_count += 1;
 
   const TfLiteEvalTensor* input =
-      tflite::micro::GetEvalInput(context, node, kInputTensor);
+      tflite_micro::micro::GetEvalInput(context, node, kInputTensor);
   TF_LITE_ENSURE(context, input != nullptr);
   const uint8_t* input_data = input->data.uint8;
   int size = NumElements(input->dims);
@@ -1503,11 +1511,11 @@ TfLiteStatus SimpleStatefulOp::Invoke(TfLiteContext* context,
   }
 
   TfLiteEvalTensor* median =
-      tflite::micro::GetEvalOutput(context, node, kMedianTensor);
+      tflite_micro::micro::GetEvalOutput(context, node, kMedianTensor);
   TF_LITE_ENSURE(context, median != nullptr);
   uint8_t* median_data = median->data.uint8;
   TfLiteEvalTensor* invoke_count =
-      tflite::micro::GetEvalOutput(context, node, kInvokeCount);
+      tflite_micro::micro::GetEvalOutput(context, node, kInvokeCount);
   TF_LITE_ENSURE(context, invoke_count != nullptr);
   int32_t* invoke_count_data = invoke_count->data.i32;
 
@@ -1546,14 +1554,14 @@ TfLiteStatus MockCustom::Prepare(TfLiteContext* context, TfLiteNode* node) {
 }
 
 TfLiteStatus MockCustom::Invoke(TfLiteContext* context, TfLiteNode* node) {
-  const TfLiteEvalTensor* input = tflite::micro::GetEvalInput(context, node, 0);
+  const TfLiteEvalTensor* input = tflite_micro::micro::GetEvalInput(context, node, 0);
   TF_LITE_ENSURE(context, input != nullptr);
   const int32_t* input_data = input->data.i32;
   const TfLiteEvalTensor* weight =
-      tflite::micro::GetEvalInput(context, node, 1);
+      tflite_micro::micro::GetEvalInput(context, node, 1);
   TF_LITE_ENSURE(context, weight != nullptr);
   const uint8_t* weight_data = weight->data.uint8;
-  TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, 0);
+  TfLiteEvalTensor* output = tflite_micro::micro::GetEvalOutput(context, node, 0);
   TF_LITE_ENSURE(context, output != nullptr);
   int32_t* output_data = output->data.i32;
   output_data[0] =
@@ -1596,19 +1604,19 @@ TfLiteStatus MultipleInputs::Prepare(TfLiteContext* context, TfLiteNode* node) {
 }
 
 TfLiteStatus MultipleInputs::Invoke(TfLiteContext* context, TfLiteNode* node) {
-  const TfLiteEvalTensor* input = tflite::micro::GetEvalInput(context, node, 0);
+  const TfLiteEvalTensor* input = tflite_micro::micro::GetEvalInput(context, node, 0);
   TF_LITE_ENSURE(context, input != nullptr);
   const int32_t* input_data = input->data.i32;
   const TfLiteEvalTensor* input1 =
-      tflite::micro::GetEvalInput(context, node, 1);
+      tflite_micro::micro::GetEvalInput(context, node, 1);
   TF_LITE_ENSURE(context, input1 != nullptr);
   const int32_t* input_data1 = input1->data.i32;
   const TfLiteEvalTensor* input2 =
-      tflite::micro::GetEvalInput(context, node, 2);
+      tflite_micro::micro::GetEvalInput(context, node, 2);
   TF_LITE_ENSURE(context, input2 != nullptr);
   const int32_t* input_data2 = input2->data.i32;
 
-  TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, 0);
+  TfLiteEvalTensor* output = tflite_micro::micro::GetEvalOutput(context, node, 0);
   TF_LITE_ENSURE(context, output != nullptr);
   int32_t* output_data = output->data.i32;
   output_data[0] =
@@ -1654,7 +1662,7 @@ TfLiteStatus NoOp::Invoke(TfLiteContext* context, TfLiteNode* node) {
 bool NoOp::freed_ = false;
 
 TfLiteStatus GetTestingOpResolver(
-    tflite::testing::TestingOpResolver& op_resolver) {
+    tflite_micro::testing::TestingOpResolver& op_resolver) {
   TF_LITE_ENSURE_STATUS(op_resolver.AddCustom(
       "mock_custom", MockCustom::GetMutableRegistration()));
   TF_LITE_ENSURE_STATUS(op_resolver.AddCustom(
@@ -1894,7 +1902,7 @@ TfLiteTensor CreateQuantizedBiasTensor(const float* data, int16_t* quantized,
                                        TfLiteIntArray* dims, float input_scale,
                                        float weights_scale, bool is_variable) {
   float bias_scale = input_scale * weights_scale;
-  tflite::SymmetricQuantize(data, quantized, ElementCount(*dims), bias_scale);
+  tflite_micro::SymmetricQuantize(data, quantized, ElementCount(*dims), bias_scale);
 
   // Quantized int16_t tensors always have a zero point of 0, since the range of
   // int16_t values is large, and because zero point costs extra cycles during
@@ -1908,7 +1916,7 @@ TfLiteTensor CreateQuantizedBiasTensor(const float* data, int32_t* quantized,
                                        TfLiteIntArray* dims, float input_scale,
                                        float weights_scale, bool is_variable) {
   float bias_scale = input_scale * weights_scale;
-  tflite::SymmetricQuantize(data, quantized, ElementCount(*dims), bias_scale);
+  tflite_micro::SymmetricQuantize(data, quantized, ElementCount(*dims), bias_scale);
 
   // Quantized int32_t tensors always have a zero point of 0, since the range of
   // int32_t values is large, and because zero point costs extra cycles during
@@ -1923,7 +1931,7 @@ TfLiteTensor CreateQuantizedBiasTensor(const float* data,
                                        TfLiteIntArray* dims, float input_scale,
                                        float weights_scale, bool is_variable) {
   float bias_scale = input_scale * weights_scale;
-  tflite::SymmetricQuantize(data, quantized, ElementCount(*dims), bias_scale);
+  tflite_micro::SymmetricQuantize(data, quantized, ElementCount(*dims), bias_scale);
 
   // Quantized int32_t tensors always have a zero point of 0, since the range of
   // int32_t values is large, and because zero point costs extra cycles during
@@ -2031,4 +2039,4 @@ void PackInt4ValuesDenselyInPlace(uint8_t* src_buffer, int buffer_size) {
 }
 
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/test_helpers.h b/tensorflow/lite/micro/test_helpers.h
index 2a6204fe..58888614 100644
--- a/tensorflow/lite/micro/test_helpers.h
+++ b/tensorflow/lite/micro/test_helpers.h
@@ -30,11 +30,11 @@ limitations under the License.
 #include "tensorflow/lite/portable_type_to_tflitetype.h"
 #include "tensorflow/lite/schema/schema_generated.h"
 
-namespace tflite {
+namespace tflite_micro {
 namespace testing {
 
 constexpr int kOfflinePlannerHeaderSize = 3;
-using TestingOpResolver = tflite::MicroMutableOpResolver<10>;
+using TestingOpResolver = tflite_micro::MicroMutableOpResolver<10>;
 
 struct NodeConnection_ {
   std::initializer_list<int32_t> input;
@@ -224,7 +224,7 @@ TfLiteTensor CreateTensor(const T* data, TfLiteIntArray* dims,
 
   if (type == kTfLiteInt4) {
     result.type = kTfLiteInt4;
-    PackInt4ValuesDenselyInPlace(tflite::GetTensorData<uint8_t>(&result),
+    PackInt4ValuesDenselyInPlace(tflite_micro::GetTensorData<uint8_t>(&result),
                                  ElementCount(*dims));
     result.bytes = ((ElementCount(*dims) + 1) / 2);
   } else {
@@ -254,7 +254,7 @@ TfLiteTensor CreateQuantizedTensor(const float* input, T* quantized,
                                    int zero_point, bool is_variable = false,
                                    TfLiteType type = kTfLiteNoType) {
   int input_size = ElementCount(*dims);
-  tflite::Quantize(input, quantized, input_size, scale, zero_point);
+  tflite_micro::Quantize(input, quantized, input_size, scale, zero_point);
   return CreateQuantizedTensor(quantized, dims, scale, zero_point, is_variable,
                                type);
 }
@@ -297,7 +297,7 @@ TfLiteTensor CreateSymmetricPerChannelQuantizedTensor(
     int quantized_dimension, bool is_variable = false,
     TfLiteType tensor_weight_type = kTfLiteNoType);
 
-// Returns the number of tensors in the default subgraph for a tflite::Model.
+// Returns the number of tensors in the default subgraph for a tflite_micro::Model.
 size_t GetModelTensorCount(const Model* model);
 
 // Derives the asymmetric quantization scaling factor from a min and max range.
@@ -329,6 +329,6 @@ inline int ZeroPointFromMinMax(const float min, const float max) {
 }
 
 }  // namespace testing
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_TEST_HELPERS_H_
diff --git a/tensorflow/lite/micro/testing/micro_test.h b/tensorflow/lite/micro/testing/micro_test.h
index a28f4b6d..a5595fe5 100644
--- a/tensorflow/lite/micro/testing/micro_test.h
+++ b/tensorflow/lite/micro/testing/micro_test.h
@@ -65,10 +65,10 @@ extern bool is_test_complete;
 extern bool did_test_fail;
 }  // namespace micro_test
 
-namespace tflite {
+namespace tflite_micro {
 
 // This additional helper function is used (instead of directly calling
-// tflite::InitializeTarget from the TF_LITE_MICRO_TESTS_BEGIN macro) to avoid
+// tflite_micro::InitializeTarget from the TF_LITE_MICRO_TESTS_BEGIN macro) to avoid
 // adding a dependency from every bazel test target to micro:system_setp (which
 // is the target that implements InitializeTarget().
 //
@@ -76,7 +76,7 @@ namespace tflite {
 // dependencies that can be containted within the micro/testing:micro_test
 // target bleeding on to all the tests.
 inline void InitializeTest() { InitializeTarget(); }
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #define TF_LITE_MICRO_TESTS_BEGIN   \
   namespace micro_test {            \
@@ -89,7 +89,7 @@ inline void InitializeTest() { InitializeTarget(); }
   int main(int argc, char** argv) { \
     micro_test::tests_passed = 0;   \
     micro_test::tests_failed = 0;   \
-    tflite::InitializeTest();
+    tflite_micro::InitializeTest();
 
 #define TF_LITE_MICRO_TESTS_END                                       \
   MicroPrintf("%d/%d tests passed", micro_test::tests_passed,         \
diff --git a/tensorflow/lite/micro/testing_helpers_test.cc b/tensorflow/lite/micro/testing_helpers_test.cc
index 95b2bcd3..e60fe332 100644
--- a/tensorflow/lite/micro/testing_helpers_test.cc
+++ b/tensorflow/lite/micro/testing_helpers_test.cc
@@ -27,9 +27,9 @@ TF_LITE_MICRO_TEST(CreateQuantizedBiasTensor) {
   float pre_quantized[] = {-10, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 10};
   int32_t expected_quantized_values[] = {-40, -20, -16, -12, -8, -4,
                                          0,   4,   8,   12,  16, 40};
-  TfLiteIntArray* dims = tflite::testing::IntArrayFromInts(dims_arr);
+  TfLiteIntArray* dims = tflite_micro::testing::IntArrayFromInts(dims_arr);
 
-  TfLiteTensor result = tflite::testing::CreateQuantizedBiasTensor(
+  TfLiteTensor result = tflite_micro::testing::CreateQuantizedBiasTensor(
       pre_quantized, quantized, dims, input_scale, weight_scale);
 
   TF_LITE_MICRO_EXPECT_EQ(result.bytes, tensor_size * sizeof(int32_t));
@@ -46,7 +46,7 @@ TF_LITE_MICRO_TEST(PackInt4Basic) {
   const int8_t expect_output[2] = {0x37, 0x52};
   int output_size = 2;
 
-  tflite::testing::PackInt4ValuesDenselyInPlace(
+  tflite_micro::testing::PackInt4ValuesDenselyInPlace(
       reinterpret_cast<uint8_t*>(input), input_size);
   for (int i = 0; i < output_size; i++) {
     TF_LITE_MICRO_EXPECT_EQ(expect_output[i], input[i]);
@@ -59,7 +59,7 @@ TF_LITE_MICRO_TEST(PackInt4BasicOddLength) {
   int output_size = 2;
   int input_size = 3;
 
-  tflite::testing::PackInt4ValuesDenselyInPlace(
+  tflite_micro::testing::PackInt4ValuesDenselyInPlace(
       reinterpret_cast<uint8_t*>(input), input_size);
   for (int i = 0; i < output_size; i++) {
     TF_LITE_MICRO_EXPECT_EQ(expect_output[i], input[i]);
@@ -78,10 +78,10 @@ TF_LITE_MICRO_TEST(CreatePerChannelQuantizedBiasTensor) {
   float pre_quantized[] = {-10, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 10};
   int32_t expected_quantized_values[] = {-40, -20, -16, -6, -4, -2,
                                          0,   1,   2,   2,  2,  5};
-  TfLiteIntArray* dims = tflite::testing::IntArrayFromInts(dims_arr);
+  TfLiteIntArray* dims = tflite_micro::testing::IntArrayFromInts(dims_arr);
 
   TfLiteAffineQuantization quant;
-  TfLiteTensor result = tflite::testing::CreatePerChannelQuantizedBiasTensor(
+  TfLiteTensor result = tflite_micro::testing::CreatePerChannelQuantizedBiasTensor(
       pre_quantized, quantized, dims, input_scale, weight_scales, scales,
       zero_points, &quant, 0);
 
@@ -108,13 +108,13 @@ TF_LITE_MICRO_TEST(CreateSymmetricPerChannelQuantizedTensor) {
   const int8_t expected_quantized_values[] = {-127, -55, -4, -3, -2, -1,
                                               0,    2,   4,  6,  8,  127};
   float expected_scales[] = {1.0, 0.5};
-  TfLiteIntArray* dims = tflite::testing::IntArrayFromInts(dims_arr);
+  TfLiteIntArray* dims = tflite_micro::testing::IntArrayFromInts(dims_arr);
 
   int zero_points[channels + 1];
   float scales[channels + 1];
   TfLiteAffineQuantization quant;
   TfLiteTensor result =
-      tflite::testing::CreateSymmetricPerChannelQuantizedTensor(
+      tflite_micro::testing::CreateSymmetricPerChannelQuantizedTensor(
           pre_quantized, quantized, dims, scales, zero_points, &quant, 0);
 
   TF_LITE_MICRO_EXPECT_EQ(result.bytes, tensor_size * sizeof(int8_t));
diff --git a/tensorflow/lite/micro/tflite_bridge/flatbuffer_conversions_bridge.cc b/tensorflow/lite/micro/tflite_bridge/flatbuffer_conversions_bridge.cc
index 20e4ae4c..4dfe3cdc 100644
--- a/tensorflow/lite/micro/tflite_bridge/flatbuffer_conversions_bridge.cc
+++ b/tensorflow/lite/micro/tflite_bridge/flatbuffer_conversions_bridge.cc
@@ -20,15 +20,15 @@ limitations under the License.
 #include "tensorflow/lite/micro/tflite_bridge/micro_error_reporter.h"
 #include "tensorflow/lite/schema/schema_generated.h"
 
-namespace tflite {
+namespace tflite_micro {
 TfLiteStatus ConvertTensorType(TensorType tensor_type, TfLiteType* type) {
-  return ConvertTensorType(tensor_type, type, tflite::GetMicroErrorReporter());
+  return ConvertTensorType(tensor_type, type, tflite_micro::GetMicroErrorReporter());
 }
 
 TfLiteStatus CallBuiltinParseFunction(TfLiteBridgeBuiltinParseFunction parser,
                                       const Operator* op,
                                       BuiltinDataAllocator* allocator,
                                       void** builtin_data) {
-  return parser(op, tflite::GetMicroErrorReporter(), allocator, builtin_data);
+  return parser(op, tflite_micro::GetMicroErrorReporter(), allocator, builtin_data);
 }
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/tflite_bridge/flatbuffer_conversions_bridge.h b/tensorflow/lite/micro/tflite_bridge/flatbuffer_conversions_bridge.h
index 2c0f3692..6580ad16 100644
--- a/tensorflow/lite/micro/tflite_bridge/flatbuffer_conversions_bridge.h
+++ b/tensorflow/lite/micro/tflite_bridge/flatbuffer_conversions_bridge.h
@@ -19,7 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/core/api/flatbuffer_conversions.h"
 #include "tensorflow/lite/schema/schema_generated.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // Forward declaration of the ErrorReporter class to hide it from the TFLM code.
 class ErrorReporter;
@@ -40,6 +40,6 @@ TfLiteStatus CallBuiltinParseFunction(TfLiteBridgeBuiltinParseFunction parser,
                                       const Operator* op,
                                       BuiltinDataAllocator* allocator,
                                       void** builtin_data);
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_TFLITE_BRIDGE_FLATBUFFER_CONVERSIONS_BRIDGE_H_
diff --git a/tensorflow/lite/micro/tflite_bridge/micro_error_reporter.cc b/tensorflow/lite/micro/tflite_bridge/micro_error_reporter.cc
index d5d77c35..672969c5 100644
--- a/tensorflow/lite/micro/tflite_bridge/micro_error_reporter.cc
+++ b/tensorflow/lite/micro/tflite_bridge/micro_error_reporter.cc
@@ -22,12 +22,12 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_log.h"
 
 namespace {
-uint8_t micro_error_reporter_buffer[sizeof(tflite::MicroErrorReporter)];
-tflite::MicroErrorReporter* error_reporter_ = nullptr;
+uint8_t micro_error_reporter_buffer[sizeof(tflite_micro::MicroErrorReporter)];
+tflite_micro::MicroErrorReporter* error_reporter_ = nullptr;
 
 }  // namespace
 
-namespace tflite {
+namespace tflite_micro {
 ErrorReporter* GetMicroErrorReporter() {
   if (error_reporter_ == nullptr) {
     error_reporter_ = new (micro_error_reporter_buffer) MicroErrorReporter();
@@ -40,4 +40,4 @@ int MicroErrorReporter::Report(const char* format, va_list args) {
   return 0;
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/tflite_bridge/micro_error_reporter.h b/tensorflow/lite/micro/tflite_bridge/micro_error_reporter.h
index d3702f46..23793558 100644
--- a/tensorflow/lite/micro/tflite_bridge/micro_error_reporter.h
+++ b/tensorflow/lite/micro/tflite_bridge/micro_error_reporter.h
@@ -20,7 +20,7 @@ limitations under the License.
 #include "tensorflow/lite/core/api/error_reporter.h"
 #include "tensorflow/lite/micro/compatibility.h"
 
-namespace tflite {
+namespace tflite_micro {
 // Get a pointer to a singleton global error reporter.
 ErrorReporter* GetMicroErrorReporter();
 class MicroErrorReporter : public ErrorReporter {
@@ -28,10 +28,9 @@ class MicroErrorReporter : public ErrorReporter {
   ~MicroErrorReporter() override {}
   int Report(const char* format, va_list args) override;
 
- private:
   TF_LITE_REMOVE_VIRTUAL_DELETE
 };
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_MICRO_TFLITE_BRIDGE_MICRO_ERROR_REPORTER_H_
diff --git a/tensorflow/lite/micro/tools/benchmarking/generic_model_benchmark.cc b/tensorflow/lite/micro/tools/benchmarking/generic_model_benchmark.cc
index 912b84a6..90f63512 100644
--- a/tensorflow/lite/micro/tools/benchmarking/generic_model_benchmark.cc
+++ b/tensorflow/lite/micro/tools/benchmarking/generic_model_benchmark.cc
@@ -39,13 +39,13 @@ limitations under the License.
  * with random inputs.
  */
 
-namespace tflite {
+namespace tflite_micro {
 
 namespace {
 
-using Profiler = ::tflite::MicroProfiler;
+using Profiler = ::tflite_micro::MicroProfiler;
 
-using TflmOpResolver = tflite::MicroMutableOpResolver<96>;
+using TflmOpResolver = tflite_micro::MicroMutableOpResolver<96>;
 
 constexpr int kTfLiteAbort = -9;
 
@@ -61,7 +61,7 @@ constexpr int kNumResourceVariable = 100;
 constexpr size_t kModelSize = 2e6;
 
 void SetRandomInput(const uint32_t random_seed,
-                    tflite::MicroInterpreter& interpreter) {
+                    tflite_micro::MicroInterpreter& interpreter) {
   std::mt19937 eng(random_seed);
   std::uniform_int_distribution<uint32_t> dist(0, 255);
 
@@ -69,7 +69,7 @@ void SetRandomInput(const uint32_t random_seed,
     TfLiteTensor* input = interpreter.input_tensor(i);
 
     // Pre-populate input tensor with random values.
-    int8_t* input_values = tflite::GetTensorData<int8_t>(input);
+    int8_t* input_values = tflite_micro::GetTensorData<int8_t>(input);
     for (size_t j = 0; j < input->bytes; ++j) {
       input_values[j] = dist(eng);
     }
@@ -110,17 +110,17 @@ int Benchmark(const char* model_file_name) {
     return -1;
   }
   uint32_t event_handle = profiler.BeginEvent("TfliteGetModel");
-  const tflite::Model* model = tflite::GetModel(model_file_content);
+  const tflite_micro::Model* model = tflite_micro::GetModel(model_file_content);
   profiler.EndEvent(event_handle);
 
   TflmOpResolver op_resolver;
   TF_LITE_ENSURE_STATUS(CreateOpResolver(op_resolver));
 
-  tflite::RecordingMicroAllocator* allocator(
-      tflite::RecordingMicroAllocator::Create(tensor_arena, kTensorArenaSize));
-  tflite::RecordingMicroInterpreter interpreter(
+  tflite_micro::RecordingMicroAllocator* allocator(
+      tflite_micro::RecordingMicroAllocator::Create(tensor_arena, kTensorArenaSize));
+  tflite_micro::RecordingMicroInterpreter interpreter(
       model, op_resolver, allocator,
-      tflite::MicroResourceVariables::Create(allocator, kNumResourceVariable),
+      tflite_micro::MicroResourceVariables::Create(allocator, kNumResourceVariable),
       &profiler);
   TF_LITE_ENSURE_STATUS(interpreter.AllocateTensors());
 
@@ -159,6 +159,6 @@ int Benchmark(const char* model_file_name) {
   return 0;
 }
 }  // namespace
-}  // namespace tflite
+}  // namespace tflite_micro
 
-int main(int argc, char** argv) { return tflite::Benchmark(argv[1]); }
+int main(int argc, char** argv) { return tflite_micro::Benchmark(argv[1]); }
diff --git a/tensorflow/lite/micro/tools/benchmarking/log_utils.cc b/tensorflow/lite/micro/tools/benchmarking/log_utils.cc
index 808e465b..4cfd332f 100644
--- a/tensorflow/lite/micro/tools/benchmarking/log_utils.cc
+++ b/tensorflow/lite/micro/tools/benchmarking/log_utils.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include <stdio.h>
 #include <stdlib.h>
 
-namespace tflite {
+namespace tflite_micro {
 
 int GetLongestStringLength(const char strings[][kMaxStringLength],
                            const int count) {
@@ -112,4 +112,4 @@ void FormatNumber<double>(char* output, double value) {
   FormatIntegerDivide(output, static_cast<int64_t>(value * kDenominator),
                       kDenominator, 3);
 }
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/tools/benchmarking/log_utils.h b/tensorflow/lite/micro/tools/benchmarking/log_utils.h
index ee73f48f..405d007f 100644
--- a/tensorflow/lite/micro/tools/benchmarking/log_utils.h
+++ b/tensorflow/lite/micro/tools/benchmarking/log_utils.h
@@ -23,7 +23,7 @@ limitations under the License.
 
 #include "tensorflow/lite/micro/micro_log.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // The maxmimum length of a string.
 static constexpr int kMaxStringLength = 32;
@@ -268,6 +268,6 @@ void PrintFormattedData(const char headers[kColumns][kMaxStringLength],
   }
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TFLM_BENCHMARK_INTERNAL_LOG_UTILS_H_
diff --git a/tensorflow/lite/micro/tools/benchmarking/metrics.cc b/tensorflow/lite/micro/tools/benchmarking/metrics.cc
index 115d8790..1ab76576 100644
--- a/tensorflow/lite/micro/tools/benchmarking/metrics.cc
+++ b/tensorflow/lite/micro/tools/benchmarking/metrics.cc
@@ -21,10 +21,10 @@ limitations under the License.
 
 #include "tensorflow/lite/micro/tools/benchmarking/log_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 void LogArenaAllocations(
-    const tflite::RecordingSingleArenaBufferAllocator* allocator,
+    const tflite_micro::RecordingSingleArenaBufferAllocator* allocator,
     const PrettyPrintType type) {
   constexpr int kArenaRows = 3;
   constexpr int kArenaCols = 3;
@@ -49,17 +49,17 @@ void LogArenaAllocations(
                                              "Arena");
 }
 
-void LogAllocations(const tflite::RecordingMicroAllocator& allocator,
+void LogAllocations(const tflite_micro::RecordingMicroAllocator& allocator,
                     const PrettyPrintType type) {
   constexpr int kAllocationTypes = 7;
-  tflite::RecordedAllocationType types[kAllocationTypes] = {
-      tflite::RecordedAllocationType::kTfLiteEvalTensorData,
-      tflite::RecordedAllocationType::kPersistentTfLiteTensorData,
-      tflite::RecordedAllocationType::kPersistentTfLiteTensorQuantizationData,
-      tflite::RecordedAllocationType::kPersistentBufferData,
-      tflite::RecordedAllocationType::kTfLiteTensorVariableBufferData,
-      tflite::RecordedAllocationType::kNodeAndRegistrationArray,
-      tflite::RecordedAllocationType::kOpData};
+  tflite_micro::RecordedAllocationType types[kAllocationTypes] = {
+      tflite_micro::RecordedAllocationType::kTfLiteEvalTensorData,
+      tflite_micro::RecordedAllocationType::kPersistentTfLiteTensorData,
+      tflite_micro::RecordedAllocationType::kPersistentTfLiteTensorQuantizationData,
+      tflite_micro::RecordedAllocationType::kPersistentBufferData,
+      tflite_micro::RecordedAllocationType::kTfLiteTensorVariableBufferData,
+      tflite_micro::RecordedAllocationType::kNodeAndRegistrationArray,
+      tflite_micro::RecordedAllocationType::kOpData};
 
   char titles[kAllocationTypes][kMaxStringLength] = {
       "Eval tensor data",
@@ -79,7 +79,7 @@ void LogAllocations(const tflite::RecordingMicroAllocator& allocator,
 
   char data[kColumns][kAllocationTypes][kMaxStringLength];
   for (int i = 0; i < kAllocationTypes; ++i) {
-    tflite::RecordedAllocation allocation =
+    tflite_micro::RecordedAllocation allocation =
         allocator.GetRecordedAllocation(types[i]);
     MicroStrcpy(data[0][i], titles[i]);
     FormatNumber<int32_t>(data[1][i], static_cast<int>(types[i]));
@@ -94,9 +94,9 @@ void LogAllocations(const tflite::RecordingMicroAllocator& allocator,
       headers, data, kAllocationTypes, type, "Allocations");
 }
 
-void LogAllocatorEvents(const tflite::RecordingMicroAllocator& allocator,
+void LogAllocatorEvents(const tflite_micro::RecordingMicroAllocator& allocator,
                         const PrettyPrintType type) {
   LogArenaAllocations(allocator.GetSimpleMemoryAllocator(), type);
   LogAllocations(allocator, type);
 }
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/micro/tools/benchmarking/metrics.h b/tensorflow/lite/micro/tools/benchmarking/metrics.h
index 5c0c8800..103c1385 100644
--- a/tensorflow/lite/micro/tools/benchmarking/metrics.h
+++ b/tensorflow/lite/micro/tools/benchmarking/metrics.h
@@ -25,7 +25,7 @@ limitations under the License.
 #include "tensorflow/lite/micro/recording_micro_allocator.h"
 #include "tensorflow/lite/micro/tools/benchmarking/log_utils.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // Logs the allocation events. Prints out two tables, one for the arena
 // allocations, and one for each type of TFLM allocation type.
@@ -34,8 +34,8 @@ namespace tflite {
 //       process.
 //   - type: Which print format should be used to output the allocation data to
 //       stdout.
-void LogAllocatorEvents(const tflite::RecordingMicroAllocator& allocator,
+void LogAllocatorEvents(const tflite_micro::RecordingMicroAllocator& allocator,
                         PrettyPrintType type);
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TFLM_BENCHMARK_INTERNAL_METRICS_H_
diff --git a/tensorflow/lite/micro/tools/benchmarking/op_resolver.h b/tensorflow/lite/micro/tools/benchmarking/op_resolver.h
index 447741bc..4f8f6b3a 100644
--- a/tensorflow/lite/micro/tools/benchmarking/op_resolver.h
+++ b/tensorflow/lite/micro/tools/benchmarking/op_resolver.h
@@ -21,10 +21,10 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_mutable_op_resolver.h"
 #include "tensorflow/lite/micro/micro_op_resolver.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 inline TfLiteStatus CreateOpResolver(
-    tflite::MicroMutableOpResolver<96>& op_resolver) {
+    tflite_micro::MicroMutableOpResolver<96>& op_resolver) {
   TF_LITE_ENSURE_STATUS(op_resolver.AddFullyConnected());
   TF_LITE_ENSURE_STATUS(op_resolver.AddAdd());
   TF_LITE_ENSURE_STATUS(op_resolver.AddAbs());
@@ -123,5 +123,5 @@ inline TfLiteStatus CreateOpResolver(
   TF_LITE_ENSURE_STATUS(op_resolver.AddMul());
   return kTfLiteOk;
 }
-}  // namespace tflite
+}  // namespace tflite_micro
 #endif  // TFLM_BENCHMARK_OP_RESOLVER_H_
diff --git a/tensorflow/lite/micro/tools/gen_micro_mutable_op_resolver/README.md b/tensorflow/lite/micro/tools/gen_micro_mutable_op_resolver/README.md
index 1837995b..eaa11aa8 100644
--- a/tensorflow/lite/micro/tools/gen_micro_mutable_op_resolver/README.md
+++ b/tensorflow/lite/micro/tools/gen_micro_mutable_op_resolver/README.md
@@ -38,7 +38,7 @@ Note that with multiple tflite files as input, the files must be placed in the s
 The generated header file can then be included in the application and used like below:
 
 ```
-tflite::MicroMutableOpResolver<kNumberOperators> op_resolver = get_resolver();
+tflite_micro::MicroMutableOpResolver<kNumberOperators> op_resolver = get_resolver();
 ```
 
 ## Verifying the content of the generated header file
diff --git a/tensorflow/lite/micro/tools/gen_micro_mutable_op_resolver/templates/micro_mutable_op_resolver.h.mako b/tensorflow/lite/micro/tools/gen_micro_mutable_op_resolver/templates/micro_mutable_op_resolver.h.mako
index 5bbab040..e74ff3a4 100644
--- a/tensorflow/lite/micro/tools/gen_micro_mutable_op_resolver/templates/micro_mutable_op_resolver.h.mako
+++ b/tensorflow/lite/micro/tools/gen_micro_mutable_op_resolver/templates/micro_mutable_op_resolver.h.mako
@@ -21,9 +21,9 @@ limitations under the License.
 
 constexpr int kNumberOperators = ${number_of_ops};
 
-inline tflite::MicroMutableOpResolver<kNumberOperators> get_resolver()
+inline tflite_micro::MicroMutableOpResolver<kNumberOperators> get_resolver()
 {
-  tflite::MicroMutableOpResolver<kNumberOperators> micro_op_resolver;
+  tflite_micro::MicroMutableOpResolver<kNumberOperators> micro_op_resolver;
 
 % for operator in operators:
   micro_op_resolver.${operator}();
diff --git a/tensorflow/lite/micro/tools/gen_micro_mutable_op_resolver/templates/micro_mutable_op_resolver_test.cc.mako b/tensorflow/lite/micro/tools/gen_micro_mutable_op_resolver/templates/micro_mutable_op_resolver_test.cc.mako
index 8f67ff38..49f951b2 100644
--- a/tensorflow/lite/micro/tools/gen_micro_mutable_op_resolver/templates/micro_mutable_op_resolver_test.cc.mako
+++ b/tensorflow/lite/micro/tools/gen_micro_mutable_op_resolver/templates/micro_mutable_op_resolver_test.cc.mako
@@ -36,7 +36,7 @@ limitations under the License.
 constexpr size_t kTensorArenaSize = ${arena_size};
 uint8_t tensor_arena[kTensorArenaSize];
 
-namespace tflite {
+namespace tflite_micro {
 namespace micro {
 namespace {
 
@@ -48,7 +48,7 @@ void RunModel(const uint8_t* model,
               const char* name) {
   InitializeTarget();
   MicroProfiler profiler;
-  tflite::MicroMutableOpResolver<kNumberOperators> op_resolver = get_resolver();
+  tflite_micro::MicroMutableOpResolver<kNumberOperators> op_resolver = get_resolver();
 
   MicroInterpreter interpreter(GetModel(model), op_resolver, tensor_arena,
                                kTensorArenaSize,
@@ -74,7 +74,7 @@ void RunModel(const uint8_t* model,
   TfLiteTensor* output_tensor = interpreter.output(0);
   TF_LITE_MICRO_EXPECT_EQ(output_tensor->bytes,
                           golden_size * sizeof(int8_t));
-  int8_t* output = ::tflite::GetTensorData<int8_t>(output_tensor);
+  int8_t* output = ::tflite_micro::GetTensorData<int8_t>(output_tensor);
   for (uint32_t i = 0; i < golden_size; i++) {
     // TODO(b/205046520): Better understand why TfLite and TFLM can sometimes be
     // off by 1.
@@ -85,13 +85,13 @@ void RunModel(const uint8_t* model,
 
 }  // namespace
 }  // namespace micro
-}  // namespace tflite
+}  // namespace tflite_micro
 
 TF_LITE_MICRO_TESTS_BEGIN
 
 TF_LITE_MICRO_TEST(gen_micro_mutable_from_${target}_test) {
 #if VERIFY_OUTPUT
-tflite::micro::RunModel(
+tflite_micro::micro::RunModel(
 g_${target}_model_data,
 g_${target}_input0_${input_dtype}_test_data,
 g_${target}_input0_${input_dtype}_test_data_size,
@@ -99,7 +99,7 @@ g_${target}_golden_${output_dtype}_test_data,
 g_${target}_golden_${output_dtype}_test_data_size,
 "${target} test");
 #else
-tflite::micro::RunModel(
+tflite_micro::micro::RunModel(
 g_${target}_model_data,
 nullptr,
 0,
diff --git a/tensorflow/lite/micro/tools/tflite_flatbuffer_align_wrapper.cc b/tensorflow/lite/micro/tools/tflite_flatbuffer_align_wrapper.cc
index 842dcee0..b729648b 100644
--- a/tensorflow/lite/micro/tools/tflite_flatbuffer_align_wrapper.cc
+++ b/tensorflow/lite/micro/tools/tflite_flatbuffer_align_wrapper.cc
@@ -28,15 +28,15 @@ void align_tflite_model(const char* input_file_name,
   flatbuffers::LoadFile(input_file_name, false, &model_file);
   // Parse the string into a C++ class.  Model is the root object of a tflite
   // flatbuffer file.
-  const tflite::Model* model = tflite::GetModel(model_file.c_str());
+  const tflite_micro::Model* model = tflite_micro::GetModel(model_file.c_str());
   // A packed model is basically the file format mmaped into memory.
   // Unpacking it and then packing it with the C++ API should yield
   // a file with the force_align attributes respected.
   // ModelT is just the unpacked version of the model file.
-  tflite::ModelT* unpacked_model = model->UnPack();
+  tflite_micro::ModelT* unpacked_model = model->UnPack();
   flatbuffers::FlatBufferBuilder fbb;
-  auto new_model = tflite::Model::Pack(fbb, unpacked_model);
-  fbb.Finish(new_model, tflite::ModelIdentifier());
+  auto new_model = tflite_micro::Model::Pack(fbb, unpacked_model);
+  fbb.Finish(new_model, tflite_micro::ModelIdentifier());
   flatbuffers::SaveFile(output_file_name,
                         reinterpret_cast<char*>(fbb.GetBufferPointer()),
                         fbb.GetSize(), /*binary*/ true);
diff --git a/tensorflow/lite/portable_type_to_tflitetype.h b/tensorflow/lite/portable_type_to_tflitetype.h
index b6005854..d0ca67b0 100644
--- a/tensorflow/lite/portable_type_to_tflitetype.h
+++ b/tensorflow/lite/portable_type_to_tflitetype.h
@@ -29,7 +29,7 @@ limitations under the License.
 
 #include "tensorflow/lite/core/c/common.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // Map statically from a C++ type to a TfLiteType. Used in interpreter for
 // safe casts.
@@ -71,5 +71,5 @@ MATCH_TYPE_AND_TFLITE_TYPE(TfLiteFloat16, kTfLiteFloat16);
 MATCH_TYPE_AND_TFLITE_TYPE(double, kTfLiteFloat64);
 MATCH_TYPE_AND_TFLITE_TYPE(uint64_t, kTfLiteUInt64);
 
-}  // namespace tflite
+}  // namespace tflite_micro
 #endif  // TENSORFLOW_LITE_PORTABLE_TYPE_TO_TFLITETYPE_H_
diff --git a/tensorflow/lite/schema/schema_generated.h b/tensorflow/lite/schema/schema_generated.h
index e8adbfd8..40f6c14c 100755
--- a/tensorflow/lite/schema/schema_generated.h
+++ b/tensorflow/lite/schema/schema_generated.h
@@ -13,7 +13,7 @@ static_assert(FLATBUFFERS_VERSION_MAJOR == 2 &&
               FLATBUFFERS_VERSION_REVISION == 6,
              "Non-compatible flatbuffers version included");
 
-namespace tflite {
+namespace tflite_micro {
 
 struct CustomQuantization;
 struct CustomQuantizationBuilder;
@@ -779,7 +779,7 @@ template<typename T> struct QuantizationDetailsTraits {
   static const QuantizationDetails enum_value = QuantizationDetails_NONE;
 };
 
-template<> struct QuantizationDetailsTraits<tflite::CustomQuantization> {
+template<> struct QuantizationDetailsTraits<tflite_micro::CustomQuantization> {
   static const QuantizationDetails enum_value = QuantizationDetails_CustomQuantization;
 };
 
@@ -787,7 +787,7 @@ template<typename T> struct QuantizationDetailsUnionTraits {
   static const QuantizationDetails enum_value = QuantizationDetails_NONE;
 };
 
-template<> struct QuantizationDetailsUnionTraits<tflite::CustomQuantizationT> {
+template<> struct QuantizationDetailsUnionTraits<tflite_micro::CustomQuantizationT> {
   static const QuantizationDetails enum_value = QuantizationDetails_CustomQuantization;
 };
 
@@ -821,13 +821,13 @@ struct QuantizationDetailsUnion {
   static void *UnPack(const void *obj, QuantizationDetails type, const flatbuffers::resolver_function_t *resolver);
   flatbuffers::Offset<void> Pack(flatbuffers::FlatBufferBuilder &_fbb, const flatbuffers::rehasher_function_t *_rehasher = nullptr) const;
 
-  tflite::CustomQuantizationT *AsCustomQuantization() {
+  tflite_micro::CustomQuantizationT *AsCustomQuantization() {
     return type == QuantizationDetails_CustomQuantization ?
-      reinterpret_cast<tflite::CustomQuantizationT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::CustomQuantizationT *>(value) : nullptr;
   }
-  const tflite::CustomQuantizationT *AsCustomQuantization() const {
+  const tflite_micro::CustomQuantizationT *AsCustomQuantization() const {
     return type == QuantizationDetails_CustomQuantization ?
-      reinterpret_cast<const tflite::CustomQuantizationT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::CustomQuantizationT *>(value) : nullptr;
   }
 };
 
@@ -904,15 +904,15 @@ template<typename T> struct SparseIndexVectorTraits {
   static const SparseIndexVector enum_value = SparseIndexVector_NONE;
 };
 
-template<> struct SparseIndexVectorTraits<tflite::Int32Vector> {
+template<> struct SparseIndexVectorTraits<tflite_micro::Int32Vector> {
   static const SparseIndexVector enum_value = SparseIndexVector_Int32Vector;
 };
 
-template<> struct SparseIndexVectorTraits<tflite::Uint16Vector> {
+template<> struct SparseIndexVectorTraits<tflite_micro::Uint16Vector> {
   static const SparseIndexVector enum_value = SparseIndexVector_Uint16Vector;
 };
 
-template<> struct SparseIndexVectorTraits<tflite::Uint8Vector> {
+template<> struct SparseIndexVectorTraits<tflite_micro::Uint8Vector> {
   static const SparseIndexVector enum_value = SparseIndexVector_Uint8Vector;
 };
 
@@ -920,15 +920,15 @@ template<typename T> struct SparseIndexVectorUnionTraits {
   static const SparseIndexVector enum_value = SparseIndexVector_NONE;
 };
 
-template<> struct SparseIndexVectorUnionTraits<tflite::Int32VectorT> {
+template<> struct SparseIndexVectorUnionTraits<tflite_micro::Int32VectorT> {
   static const SparseIndexVector enum_value = SparseIndexVector_Int32Vector;
 };
 
-template<> struct SparseIndexVectorUnionTraits<tflite::Uint16VectorT> {
+template<> struct SparseIndexVectorUnionTraits<tflite_micro::Uint16VectorT> {
   static const SparseIndexVector enum_value = SparseIndexVector_Uint16Vector;
 };
 
-template<> struct SparseIndexVectorUnionTraits<tflite::Uint8VectorT> {
+template<> struct SparseIndexVectorUnionTraits<tflite_micro::Uint8VectorT> {
   static const SparseIndexVector enum_value = SparseIndexVector_Uint8Vector;
 };
 
@@ -962,29 +962,29 @@ struct SparseIndexVectorUnion {
   static void *UnPack(const void *obj, SparseIndexVector type, const flatbuffers::resolver_function_t *resolver);
   flatbuffers::Offset<void> Pack(flatbuffers::FlatBufferBuilder &_fbb, const flatbuffers::rehasher_function_t *_rehasher = nullptr) const;
 
-  tflite::Int32VectorT *AsInt32Vector() {
+  tflite_micro::Int32VectorT *AsInt32Vector() {
     return type == SparseIndexVector_Int32Vector ?
-      reinterpret_cast<tflite::Int32VectorT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::Int32VectorT *>(value) : nullptr;
   }
-  const tflite::Int32VectorT *AsInt32Vector() const {
+  const tflite_micro::Int32VectorT *AsInt32Vector() const {
     return type == SparseIndexVector_Int32Vector ?
-      reinterpret_cast<const tflite::Int32VectorT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::Int32VectorT *>(value) : nullptr;
   }
-  tflite::Uint16VectorT *AsUint16Vector() {
+  tflite_micro::Uint16VectorT *AsUint16Vector() {
     return type == SparseIndexVector_Uint16Vector ?
-      reinterpret_cast<tflite::Uint16VectorT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::Uint16VectorT *>(value) : nullptr;
   }
-  const tflite::Uint16VectorT *AsUint16Vector() const {
+  const tflite_micro::Uint16VectorT *AsUint16Vector() const {
     return type == SparseIndexVector_Uint16Vector ?
-      reinterpret_cast<const tflite::Uint16VectorT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::Uint16VectorT *>(value) : nullptr;
   }
-  tflite::Uint8VectorT *AsUint8Vector() {
+  tflite_micro::Uint8VectorT *AsUint8Vector() {
     return type == SparseIndexVector_Uint8Vector ?
-      reinterpret_cast<tflite::Uint8VectorT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::Uint8VectorT *>(value) : nullptr;
   }
-  const tflite::Uint8VectorT *AsUint8Vector() const {
+  const tflite_micro::Uint8VectorT *AsUint8Vector() const {
     return type == SparseIndexVector_Uint8Vector ?
-      reinterpret_cast<const tflite::Uint8VectorT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::Uint8VectorT *>(value) : nullptr;
   }
 };
 
@@ -2042,507 +2042,507 @@ template<typename T> struct BuiltinOptionsTraits {
   static const BuiltinOptions enum_value = BuiltinOptions_NONE;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::Conv2DOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::Conv2DOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_Conv2DOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::DepthwiseConv2DOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::DepthwiseConv2DOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_DepthwiseConv2DOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::ConcatEmbeddingsOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::ConcatEmbeddingsOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_ConcatEmbeddingsOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::LSHProjectionOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::LSHProjectionOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_LSHProjectionOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::Pool2DOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::Pool2DOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_Pool2DOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::SVDFOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::SVDFOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_SVDFOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::RNNOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::RNNOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_RNNOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::FullyConnectedOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::FullyConnectedOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_FullyConnectedOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::SoftmaxOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::SoftmaxOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_SoftmaxOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::ConcatenationOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::ConcatenationOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_ConcatenationOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::AddOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::AddOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_AddOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::L2NormOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::L2NormOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_L2NormOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::LocalResponseNormalizationOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::LocalResponseNormalizationOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_LocalResponseNormalizationOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::LSTMOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::LSTMOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_LSTMOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::ResizeBilinearOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::ResizeBilinearOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_ResizeBilinearOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::CallOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::CallOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_CallOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::ReshapeOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::ReshapeOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_ReshapeOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::SkipGramOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::SkipGramOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_SkipGramOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::SpaceToDepthOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::SpaceToDepthOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_SpaceToDepthOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::EmbeddingLookupSparseOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::EmbeddingLookupSparseOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_EmbeddingLookupSparseOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::MulOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::MulOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_MulOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::PadOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::PadOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_PadOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::GatherOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::GatherOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_GatherOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::BatchToSpaceNDOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::BatchToSpaceNDOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_BatchToSpaceNDOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::SpaceToBatchNDOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::SpaceToBatchNDOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_SpaceToBatchNDOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::TransposeOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::TransposeOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_TransposeOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::ReducerOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::ReducerOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_ReducerOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::SubOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::SubOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_SubOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::DivOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::DivOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_DivOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::SqueezeOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::SqueezeOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_SqueezeOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::SequenceRNNOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::SequenceRNNOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_SequenceRNNOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::StridedSliceOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::StridedSliceOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_StridedSliceOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::ExpOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::ExpOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_ExpOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::TopKV2Options> {
+template<> struct BuiltinOptionsTraits<tflite_micro::TopKV2Options> {
   static const BuiltinOptions enum_value = BuiltinOptions_TopKV2Options;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::SplitOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::SplitOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_SplitOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::LogSoftmaxOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::LogSoftmaxOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_LogSoftmaxOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::CastOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::CastOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_CastOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::DequantizeOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::DequantizeOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_DequantizeOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::MaximumMinimumOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::MaximumMinimumOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_MaximumMinimumOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::ArgMaxOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::ArgMaxOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_ArgMaxOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::LessOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::LessOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_LessOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::NegOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::NegOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_NegOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::PadV2Options> {
+template<> struct BuiltinOptionsTraits<tflite_micro::PadV2Options> {
   static const BuiltinOptions enum_value = BuiltinOptions_PadV2Options;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::GreaterOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::GreaterOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_GreaterOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::GreaterEqualOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::GreaterEqualOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_GreaterEqualOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::LessEqualOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::LessEqualOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_LessEqualOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::SelectOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::SelectOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_SelectOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::SliceOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::SliceOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_SliceOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::TransposeConvOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::TransposeConvOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_TransposeConvOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::SparseToDenseOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::SparseToDenseOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_SparseToDenseOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::TileOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::TileOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_TileOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::ExpandDimsOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::ExpandDimsOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_ExpandDimsOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::EqualOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::EqualOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_EqualOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::NotEqualOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::NotEqualOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_NotEqualOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::ShapeOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::ShapeOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_ShapeOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::PowOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::PowOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_PowOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::ArgMinOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::ArgMinOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_ArgMinOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::FakeQuantOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::FakeQuantOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_FakeQuantOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::PackOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::PackOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_PackOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::LogicalOrOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::LogicalOrOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_LogicalOrOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::OneHotOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::OneHotOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_OneHotOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::LogicalAndOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::LogicalAndOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_LogicalAndOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::LogicalNotOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::LogicalNotOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_LogicalNotOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::UnpackOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::UnpackOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_UnpackOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::FloorDivOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::FloorDivOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_FloorDivOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::SquareOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::SquareOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_SquareOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::ZerosLikeOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::ZerosLikeOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_ZerosLikeOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::FillOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::FillOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_FillOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::BidirectionalSequenceLSTMOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::BidirectionalSequenceLSTMOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_BidirectionalSequenceLSTMOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::BidirectionalSequenceRNNOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::BidirectionalSequenceRNNOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_BidirectionalSequenceRNNOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::UnidirectionalSequenceLSTMOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::UnidirectionalSequenceLSTMOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_UnidirectionalSequenceLSTMOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::FloorModOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::FloorModOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_FloorModOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::RangeOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::RangeOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_RangeOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::ResizeNearestNeighborOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::ResizeNearestNeighborOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_ResizeNearestNeighborOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::LeakyReluOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::LeakyReluOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_LeakyReluOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::SquaredDifferenceOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::SquaredDifferenceOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_SquaredDifferenceOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::MirrorPadOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::MirrorPadOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_MirrorPadOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::AbsOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::AbsOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_AbsOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::SplitVOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::SplitVOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_SplitVOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::UniqueOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::UniqueOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_UniqueOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::ReverseV2Options> {
+template<> struct BuiltinOptionsTraits<tflite_micro::ReverseV2Options> {
   static const BuiltinOptions enum_value = BuiltinOptions_ReverseV2Options;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::AddNOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::AddNOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_AddNOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::GatherNdOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::GatherNdOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_GatherNdOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::CosOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::CosOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_CosOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::WhereOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::WhereOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_WhereOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::RankOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::RankOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_RankOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::ReverseSequenceOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::ReverseSequenceOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_ReverseSequenceOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::MatrixDiagOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::MatrixDiagOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_MatrixDiagOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::QuantizeOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::QuantizeOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_QuantizeOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::MatrixSetDiagOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::MatrixSetDiagOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_MatrixSetDiagOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::HardSwishOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::HardSwishOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_HardSwishOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::IfOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::IfOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_IfOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::WhileOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::WhileOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_WhileOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::DepthToSpaceOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::DepthToSpaceOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_DepthToSpaceOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::NonMaxSuppressionV4Options> {
+template<> struct BuiltinOptionsTraits<tflite_micro::NonMaxSuppressionV4Options> {
   static const BuiltinOptions enum_value = BuiltinOptions_NonMaxSuppressionV4Options;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::NonMaxSuppressionV5Options> {
+template<> struct BuiltinOptionsTraits<tflite_micro::NonMaxSuppressionV5Options> {
   static const BuiltinOptions enum_value = BuiltinOptions_NonMaxSuppressionV5Options;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::ScatterNdOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::ScatterNdOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_ScatterNdOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::SelectV2Options> {
+template<> struct BuiltinOptionsTraits<tflite_micro::SelectV2Options> {
   static const BuiltinOptions enum_value = BuiltinOptions_SelectV2Options;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::DensifyOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::DensifyOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_DensifyOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::SegmentSumOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::SegmentSumOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_SegmentSumOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::BatchMatMulOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::BatchMatMulOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_BatchMatMulOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::CumsumOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::CumsumOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_CumsumOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::CallOnceOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::CallOnceOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_CallOnceOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::BroadcastToOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::BroadcastToOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_BroadcastToOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::Rfft2dOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::Rfft2dOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_Rfft2dOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::Conv3DOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::Conv3DOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_Conv3DOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::HashtableOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::HashtableOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_HashtableOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::HashtableFindOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::HashtableFindOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_HashtableFindOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::HashtableImportOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::HashtableImportOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_HashtableImportOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::HashtableSizeOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::HashtableSizeOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_HashtableSizeOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::VarHandleOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::VarHandleOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_VarHandleOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::ReadVariableOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::ReadVariableOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_ReadVariableOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::AssignVariableOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::AssignVariableOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_AssignVariableOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::RandomOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::RandomOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_RandomOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::BucketizeOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::BucketizeOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_BucketizeOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::GeluOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::GeluOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_GeluOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::DynamicUpdateSliceOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::DynamicUpdateSliceOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_DynamicUpdateSliceOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::UnsortedSegmentProdOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::UnsortedSegmentProdOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_UnsortedSegmentProdOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::UnsortedSegmentMaxOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::UnsortedSegmentMaxOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_UnsortedSegmentMaxOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::UnsortedSegmentMinOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::UnsortedSegmentMinOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_UnsortedSegmentMinOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::UnsortedSegmentSumOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::UnsortedSegmentSumOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_UnsortedSegmentSumOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::ATan2Options> {
+template<> struct BuiltinOptionsTraits<tflite_micro::ATan2Options> {
   static const BuiltinOptions enum_value = BuiltinOptions_ATan2Options;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::SignOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::SignOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_SignOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::BitcastOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::BitcastOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_BitcastOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::BitwiseXorOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::BitwiseXorOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_BitwiseXorOptions;
 };
 
-template<> struct BuiltinOptionsTraits<tflite::RightShiftOptions> {
+template<> struct BuiltinOptionsTraits<tflite_micro::RightShiftOptions> {
   static const BuiltinOptions enum_value = BuiltinOptions_RightShiftOptions;
 };
 
@@ -2550,507 +2550,507 @@ template<typename T> struct BuiltinOptionsUnionTraits {
   static const BuiltinOptions enum_value = BuiltinOptions_NONE;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::Conv2DOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::Conv2DOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_Conv2DOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::DepthwiseConv2DOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::DepthwiseConv2DOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_DepthwiseConv2DOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::ConcatEmbeddingsOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::ConcatEmbeddingsOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_ConcatEmbeddingsOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::LSHProjectionOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::LSHProjectionOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_LSHProjectionOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::Pool2DOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::Pool2DOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_Pool2DOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::SVDFOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::SVDFOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_SVDFOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::RNNOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::RNNOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_RNNOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::FullyConnectedOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::FullyConnectedOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_FullyConnectedOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::SoftmaxOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::SoftmaxOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_SoftmaxOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::ConcatenationOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::ConcatenationOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_ConcatenationOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::AddOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::AddOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_AddOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::L2NormOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::L2NormOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_L2NormOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::LocalResponseNormalizationOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::LocalResponseNormalizationOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_LocalResponseNormalizationOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::LSTMOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::LSTMOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_LSTMOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::ResizeBilinearOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::ResizeBilinearOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_ResizeBilinearOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::CallOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::CallOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_CallOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::ReshapeOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::ReshapeOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_ReshapeOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::SkipGramOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::SkipGramOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_SkipGramOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::SpaceToDepthOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::SpaceToDepthOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_SpaceToDepthOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::EmbeddingLookupSparseOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::EmbeddingLookupSparseOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_EmbeddingLookupSparseOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::MulOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::MulOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_MulOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::PadOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::PadOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_PadOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::GatherOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::GatherOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_GatherOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::BatchToSpaceNDOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::BatchToSpaceNDOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_BatchToSpaceNDOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::SpaceToBatchNDOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::SpaceToBatchNDOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_SpaceToBatchNDOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::TransposeOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::TransposeOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_TransposeOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::ReducerOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::ReducerOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_ReducerOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::SubOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::SubOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_SubOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::DivOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::DivOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_DivOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::SqueezeOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::SqueezeOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_SqueezeOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::SequenceRNNOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::SequenceRNNOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_SequenceRNNOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::StridedSliceOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::StridedSliceOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_StridedSliceOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::ExpOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::ExpOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_ExpOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::TopKV2OptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::TopKV2OptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_TopKV2Options;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::SplitOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::SplitOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_SplitOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::LogSoftmaxOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::LogSoftmaxOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_LogSoftmaxOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::CastOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::CastOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_CastOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::DequantizeOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::DequantizeOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_DequantizeOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::MaximumMinimumOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::MaximumMinimumOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_MaximumMinimumOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::ArgMaxOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::ArgMaxOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_ArgMaxOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::LessOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::LessOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_LessOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::NegOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::NegOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_NegOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::PadV2OptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::PadV2OptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_PadV2Options;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::GreaterOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::GreaterOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_GreaterOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::GreaterEqualOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::GreaterEqualOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_GreaterEqualOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::LessEqualOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::LessEqualOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_LessEqualOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::SelectOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::SelectOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_SelectOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::SliceOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::SliceOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_SliceOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::TransposeConvOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::TransposeConvOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_TransposeConvOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::SparseToDenseOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::SparseToDenseOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_SparseToDenseOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::TileOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::TileOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_TileOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::ExpandDimsOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::ExpandDimsOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_ExpandDimsOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::EqualOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::EqualOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_EqualOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::NotEqualOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::NotEqualOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_NotEqualOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::ShapeOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::ShapeOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_ShapeOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::PowOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::PowOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_PowOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::ArgMinOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::ArgMinOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_ArgMinOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::FakeQuantOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::FakeQuantOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_FakeQuantOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::PackOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::PackOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_PackOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::LogicalOrOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::LogicalOrOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_LogicalOrOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::OneHotOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::OneHotOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_OneHotOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::LogicalAndOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::LogicalAndOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_LogicalAndOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::LogicalNotOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::LogicalNotOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_LogicalNotOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::UnpackOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::UnpackOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_UnpackOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::FloorDivOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::FloorDivOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_FloorDivOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::SquareOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::SquareOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_SquareOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::ZerosLikeOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::ZerosLikeOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_ZerosLikeOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::FillOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::FillOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_FillOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::BidirectionalSequenceLSTMOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::BidirectionalSequenceLSTMOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_BidirectionalSequenceLSTMOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::BidirectionalSequenceRNNOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::BidirectionalSequenceRNNOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_BidirectionalSequenceRNNOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::UnidirectionalSequenceLSTMOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::UnidirectionalSequenceLSTMOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_UnidirectionalSequenceLSTMOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::FloorModOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::FloorModOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_FloorModOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::RangeOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::RangeOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_RangeOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::ResizeNearestNeighborOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::ResizeNearestNeighborOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_ResizeNearestNeighborOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::LeakyReluOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::LeakyReluOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_LeakyReluOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::SquaredDifferenceOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::SquaredDifferenceOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_SquaredDifferenceOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::MirrorPadOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::MirrorPadOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_MirrorPadOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::AbsOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::AbsOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_AbsOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::SplitVOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::SplitVOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_SplitVOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::UniqueOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::UniqueOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_UniqueOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::ReverseV2OptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::ReverseV2OptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_ReverseV2Options;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::AddNOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::AddNOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_AddNOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::GatherNdOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::GatherNdOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_GatherNdOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::CosOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::CosOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_CosOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::WhereOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::WhereOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_WhereOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::RankOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::RankOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_RankOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::ReverseSequenceOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::ReverseSequenceOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_ReverseSequenceOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::MatrixDiagOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::MatrixDiagOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_MatrixDiagOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::QuantizeOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::QuantizeOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_QuantizeOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::MatrixSetDiagOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::MatrixSetDiagOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_MatrixSetDiagOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::HardSwishOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::HardSwishOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_HardSwishOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::IfOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::IfOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_IfOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::WhileOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::WhileOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_WhileOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::DepthToSpaceOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::DepthToSpaceOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_DepthToSpaceOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::NonMaxSuppressionV4OptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::NonMaxSuppressionV4OptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_NonMaxSuppressionV4Options;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::NonMaxSuppressionV5OptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::NonMaxSuppressionV5OptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_NonMaxSuppressionV5Options;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::ScatterNdOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::ScatterNdOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_ScatterNdOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::SelectV2OptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::SelectV2OptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_SelectV2Options;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::DensifyOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::DensifyOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_DensifyOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::SegmentSumOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::SegmentSumOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_SegmentSumOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::BatchMatMulOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::BatchMatMulOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_BatchMatMulOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::CumsumOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::CumsumOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_CumsumOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::CallOnceOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::CallOnceOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_CallOnceOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::BroadcastToOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::BroadcastToOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_BroadcastToOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::Rfft2dOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::Rfft2dOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_Rfft2dOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::Conv3DOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::Conv3DOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_Conv3DOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::HashtableOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::HashtableOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_HashtableOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::HashtableFindOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::HashtableFindOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_HashtableFindOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::HashtableImportOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::HashtableImportOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_HashtableImportOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::HashtableSizeOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::HashtableSizeOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_HashtableSizeOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::VarHandleOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::VarHandleOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_VarHandleOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::ReadVariableOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::ReadVariableOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_ReadVariableOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::AssignVariableOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::AssignVariableOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_AssignVariableOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::RandomOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::RandomOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_RandomOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::BucketizeOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::BucketizeOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_BucketizeOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::GeluOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::GeluOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_GeluOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::DynamicUpdateSliceOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::DynamicUpdateSliceOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_DynamicUpdateSliceOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::UnsortedSegmentProdOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::UnsortedSegmentProdOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_UnsortedSegmentProdOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::UnsortedSegmentMaxOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::UnsortedSegmentMaxOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_UnsortedSegmentMaxOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::UnsortedSegmentMinOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::UnsortedSegmentMinOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_UnsortedSegmentMinOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::UnsortedSegmentSumOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::UnsortedSegmentSumOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_UnsortedSegmentSumOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::ATan2OptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::ATan2OptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_ATan2Options;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::SignOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::SignOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_SignOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::BitcastOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::BitcastOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_BitcastOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::BitwiseXorOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::BitwiseXorOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_BitwiseXorOptions;
 };
 
-template<> struct BuiltinOptionsUnionTraits<tflite::RightShiftOptionsT> {
+template<> struct BuiltinOptionsUnionTraits<tflite_micro::RightShiftOptionsT> {
   static const BuiltinOptions enum_value = BuiltinOptions_RightShiftOptions;
 };
 
@@ -3084,1013 +3084,1013 @@ struct BuiltinOptionsUnion {
   static void *UnPack(const void *obj, BuiltinOptions type, const flatbuffers::resolver_function_t *resolver);
   flatbuffers::Offset<void> Pack(flatbuffers::FlatBufferBuilder &_fbb, const flatbuffers::rehasher_function_t *_rehasher = nullptr) const;
 
-  tflite::Conv2DOptionsT *AsConv2DOptions() {
+  tflite_micro::Conv2DOptionsT *AsConv2DOptions() {
     return type == BuiltinOptions_Conv2DOptions ?
-      reinterpret_cast<tflite::Conv2DOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::Conv2DOptionsT *>(value) : nullptr;
   }
-  const tflite::Conv2DOptionsT *AsConv2DOptions() const {
+  const tflite_micro::Conv2DOptionsT *AsConv2DOptions() const {
     return type == BuiltinOptions_Conv2DOptions ?
-      reinterpret_cast<const tflite::Conv2DOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::Conv2DOptionsT *>(value) : nullptr;
   }
-  tflite::DepthwiseConv2DOptionsT *AsDepthwiseConv2DOptions() {
+  tflite_micro::DepthwiseConv2DOptionsT *AsDepthwiseConv2DOptions() {
     return type == BuiltinOptions_DepthwiseConv2DOptions ?
-      reinterpret_cast<tflite::DepthwiseConv2DOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::DepthwiseConv2DOptionsT *>(value) : nullptr;
   }
-  const tflite::DepthwiseConv2DOptionsT *AsDepthwiseConv2DOptions() const {
+  const tflite_micro::DepthwiseConv2DOptionsT *AsDepthwiseConv2DOptions() const {
     return type == BuiltinOptions_DepthwiseConv2DOptions ?
-      reinterpret_cast<const tflite::DepthwiseConv2DOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::DepthwiseConv2DOptionsT *>(value) : nullptr;
   }
-  tflite::ConcatEmbeddingsOptionsT *AsConcatEmbeddingsOptions() {
+  tflite_micro::ConcatEmbeddingsOptionsT *AsConcatEmbeddingsOptions() {
     return type == BuiltinOptions_ConcatEmbeddingsOptions ?
-      reinterpret_cast<tflite::ConcatEmbeddingsOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::ConcatEmbeddingsOptionsT *>(value) : nullptr;
   }
-  const tflite::ConcatEmbeddingsOptionsT *AsConcatEmbeddingsOptions() const {
+  const tflite_micro::ConcatEmbeddingsOptionsT *AsConcatEmbeddingsOptions() const {
     return type == BuiltinOptions_ConcatEmbeddingsOptions ?
-      reinterpret_cast<const tflite::ConcatEmbeddingsOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::ConcatEmbeddingsOptionsT *>(value) : nullptr;
   }
-  tflite::LSHProjectionOptionsT *AsLSHProjectionOptions() {
+  tflite_micro::LSHProjectionOptionsT *AsLSHProjectionOptions() {
     return type == BuiltinOptions_LSHProjectionOptions ?
-      reinterpret_cast<tflite::LSHProjectionOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::LSHProjectionOptionsT *>(value) : nullptr;
   }
-  const tflite::LSHProjectionOptionsT *AsLSHProjectionOptions() const {
+  const tflite_micro::LSHProjectionOptionsT *AsLSHProjectionOptions() const {
     return type == BuiltinOptions_LSHProjectionOptions ?
-      reinterpret_cast<const tflite::LSHProjectionOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::LSHProjectionOptionsT *>(value) : nullptr;
   }
-  tflite::Pool2DOptionsT *AsPool2DOptions() {
+  tflite_micro::Pool2DOptionsT *AsPool2DOptions() {
     return type == BuiltinOptions_Pool2DOptions ?
-      reinterpret_cast<tflite::Pool2DOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::Pool2DOptionsT *>(value) : nullptr;
   }
-  const tflite::Pool2DOptionsT *AsPool2DOptions() const {
+  const tflite_micro::Pool2DOptionsT *AsPool2DOptions() const {
     return type == BuiltinOptions_Pool2DOptions ?
-      reinterpret_cast<const tflite::Pool2DOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::Pool2DOptionsT *>(value) : nullptr;
   }
-  tflite::SVDFOptionsT *AsSVDFOptions() {
+  tflite_micro::SVDFOptionsT *AsSVDFOptions() {
     return type == BuiltinOptions_SVDFOptions ?
-      reinterpret_cast<tflite::SVDFOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::SVDFOptionsT *>(value) : nullptr;
   }
-  const tflite::SVDFOptionsT *AsSVDFOptions() const {
+  const tflite_micro::SVDFOptionsT *AsSVDFOptions() const {
     return type == BuiltinOptions_SVDFOptions ?
-      reinterpret_cast<const tflite::SVDFOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::SVDFOptionsT *>(value) : nullptr;
   }
-  tflite::RNNOptionsT *AsRNNOptions() {
+  tflite_micro::RNNOptionsT *AsRNNOptions() {
     return type == BuiltinOptions_RNNOptions ?
-      reinterpret_cast<tflite::RNNOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::RNNOptionsT *>(value) : nullptr;
   }
-  const tflite::RNNOptionsT *AsRNNOptions() const {
+  const tflite_micro::RNNOptionsT *AsRNNOptions() const {
     return type == BuiltinOptions_RNNOptions ?
-      reinterpret_cast<const tflite::RNNOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::RNNOptionsT *>(value) : nullptr;
   }
-  tflite::FullyConnectedOptionsT *AsFullyConnectedOptions() {
+  tflite_micro::FullyConnectedOptionsT *AsFullyConnectedOptions() {
     return type == BuiltinOptions_FullyConnectedOptions ?
-      reinterpret_cast<tflite::FullyConnectedOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::FullyConnectedOptionsT *>(value) : nullptr;
   }
-  const tflite::FullyConnectedOptionsT *AsFullyConnectedOptions() const {
+  const tflite_micro::FullyConnectedOptionsT *AsFullyConnectedOptions() const {
     return type == BuiltinOptions_FullyConnectedOptions ?
-      reinterpret_cast<const tflite::FullyConnectedOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::FullyConnectedOptionsT *>(value) : nullptr;
   }
-  tflite::SoftmaxOptionsT *AsSoftmaxOptions() {
+  tflite_micro::SoftmaxOptionsT *AsSoftmaxOptions() {
     return type == BuiltinOptions_SoftmaxOptions ?
-      reinterpret_cast<tflite::SoftmaxOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::SoftmaxOptionsT *>(value) : nullptr;
   }
-  const tflite::SoftmaxOptionsT *AsSoftmaxOptions() const {
+  const tflite_micro::SoftmaxOptionsT *AsSoftmaxOptions() const {
     return type == BuiltinOptions_SoftmaxOptions ?
-      reinterpret_cast<const tflite::SoftmaxOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::SoftmaxOptionsT *>(value) : nullptr;
   }
-  tflite::ConcatenationOptionsT *AsConcatenationOptions() {
+  tflite_micro::ConcatenationOptionsT *AsConcatenationOptions() {
     return type == BuiltinOptions_ConcatenationOptions ?
-      reinterpret_cast<tflite::ConcatenationOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::ConcatenationOptionsT *>(value) : nullptr;
   }
-  const tflite::ConcatenationOptionsT *AsConcatenationOptions() const {
+  const tflite_micro::ConcatenationOptionsT *AsConcatenationOptions() const {
     return type == BuiltinOptions_ConcatenationOptions ?
-      reinterpret_cast<const tflite::ConcatenationOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::ConcatenationOptionsT *>(value) : nullptr;
   }
-  tflite::AddOptionsT *AsAddOptions() {
+  tflite_micro::AddOptionsT *AsAddOptions() {
     return type == BuiltinOptions_AddOptions ?
-      reinterpret_cast<tflite::AddOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::AddOptionsT *>(value) : nullptr;
   }
-  const tflite::AddOptionsT *AsAddOptions() const {
+  const tflite_micro::AddOptionsT *AsAddOptions() const {
     return type == BuiltinOptions_AddOptions ?
-      reinterpret_cast<const tflite::AddOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::AddOptionsT *>(value) : nullptr;
   }
-  tflite::L2NormOptionsT *AsL2NormOptions() {
+  tflite_micro::L2NormOptionsT *AsL2NormOptions() {
     return type == BuiltinOptions_L2NormOptions ?
-      reinterpret_cast<tflite::L2NormOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::L2NormOptionsT *>(value) : nullptr;
   }
-  const tflite::L2NormOptionsT *AsL2NormOptions() const {
+  const tflite_micro::L2NormOptionsT *AsL2NormOptions() const {
     return type == BuiltinOptions_L2NormOptions ?
-      reinterpret_cast<const tflite::L2NormOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::L2NormOptionsT *>(value) : nullptr;
   }
-  tflite::LocalResponseNormalizationOptionsT *AsLocalResponseNormalizationOptions() {
+  tflite_micro::LocalResponseNormalizationOptionsT *AsLocalResponseNormalizationOptions() {
     return type == BuiltinOptions_LocalResponseNormalizationOptions ?
-      reinterpret_cast<tflite::LocalResponseNormalizationOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::LocalResponseNormalizationOptionsT *>(value) : nullptr;
   }
-  const tflite::LocalResponseNormalizationOptionsT *AsLocalResponseNormalizationOptions() const {
+  const tflite_micro::LocalResponseNormalizationOptionsT *AsLocalResponseNormalizationOptions() const {
     return type == BuiltinOptions_LocalResponseNormalizationOptions ?
-      reinterpret_cast<const tflite::LocalResponseNormalizationOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::LocalResponseNormalizationOptionsT *>(value) : nullptr;
   }
-  tflite::LSTMOptionsT *AsLSTMOptions() {
+  tflite_micro::LSTMOptionsT *AsLSTMOptions() {
     return type == BuiltinOptions_LSTMOptions ?
-      reinterpret_cast<tflite::LSTMOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::LSTMOptionsT *>(value) : nullptr;
   }
-  const tflite::LSTMOptionsT *AsLSTMOptions() const {
+  const tflite_micro::LSTMOptionsT *AsLSTMOptions() const {
     return type == BuiltinOptions_LSTMOptions ?
-      reinterpret_cast<const tflite::LSTMOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::LSTMOptionsT *>(value) : nullptr;
   }
-  tflite::ResizeBilinearOptionsT *AsResizeBilinearOptions() {
+  tflite_micro::ResizeBilinearOptionsT *AsResizeBilinearOptions() {
     return type == BuiltinOptions_ResizeBilinearOptions ?
-      reinterpret_cast<tflite::ResizeBilinearOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::ResizeBilinearOptionsT *>(value) : nullptr;
   }
-  const tflite::ResizeBilinearOptionsT *AsResizeBilinearOptions() const {
+  const tflite_micro::ResizeBilinearOptionsT *AsResizeBilinearOptions() const {
     return type == BuiltinOptions_ResizeBilinearOptions ?
-      reinterpret_cast<const tflite::ResizeBilinearOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::ResizeBilinearOptionsT *>(value) : nullptr;
   }
-  tflite::CallOptionsT *AsCallOptions() {
+  tflite_micro::CallOptionsT *AsCallOptions() {
     return type == BuiltinOptions_CallOptions ?
-      reinterpret_cast<tflite::CallOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::CallOptionsT *>(value) : nullptr;
   }
-  const tflite::CallOptionsT *AsCallOptions() const {
+  const tflite_micro::CallOptionsT *AsCallOptions() const {
     return type == BuiltinOptions_CallOptions ?
-      reinterpret_cast<const tflite::CallOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::CallOptionsT *>(value) : nullptr;
   }
-  tflite::ReshapeOptionsT *AsReshapeOptions() {
+  tflite_micro::ReshapeOptionsT *AsReshapeOptions() {
     return type == BuiltinOptions_ReshapeOptions ?
-      reinterpret_cast<tflite::ReshapeOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::ReshapeOptionsT *>(value) : nullptr;
   }
-  const tflite::ReshapeOptionsT *AsReshapeOptions() const {
+  const tflite_micro::ReshapeOptionsT *AsReshapeOptions() const {
     return type == BuiltinOptions_ReshapeOptions ?
-      reinterpret_cast<const tflite::ReshapeOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::ReshapeOptionsT *>(value) : nullptr;
   }
-  tflite::SkipGramOptionsT *AsSkipGramOptions() {
+  tflite_micro::SkipGramOptionsT *AsSkipGramOptions() {
     return type == BuiltinOptions_SkipGramOptions ?
-      reinterpret_cast<tflite::SkipGramOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::SkipGramOptionsT *>(value) : nullptr;
   }
-  const tflite::SkipGramOptionsT *AsSkipGramOptions() const {
+  const tflite_micro::SkipGramOptionsT *AsSkipGramOptions() const {
     return type == BuiltinOptions_SkipGramOptions ?
-      reinterpret_cast<const tflite::SkipGramOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::SkipGramOptionsT *>(value) : nullptr;
   }
-  tflite::SpaceToDepthOptionsT *AsSpaceToDepthOptions() {
+  tflite_micro::SpaceToDepthOptionsT *AsSpaceToDepthOptions() {
     return type == BuiltinOptions_SpaceToDepthOptions ?
-      reinterpret_cast<tflite::SpaceToDepthOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::SpaceToDepthOptionsT *>(value) : nullptr;
   }
-  const tflite::SpaceToDepthOptionsT *AsSpaceToDepthOptions() const {
+  const tflite_micro::SpaceToDepthOptionsT *AsSpaceToDepthOptions() const {
     return type == BuiltinOptions_SpaceToDepthOptions ?
-      reinterpret_cast<const tflite::SpaceToDepthOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::SpaceToDepthOptionsT *>(value) : nullptr;
   }
-  tflite::EmbeddingLookupSparseOptionsT *AsEmbeddingLookupSparseOptions() {
+  tflite_micro::EmbeddingLookupSparseOptionsT *AsEmbeddingLookupSparseOptions() {
     return type == BuiltinOptions_EmbeddingLookupSparseOptions ?
-      reinterpret_cast<tflite::EmbeddingLookupSparseOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::EmbeddingLookupSparseOptionsT *>(value) : nullptr;
   }
-  const tflite::EmbeddingLookupSparseOptionsT *AsEmbeddingLookupSparseOptions() const {
+  const tflite_micro::EmbeddingLookupSparseOptionsT *AsEmbeddingLookupSparseOptions() const {
     return type == BuiltinOptions_EmbeddingLookupSparseOptions ?
-      reinterpret_cast<const tflite::EmbeddingLookupSparseOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::EmbeddingLookupSparseOptionsT *>(value) : nullptr;
   }
-  tflite::MulOptionsT *AsMulOptions() {
+  tflite_micro::MulOptionsT *AsMulOptions() {
     return type == BuiltinOptions_MulOptions ?
-      reinterpret_cast<tflite::MulOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::MulOptionsT *>(value) : nullptr;
   }
-  const tflite::MulOptionsT *AsMulOptions() const {
+  const tflite_micro::MulOptionsT *AsMulOptions() const {
     return type == BuiltinOptions_MulOptions ?
-      reinterpret_cast<const tflite::MulOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::MulOptionsT *>(value) : nullptr;
   }
-  tflite::PadOptionsT *AsPadOptions() {
+  tflite_micro::PadOptionsT *AsPadOptions() {
     return type == BuiltinOptions_PadOptions ?
-      reinterpret_cast<tflite::PadOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::PadOptionsT *>(value) : nullptr;
   }
-  const tflite::PadOptionsT *AsPadOptions() const {
+  const tflite_micro::PadOptionsT *AsPadOptions() const {
     return type == BuiltinOptions_PadOptions ?
-      reinterpret_cast<const tflite::PadOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::PadOptionsT *>(value) : nullptr;
   }
-  tflite::GatherOptionsT *AsGatherOptions() {
+  tflite_micro::GatherOptionsT *AsGatherOptions() {
     return type == BuiltinOptions_GatherOptions ?
-      reinterpret_cast<tflite::GatherOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::GatherOptionsT *>(value) : nullptr;
   }
-  const tflite::GatherOptionsT *AsGatherOptions() const {
+  const tflite_micro::GatherOptionsT *AsGatherOptions() const {
     return type == BuiltinOptions_GatherOptions ?
-      reinterpret_cast<const tflite::GatherOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::GatherOptionsT *>(value) : nullptr;
   }
-  tflite::BatchToSpaceNDOptionsT *AsBatchToSpaceNDOptions() {
+  tflite_micro::BatchToSpaceNDOptionsT *AsBatchToSpaceNDOptions() {
     return type == BuiltinOptions_BatchToSpaceNDOptions ?
-      reinterpret_cast<tflite::BatchToSpaceNDOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::BatchToSpaceNDOptionsT *>(value) : nullptr;
   }
-  const tflite::BatchToSpaceNDOptionsT *AsBatchToSpaceNDOptions() const {
+  const tflite_micro::BatchToSpaceNDOptionsT *AsBatchToSpaceNDOptions() const {
     return type == BuiltinOptions_BatchToSpaceNDOptions ?
-      reinterpret_cast<const tflite::BatchToSpaceNDOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::BatchToSpaceNDOptionsT *>(value) : nullptr;
   }
-  tflite::SpaceToBatchNDOptionsT *AsSpaceToBatchNDOptions() {
+  tflite_micro::SpaceToBatchNDOptionsT *AsSpaceToBatchNDOptions() {
     return type == BuiltinOptions_SpaceToBatchNDOptions ?
-      reinterpret_cast<tflite::SpaceToBatchNDOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::SpaceToBatchNDOptionsT *>(value) : nullptr;
   }
-  const tflite::SpaceToBatchNDOptionsT *AsSpaceToBatchNDOptions() const {
+  const tflite_micro::SpaceToBatchNDOptionsT *AsSpaceToBatchNDOptions() const {
     return type == BuiltinOptions_SpaceToBatchNDOptions ?
-      reinterpret_cast<const tflite::SpaceToBatchNDOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::SpaceToBatchNDOptionsT *>(value) : nullptr;
   }
-  tflite::TransposeOptionsT *AsTransposeOptions() {
+  tflite_micro::TransposeOptionsT *AsTransposeOptions() {
     return type == BuiltinOptions_TransposeOptions ?
-      reinterpret_cast<tflite::TransposeOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::TransposeOptionsT *>(value) : nullptr;
   }
-  const tflite::TransposeOptionsT *AsTransposeOptions() const {
+  const tflite_micro::TransposeOptionsT *AsTransposeOptions() const {
     return type == BuiltinOptions_TransposeOptions ?
-      reinterpret_cast<const tflite::TransposeOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::TransposeOptionsT *>(value) : nullptr;
   }
-  tflite::ReducerOptionsT *AsReducerOptions() {
+  tflite_micro::ReducerOptionsT *AsReducerOptions() {
     return type == BuiltinOptions_ReducerOptions ?
-      reinterpret_cast<tflite::ReducerOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::ReducerOptionsT *>(value) : nullptr;
   }
-  const tflite::ReducerOptionsT *AsReducerOptions() const {
+  const tflite_micro::ReducerOptionsT *AsReducerOptions() const {
     return type == BuiltinOptions_ReducerOptions ?
-      reinterpret_cast<const tflite::ReducerOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::ReducerOptionsT *>(value) : nullptr;
   }
-  tflite::SubOptionsT *AsSubOptions() {
+  tflite_micro::SubOptionsT *AsSubOptions() {
     return type == BuiltinOptions_SubOptions ?
-      reinterpret_cast<tflite::SubOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::SubOptionsT *>(value) : nullptr;
   }
-  const tflite::SubOptionsT *AsSubOptions() const {
+  const tflite_micro::SubOptionsT *AsSubOptions() const {
     return type == BuiltinOptions_SubOptions ?
-      reinterpret_cast<const tflite::SubOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::SubOptionsT *>(value) : nullptr;
   }
-  tflite::DivOptionsT *AsDivOptions() {
+  tflite_micro::DivOptionsT *AsDivOptions() {
     return type == BuiltinOptions_DivOptions ?
-      reinterpret_cast<tflite::DivOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::DivOptionsT *>(value) : nullptr;
   }
-  const tflite::DivOptionsT *AsDivOptions() const {
+  const tflite_micro::DivOptionsT *AsDivOptions() const {
     return type == BuiltinOptions_DivOptions ?
-      reinterpret_cast<const tflite::DivOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::DivOptionsT *>(value) : nullptr;
   }
-  tflite::SqueezeOptionsT *AsSqueezeOptions() {
+  tflite_micro::SqueezeOptionsT *AsSqueezeOptions() {
     return type == BuiltinOptions_SqueezeOptions ?
-      reinterpret_cast<tflite::SqueezeOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::SqueezeOptionsT *>(value) : nullptr;
   }
-  const tflite::SqueezeOptionsT *AsSqueezeOptions() const {
+  const tflite_micro::SqueezeOptionsT *AsSqueezeOptions() const {
     return type == BuiltinOptions_SqueezeOptions ?
-      reinterpret_cast<const tflite::SqueezeOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::SqueezeOptionsT *>(value) : nullptr;
   }
-  tflite::SequenceRNNOptionsT *AsSequenceRNNOptions() {
+  tflite_micro::SequenceRNNOptionsT *AsSequenceRNNOptions() {
     return type == BuiltinOptions_SequenceRNNOptions ?
-      reinterpret_cast<tflite::SequenceRNNOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::SequenceRNNOptionsT *>(value) : nullptr;
   }
-  const tflite::SequenceRNNOptionsT *AsSequenceRNNOptions() const {
+  const tflite_micro::SequenceRNNOptionsT *AsSequenceRNNOptions() const {
     return type == BuiltinOptions_SequenceRNNOptions ?
-      reinterpret_cast<const tflite::SequenceRNNOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::SequenceRNNOptionsT *>(value) : nullptr;
   }
-  tflite::StridedSliceOptionsT *AsStridedSliceOptions() {
+  tflite_micro::StridedSliceOptionsT *AsStridedSliceOptions() {
     return type == BuiltinOptions_StridedSliceOptions ?
-      reinterpret_cast<tflite::StridedSliceOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::StridedSliceOptionsT *>(value) : nullptr;
   }
-  const tflite::StridedSliceOptionsT *AsStridedSliceOptions() const {
+  const tflite_micro::StridedSliceOptionsT *AsStridedSliceOptions() const {
     return type == BuiltinOptions_StridedSliceOptions ?
-      reinterpret_cast<const tflite::StridedSliceOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::StridedSliceOptionsT *>(value) : nullptr;
   }
-  tflite::ExpOptionsT *AsExpOptions() {
+  tflite_micro::ExpOptionsT *AsExpOptions() {
     return type == BuiltinOptions_ExpOptions ?
-      reinterpret_cast<tflite::ExpOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::ExpOptionsT *>(value) : nullptr;
   }
-  const tflite::ExpOptionsT *AsExpOptions() const {
+  const tflite_micro::ExpOptionsT *AsExpOptions() const {
     return type == BuiltinOptions_ExpOptions ?
-      reinterpret_cast<const tflite::ExpOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::ExpOptionsT *>(value) : nullptr;
   }
-  tflite::TopKV2OptionsT *AsTopKV2Options() {
+  tflite_micro::TopKV2OptionsT *AsTopKV2Options() {
     return type == BuiltinOptions_TopKV2Options ?
-      reinterpret_cast<tflite::TopKV2OptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::TopKV2OptionsT *>(value) : nullptr;
   }
-  const tflite::TopKV2OptionsT *AsTopKV2Options() const {
+  const tflite_micro::TopKV2OptionsT *AsTopKV2Options() const {
     return type == BuiltinOptions_TopKV2Options ?
-      reinterpret_cast<const tflite::TopKV2OptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::TopKV2OptionsT *>(value) : nullptr;
   }
-  tflite::SplitOptionsT *AsSplitOptions() {
+  tflite_micro::SplitOptionsT *AsSplitOptions() {
     return type == BuiltinOptions_SplitOptions ?
-      reinterpret_cast<tflite::SplitOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::SplitOptionsT *>(value) : nullptr;
   }
-  const tflite::SplitOptionsT *AsSplitOptions() const {
+  const tflite_micro::SplitOptionsT *AsSplitOptions() const {
     return type == BuiltinOptions_SplitOptions ?
-      reinterpret_cast<const tflite::SplitOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::SplitOptionsT *>(value) : nullptr;
   }
-  tflite::LogSoftmaxOptionsT *AsLogSoftmaxOptions() {
+  tflite_micro::LogSoftmaxOptionsT *AsLogSoftmaxOptions() {
     return type == BuiltinOptions_LogSoftmaxOptions ?
-      reinterpret_cast<tflite::LogSoftmaxOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::LogSoftmaxOptionsT *>(value) : nullptr;
   }
-  const tflite::LogSoftmaxOptionsT *AsLogSoftmaxOptions() const {
+  const tflite_micro::LogSoftmaxOptionsT *AsLogSoftmaxOptions() const {
     return type == BuiltinOptions_LogSoftmaxOptions ?
-      reinterpret_cast<const tflite::LogSoftmaxOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::LogSoftmaxOptionsT *>(value) : nullptr;
   }
-  tflite::CastOptionsT *AsCastOptions() {
+  tflite_micro::CastOptionsT *AsCastOptions() {
     return type == BuiltinOptions_CastOptions ?
-      reinterpret_cast<tflite::CastOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::CastOptionsT *>(value) : nullptr;
   }
-  const tflite::CastOptionsT *AsCastOptions() const {
+  const tflite_micro::CastOptionsT *AsCastOptions() const {
     return type == BuiltinOptions_CastOptions ?
-      reinterpret_cast<const tflite::CastOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::CastOptionsT *>(value) : nullptr;
   }
-  tflite::DequantizeOptionsT *AsDequantizeOptions() {
+  tflite_micro::DequantizeOptionsT *AsDequantizeOptions() {
     return type == BuiltinOptions_DequantizeOptions ?
-      reinterpret_cast<tflite::DequantizeOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::DequantizeOptionsT *>(value) : nullptr;
   }
-  const tflite::DequantizeOptionsT *AsDequantizeOptions() const {
+  const tflite_micro::DequantizeOptionsT *AsDequantizeOptions() const {
     return type == BuiltinOptions_DequantizeOptions ?
-      reinterpret_cast<const tflite::DequantizeOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::DequantizeOptionsT *>(value) : nullptr;
   }
-  tflite::MaximumMinimumOptionsT *AsMaximumMinimumOptions() {
+  tflite_micro::MaximumMinimumOptionsT *AsMaximumMinimumOptions() {
     return type == BuiltinOptions_MaximumMinimumOptions ?
-      reinterpret_cast<tflite::MaximumMinimumOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::MaximumMinimumOptionsT *>(value) : nullptr;
   }
-  const tflite::MaximumMinimumOptionsT *AsMaximumMinimumOptions() const {
+  const tflite_micro::MaximumMinimumOptionsT *AsMaximumMinimumOptions() const {
     return type == BuiltinOptions_MaximumMinimumOptions ?
-      reinterpret_cast<const tflite::MaximumMinimumOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::MaximumMinimumOptionsT *>(value) : nullptr;
   }
-  tflite::ArgMaxOptionsT *AsArgMaxOptions() {
+  tflite_micro::ArgMaxOptionsT *AsArgMaxOptions() {
     return type == BuiltinOptions_ArgMaxOptions ?
-      reinterpret_cast<tflite::ArgMaxOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::ArgMaxOptionsT *>(value) : nullptr;
   }
-  const tflite::ArgMaxOptionsT *AsArgMaxOptions() const {
+  const tflite_micro::ArgMaxOptionsT *AsArgMaxOptions() const {
     return type == BuiltinOptions_ArgMaxOptions ?
-      reinterpret_cast<const tflite::ArgMaxOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::ArgMaxOptionsT *>(value) : nullptr;
   }
-  tflite::LessOptionsT *AsLessOptions() {
+  tflite_micro::LessOptionsT *AsLessOptions() {
     return type == BuiltinOptions_LessOptions ?
-      reinterpret_cast<tflite::LessOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::LessOptionsT *>(value) : nullptr;
   }
-  const tflite::LessOptionsT *AsLessOptions() const {
+  const tflite_micro::LessOptionsT *AsLessOptions() const {
     return type == BuiltinOptions_LessOptions ?
-      reinterpret_cast<const tflite::LessOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::LessOptionsT *>(value) : nullptr;
   }
-  tflite::NegOptionsT *AsNegOptions() {
+  tflite_micro::NegOptionsT *AsNegOptions() {
     return type == BuiltinOptions_NegOptions ?
-      reinterpret_cast<tflite::NegOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::NegOptionsT *>(value) : nullptr;
   }
-  const tflite::NegOptionsT *AsNegOptions() const {
+  const tflite_micro::NegOptionsT *AsNegOptions() const {
     return type == BuiltinOptions_NegOptions ?
-      reinterpret_cast<const tflite::NegOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::NegOptionsT *>(value) : nullptr;
   }
-  tflite::PadV2OptionsT *AsPadV2Options() {
+  tflite_micro::PadV2OptionsT *AsPadV2Options() {
     return type == BuiltinOptions_PadV2Options ?
-      reinterpret_cast<tflite::PadV2OptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::PadV2OptionsT *>(value) : nullptr;
   }
-  const tflite::PadV2OptionsT *AsPadV2Options() const {
+  const tflite_micro::PadV2OptionsT *AsPadV2Options() const {
     return type == BuiltinOptions_PadV2Options ?
-      reinterpret_cast<const tflite::PadV2OptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::PadV2OptionsT *>(value) : nullptr;
   }
-  tflite::GreaterOptionsT *AsGreaterOptions() {
+  tflite_micro::GreaterOptionsT *AsGreaterOptions() {
     return type == BuiltinOptions_GreaterOptions ?
-      reinterpret_cast<tflite::GreaterOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::GreaterOptionsT *>(value) : nullptr;
   }
-  const tflite::GreaterOptionsT *AsGreaterOptions() const {
+  const tflite_micro::GreaterOptionsT *AsGreaterOptions() const {
     return type == BuiltinOptions_GreaterOptions ?
-      reinterpret_cast<const tflite::GreaterOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::GreaterOptionsT *>(value) : nullptr;
   }
-  tflite::GreaterEqualOptionsT *AsGreaterEqualOptions() {
+  tflite_micro::GreaterEqualOptionsT *AsGreaterEqualOptions() {
     return type == BuiltinOptions_GreaterEqualOptions ?
-      reinterpret_cast<tflite::GreaterEqualOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::GreaterEqualOptionsT *>(value) : nullptr;
   }
-  const tflite::GreaterEqualOptionsT *AsGreaterEqualOptions() const {
+  const tflite_micro::GreaterEqualOptionsT *AsGreaterEqualOptions() const {
     return type == BuiltinOptions_GreaterEqualOptions ?
-      reinterpret_cast<const tflite::GreaterEqualOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::GreaterEqualOptionsT *>(value) : nullptr;
   }
-  tflite::LessEqualOptionsT *AsLessEqualOptions() {
+  tflite_micro::LessEqualOptionsT *AsLessEqualOptions() {
     return type == BuiltinOptions_LessEqualOptions ?
-      reinterpret_cast<tflite::LessEqualOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::LessEqualOptionsT *>(value) : nullptr;
   }
-  const tflite::LessEqualOptionsT *AsLessEqualOptions() const {
+  const tflite_micro::LessEqualOptionsT *AsLessEqualOptions() const {
     return type == BuiltinOptions_LessEqualOptions ?
-      reinterpret_cast<const tflite::LessEqualOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::LessEqualOptionsT *>(value) : nullptr;
   }
-  tflite::SelectOptionsT *AsSelectOptions() {
+  tflite_micro::SelectOptionsT *AsSelectOptions() {
     return type == BuiltinOptions_SelectOptions ?
-      reinterpret_cast<tflite::SelectOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::SelectOptionsT *>(value) : nullptr;
   }
-  const tflite::SelectOptionsT *AsSelectOptions() const {
+  const tflite_micro::SelectOptionsT *AsSelectOptions() const {
     return type == BuiltinOptions_SelectOptions ?
-      reinterpret_cast<const tflite::SelectOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::SelectOptionsT *>(value) : nullptr;
   }
-  tflite::SliceOptionsT *AsSliceOptions() {
+  tflite_micro::SliceOptionsT *AsSliceOptions() {
     return type == BuiltinOptions_SliceOptions ?
-      reinterpret_cast<tflite::SliceOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::SliceOptionsT *>(value) : nullptr;
   }
-  const tflite::SliceOptionsT *AsSliceOptions() const {
+  const tflite_micro::SliceOptionsT *AsSliceOptions() const {
     return type == BuiltinOptions_SliceOptions ?
-      reinterpret_cast<const tflite::SliceOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::SliceOptionsT *>(value) : nullptr;
   }
-  tflite::TransposeConvOptionsT *AsTransposeConvOptions() {
+  tflite_micro::TransposeConvOptionsT *AsTransposeConvOptions() {
     return type == BuiltinOptions_TransposeConvOptions ?
-      reinterpret_cast<tflite::TransposeConvOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::TransposeConvOptionsT *>(value) : nullptr;
   }
-  const tflite::TransposeConvOptionsT *AsTransposeConvOptions() const {
+  const tflite_micro::TransposeConvOptionsT *AsTransposeConvOptions() const {
     return type == BuiltinOptions_TransposeConvOptions ?
-      reinterpret_cast<const tflite::TransposeConvOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::TransposeConvOptionsT *>(value) : nullptr;
   }
-  tflite::SparseToDenseOptionsT *AsSparseToDenseOptions() {
+  tflite_micro::SparseToDenseOptionsT *AsSparseToDenseOptions() {
     return type == BuiltinOptions_SparseToDenseOptions ?
-      reinterpret_cast<tflite::SparseToDenseOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::SparseToDenseOptionsT *>(value) : nullptr;
   }
-  const tflite::SparseToDenseOptionsT *AsSparseToDenseOptions() const {
+  const tflite_micro::SparseToDenseOptionsT *AsSparseToDenseOptions() const {
     return type == BuiltinOptions_SparseToDenseOptions ?
-      reinterpret_cast<const tflite::SparseToDenseOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::SparseToDenseOptionsT *>(value) : nullptr;
   }
-  tflite::TileOptionsT *AsTileOptions() {
+  tflite_micro::TileOptionsT *AsTileOptions() {
     return type == BuiltinOptions_TileOptions ?
-      reinterpret_cast<tflite::TileOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::TileOptionsT *>(value) : nullptr;
   }
-  const tflite::TileOptionsT *AsTileOptions() const {
+  const tflite_micro::TileOptionsT *AsTileOptions() const {
     return type == BuiltinOptions_TileOptions ?
-      reinterpret_cast<const tflite::TileOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::TileOptionsT *>(value) : nullptr;
   }
-  tflite::ExpandDimsOptionsT *AsExpandDimsOptions() {
+  tflite_micro::ExpandDimsOptionsT *AsExpandDimsOptions() {
     return type == BuiltinOptions_ExpandDimsOptions ?
-      reinterpret_cast<tflite::ExpandDimsOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::ExpandDimsOptionsT *>(value) : nullptr;
   }
-  const tflite::ExpandDimsOptionsT *AsExpandDimsOptions() const {
+  const tflite_micro::ExpandDimsOptionsT *AsExpandDimsOptions() const {
     return type == BuiltinOptions_ExpandDimsOptions ?
-      reinterpret_cast<const tflite::ExpandDimsOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::ExpandDimsOptionsT *>(value) : nullptr;
   }
-  tflite::EqualOptionsT *AsEqualOptions() {
+  tflite_micro::EqualOptionsT *AsEqualOptions() {
     return type == BuiltinOptions_EqualOptions ?
-      reinterpret_cast<tflite::EqualOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::EqualOptionsT *>(value) : nullptr;
   }
-  const tflite::EqualOptionsT *AsEqualOptions() const {
+  const tflite_micro::EqualOptionsT *AsEqualOptions() const {
     return type == BuiltinOptions_EqualOptions ?
-      reinterpret_cast<const tflite::EqualOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::EqualOptionsT *>(value) : nullptr;
   }
-  tflite::NotEqualOptionsT *AsNotEqualOptions() {
+  tflite_micro::NotEqualOptionsT *AsNotEqualOptions() {
     return type == BuiltinOptions_NotEqualOptions ?
-      reinterpret_cast<tflite::NotEqualOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::NotEqualOptionsT *>(value) : nullptr;
   }
-  const tflite::NotEqualOptionsT *AsNotEqualOptions() const {
+  const tflite_micro::NotEqualOptionsT *AsNotEqualOptions() const {
     return type == BuiltinOptions_NotEqualOptions ?
-      reinterpret_cast<const tflite::NotEqualOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::NotEqualOptionsT *>(value) : nullptr;
   }
-  tflite::ShapeOptionsT *AsShapeOptions() {
+  tflite_micro::ShapeOptionsT *AsShapeOptions() {
     return type == BuiltinOptions_ShapeOptions ?
-      reinterpret_cast<tflite::ShapeOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::ShapeOptionsT *>(value) : nullptr;
   }
-  const tflite::ShapeOptionsT *AsShapeOptions() const {
+  const tflite_micro::ShapeOptionsT *AsShapeOptions() const {
     return type == BuiltinOptions_ShapeOptions ?
-      reinterpret_cast<const tflite::ShapeOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::ShapeOptionsT *>(value) : nullptr;
   }
-  tflite::PowOptionsT *AsPowOptions() {
+  tflite_micro::PowOptionsT *AsPowOptions() {
     return type == BuiltinOptions_PowOptions ?
-      reinterpret_cast<tflite::PowOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::PowOptionsT *>(value) : nullptr;
   }
-  const tflite::PowOptionsT *AsPowOptions() const {
+  const tflite_micro::PowOptionsT *AsPowOptions() const {
     return type == BuiltinOptions_PowOptions ?
-      reinterpret_cast<const tflite::PowOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::PowOptionsT *>(value) : nullptr;
   }
-  tflite::ArgMinOptionsT *AsArgMinOptions() {
+  tflite_micro::ArgMinOptionsT *AsArgMinOptions() {
     return type == BuiltinOptions_ArgMinOptions ?
-      reinterpret_cast<tflite::ArgMinOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::ArgMinOptionsT *>(value) : nullptr;
   }
-  const tflite::ArgMinOptionsT *AsArgMinOptions() const {
+  const tflite_micro::ArgMinOptionsT *AsArgMinOptions() const {
     return type == BuiltinOptions_ArgMinOptions ?
-      reinterpret_cast<const tflite::ArgMinOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::ArgMinOptionsT *>(value) : nullptr;
   }
-  tflite::FakeQuantOptionsT *AsFakeQuantOptions() {
+  tflite_micro::FakeQuantOptionsT *AsFakeQuantOptions() {
     return type == BuiltinOptions_FakeQuantOptions ?
-      reinterpret_cast<tflite::FakeQuantOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::FakeQuantOptionsT *>(value) : nullptr;
   }
-  const tflite::FakeQuantOptionsT *AsFakeQuantOptions() const {
+  const tflite_micro::FakeQuantOptionsT *AsFakeQuantOptions() const {
     return type == BuiltinOptions_FakeQuantOptions ?
-      reinterpret_cast<const tflite::FakeQuantOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::FakeQuantOptionsT *>(value) : nullptr;
   }
-  tflite::PackOptionsT *AsPackOptions() {
+  tflite_micro::PackOptionsT *AsPackOptions() {
     return type == BuiltinOptions_PackOptions ?
-      reinterpret_cast<tflite::PackOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::PackOptionsT *>(value) : nullptr;
   }
-  const tflite::PackOptionsT *AsPackOptions() const {
+  const tflite_micro::PackOptionsT *AsPackOptions() const {
     return type == BuiltinOptions_PackOptions ?
-      reinterpret_cast<const tflite::PackOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::PackOptionsT *>(value) : nullptr;
   }
-  tflite::LogicalOrOptionsT *AsLogicalOrOptions() {
+  tflite_micro::LogicalOrOptionsT *AsLogicalOrOptions() {
     return type == BuiltinOptions_LogicalOrOptions ?
-      reinterpret_cast<tflite::LogicalOrOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::LogicalOrOptionsT *>(value) : nullptr;
   }
-  const tflite::LogicalOrOptionsT *AsLogicalOrOptions() const {
+  const tflite_micro::LogicalOrOptionsT *AsLogicalOrOptions() const {
     return type == BuiltinOptions_LogicalOrOptions ?
-      reinterpret_cast<const tflite::LogicalOrOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::LogicalOrOptionsT *>(value) : nullptr;
   }
-  tflite::OneHotOptionsT *AsOneHotOptions() {
+  tflite_micro::OneHotOptionsT *AsOneHotOptions() {
     return type == BuiltinOptions_OneHotOptions ?
-      reinterpret_cast<tflite::OneHotOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::OneHotOptionsT *>(value) : nullptr;
   }
-  const tflite::OneHotOptionsT *AsOneHotOptions() const {
+  const tflite_micro::OneHotOptionsT *AsOneHotOptions() const {
     return type == BuiltinOptions_OneHotOptions ?
-      reinterpret_cast<const tflite::OneHotOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::OneHotOptionsT *>(value) : nullptr;
   }
-  tflite::LogicalAndOptionsT *AsLogicalAndOptions() {
+  tflite_micro::LogicalAndOptionsT *AsLogicalAndOptions() {
     return type == BuiltinOptions_LogicalAndOptions ?
-      reinterpret_cast<tflite::LogicalAndOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::LogicalAndOptionsT *>(value) : nullptr;
   }
-  const tflite::LogicalAndOptionsT *AsLogicalAndOptions() const {
+  const tflite_micro::LogicalAndOptionsT *AsLogicalAndOptions() const {
     return type == BuiltinOptions_LogicalAndOptions ?
-      reinterpret_cast<const tflite::LogicalAndOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::LogicalAndOptionsT *>(value) : nullptr;
   }
-  tflite::LogicalNotOptionsT *AsLogicalNotOptions() {
+  tflite_micro::LogicalNotOptionsT *AsLogicalNotOptions() {
     return type == BuiltinOptions_LogicalNotOptions ?
-      reinterpret_cast<tflite::LogicalNotOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::LogicalNotOptionsT *>(value) : nullptr;
   }
-  const tflite::LogicalNotOptionsT *AsLogicalNotOptions() const {
+  const tflite_micro::LogicalNotOptionsT *AsLogicalNotOptions() const {
     return type == BuiltinOptions_LogicalNotOptions ?
-      reinterpret_cast<const tflite::LogicalNotOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::LogicalNotOptionsT *>(value) : nullptr;
   }
-  tflite::UnpackOptionsT *AsUnpackOptions() {
+  tflite_micro::UnpackOptionsT *AsUnpackOptions() {
     return type == BuiltinOptions_UnpackOptions ?
-      reinterpret_cast<tflite::UnpackOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::UnpackOptionsT *>(value) : nullptr;
   }
-  const tflite::UnpackOptionsT *AsUnpackOptions() const {
+  const tflite_micro::UnpackOptionsT *AsUnpackOptions() const {
     return type == BuiltinOptions_UnpackOptions ?
-      reinterpret_cast<const tflite::UnpackOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::UnpackOptionsT *>(value) : nullptr;
   }
-  tflite::FloorDivOptionsT *AsFloorDivOptions() {
+  tflite_micro::FloorDivOptionsT *AsFloorDivOptions() {
     return type == BuiltinOptions_FloorDivOptions ?
-      reinterpret_cast<tflite::FloorDivOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::FloorDivOptionsT *>(value) : nullptr;
   }
-  const tflite::FloorDivOptionsT *AsFloorDivOptions() const {
+  const tflite_micro::FloorDivOptionsT *AsFloorDivOptions() const {
     return type == BuiltinOptions_FloorDivOptions ?
-      reinterpret_cast<const tflite::FloorDivOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::FloorDivOptionsT *>(value) : nullptr;
   }
-  tflite::SquareOptionsT *AsSquareOptions() {
+  tflite_micro::SquareOptionsT *AsSquareOptions() {
     return type == BuiltinOptions_SquareOptions ?
-      reinterpret_cast<tflite::SquareOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::SquareOptionsT *>(value) : nullptr;
   }
-  const tflite::SquareOptionsT *AsSquareOptions() const {
+  const tflite_micro::SquareOptionsT *AsSquareOptions() const {
     return type == BuiltinOptions_SquareOptions ?
-      reinterpret_cast<const tflite::SquareOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::SquareOptionsT *>(value) : nullptr;
   }
-  tflite::ZerosLikeOptionsT *AsZerosLikeOptions() {
+  tflite_micro::ZerosLikeOptionsT *AsZerosLikeOptions() {
     return type == BuiltinOptions_ZerosLikeOptions ?
-      reinterpret_cast<tflite::ZerosLikeOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::ZerosLikeOptionsT *>(value) : nullptr;
   }
-  const tflite::ZerosLikeOptionsT *AsZerosLikeOptions() const {
+  const tflite_micro::ZerosLikeOptionsT *AsZerosLikeOptions() const {
     return type == BuiltinOptions_ZerosLikeOptions ?
-      reinterpret_cast<const tflite::ZerosLikeOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::ZerosLikeOptionsT *>(value) : nullptr;
   }
-  tflite::FillOptionsT *AsFillOptions() {
+  tflite_micro::FillOptionsT *AsFillOptions() {
     return type == BuiltinOptions_FillOptions ?
-      reinterpret_cast<tflite::FillOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::FillOptionsT *>(value) : nullptr;
   }
-  const tflite::FillOptionsT *AsFillOptions() const {
+  const tflite_micro::FillOptionsT *AsFillOptions() const {
     return type == BuiltinOptions_FillOptions ?
-      reinterpret_cast<const tflite::FillOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::FillOptionsT *>(value) : nullptr;
   }
-  tflite::BidirectionalSequenceLSTMOptionsT *AsBidirectionalSequenceLSTMOptions() {
+  tflite_micro::BidirectionalSequenceLSTMOptionsT *AsBidirectionalSequenceLSTMOptions() {
     return type == BuiltinOptions_BidirectionalSequenceLSTMOptions ?
-      reinterpret_cast<tflite::BidirectionalSequenceLSTMOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::BidirectionalSequenceLSTMOptionsT *>(value) : nullptr;
   }
-  const tflite::BidirectionalSequenceLSTMOptionsT *AsBidirectionalSequenceLSTMOptions() const {
+  const tflite_micro::BidirectionalSequenceLSTMOptionsT *AsBidirectionalSequenceLSTMOptions() const {
     return type == BuiltinOptions_BidirectionalSequenceLSTMOptions ?
-      reinterpret_cast<const tflite::BidirectionalSequenceLSTMOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::BidirectionalSequenceLSTMOptionsT *>(value) : nullptr;
   }
-  tflite::BidirectionalSequenceRNNOptionsT *AsBidirectionalSequenceRNNOptions() {
+  tflite_micro::BidirectionalSequenceRNNOptionsT *AsBidirectionalSequenceRNNOptions() {
     return type == BuiltinOptions_BidirectionalSequenceRNNOptions ?
-      reinterpret_cast<tflite::BidirectionalSequenceRNNOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::BidirectionalSequenceRNNOptionsT *>(value) : nullptr;
   }
-  const tflite::BidirectionalSequenceRNNOptionsT *AsBidirectionalSequenceRNNOptions() const {
+  const tflite_micro::BidirectionalSequenceRNNOptionsT *AsBidirectionalSequenceRNNOptions() const {
     return type == BuiltinOptions_BidirectionalSequenceRNNOptions ?
-      reinterpret_cast<const tflite::BidirectionalSequenceRNNOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::BidirectionalSequenceRNNOptionsT *>(value) : nullptr;
   }
-  tflite::UnidirectionalSequenceLSTMOptionsT *AsUnidirectionalSequenceLSTMOptions() {
+  tflite_micro::UnidirectionalSequenceLSTMOptionsT *AsUnidirectionalSequenceLSTMOptions() {
     return type == BuiltinOptions_UnidirectionalSequenceLSTMOptions ?
-      reinterpret_cast<tflite::UnidirectionalSequenceLSTMOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::UnidirectionalSequenceLSTMOptionsT *>(value) : nullptr;
   }
-  const tflite::UnidirectionalSequenceLSTMOptionsT *AsUnidirectionalSequenceLSTMOptions() const {
+  const tflite_micro::UnidirectionalSequenceLSTMOptionsT *AsUnidirectionalSequenceLSTMOptions() const {
     return type == BuiltinOptions_UnidirectionalSequenceLSTMOptions ?
-      reinterpret_cast<const tflite::UnidirectionalSequenceLSTMOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::UnidirectionalSequenceLSTMOptionsT *>(value) : nullptr;
   }
-  tflite::FloorModOptionsT *AsFloorModOptions() {
+  tflite_micro::FloorModOptionsT *AsFloorModOptions() {
     return type == BuiltinOptions_FloorModOptions ?
-      reinterpret_cast<tflite::FloorModOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::FloorModOptionsT *>(value) : nullptr;
   }
-  const tflite::FloorModOptionsT *AsFloorModOptions() const {
+  const tflite_micro::FloorModOptionsT *AsFloorModOptions() const {
     return type == BuiltinOptions_FloorModOptions ?
-      reinterpret_cast<const tflite::FloorModOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::FloorModOptionsT *>(value) : nullptr;
   }
-  tflite::RangeOptionsT *AsRangeOptions() {
+  tflite_micro::RangeOptionsT *AsRangeOptions() {
     return type == BuiltinOptions_RangeOptions ?
-      reinterpret_cast<tflite::RangeOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::RangeOptionsT *>(value) : nullptr;
   }
-  const tflite::RangeOptionsT *AsRangeOptions() const {
+  const tflite_micro::RangeOptionsT *AsRangeOptions() const {
     return type == BuiltinOptions_RangeOptions ?
-      reinterpret_cast<const tflite::RangeOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::RangeOptionsT *>(value) : nullptr;
   }
-  tflite::ResizeNearestNeighborOptionsT *AsResizeNearestNeighborOptions() {
+  tflite_micro::ResizeNearestNeighborOptionsT *AsResizeNearestNeighborOptions() {
     return type == BuiltinOptions_ResizeNearestNeighborOptions ?
-      reinterpret_cast<tflite::ResizeNearestNeighborOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::ResizeNearestNeighborOptionsT *>(value) : nullptr;
   }
-  const tflite::ResizeNearestNeighborOptionsT *AsResizeNearestNeighborOptions() const {
+  const tflite_micro::ResizeNearestNeighborOptionsT *AsResizeNearestNeighborOptions() const {
     return type == BuiltinOptions_ResizeNearestNeighborOptions ?
-      reinterpret_cast<const tflite::ResizeNearestNeighborOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::ResizeNearestNeighborOptionsT *>(value) : nullptr;
   }
-  tflite::LeakyReluOptionsT *AsLeakyReluOptions() {
+  tflite_micro::LeakyReluOptionsT *AsLeakyReluOptions() {
     return type == BuiltinOptions_LeakyReluOptions ?
-      reinterpret_cast<tflite::LeakyReluOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::LeakyReluOptionsT *>(value) : nullptr;
   }
-  const tflite::LeakyReluOptionsT *AsLeakyReluOptions() const {
+  const tflite_micro::LeakyReluOptionsT *AsLeakyReluOptions() const {
     return type == BuiltinOptions_LeakyReluOptions ?
-      reinterpret_cast<const tflite::LeakyReluOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::LeakyReluOptionsT *>(value) : nullptr;
   }
-  tflite::SquaredDifferenceOptionsT *AsSquaredDifferenceOptions() {
+  tflite_micro::SquaredDifferenceOptionsT *AsSquaredDifferenceOptions() {
     return type == BuiltinOptions_SquaredDifferenceOptions ?
-      reinterpret_cast<tflite::SquaredDifferenceOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::SquaredDifferenceOptionsT *>(value) : nullptr;
   }
-  const tflite::SquaredDifferenceOptionsT *AsSquaredDifferenceOptions() const {
+  const tflite_micro::SquaredDifferenceOptionsT *AsSquaredDifferenceOptions() const {
     return type == BuiltinOptions_SquaredDifferenceOptions ?
-      reinterpret_cast<const tflite::SquaredDifferenceOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::SquaredDifferenceOptionsT *>(value) : nullptr;
   }
-  tflite::MirrorPadOptionsT *AsMirrorPadOptions() {
+  tflite_micro::MirrorPadOptionsT *AsMirrorPadOptions() {
     return type == BuiltinOptions_MirrorPadOptions ?
-      reinterpret_cast<tflite::MirrorPadOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::MirrorPadOptionsT *>(value) : nullptr;
   }
-  const tflite::MirrorPadOptionsT *AsMirrorPadOptions() const {
+  const tflite_micro::MirrorPadOptionsT *AsMirrorPadOptions() const {
     return type == BuiltinOptions_MirrorPadOptions ?
-      reinterpret_cast<const tflite::MirrorPadOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::MirrorPadOptionsT *>(value) : nullptr;
   }
-  tflite::AbsOptionsT *AsAbsOptions() {
+  tflite_micro::AbsOptionsT *AsAbsOptions() {
     return type == BuiltinOptions_AbsOptions ?
-      reinterpret_cast<tflite::AbsOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::AbsOptionsT *>(value) : nullptr;
   }
-  const tflite::AbsOptionsT *AsAbsOptions() const {
+  const tflite_micro::AbsOptionsT *AsAbsOptions() const {
     return type == BuiltinOptions_AbsOptions ?
-      reinterpret_cast<const tflite::AbsOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::AbsOptionsT *>(value) : nullptr;
   }
-  tflite::SplitVOptionsT *AsSplitVOptions() {
+  tflite_micro::SplitVOptionsT *AsSplitVOptions() {
     return type == BuiltinOptions_SplitVOptions ?
-      reinterpret_cast<tflite::SplitVOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::SplitVOptionsT *>(value) : nullptr;
   }
-  const tflite::SplitVOptionsT *AsSplitVOptions() const {
+  const tflite_micro::SplitVOptionsT *AsSplitVOptions() const {
     return type == BuiltinOptions_SplitVOptions ?
-      reinterpret_cast<const tflite::SplitVOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::SplitVOptionsT *>(value) : nullptr;
   }
-  tflite::UniqueOptionsT *AsUniqueOptions() {
+  tflite_micro::UniqueOptionsT *AsUniqueOptions() {
     return type == BuiltinOptions_UniqueOptions ?
-      reinterpret_cast<tflite::UniqueOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::UniqueOptionsT *>(value) : nullptr;
   }
-  const tflite::UniqueOptionsT *AsUniqueOptions() const {
+  const tflite_micro::UniqueOptionsT *AsUniqueOptions() const {
     return type == BuiltinOptions_UniqueOptions ?
-      reinterpret_cast<const tflite::UniqueOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::UniqueOptionsT *>(value) : nullptr;
   }
-  tflite::ReverseV2OptionsT *AsReverseV2Options() {
+  tflite_micro::ReverseV2OptionsT *AsReverseV2Options() {
     return type == BuiltinOptions_ReverseV2Options ?
-      reinterpret_cast<tflite::ReverseV2OptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::ReverseV2OptionsT *>(value) : nullptr;
   }
-  const tflite::ReverseV2OptionsT *AsReverseV2Options() const {
+  const tflite_micro::ReverseV2OptionsT *AsReverseV2Options() const {
     return type == BuiltinOptions_ReverseV2Options ?
-      reinterpret_cast<const tflite::ReverseV2OptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::ReverseV2OptionsT *>(value) : nullptr;
   }
-  tflite::AddNOptionsT *AsAddNOptions() {
+  tflite_micro::AddNOptionsT *AsAddNOptions() {
     return type == BuiltinOptions_AddNOptions ?
-      reinterpret_cast<tflite::AddNOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::AddNOptionsT *>(value) : nullptr;
   }
-  const tflite::AddNOptionsT *AsAddNOptions() const {
+  const tflite_micro::AddNOptionsT *AsAddNOptions() const {
     return type == BuiltinOptions_AddNOptions ?
-      reinterpret_cast<const tflite::AddNOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::AddNOptionsT *>(value) : nullptr;
   }
-  tflite::GatherNdOptionsT *AsGatherNdOptions() {
+  tflite_micro::GatherNdOptionsT *AsGatherNdOptions() {
     return type == BuiltinOptions_GatherNdOptions ?
-      reinterpret_cast<tflite::GatherNdOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::GatherNdOptionsT *>(value) : nullptr;
   }
-  const tflite::GatherNdOptionsT *AsGatherNdOptions() const {
+  const tflite_micro::GatherNdOptionsT *AsGatherNdOptions() const {
     return type == BuiltinOptions_GatherNdOptions ?
-      reinterpret_cast<const tflite::GatherNdOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::GatherNdOptionsT *>(value) : nullptr;
   }
-  tflite::CosOptionsT *AsCosOptions() {
+  tflite_micro::CosOptionsT *AsCosOptions() {
     return type == BuiltinOptions_CosOptions ?
-      reinterpret_cast<tflite::CosOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::CosOptionsT *>(value) : nullptr;
   }
-  const tflite::CosOptionsT *AsCosOptions() const {
+  const tflite_micro::CosOptionsT *AsCosOptions() const {
     return type == BuiltinOptions_CosOptions ?
-      reinterpret_cast<const tflite::CosOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::CosOptionsT *>(value) : nullptr;
   }
-  tflite::WhereOptionsT *AsWhereOptions() {
+  tflite_micro::WhereOptionsT *AsWhereOptions() {
     return type == BuiltinOptions_WhereOptions ?
-      reinterpret_cast<tflite::WhereOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::WhereOptionsT *>(value) : nullptr;
   }
-  const tflite::WhereOptionsT *AsWhereOptions() const {
+  const tflite_micro::WhereOptionsT *AsWhereOptions() const {
     return type == BuiltinOptions_WhereOptions ?
-      reinterpret_cast<const tflite::WhereOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::WhereOptionsT *>(value) : nullptr;
   }
-  tflite::RankOptionsT *AsRankOptions() {
+  tflite_micro::RankOptionsT *AsRankOptions() {
     return type == BuiltinOptions_RankOptions ?
-      reinterpret_cast<tflite::RankOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::RankOptionsT *>(value) : nullptr;
   }
-  const tflite::RankOptionsT *AsRankOptions() const {
+  const tflite_micro::RankOptionsT *AsRankOptions() const {
     return type == BuiltinOptions_RankOptions ?
-      reinterpret_cast<const tflite::RankOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::RankOptionsT *>(value) : nullptr;
   }
-  tflite::ReverseSequenceOptionsT *AsReverseSequenceOptions() {
+  tflite_micro::ReverseSequenceOptionsT *AsReverseSequenceOptions() {
     return type == BuiltinOptions_ReverseSequenceOptions ?
-      reinterpret_cast<tflite::ReverseSequenceOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::ReverseSequenceOptionsT *>(value) : nullptr;
   }
-  const tflite::ReverseSequenceOptionsT *AsReverseSequenceOptions() const {
+  const tflite_micro::ReverseSequenceOptionsT *AsReverseSequenceOptions() const {
     return type == BuiltinOptions_ReverseSequenceOptions ?
-      reinterpret_cast<const tflite::ReverseSequenceOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::ReverseSequenceOptionsT *>(value) : nullptr;
   }
-  tflite::MatrixDiagOptionsT *AsMatrixDiagOptions() {
+  tflite_micro::MatrixDiagOptionsT *AsMatrixDiagOptions() {
     return type == BuiltinOptions_MatrixDiagOptions ?
-      reinterpret_cast<tflite::MatrixDiagOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::MatrixDiagOptionsT *>(value) : nullptr;
   }
-  const tflite::MatrixDiagOptionsT *AsMatrixDiagOptions() const {
+  const tflite_micro::MatrixDiagOptionsT *AsMatrixDiagOptions() const {
     return type == BuiltinOptions_MatrixDiagOptions ?
-      reinterpret_cast<const tflite::MatrixDiagOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::MatrixDiagOptionsT *>(value) : nullptr;
   }
-  tflite::QuantizeOptionsT *AsQuantizeOptions() {
+  tflite_micro::QuantizeOptionsT *AsQuantizeOptions() {
     return type == BuiltinOptions_QuantizeOptions ?
-      reinterpret_cast<tflite::QuantizeOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::QuantizeOptionsT *>(value) : nullptr;
   }
-  const tflite::QuantizeOptionsT *AsQuantizeOptions() const {
+  const tflite_micro::QuantizeOptionsT *AsQuantizeOptions() const {
     return type == BuiltinOptions_QuantizeOptions ?
-      reinterpret_cast<const tflite::QuantizeOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::QuantizeOptionsT *>(value) : nullptr;
   }
-  tflite::MatrixSetDiagOptionsT *AsMatrixSetDiagOptions() {
+  tflite_micro::MatrixSetDiagOptionsT *AsMatrixSetDiagOptions() {
     return type == BuiltinOptions_MatrixSetDiagOptions ?
-      reinterpret_cast<tflite::MatrixSetDiagOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::MatrixSetDiagOptionsT *>(value) : nullptr;
   }
-  const tflite::MatrixSetDiagOptionsT *AsMatrixSetDiagOptions() const {
+  const tflite_micro::MatrixSetDiagOptionsT *AsMatrixSetDiagOptions() const {
     return type == BuiltinOptions_MatrixSetDiagOptions ?
-      reinterpret_cast<const tflite::MatrixSetDiagOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::MatrixSetDiagOptionsT *>(value) : nullptr;
   }
-  tflite::HardSwishOptionsT *AsHardSwishOptions() {
+  tflite_micro::HardSwishOptionsT *AsHardSwishOptions() {
     return type == BuiltinOptions_HardSwishOptions ?
-      reinterpret_cast<tflite::HardSwishOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::HardSwishOptionsT *>(value) : nullptr;
   }
-  const tflite::HardSwishOptionsT *AsHardSwishOptions() const {
+  const tflite_micro::HardSwishOptionsT *AsHardSwishOptions() const {
     return type == BuiltinOptions_HardSwishOptions ?
-      reinterpret_cast<const tflite::HardSwishOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::HardSwishOptionsT *>(value) : nullptr;
   }
-  tflite::IfOptionsT *AsIfOptions() {
+  tflite_micro::IfOptionsT *AsIfOptions() {
     return type == BuiltinOptions_IfOptions ?
-      reinterpret_cast<tflite::IfOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::IfOptionsT *>(value) : nullptr;
   }
-  const tflite::IfOptionsT *AsIfOptions() const {
+  const tflite_micro::IfOptionsT *AsIfOptions() const {
     return type == BuiltinOptions_IfOptions ?
-      reinterpret_cast<const tflite::IfOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::IfOptionsT *>(value) : nullptr;
   }
-  tflite::WhileOptionsT *AsWhileOptions() {
+  tflite_micro::WhileOptionsT *AsWhileOptions() {
     return type == BuiltinOptions_WhileOptions ?
-      reinterpret_cast<tflite::WhileOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::WhileOptionsT *>(value) : nullptr;
   }
-  const tflite::WhileOptionsT *AsWhileOptions() const {
+  const tflite_micro::WhileOptionsT *AsWhileOptions() const {
     return type == BuiltinOptions_WhileOptions ?
-      reinterpret_cast<const tflite::WhileOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::WhileOptionsT *>(value) : nullptr;
   }
-  tflite::DepthToSpaceOptionsT *AsDepthToSpaceOptions() {
+  tflite_micro::DepthToSpaceOptionsT *AsDepthToSpaceOptions() {
     return type == BuiltinOptions_DepthToSpaceOptions ?
-      reinterpret_cast<tflite::DepthToSpaceOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::DepthToSpaceOptionsT *>(value) : nullptr;
   }
-  const tflite::DepthToSpaceOptionsT *AsDepthToSpaceOptions() const {
+  const tflite_micro::DepthToSpaceOptionsT *AsDepthToSpaceOptions() const {
     return type == BuiltinOptions_DepthToSpaceOptions ?
-      reinterpret_cast<const tflite::DepthToSpaceOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::DepthToSpaceOptionsT *>(value) : nullptr;
   }
-  tflite::NonMaxSuppressionV4OptionsT *AsNonMaxSuppressionV4Options() {
+  tflite_micro::NonMaxSuppressionV4OptionsT *AsNonMaxSuppressionV4Options() {
     return type == BuiltinOptions_NonMaxSuppressionV4Options ?
-      reinterpret_cast<tflite::NonMaxSuppressionV4OptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::NonMaxSuppressionV4OptionsT *>(value) : nullptr;
   }
-  const tflite::NonMaxSuppressionV4OptionsT *AsNonMaxSuppressionV4Options() const {
+  const tflite_micro::NonMaxSuppressionV4OptionsT *AsNonMaxSuppressionV4Options() const {
     return type == BuiltinOptions_NonMaxSuppressionV4Options ?
-      reinterpret_cast<const tflite::NonMaxSuppressionV4OptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::NonMaxSuppressionV4OptionsT *>(value) : nullptr;
   }
-  tflite::NonMaxSuppressionV5OptionsT *AsNonMaxSuppressionV5Options() {
+  tflite_micro::NonMaxSuppressionV5OptionsT *AsNonMaxSuppressionV5Options() {
     return type == BuiltinOptions_NonMaxSuppressionV5Options ?
-      reinterpret_cast<tflite::NonMaxSuppressionV5OptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::NonMaxSuppressionV5OptionsT *>(value) : nullptr;
   }
-  const tflite::NonMaxSuppressionV5OptionsT *AsNonMaxSuppressionV5Options() const {
+  const tflite_micro::NonMaxSuppressionV5OptionsT *AsNonMaxSuppressionV5Options() const {
     return type == BuiltinOptions_NonMaxSuppressionV5Options ?
-      reinterpret_cast<const tflite::NonMaxSuppressionV5OptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::NonMaxSuppressionV5OptionsT *>(value) : nullptr;
   }
-  tflite::ScatterNdOptionsT *AsScatterNdOptions() {
+  tflite_micro::ScatterNdOptionsT *AsScatterNdOptions() {
     return type == BuiltinOptions_ScatterNdOptions ?
-      reinterpret_cast<tflite::ScatterNdOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::ScatterNdOptionsT *>(value) : nullptr;
   }
-  const tflite::ScatterNdOptionsT *AsScatterNdOptions() const {
+  const tflite_micro::ScatterNdOptionsT *AsScatterNdOptions() const {
     return type == BuiltinOptions_ScatterNdOptions ?
-      reinterpret_cast<const tflite::ScatterNdOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::ScatterNdOptionsT *>(value) : nullptr;
   }
-  tflite::SelectV2OptionsT *AsSelectV2Options() {
+  tflite_micro::SelectV2OptionsT *AsSelectV2Options() {
     return type == BuiltinOptions_SelectV2Options ?
-      reinterpret_cast<tflite::SelectV2OptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::SelectV2OptionsT *>(value) : nullptr;
   }
-  const tflite::SelectV2OptionsT *AsSelectV2Options() const {
+  const tflite_micro::SelectV2OptionsT *AsSelectV2Options() const {
     return type == BuiltinOptions_SelectV2Options ?
-      reinterpret_cast<const tflite::SelectV2OptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::SelectV2OptionsT *>(value) : nullptr;
   }
-  tflite::DensifyOptionsT *AsDensifyOptions() {
+  tflite_micro::DensifyOptionsT *AsDensifyOptions() {
     return type == BuiltinOptions_DensifyOptions ?
-      reinterpret_cast<tflite::DensifyOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::DensifyOptionsT *>(value) : nullptr;
   }
-  const tflite::DensifyOptionsT *AsDensifyOptions() const {
+  const tflite_micro::DensifyOptionsT *AsDensifyOptions() const {
     return type == BuiltinOptions_DensifyOptions ?
-      reinterpret_cast<const tflite::DensifyOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::DensifyOptionsT *>(value) : nullptr;
   }
-  tflite::SegmentSumOptionsT *AsSegmentSumOptions() {
+  tflite_micro::SegmentSumOptionsT *AsSegmentSumOptions() {
     return type == BuiltinOptions_SegmentSumOptions ?
-      reinterpret_cast<tflite::SegmentSumOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::SegmentSumOptionsT *>(value) : nullptr;
   }
-  const tflite::SegmentSumOptionsT *AsSegmentSumOptions() const {
+  const tflite_micro::SegmentSumOptionsT *AsSegmentSumOptions() const {
     return type == BuiltinOptions_SegmentSumOptions ?
-      reinterpret_cast<const tflite::SegmentSumOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::SegmentSumOptionsT *>(value) : nullptr;
   }
-  tflite::BatchMatMulOptionsT *AsBatchMatMulOptions() {
+  tflite_micro::BatchMatMulOptionsT *AsBatchMatMulOptions() {
     return type == BuiltinOptions_BatchMatMulOptions ?
-      reinterpret_cast<tflite::BatchMatMulOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::BatchMatMulOptionsT *>(value) : nullptr;
   }
-  const tflite::BatchMatMulOptionsT *AsBatchMatMulOptions() const {
+  const tflite_micro::BatchMatMulOptionsT *AsBatchMatMulOptions() const {
     return type == BuiltinOptions_BatchMatMulOptions ?
-      reinterpret_cast<const tflite::BatchMatMulOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::BatchMatMulOptionsT *>(value) : nullptr;
   }
-  tflite::CumsumOptionsT *AsCumsumOptions() {
+  tflite_micro::CumsumOptionsT *AsCumsumOptions() {
     return type == BuiltinOptions_CumsumOptions ?
-      reinterpret_cast<tflite::CumsumOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::CumsumOptionsT *>(value) : nullptr;
   }
-  const tflite::CumsumOptionsT *AsCumsumOptions() const {
+  const tflite_micro::CumsumOptionsT *AsCumsumOptions() const {
     return type == BuiltinOptions_CumsumOptions ?
-      reinterpret_cast<const tflite::CumsumOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::CumsumOptionsT *>(value) : nullptr;
   }
-  tflite::CallOnceOptionsT *AsCallOnceOptions() {
+  tflite_micro::CallOnceOptionsT *AsCallOnceOptions() {
     return type == BuiltinOptions_CallOnceOptions ?
-      reinterpret_cast<tflite::CallOnceOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::CallOnceOptionsT *>(value) : nullptr;
   }
-  const tflite::CallOnceOptionsT *AsCallOnceOptions() const {
+  const tflite_micro::CallOnceOptionsT *AsCallOnceOptions() const {
     return type == BuiltinOptions_CallOnceOptions ?
-      reinterpret_cast<const tflite::CallOnceOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::CallOnceOptionsT *>(value) : nullptr;
   }
-  tflite::BroadcastToOptionsT *AsBroadcastToOptions() {
+  tflite_micro::BroadcastToOptionsT *AsBroadcastToOptions() {
     return type == BuiltinOptions_BroadcastToOptions ?
-      reinterpret_cast<tflite::BroadcastToOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::BroadcastToOptionsT *>(value) : nullptr;
   }
-  const tflite::BroadcastToOptionsT *AsBroadcastToOptions() const {
+  const tflite_micro::BroadcastToOptionsT *AsBroadcastToOptions() const {
     return type == BuiltinOptions_BroadcastToOptions ?
-      reinterpret_cast<const tflite::BroadcastToOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::BroadcastToOptionsT *>(value) : nullptr;
   }
-  tflite::Rfft2dOptionsT *AsRfft2dOptions() {
+  tflite_micro::Rfft2dOptionsT *AsRfft2dOptions() {
     return type == BuiltinOptions_Rfft2dOptions ?
-      reinterpret_cast<tflite::Rfft2dOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::Rfft2dOptionsT *>(value) : nullptr;
   }
-  const tflite::Rfft2dOptionsT *AsRfft2dOptions() const {
+  const tflite_micro::Rfft2dOptionsT *AsRfft2dOptions() const {
     return type == BuiltinOptions_Rfft2dOptions ?
-      reinterpret_cast<const tflite::Rfft2dOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::Rfft2dOptionsT *>(value) : nullptr;
   }
-  tflite::Conv3DOptionsT *AsConv3DOptions() {
+  tflite_micro::Conv3DOptionsT *AsConv3DOptions() {
     return type == BuiltinOptions_Conv3DOptions ?
-      reinterpret_cast<tflite::Conv3DOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::Conv3DOptionsT *>(value) : nullptr;
   }
-  const tflite::Conv3DOptionsT *AsConv3DOptions() const {
+  const tflite_micro::Conv3DOptionsT *AsConv3DOptions() const {
     return type == BuiltinOptions_Conv3DOptions ?
-      reinterpret_cast<const tflite::Conv3DOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::Conv3DOptionsT *>(value) : nullptr;
   }
-  tflite::HashtableOptionsT *AsHashtableOptions() {
+  tflite_micro::HashtableOptionsT *AsHashtableOptions() {
     return type == BuiltinOptions_HashtableOptions ?
-      reinterpret_cast<tflite::HashtableOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::HashtableOptionsT *>(value) : nullptr;
   }
-  const tflite::HashtableOptionsT *AsHashtableOptions() const {
+  const tflite_micro::HashtableOptionsT *AsHashtableOptions() const {
     return type == BuiltinOptions_HashtableOptions ?
-      reinterpret_cast<const tflite::HashtableOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::HashtableOptionsT *>(value) : nullptr;
   }
-  tflite::HashtableFindOptionsT *AsHashtableFindOptions() {
+  tflite_micro::HashtableFindOptionsT *AsHashtableFindOptions() {
     return type == BuiltinOptions_HashtableFindOptions ?
-      reinterpret_cast<tflite::HashtableFindOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::HashtableFindOptionsT *>(value) : nullptr;
   }
-  const tflite::HashtableFindOptionsT *AsHashtableFindOptions() const {
+  const tflite_micro::HashtableFindOptionsT *AsHashtableFindOptions() const {
     return type == BuiltinOptions_HashtableFindOptions ?
-      reinterpret_cast<const tflite::HashtableFindOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::HashtableFindOptionsT *>(value) : nullptr;
   }
-  tflite::HashtableImportOptionsT *AsHashtableImportOptions() {
+  tflite_micro::HashtableImportOptionsT *AsHashtableImportOptions() {
     return type == BuiltinOptions_HashtableImportOptions ?
-      reinterpret_cast<tflite::HashtableImportOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::HashtableImportOptionsT *>(value) : nullptr;
   }
-  const tflite::HashtableImportOptionsT *AsHashtableImportOptions() const {
+  const tflite_micro::HashtableImportOptionsT *AsHashtableImportOptions() const {
     return type == BuiltinOptions_HashtableImportOptions ?
-      reinterpret_cast<const tflite::HashtableImportOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::HashtableImportOptionsT *>(value) : nullptr;
   }
-  tflite::HashtableSizeOptionsT *AsHashtableSizeOptions() {
+  tflite_micro::HashtableSizeOptionsT *AsHashtableSizeOptions() {
     return type == BuiltinOptions_HashtableSizeOptions ?
-      reinterpret_cast<tflite::HashtableSizeOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::HashtableSizeOptionsT *>(value) : nullptr;
   }
-  const tflite::HashtableSizeOptionsT *AsHashtableSizeOptions() const {
+  const tflite_micro::HashtableSizeOptionsT *AsHashtableSizeOptions() const {
     return type == BuiltinOptions_HashtableSizeOptions ?
-      reinterpret_cast<const tflite::HashtableSizeOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::HashtableSizeOptionsT *>(value) : nullptr;
   }
-  tflite::VarHandleOptionsT *AsVarHandleOptions() {
+  tflite_micro::VarHandleOptionsT *AsVarHandleOptions() {
     return type == BuiltinOptions_VarHandleOptions ?
-      reinterpret_cast<tflite::VarHandleOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::VarHandleOptionsT *>(value) : nullptr;
   }
-  const tflite::VarHandleOptionsT *AsVarHandleOptions() const {
+  const tflite_micro::VarHandleOptionsT *AsVarHandleOptions() const {
     return type == BuiltinOptions_VarHandleOptions ?
-      reinterpret_cast<const tflite::VarHandleOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::VarHandleOptionsT *>(value) : nullptr;
   }
-  tflite::ReadVariableOptionsT *AsReadVariableOptions() {
+  tflite_micro::ReadVariableOptionsT *AsReadVariableOptions() {
     return type == BuiltinOptions_ReadVariableOptions ?
-      reinterpret_cast<tflite::ReadVariableOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::ReadVariableOptionsT *>(value) : nullptr;
   }
-  const tflite::ReadVariableOptionsT *AsReadVariableOptions() const {
+  const tflite_micro::ReadVariableOptionsT *AsReadVariableOptions() const {
     return type == BuiltinOptions_ReadVariableOptions ?
-      reinterpret_cast<const tflite::ReadVariableOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::ReadVariableOptionsT *>(value) : nullptr;
   }
-  tflite::AssignVariableOptionsT *AsAssignVariableOptions() {
+  tflite_micro::AssignVariableOptionsT *AsAssignVariableOptions() {
     return type == BuiltinOptions_AssignVariableOptions ?
-      reinterpret_cast<tflite::AssignVariableOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::AssignVariableOptionsT *>(value) : nullptr;
   }
-  const tflite::AssignVariableOptionsT *AsAssignVariableOptions() const {
+  const tflite_micro::AssignVariableOptionsT *AsAssignVariableOptions() const {
     return type == BuiltinOptions_AssignVariableOptions ?
-      reinterpret_cast<const tflite::AssignVariableOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::AssignVariableOptionsT *>(value) : nullptr;
   }
-  tflite::RandomOptionsT *AsRandomOptions() {
+  tflite_micro::RandomOptionsT *AsRandomOptions() {
     return type == BuiltinOptions_RandomOptions ?
-      reinterpret_cast<tflite::RandomOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::RandomOptionsT *>(value) : nullptr;
   }
-  const tflite::RandomOptionsT *AsRandomOptions() const {
+  const tflite_micro::RandomOptionsT *AsRandomOptions() const {
     return type == BuiltinOptions_RandomOptions ?
-      reinterpret_cast<const tflite::RandomOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::RandomOptionsT *>(value) : nullptr;
   }
-  tflite::BucketizeOptionsT *AsBucketizeOptions() {
+  tflite_micro::BucketizeOptionsT *AsBucketizeOptions() {
     return type == BuiltinOptions_BucketizeOptions ?
-      reinterpret_cast<tflite::BucketizeOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::BucketizeOptionsT *>(value) : nullptr;
   }
-  const tflite::BucketizeOptionsT *AsBucketizeOptions() const {
+  const tflite_micro::BucketizeOptionsT *AsBucketizeOptions() const {
     return type == BuiltinOptions_BucketizeOptions ?
-      reinterpret_cast<const tflite::BucketizeOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::BucketizeOptionsT *>(value) : nullptr;
   }
-  tflite::GeluOptionsT *AsGeluOptions() {
+  tflite_micro::GeluOptionsT *AsGeluOptions() {
     return type == BuiltinOptions_GeluOptions ?
-      reinterpret_cast<tflite::GeluOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::GeluOptionsT *>(value) : nullptr;
   }
-  const tflite::GeluOptionsT *AsGeluOptions() const {
+  const tflite_micro::GeluOptionsT *AsGeluOptions() const {
     return type == BuiltinOptions_GeluOptions ?
-      reinterpret_cast<const tflite::GeluOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::GeluOptionsT *>(value) : nullptr;
   }
-  tflite::DynamicUpdateSliceOptionsT *AsDynamicUpdateSliceOptions() {
+  tflite_micro::DynamicUpdateSliceOptionsT *AsDynamicUpdateSliceOptions() {
     return type == BuiltinOptions_DynamicUpdateSliceOptions ?
-      reinterpret_cast<tflite::DynamicUpdateSliceOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::DynamicUpdateSliceOptionsT *>(value) : nullptr;
   }
-  const tflite::DynamicUpdateSliceOptionsT *AsDynamicUpdateSliceOptions() const {
+  const tflite_micro::DynamicUpdateSliceOptionsT *AsDynamicUpdateSliceOptions() const {
     return type == BuiltinOptions_DynamicUpdateSliceOptions ?
-      reinterpret_cast<const tflite::DynamicUpdateSliceOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::DynamicUpdateSliceOptionsT *>(value) : nullptr;
   }
-  tflite::UnsortedSegmentProdOptionsT *AsUnsortedSegmentProdOptions() {
+  tflite_micro::UnsortedSegmentProdOptionsT *AsUnsortedSegmentProdOptions() {
     return type == BuiltinOptions_UnsortedSegmentProdOptions ?
-      reinterpret_cast<tflite::UnsortedSegmentProdOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::UnsortedSegmentProdOptionsT *>(value) : nullptr;
   }
-  const tflite::UnsortedSegmentProdOptionsT *AsUnsortedSegmentProdOptions() const {
+  const tflite_micro::UnsortedSegmentProdOptionsT *AsUnsortedSegmentProdOptions() const {
     return type == BuiltinOptions_UnsortedSegmentProdOptions ?
-      reinterpret_cast<const tflite::UnsortedSegmentProdOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::UnsortedSegmentProdOptionsT *>(value) : nullptr;
   }
-  tflite::UnsortedSegmentMaxOptionsT *AsUnsortedSegmentMaxOptions() {
+  tflite_micro::UnsortedSegmentMaxOptionsT *AsUnsortedSegmentMaxOptions() {
     return type == BuiltinOptions_UnsortedSegmentMaxOptions ?
-      reinterpret_cast<tflite::UnsortedSegmentMaxOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::UnsortedSegmentMaxOptionsT *>(value) : nullptr;
   }
-  const tflite::UnsortedSegmentMaxOptionsT *AsUnsortedSegmentMaxOptions() const {
+  const tflite_micro::UnsortedSegmentMaxOptionsT *AsUnsortedSegmentMaxOptions() const {
     return type == BuiltinOptions_UnsortedSegmentMaxOptions ?
-      reinterpret_cast<const tflite::UnsortedSegmentMaxOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::UnsortedSegmentMaxOptionsT *>(value) : nullptr;
   }
-  tflite::UnsortedSegmentMinOptionsT *AsUnsortedSegmentMinOptions() {
+  tflite_micro::UnsortedSegmentMinOptionsT *AsUnsortedSegmentMinOptions() {
     return type == BuiltinOptions_UnsortedSegmentMinOptions ?
-      reinterpret_cast<tflite::UnsortedSegmentMinOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::UnsortedSegmentMinOptionsT *>(value) : nullptr;
   }
-  const tflite::UnsortedSegmentMinOptionsT *AsUnsortedSegmentMinOptions() const {
+  const tflite_micro::UnsortedSegmentMinOptionsT *AsUnsortedSegmentMinOptions() const {
     return type == BuiltinOptions_UnsortedSegmentMinOptions ?
-      reinterpret_cast<const tflite::UnsortedSegmentMinOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::UnsortedSegmentMinOptionsT *>(value) : nullptr;
   }
-  tflite::UnsortedSegmentSumOptionsT *AsUnsortedSegmentSumOptions() {
+  tflite_micro::UnsortedSegmentSumOptionsT *AsUnsortedSegmentSumOptions() {
     return type == BuiltinOptions_UnsortedSegmentSumOptions ?
-      reinterpret_cast<tflite::UnsortedSegmentSumOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::UnsortedSegmentSumOptionsT *>(value) : nullptr;
   }
-  const tflite::UnsortedSegmentSumOptionsT *AsUnsortedSegmentSumOptions() const {
+  const tflite_micro::UnsortedSegmentSumOptionsT *AsUnsortedSegmentSumOptions() const {
     return type == BuiltinOptions_UnsortedSegmentSumOptions ?
-      reinterpret_cast<const tflite::UnsortedSegmentSumOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::UnsortedSegmentSumOptionsT *>(value) : nullptr;
   }
-  tflite::ATan2OptionsT *AsATan2Options() {
+  tflite_micro::ATan2OptionsT *AsATan2Options() {
     return type == BuiltinOptions_ATan2Options ?
-      reinterpret_cast<tflite::ATan2OptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::ATan2OptionsT *>(value) : nullptr;
   }
-  const tflite::ATan2OptionsT *AsATan2Options() const {
+  const tflite_micro::ATan2OptionsT *AsATan2Options() const {
     return type == BuiltinOptions_ATan2Options ?
-      reinterpret_cast<const tflite::ATan2OptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::ATan2OptionsT *>(value) : nullptr;
   }
-  tflite::SignOptionsT *AsSignOptions() {
+  tflite_micro::SignOptionsT *AsSignOptions() {
     return type == BuiltinOptions_SignOptions ?
-      reinterpret_cast<tflite::SignOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::SignOptionsT *>(value) : nullptr;
   }
-  const tflite::SignOptionsT *AsSignOptions() const {
+  const tflite_micro::SignOptionsT *AsSignOptions() const {
     return type == BuiltinOptions_SignOptions ?
-      reinterpret_cast<const tflite::SignOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::SignOptionsT *>(value) : nullptr;
   }
-  tflite::BitcastOptionsT *AsBitcastOptions() {
+  tflite_micro::BitcastOptionsT *AsBitcastOptions() {
     return type == BuiltinOptions_BitcastOptions ?
-      reinterpret_cast<tflite::BitcastOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::BitcastOptionsT *>(value) : nullptr;
   }
-  const tflite::BitcastOptionsT *AsBitcastOptions() const {
+  const tflite_micro::BitcastOptionsT *AsBitcastOptions() const {
     return type == BuiltinOptions_BitcastOptions ?
-      reinterpret_cast<const tflite::BitcastOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::BitcastOptionsT *>(value) : nullptr;
   }
-  tflite::BitwiseXorOptionsT *AsBitwiseXorOptions() {
+  tflite_micro::BitwiseXorOptionsT *AsBitwiseXorOptions() {
     return type == BuiltinOptions_BitwiseXorOptions ?
-      reinterpret_cast<tflite::BitwiseXorOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::BitwiseXorOptionsT *>(value) : nullptr;
   }
-  const tflite::BitwiseXorOptionsT *AsBitwiseXorOptions() const {
+  const tflite_micro::BitwiseXorOptionsT *AsBitwiseXorOptions() const {
     return type == BuiltinOptions_BitwiseXorOptions ?
-      reinterpret_cast<const tflite::BitwiseXorOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::BitwiseXorOptionsT *>(value) : nullptr;
   }
-  tflite::RightShiftOptionsT *AsRightShiftOptions() {
+  tflite_micro::RightShiftOptionsT *AsRightShiftOptions() {
     return type == BuiltinOptions_RightShiftOptions ?
-      reinterpret_cast<tflite::RightShiftOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::RightShiftOptionsT *>(value) : nullptr;
   }
-  const tflite::RightShiftOptionsT *AsRightShiftOptions() const {
+  const tflite_micro::RightShiftOptionsT *AsRightShiftOptions() const {
     return type == BuiltinOptions_RightShiftOptions ?
-      reinterpret_cast<const tflite::RightShiftOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::RightShiftOptionsT *>(value) : nullptr;
   }
 };
 
@@ -4188,83 +4188,83 @@ template<typename T> struct BuiltinOptions2Traits {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_NONE;
 };
 
-template<> struct BuiltinOptions2Traits<tflite::StablehloConcatenateOptions> {
+template<> struct BuiltinOptions2Traits<tflite_micro::StablehloConcatenateOptions> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloConcatenateOptions;
 };
 
-template<> struct BuiltinOptions2Traits<tflite::StablehloBroadcastInDimOptions> {
+template<> struct BuiltinOptions2Traits<tflite_micro::StablehloBroadcastInDimOptions> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloBroadcastInDimOptions;
 };
 
-template<> struct BuiltinOptions2Traits<tflite::StablehloSliceOptions> {
+template<> struct BuiltinOptions2Traits<tflite_micro::StablehloSliceOptions> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloSliceOptions;
 };
 
-template<> struct BuiltinOptions2Traits<tflite::StablehloConvolutionOptions> {
+template<> struct BuiltinOptions2Traits<tflite_micro::StablehloConvolutionOptions> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloConvolutionOptions;
 };
 
-template<> struct BuiltinOptions2Traits<tflite::StablehloCustomCallOptions> {
+template<> struct BuiltinOptions2Traits<tflite_micro::StablehloCustomCallOptions> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloCustomCallOptions;
 };
 
-template<> struct BuiltinOptions2Traits<tflite::StablehloReduceOptions> {
+template<> struct BuiltinOptions2Traits<tflite_micro::StablehloReduceOptions> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloReduceOptions;
 };
 
-template<> struct BuiltinOptions2Traits<tflite::StablehloScatterOptions> {
+template<> struct BuiltinOptions2Traits<tflite_micro::StablehloScatterOptions> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloScatterOptions;
 };
 
-template<> struct BuiltinOptions2Traits<tflite::StablehloCompareOptions> {
+template<> struct BuiltinOptions2Traits<tflite_micro::StablehloCompareOptions> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloCompareOptions;
 };
 
-template<> struct BuiltinOptions2Traits<tflite::StablehloDynamicSliceOptions> {
+template<> struct BuiltinOptions2Traits<tflite_micro::StablehloDynamicSliceOptions> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloDynamicSliceOptions;
 };
 
-template<> struct BuiltinOptions2Traits<tflite::StablehloPadOptions> {
+template<> struct BuiltinOptions2Traits<tflite_micro::StablehloPadOptions> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloPadOptions;
 };
 
-template<> struct BuiltinOptions2Traits<tflite::StablehloIotaOptions> {
+template<> struct BuiltinOptions2Traits<tflite_micro::StablehloIotaOptions> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloIotaOptions;
 };
 
-template<> struct BuiltinOptions2Traits<tflite::StablehloDotGeneralOptions> {
+template<> struct BuiltinOptions2Traits<tflite_micro::StablehloDotGeneralOptions> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloDotGeneralOptions;
 };
 
-template<> struct BuiltinOptions2Traits<tflite::StablehloReduceWindowOptions> {
+template<> struct BuiltinOptions2Traits<tflite_micro::StablehloReduceWindowOptions> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloReduceWindowOptions;
 };
 
-template<> struct BuiltinOptions2Traits<tflite::StablehloSortOptions> {
+template<> struct BuiltinOptions2Traits<tflite_micro::StablehloSortOptions> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloSortOptions;
 };
 
-template<> struct BuiltinOptions2Traits<tflite::StablehloWhileOptions> {
+template<> struct BuiltinOptions2Traits<tflite_micro::StablehloWhileOptions> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloWhileOptions;
 };
 
-template<> struct BuiltinOptions2Traits<tflite::StablehloGatherOptions> {
+template<> struct BuiltinOptions2Traits<tflite_micro::StablehloGatherOptions> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloGatherOptions;
 };
 
-template<> struct BuiltinOptions2Traits<tflite::StablehloTransposeOptions> {
+template<> struct BuiltinOptions2Traits<tflite_micro::StablehloTransposeOptions> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloTransposeOptions;
 };
 
-template<> struct BuiltinOptions2Traits<tflite::DilateOptions> {
+template<> struct BuiltinOptions2Traits<tflite_micro::DilateOptions> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_DilateOptions;
 };
 
-template<> struct BuiltinOptions2Traits<tflite::StablehloRngBitGeneratorOptions> {
+template<> struct BuiltinOptions2Traits<tflite_micro::StablehloRngBitGeneratorOptions> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloRngBitGeneratorOptions;
 };
 
-template<> struct BuiltinOptions2Traits<tflite::ReduceWindowOptions> {
+template<> struct BuiltinOptions2Traits<tflite_micro::ReduceWindowOptions> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_ReduceWindowOptions;
 };
 
@@ -4272,83 +4272,83 @@ template<typename T> struct BuiltinOptions2UnionTraits {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_NONE;
 };
 
-template<> struct BuiltinOptions2UnionTraits<tflite::StablehloConcatenateOptionsT> {
+template<> struct BuiltinOptions2UnionTraits<tflite_micro::StablehloConcatenateOptionsT> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloConcatenateOptions;
 };
 
-template<> struct BuiltinOptions2UnionTraits<tflite::StablehloBroadcastInDimOptionsT> {
+template<> struct BuiltinOptions2UnionTraits<tflite_micro::StablehloBroadcastInDimOptionsT> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloBroadcastInDimOptions;
 };
 
-template<> struct BuiltinOptions2UnionTraits<tflite::StablehloSliceOptionsT> {
+template<> struct BuiltinOptions2UnionTraits<tflite_micro::StablehloSliceOptionsT> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloSliceOptions;
 };
 
-template<> struct BuiltinOptions2UnionTraits<tflite::StablehloConvolutionOptionsT> {
+template<> struct BuiltinOptions2UnionTraits<tflite_micro::StablehloConvolutionOptionsT> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloConvolutionOptions;
 };
 
-template<> struct BuiltinOptions2UnionTraits<tflite::StablehloCustomCallOptionsT> {
+template<> struct BuiltinOptions2UnionTraits<tflite_micro::StablehloCustomCallOptionsT> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloCustomCallOptions;
 };
 
-template<> struct BuiltinOptions2UnionTraits<tflite::StablehloReduceOptionsT> {
+template<> struct BuiltinOptions2UnionTraits<tflite_micro::StablehloReduceOptionsT> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloReduceOptions;
 };
 
-template<> struct BuiltinOptions2UnionTraits<tflite::StablehloScatterOptionsT> {
+template<> struct BuiltinOptions2UnionTraits<tflite_micro::StablehloScatterOptionsT> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloScatterOptions;
 };
 
-template<> struct BuiltinOptions2UnionTraits<tflite::StablehloCompareOptionsT> {
+template<> struct BuiltinOptions2UnionTraits<tflite_micro::StablehloCompareOptionsT> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloCompareOptions;
 };
 
-template<> struct BuiltinOptions2UnionTraits<tflite::StablehloDynamicSliceOptionsT> {
+template<> struct BuiltinOptions2UnionTraits<tflite_micro::StablehloDynamicSliceOptionsT> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloDynamicSliceOptions;
 };
 
-template<> struct BuiltinOptions2UnionTraits<tflite::StablehloPadOptionsT> {
+template<> struct BuiltinOptions2UnionTraits<tflite_micro::StablehloPadOptionsT> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloPadOptions;
 };
 
-template<> struct BuiltinOptions2UnionTraits<tflite::StablehloIotaOptionsT> {
+template<> struct BuiltinOptions2UnionTraits<tflite_micro::StablehloIotaOptionsT> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloIotaOptions;
 };
 
-template<> struct BuiltinOptions2UnionTraits<tflite::StablehloDotGeneralOptionsT> {
+template<> struct BuiltinOptions2UnionTraits<tflite_micro::StablehloDotGeneralOptionsT> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloDotGeneralOptions;
 };
 
-template<> struct BuiltinOptions2UnionTraits<tflite::StablehloReduceWindowOptionsT> {
+template<> struct BuiltinOptions2UnionTraits<tflite_micro::StablehloReduceWindowOptionsT> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloReduceWindowOptions;
 };
 
-template<> struct BuiltinOptions2UnionTraits<tflite::StablehloSortOptionsT> {
+template<> struct BuiltinOptions2UnionTraits<tflite_micro::StablehloSortOptionsT> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloSortOptions;
 };
 
-template<> struct BuiltinOptions2UnionTraits<tflite::StablehloWhileOptionsT> {
+template<> struct BuiltinOptions2UnionTraits<tflite_micro::StablehloWhileOptionsT> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloWhileOptions;
 };
 
-template<> struct BuiltinOptions2UnionTraits<tflite::StablehloGatherOptionsT> {
+template<> struct BuiltinOptions2UnionTraits<tflite_micro::StablehloGatherOptionsT> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloGatherOptions;
 };
 
-template<> struct BuiltinOptions2UnionTraits<tflite::StablehloTransposeOptionsT> {
+template<> struct BuiltinOptions2UnionTraits<tflite_micro::StablehloTransposeOptionsT> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloTransposeOptions;
 };
 
-template<> struct BuiltinOptions2UnionTraits<tflite::DilateOptionsT> {
+template<> struct BuiltinOptions2UnionTraits<tflite_micro::DilateOptionsT> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_DilateOptions;
 };
 
-template<> struct BuiltinOptions2UnionTraits<tflite::StablehloRngBitGeneratorOptionsT> {
+template<> struct BuiltinOptions2UnionTraits<tflite_micro::StablehloRngBitGeneratorOptionsT> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_StablehloRngBitGeneratorOptions;
 };
 
-template<> struct BuiltinOptions2UnionTraits<tflite::ReduceWindowOptionsT> {
+template<> struct BuiltinOptions2UnionTraits<tflite_micro::ReduceWindowOptionsT> {
   static const BuiltinOptions2 enum_value = BuiltinOptions2_ReduceWindowOptions;
 };
 
@@ -4382,165 +4382,165 @@ struct BuiltinOptions2Union {
   static void *UnPack(const void *obj, BuiltinOptions2 type, const flatbuffers::resolver_function_t *resolver);
   flatbuffers::Offset<void> Pack(flatbuffers::FlatBufferBuilder &_fbb, const flatbuffers::rehasher_function_t *_rehasher = nullptr) const;
 
-  tflite::StablehloConcatenateOptionsT *AsStablehloConcatenateOptions() {
+  tflite_micro::StablehloConcatenateOptionsT *AsStablehloConcatenateOptions() {
     return type == BuiltinOptions2_StablehloConcatenateOptions ?
-      reinterpret_cast<tflite::StablehloConcatenateOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::StablehloConcatenateOptionsT *>(value) : nullptr;
   }
-  const tflite::StablehloConcatenateOptionsT *AsStablehloConcatenateOptions() const {
+  const tflite_micro::StablehloConcatenateOptionsT *AsStablehloConcatenateOptions() const {
     return type == BuiltinOptions2_StablehloConcatenateOptions ?
-      reinterpret_cast<const tflite::StablehloConcatenateOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::StablehloConcatenateOptionsT *>(value) : nullptr;
   }
-  tflite::StablehloBroadcastInDimOptionsT *AsStablehloBroadcastInDimOptions() {
+  tflite_micro::StablehloBroadcastInDimOptionsT *AsStablehloBroadcastInDimOptions() {
     return type == BuiltinOptions2_StablehloBroadcastInDimOptions ?
-      reinterpret_cast<tflite::StablehloBroadcastInDimOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::StablehloBroadcastInDimOptionsT *>(value) : nullptr;
   }
-  const tflite::StablehloBroadcastInDimOptionsT *AsStablehloBroadcastInDimOptions() const {
+  const tflite_micro::StablehloBroadcastInDimOptionsT *AsStablehloBroadcastInDimOptions() const {
     return type == BuiltinOptions2_StablehloBroadcastInDimOptions ?
-      reinterpret_cast<const tflite::StablehloBroadcastInDimOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::StablehloBroadcastInDimOptionsT *>(value) : nullptr;
   }
-  tflite::StablehloSliceOptionsT *AsStablehloSliceOptions() {
+  tflite_micro::StablehloSliceOptionsT *AsStablehloSliceOptions() {
     return type == BuiltinOptions2_StablehloSliceOptions ?
-      reinterpret_cast<tflite::StablehloSliceOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::StablehloSliceOptionsT *>(value) : nullptr;
   }
-  const tflite::StablehloSliceOptionsT *AsStablehloSliceOptions() const {
+  const tflite_micro::StablehloSliceOptionsT *AsStablehloSliceOptions() const {
     return type == BuiltinOptions2_StablehloSliceOptions ?
-      reinterpret_cast<const tflite::StablehloSliceOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::StablehloSliceOptionsT *>(value) : nullptr;
   }
-  tflite::StablehloConvolutionOptionsT *AsStablehloConvolutionOptions() {
+  tflite_micro::StablehloConvolutionOptionsT *AsStablehloConvolutionOptions() {
     return type == BuiltinOptions2_StablehloConvolutionOptions ?
-      reinterpret_cast<tflite::StablehloConvolutionOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::StablehloConvolutionOptionsT *>(value) : nullptr;
   }
-  const tflite::StablehloConvolutionOptionsT *AsStablehloConvolutionOptions() const {
+  const tflite_micro::StablehloConvolutionOptionsT *AsStablehloConvolutionOptions() const {
     return type == BuiltinOptions2_StablehloConvolutionOptions ?
-      reinterpret_cast<const tflite::StablehloConvolutionOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::StablehloConvolutionOptionsT *>(value) : nullptr;
   }
-  tflite::StablehloCustomCallOptionsT *AsStablehloCustomCallOptions() {
+  tflite_micro::StablehloCustomCallOptionsT *AsStablehloCustomCallOptions() {
     return type == BuiltinOptions2_StablehloCustomCallOptions ?
-      reinterpret_cast<tflite::StablehloCustomCallOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::StablehloCustomCallOptionsT *>(value) : nullptr;
   }
-  const tflite::StablehloCustomCallOptionsT *AsStablehloCustomCallOptions() const {
+  const tflite_micro::StablehloCustomCallOptionsT *AsStablehloCustomCallOptions() const {
     return type == BuiltinOptions2_StablehloCustomCallOptions ?
-      reinterpret_cast<const tflite::StablehloCustomCallOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::StablehloCustomCallOptionsT *>(value) : nullptr;
   }
-  tflite::StablehloReduceOptionsT *AsStablehloReduceOptions() {
+  tflite_micro::StablehloReduceOptionsT *AsStablehloReduceOptions() {
     return type == BuiltinOptions2_StablehloReduceOptions ?
-      reinterpret_cast<tflite::StablehloReduceOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::StablehloReduceOptionsT *>(value) : nullptr;
   }
-  const tflite::StablehloReduceOptionsT *AsStablehloReduceOptions() const {
+  const tflite_micro::StablehloReduceOptionsT *AsStablehloReduceOptions() const {
     return type == BuiltinOptions2_StablehloReduceOptions ?
-      reinterpret_cast<const tflite::StablehloReduceOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::StablehloReduceOptionsT *>(value) : nullptr;
   }
-  tflite::StablehloScatterOptionsT *AsStablehloScatterOptions() {
+  tflite_micro::StablehloScatterOptionsT *AsStablehloScatterOptions() {
     return type == BuiltinOptions2_StablehloScatterOptions ?
-      reinterpret_cast<tflite::StablehloScatterOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::StablehloScatterOptionsT *>(value) : nullptr;
   }
-  const tflite::StablehloScatterOptionsT *AsStablehloScatterOptions() const {
+  const tflite_micro::StablehloScatterOptionsT *AsStablehloScatterOptions() const {
     return type == BuiltinOptions2_StablehloScatterOptions ?
-      reinterpret_cast<const tflite::StablehloScatterOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::StablehloScatterOptionsT *>(value) : nullptr;
   }
-  tflite::StablehloCompareOptionsT *AsStablehloCompareOptions() {
+  tflite_micro::StablehloCompareOptionsT *AsStablehloCompareOptions() {
     return type == BuiltinOptions2_StablehloCompareOptions ?
-      reinterpret_cast<tflite::StablehloCompareOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::StablehloCompareOptionsT *>(value) : nullptr;
   }
-  const tflite::StablehloCompareOptionsT *AsStablehloCompareOptions() const {
+  const tflite_micro::StablehloCompareOptionsT *AsStablehloCompareOptions() const {
     return type == BuiltinOptions2_StablehloCompareOptions ?
-      reinterpret_cast<const tflite::StablehloCompareOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::StablehloCompareOptionsT *>(value) : nullptr;
   }
-  tflite::StablehloDynamicSliceOptionsT *AsStablehloDynamicSliceOptions() {
+  tflite_micro::StablehloDynamicSliceOptionsT *AsStablehloDynamicSliceOptions() {
     return type == BuiltinOptions2_StablehloDynamicSliceOptions ?
-      reinterpret_cast<tflite::StablehloDynamicSliceOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::StablehloDynamicSliceOptionsT *>(value) : nullptr;
   }
-  const tflite::StablehloDynamicSliceOptionsT *AsStablehloDynamicSliceOptions() const {
+  const tflite_micro::StablehloDynamicSliceOptionsT *AsStablehloDynamicSliceOptions() const {
     return type == BuiltinOptions2_StablehloDynamicSliceOptions ?
-      reinterpret_cast<const tflite::StablehloDynamicSliceOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::StablehloDynamicSliceOptionsT *>(value) : nullptr;
   }
-  tflite::StablehloPadOptionsT *AsStablehloPadOptions() {
+  tflite_micro::StablehloPadOptionsT *AsStablehloPadOptions() {
     return type == BuiltinOptions2_StablehloPadOptions ?
-      reinterpret_cast<tflite::StablehloPadOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::StablehloPadOptionsT *>(value) : nullptr;
   }
-  const tflite::StablehloPadOptionsT *AsStablehloPadOptions() const {
+  const tflite_micro::StablehloPadOptionsT *AsStablehloPadOptions() const {
     return type == BuiltinOptions2_StablehloPadOptions ?
-      reinterpret_cast<const tflite::StablehloPadOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::StablehloPadOptionsT *>(value) : nullptr;
   }
-  tflite::StablehloIotaOptionsT *AsStablehloIotaOptions() {
+  tflite_micro::StablehloIotaOptionsT *AsStablehloIotaOptions() {
     return type == BuiltinOptions2_StablehloIotaOptions ?
-      reinterpret_cast<tflite::StablehloIotaOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::StablehloIotaOptionsT *>(value) : nullptr;
   }
-  const tflite::StablehloIotaOptionsT *AsStablehloIotaOptions() const {
+  const tflite_micro::StablehloIotaOptionsT *AsStablehloIotaOptions() const {
     return type == BuiltinOptions2_StablehloIotaOptions ?
-      reinterpret_cast<const tflite::StablehloIotaOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::StablehloIotaOptionsT *>(value) : nullptr;
   }
-  tflite::StablehloDotGeneralOptionsT *AsStablehloDotGeneralOptions() {
+  tflite_micro::StablehloDotGeneralOptionsT *AsStablehloDotGeneralOptions() {
     return type == BuiltinOptions2_StablehloDotGeneralOptions ?
-      reinterpret_cast<tflite::StablehloDotGeneralOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::StablehloDotGeneralOptionsT *>(value) : nullptr;
   }
-  const tflite::StablehloDotGeneralOptionsT *AsStablehloDotGeneralOptions() const {
+  const tflite_micro::StablehloDotGeneralOptionsT *AsStablehloDotGeneralOptions() const {
     return type == BuiltinOptions2_StablehloDotGeneralOptions ?
-      reinterpret_cast<const tflite::StablehloDotGeneralOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::StablehloDotGeneralOptionsT *>(value) : nullptr;
   }
-  tflite::StablehloReduceWindowOptionsT *AsStablehloReduceWindowOptions() {
+  tflite_micro::StablehloReduceWindowOptionsT *AsStablehloReduceWindowOptions() {
     return type == BuiltinOptions2_StablehloReduceWindowOptions ?
-      reinterpret_cast<tflite::StablehloReduceWindowOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::StablehloReduceWindowOptionsT *>(value) : nullptr;
   }
-  const tflite::StablehloReduceWindowOptionsT *AsStablehloReduceWindowOptions() const {
+  const tflite_micro::StablehloReduceWindowOptionsT *AsStablehloReduceWindowOptions() const {
     return type == BuiltinOptions2_StablehloReduceWindowOptions ?
-      reinterpret_cast<const tflite::StablehloReduceWindowOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::StablehloReduceWindowOptionsT *>(value) : nullptr;
   }
-  tflite::StablehloSortOptionsT *AsStablehloSortOptions() {
+  tflite_micro::StablehloSortOptionsT *AsStablehloSortOptions() {
     return type == BuiltinOptions2_StablehloSortOptions ?
-      reinterpret_cast<tflite::StablehloSortOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::StablehloSortOptionsT *>(value) : nullptr;
   }
-  const tflite::StablehloSortOptionsT *AsStablehloSortOptions() const {
+  const tflite_micro::StablehloSortOptionsT *AsStablehloSortOptions() const {
     return type == BuiltinOptions2_StablehloSortOptions ?
-      reinterpret_cast<const tflite::StablehloSortOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::StablehloSortOptionsT *>(value) : nullptr;
   }
-  tflite::StablehloWhileOptionsT *AsStablehloWhileOptions() {
+  tflite_micro::StablehloWhileOptionsT *AsStablehloWhileOptions() {
     return type == BuiltinOptions2_StablehloWhileOptions ?
-      reinterpret_cast<tflite::StablehloWhileOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::StablehloWhileOptionsT *>(value) : nullptr;
   }
-  const tflite::StablehloWhileOptionsT *AsStablehloWhileOptions() const {
+  const tflite_micro::StablehloWhileOptionsT *AsStablehloWhileOptions() const {
     return type == BuiltinOptions2_StablehloWhileOptions ?
-      reinterpret_cast<const tflite::StablehloWhileOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::StablehloWhileOptionsT *>(value) : nullptr;
   }
-  tflite::StablehloGatherOptionsT *AsStablehloGatherOptions() {
+  tflite_micro::StablehloGatherOptionsT *AsStablehloGatherOptions() {
     return type == BuiltinOptions2_StablehloGatherOptions ?
-      reinterpret_cast<tflite::StablehloGatherOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::StablehloGatherOptionsT *>(value) : nullptr;
   }
-  const tflite::StablehloGatherOptionsT *AsStablehloGatherOptions() const {
+  const tflite_micro::StablehloGatherOptionsT *AsStablehloGatherOptions() const {
     return type == BuiltinOptions2_StablehloGatherOptions ?
-      reinterpret_cast<const tflite::StablehloGatherOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::StablehloGatherOptionsT *>(value) : nullptr;
   }
-  tflite::StablehloTransposeOptionsT *AsStablehloTransposeOptions() {
+  tflite_micro::StablehloTransposeOptionsT *AsStablehloTransposeOptions() {
     return type == BuiltinOptions2_StablehloTransposeOptions ?
-      reinterpret_cast<tflite::StablehloTransposeOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::StablehloTransposeOptionsT *>(value) : nullptr;
   }
-  const tflite::StablehloTransposeOptionsT *AsStablehloTransposeOptions() const {
+  const tflite_micro::StablehloTransposeOptionsT *AsStablehloTransposeOptions() const {
     return type == BuiltinOptions2_StablehloTransposeOptions ?
-      reinterpret_cast<const tflite::StablehloTransposeOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::StablehloTransposeOptionsT *>(value) : nullptr;
   }
-  tflite::DilateOptionsT *AsDilateOptions() {
+  tflite_micro::DilateOptionsT *AsDilateOptions() {
     return type == BuiltinOptions2_DilateOptions ?
-      reinterpret_cast<tflite::DilateOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::DilateOptionsT *>(value) : nullptr;
   }
-  const tflite::DilateOptionsT *AsDilateOptions() const {
+  const tflite_micro::DilateOptionsT *AsDilateOptions() const {
     return type == BuiltinOptions2_DilateOptions ?
-      reinterpret_cast<const tflite::DilateOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::DilateOptionsT *>(value) : nullptr;
   }
-  tflite::StablehloRngBitGeneratorOptionsT *AsStablehloRngBitGeneratorOptions() {
+  tflite_micro::StablehloRngBitGeneratorOptionsT *AsStablehloRngBitGeneratorOptions() {
     return type == BuiltinOptions2_StablehloRngBitGeneratorOptions ?
-      reinterpret_cast<tflite::StablehloRngBitGeneratorOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::StablehloRngBitGeneratorOptionsT *>(value) : nullptr;
   }
-  const tflite::StablehloRngBitGeneratorOptionsT *AsStablehloRngBitGeneratorOptions() const {
+  const tflite_micro::StablehloRngBitGeneratorOptionsT *AsStablehloRngBitGeneratorOptions() const {
     return type == BuiltinOptions2_StablehloRngBitGeneratorOptions ?
-      reinterpret_cast<const tflite::StablehloRngBitGeneratorOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::StablehloRngBitGeneratorOptionsT *>(value) : nullptr;
   }
-  tflite::ReduceWindowOptionsT *AsReduceWindowOptions() {
+  tflite_micro::ReduceWindowOptionsT *AsReduceWindowOptions() {
     return type == BuiltinOptions2_ReduceWindowOptions ?
-      reinterpret_cast<tflite::ReduceWindowOptionsT *>(value) : nullptr;
+      reinterpret_cast<tflite_micro::ReduceWindowOptionsT *>(value) : nullptr;
   }
-  const tflite::ReduceWindowOptionsT *AsReduceWindowOptions() const {
+  const tflite_micro::ReduceWindowOptionsT *AsReduceWindowOptions() const {
     return type == BuiltinOptions2_ReduceWindowOptions ?
-      reinterpret_cast<const tflite::ReduceWindowOptionsT *>(value) : nullptr;
+      reinterpret_cast<const tflite_micro::ReduceWindowOptionsT *>(value) : nullptr;
   }
 };
 
@@ -5050,7 +5050,7 @@ inline flatbuffers::Offset<CustomQuantization> CreateCustomQuantizationDirect(
     const std::vector<uint8_t> *custom = nullptr) {
   if (custom) { _fbb.ForceVectorAlignment(custom->size(), sizeof(uint8_t), 16); }
   auto custom__ = custom ? _fbb.CreateVector<uint8_t>(*custom) : 0;
-  return tflite::CreateCustomQuantization(
+  return tflite_micro::CreateCustomQuantization(
       _fbb,
       custom__);
 }
@@ -5063,7 +5063,7 @@ struct QuantizationParametersT : public flatbuffers::NativeTable {
   std::vector<float> max{};
   std::vector<float> scale{};
   std::vector<int64_t> zero_point{};
-  tflite::QuantizationDetailsUnion details{};
+  tflite_micro::QuantizationDetailsUnion details{};
   int32_t quantized_dimension = 0;
 };
 
@@ -5091,15 +5091,15 @@ struct QuantizationParameters FLATBUFFERS_FINAL_CLASS : private flatbuffers::Tab
   const flatbuffers::Vector<int64_t> *zero_point() const {
     return GetPointer<const flatbuffers::Vector<int64_t> *>(VT_ZERO_POINT);
   }
-  tflite::QuantizationDetails details_type() const {
-    return static_cast<tflite::QuantizationDetails>(GetField<uint8_t>(VT_DETAILS_TYPE, 0));
+  tflite_micro::QuantizationDetails details_type() const {
+    return static_cast<tflite_micro::QuantizationDetails>(GetField<uint8_t>(VT_DETAILS_TYPE, 0));
   }
   const void *details() const {
     return GetPointer<const void *>(VT_DETAILS);
   }
   template<typename T> const T *details_as() const;
-  const tflite::CustomQuantization *details_as_CustomQuantization() const {
-    return details_type() == tflite::QuantizationDetails_CustomQuantization ? static_cast<const tflite::CustomQuantization *>(details()) : nullptr;
+  const tflite_micro::CustomQuantization *details_as_CustomQuantization() const {
+    return details_type() == tflite_micro::QuantizationDetails_CustomQuantization ? static_cast<const tflite_micro::CustomQuantization *>(details()) : nullptr;
   }
   int32_t quantized_dimension() const {
     return GetField<int32_t>(VT_QUANTIZED_DIMENSION, 0);
@@ -5125,7 +5125,7 @@ struct QuantizationParameters FLATBUFFERS_FINAL_CLASS : private flatbuffers::Tab
   static flatbuffers::Offset<QuantizationParameters> Pack(flatbuffers::FlatBufferBuilder &_fbb, const QuantizationParametersT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
 };
 
-template<> inline const tflite::CustomQuantization *QuantizationParameters::details_as<tflite::CustomQuantization>() const {
+template<> inline const tflite_micro::CustomQuantization *QuantizationParameters::details_as<tflite_micro::CustomQuantization>() const {
   return details_as_CustomQuantization();
 }
 
@@ -5145,7 +5145,7 @@ struct QuantizationParametersBuilder {
   void add_zero_point(flatbuffers::Offset<flatbuffers::Vector<int64_t>> zero_point) {
     fbb_.AddOffset(QuantizationParameters::VT_ZERO_POINT, zero_point);
   }
-  void add_details_type(tflite::QuantizationDetails details_type) {
+  void add_details_type(tflite_micro::QuantizationDetails details_type) {
     fbb_.AddElement<uint8_t>(QuantizationParameters::VT_DETAILS_TYPE, static_cast<uint8_t>(details_type), 0);
   }
   void add_details(flatbuffers::Offset<void> details) {
@@ -5171,7 +5171,7 @@ inline flatbuffers::Offset<QuantizationParameters> CreateQuantizationParameters(
     flatbuffers::Offset<flatbuffers::Vector<float>> max = 0,
     flatbuffers::Offset<flatbuffers::Vector<float>> scale = 0,
     flatbuffers::Offset<flatbuffers::Vector<int64_t>> zero_point = 0,
-    tflite::QuantizationDetails details_type = tflite::QuantizationDetails_NONE,
+    tflite_micro::QuantizationDetails details_type = tflite_micro::QuantizationDetails_NONE,
     flatbuffers::Offset<void> details = 0,
     int32_t quantized_dimension = 0) {
   QuantizationParametersBuilder builder_(_fbb);
@@ -5191,14 +5191,14 @@ inline flatbuffers::Offset<QuantizationParameters> CreateQuantizationParametersD
     const std::vector<float> *max = nullptr,
     const std::vector<float> *scale = nullptr,
     const std::vector<int64_t> *zero_point = nullptr,
-    tflite::QuantizationDetails details_type = tflite::QuantizationDetails_NONE,
+    tflite_micro::QuantizationDetails details_type = tflite_micro::QuantizationDetails_NONE,
     flatbuffers::Offset<void> details = 0,
     int32_t quantized_dimension = 0) {
   auto min__ = min ? _fbb.CreateVector<float>(*min) : 0;
   auto max__ = max ? _fbb.CreateVector<float>(*max) : 0;
   auto scale__ = scale ? _fbb.CreateVector<float>(*scale) : 0;
   auto zero_point__ = zero_point ? _fbb.CreateVector<int64_t>(*zero_point) : 0;
-  return tflite::CreateQuantizationParameters(
+  return tflite_micro::CreateQuantizationParameters(
       _fbb,
       min__,
       max__,
@@ -5266,7 +5266,7 @@ inline flatbuffers::Offset<Int32Vector> CreateInt32VectorDirect(
     flatbuffers::FlatBufferBuilder &_fbb,
     const std::vector<int32_t> *values = nullptr) {
   auto values__ = values ? _fbb.CreateVector<int32_t>(*values) : 0;
-  return tflite::CreateInt32Vector(
+  return tflite_micro::CreateInt32Vector(
       _fbb,
       values__);
 }
@@ -5329,7 +5329,7 @@ inline flatbuffers::Offset<Uint16Vector> CreateUint16VectorDirect(
     const std::vector<uint16_t> *values = nullptr) {
   if (values) { _fbb.ForceVectorAlignment(values->size(), sizeof(uint16_t), 4); }
   auto values__ = values ? _fbb.CreateVector<uint16_t>(*values) : 0;
-  return tflite::CreateUint16Vector(
+  return tflite_micro::CreateUint16Vector(
       _fbb,
       values__);
 }
@@ -5392,7 +5392,7 @@ inline flatbuffers::Offset<Uint8Vector> CreateUint8VectorDirect(
     const std::vector<uint8_t> *values = nullptr) {
   if (values) { _fbb.ForceVectorAlignment(values->size(), sizeof(uint8_t), 4); }
   auto values__ = values ? _fbb.CreateVector<uint8_t>(*values) : 0;
-  return tflite::CreateUint8Vector(
+  return tflite_micro::CreateUint8Vector(
       _fbb,
       values__);
 }
@@ -5401,10 +5401,10 @@ flatbuffers::Offset<Uint8Vector> CreateUint8Vector(flatbuffers::FlatBufferBuilde
 
 struct DimensionMetadataT : public flatbuffers::NativeTable {
   typedef DimensionMetadata TableType;
-  tflite::DimensionType format = tflite::DimensionType_DENSE;
+  tflite_micro::DimensionType format = tflite_micro::DimensionType_DENSE;
   int32_t dense_size = 0;
-  tflite::SparseIndexVectorUnion array_segments{};
-  tflite::SparseIndexVectorUnion array_indices{};
+  tflite_micro::SparseIndexVectorUnion array_segments{};
+  tflite_micro::SparseIndexVectorUnion array_indices{};
 };
 
 struct DimensionMetadata FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -5418,43 +5418,43 @@ struct DimensionMetadata FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
     VT_ARRAY_INDICES_TYPE = 12,
     VT_ARRAY_INDICES = 14
   };
-  tflite::DimensionType format() const {
-    return static_cast<tflite::DimensionType>(GetField<int8_t>(VT_FORMAT, 0));
+  tflite_micro::DimensionType format() const {
+    return static_cast<tflite_micro::DimensionType>(GetField<int8_t>(VT_FORMAT, 0));
   }
   int32_t dense_size() const {
     return GetField<int32_t>(VT_DENSE_SIZE, 0);
   }
-  tflite::SparseIndexVector array_segments_type() const {
-    return static_cast<tflite::SparseIndexVector>(GetField<uint8_t>(VT_ARRAY_SEGMENTS_TYPE, 0));
+  tflite_micro::SparseIndexVector array_segments_type() const {
+    return static_cast<tflite_micro::SparseIndexVector>(GetField<uint8_t>(VT_ARRAY_SEGMENTS_TYPE, 0));
   }
   const void *array_segments() const {
     return GetPointer<const void *>(VT_ARRAY_SEGMENTS);
   }
   template<typename T> const T *array_segments_as() const;
-  const tflite::Int32Vector *array_segments_as_Int32Vector() const {
-    return array_segments_type() == tflite::SparseIndexVector_Int32Vector ? static_cast<const tflite::Int32Vector *>(array_segments()) : nullptr;
+  const tflite_micro::Int32Vector *array_segments_as_Int32Vector() const {
+    return array_segments_type() == tflite_micro::SparseIndexVector_Int32Vector ? static_cast<const tflite_micro::Int32Vector *>(array_segments()) : nullptr;
   }
-  const tflite::Uint16Vector *array_segments_as_Uint16Vector() const {
-    return array_segments_type() == tflite::SparseIndexVector_Uint16Vector ? static_cast<const tflite::Uint16Vector *>(array_segments()) : nullptr;
+  const tflite_micro::Uint16Vector *array_segments_as_Uint16Vector() const {
+    return array_segments_type() == tflite_micro::SparseIndexVector_Uint16Vector ? static_cast<const tflite_micro::Uint16Vector *>(array_segments()) : nullptr;
   }
-  const tflite::Uint8Vector *array_segments_as_Uint8Vector() const {
-    return array_segments_type() == tflite::SparseIndexVector_Uint8Vector ? static_cast<const tflite::Uint8Vector *>(array_segments()) : nullptr;
+  const tflite_micro::Uint8Vector *array_segments_as_Uint8Vector() const {
+    return array_segments_type() == tflite_micro::SparseIndexVector_Uint8Vector ? static_cast<const tflite_micro::Uint8Vector *>(array_segments()) : nullptr;
   }
-  tflite::SparseIndexVector array_indices_type() const {
-    return static_cast<tflite::SparseIndexVector>(GetField<uint8_t>(VT_ARRAY_INDICES_TYPE, 0));
+  tflite_micro::SparseIndexVector array_indices_type() const {
+    return static_cast<tflite_micro::SparseIndexVector>(GetField<uint8_t>(VT_ARRAY_INDICES_TYPE, 0));
   }
   const void *array_indices() const {
     return GetPointer<const void *>(VT_ARRAY_INDICES);
   }
   template<typename T> const T *array_indices_as() const;
-  const tflite::Int32Vector *array_indices_as_Int32Vector() const {
-    return array_indices_type() == tflite::SparseIndexVector_Int32Vector ? static_cast<const tflite::Int32Vector *>(array_indices()) : nullptr;
+  const tflite_micro::Int32Vector *array_indices_as_Int32Vector() const {
+    return array_indices_type() == tflite_micro::SparseIndexVector_Int32Vector ? static_cast<const tflite_micro::Int32Vector *>(array_indices()) : nullptr;
   }
-  const tflite::Uint16Vector *array_indices_as_Uint16Vector() const {
-    return array_indices_type() == tflite::SparseIndexVector_Uint16Vector ? static_cast<const tflite::Uint16Vector *>(array_indices()) : nullptr;
+  const tflite_micro::Uint16Vector *array_indices_as_Uint16Vector() const {
+    return array_indices_type() == tflite_micro::SparseIndexVector_Uint16Vector ? static_cast<const tflite_micro::Uint16Vector *>(array_indices()) : nullptr;
   }
-  const tflite::Uint8Vector *array_indices_as_Uint8Vector() const {
-    return array_indices_type() == tflite::SparseIndexVector_Uint8Vector ? static_cast<const tflite::Uint8Vector *>(array_indices()) : nullptr;
+  const tflite_micro::Uint8Vector *array_indices_as_Uint8Vector() const {
+    return array_indices_type() == tflite_micro::SparseIndexVector_Uint8Vector ? static_cast<const tflite_micro::Uint8Vector *>(array_indices()) : nullptr;
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
@@ -5473,27 +5473,27 @@ struct DimensionMetadata FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   static flatbuffers::Offset<DimensionMetadata> Pack(flatbuffers::FlatBufferBuilder &_fbb, const DimensionMetadataT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
 };
 
-template<> inline const tflite::Int32Vector *DimensionMetadata::array_segments_as<tflite::Int32Vector>() const {
+template<> inline const tflite_micro::Int32Vector *DimensionMetadata::array_segments_as<tflite_micro::Int32Vector>() const {
   return array_segments_as_Int32Vector();
 }
 
-template<> inline const tflite::Uint16Vector *DimensionMetadata::array_segments_as<tflite::Uint16Vector>() const {
+template<> inline const tflite_micro::Uint16Vector *DimensionMetadata::array_segments_as<tflite_micro::Uint16Vector>() const {
   return array_segments_as_Uint16Vector();
 }
 
-template<> inline const tflite::Uint8Vector *DimensionMetadata::array_segments_as<tflite::Uint8Vector>() const {
+template<> inline const tflite_micro::Uint8Vector *DimensionMetadata::array_segments_as<tflite_micro::Uint8Vector>() const {
   return array_segments_as_Uint8Vector();
 }
 
-template<> inline const tflite::Int32Vector *DimensionMetadata::array_indices_as<tflite::Int32Vector>() const {
+template<> inline const tflite_micro::Int32Vector *DimensionMetadata::array_indices_as<tflite_micro::Int32Vector>() const {
   return array_indices_as_Int32Vector();
 }
 
-template<> inline const tflite::Uint16Vector *DimensionMetadata::array_indices_as<tflite::Uint16Vector>() const {
+template<> inline const tflite_micro::Uint16Vector *DimensionMetadata::array_indices_as<tflite_micro::Uint16Vector>() const {
   return array_indices_as_Uint16Vector();
 }
 
-template<> inline const tflite::Uint8Vector *DimensionMetadata::array_indices_as<tflite::Uint8Vector>() const {
+template<> inline const tflite_micro::Uint8Vector *DimensionMetadata::array_indices_as<tflite_micro::Uint8Vector>() const {
   return array_indices_as_Uint8Vector();
 }
 
@@ -5501,19 +5501,19 @@ struct DimensionMetadataBuilder {
   typedef DimensionMetadata Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_format(tflite::DimensionType format) {
+  void add_format(tflite_micro::DimensionType format) {
     fbb_.AddElement<int8_t>(DimensionMetadata::VT_FORMAT, static_cast<int8_t>(format), 0);
   }
   void add_dense_size(int32_t dense_size) {
     fbb_.AddElement<int32_t>(DimensionMetadata::VT_DENSE_SIZE, dense_size, 0);
   }
-  void add_array_segments_type(tflite::SparseIndexVector array_segments_type) {
+  void add_array_segments_type(tflite_micro::SparseIndexVector array_segments_type) {
     fbb_.AddElement<uint8_t>(DimensionMetadata::VT_ARRAY_SEGMENTS_TYPE, static_cast<uint8_t>(array_segments_type), 0);
   }
   void add_array_segments(flatbuffers::Offset<void> array_segments) {
     fbb_.AddOffset(DimensionMetadata::VT_ARRAY_SEGMENTS, array_segments);
   }
-  void add_array_indices_type(tflite::SparseIndexVector array_indices_type) {
+  void add_array_indices_type(tflite_micro::SparseIndexVector array_indices_type) {
     fbb_.AddElement<uint8_t>(DimensionMetadata::VT_ARRAY_INDICES_TYPE, static_cast<uint8_t>(array_indices_type), 0);
   }
   void add_array_indices(flatbuffers::Offset<void> array_indices) {
@@ -5532,11 +5532,11 @@ struct DimensionMetadataBuilder {
 
 inline flatbuffers::Offset<DimensionMetadata> CreateDimensionMetadata(
     flatbuffers::FlatBufferBuilder &_fbb,
-    tflite::DimensionType format = tflite::DimensionType_DENSE,
+    tflite_micro::DimensionType format = tflite_micro::DimensionType_DENSE,
     int32_t dense_size = 0,
-    tflite::SparseIndexVector array_segments_type = tflite::SparseIndexVector_NONE,
+    tflite_micro::SparseIndexVector array_segments_type = tflite_micro::SparseIndexVector_NONE,
     flatbuffers::Offset<void> array_segments = 0,
-    tflite::SparseIndexVector array_indices_type = tflite::SparseIndexVector_NONE,
+    tflite_micro::SparseIndexVector array_indices_type = tflite_micro::SparseIndexVector_NONE,
     flatbuffers::Offset<void> array_indices = 0) {
   DimensionMetadataBuilder builder_(_fbb);
   builder_.add_array_indices(array_indices);
@@ -5554,7 +5554,7 @@ struct SparsityParametersT : public flatbuffers::NativeTable {
   typedef SparsityParameters TableType;
   std::vector<int32_t> traversal_order{};
   std::vector<int32_t> block_map{};
-  std::vector<std::unique_ptr<tflite::DimensionMetadataT>> dim_metadata{};
+  std::vector<std::unique_ptr<tflite_micro::DimensionMetadataT>> dim_metadata{};
   SparsityParametersT() = default;
   SparsityParametersT(const SparsityParametersT &o);
   SparsityParametersT(SparsityParametersT&&) FLATBUFFERS_NOEXCEPT = default;
@@ -5575,8 +5575,8 @@ struct SparsityParameters FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   const flatbuffers::Vector<int32_t> *block_map() const {
     return GetPointer<const flatbuffers::Vector<int32_t> *>(VT_BLOCK_MAP);
   }
-  const flatbuffers::Vector<flatbuffers::Offset<tflite::DimensionMetadata>> *dim_metadata() const {
-    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite::DimensionMetadata>> *>(VT_DIM_METADATA);
+  const flatbuffers::Vector<flatbuffers::Offset<tflite_micro::DimensionMetadata>> *dim_metadata() const {
+    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite_micro::DimensionMetadata>> *>(VT_DIM_METADATA);
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
@@ -5604,7 +5604,7 @@ struct SparsityParametersBuilder {
   void add_block_map(flatbuffers::Offset<flatbuffers::Vector<int32_t>> block_map) {
     fbb_.AddOffset(SparsityParameters::VT_BLOCK_MAP, block_map);
   }
-  void add_dim_metadata(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::DimensionMetadata>>> dim_metadata) {
+  void add_dim_metadata(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite_micro::DimensionMetadata>>> dim_metadata) {
     fbb_.AddOffset(SparsityParameters::VT_DIM_METADATA, dim_metadata);
   }
   explicit SparsityParametersBuilder(flatbuffers::FlatBufferBuilder &_fbb)
@@ -5622,7 +5622,7 @@ inline flatbuffers::Offset<SparsityParameters> CreateSparsityParameters(
     flatbuffers::FlatBufferBuilder &_fbb,
     flatbuffers::Offset<flatbuffers::Vector<int32_t>> traversal_order = 0,
     flatbuffers::Offset<flatbuffers::Vector<int32_t>> block_map = 0,
-    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::DimensionMetadata>>> dim_metadata = 0) {
+    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite_micro::DimensionMetadata>>> dim_metadata = 0) {
   SparsityParametersBuilder builder_(_fbb);
   builder_.add_dim_metadata(dim_metadata);
   builder_.add_block_map(block_map);
@@ -5634,11 +5634,11 @@ inline flatbuffers::Offset<SparsityParameters> CreateSparsityParametersDirect(
     flatbuffers::FlatBufferBuilder &_fbb,
     const std::vector<int32_t> *traversal_order = nullptr,
     const std::vector<int32_t> *block_map = nullptr,
-    const std::vector<flatbuffers::Offset<tflite::DimensionMetadata>> *dim_metadata = nullptr) {
+    const std::vector<flatbuffers::Offset<tflite_micro::DimensionMetadata>> *dim_metadata = nullptr) {
   auto traversal_order__ = traversal_order ? _fbb.CreateVector<int32_t>(*traversal_order) : 0;
   auto block_map__ = block_map ? _fbb.CreateVector<int32_t>(*block_map) : 0;
-  auto dim_metadata__ = dim_metadata ? _fbb.CreateVector<flatbuffers::Offset<tflite::DimensionMetadata>>(*dim_metadata) : 0;
-  return tflite::CreateSparsityParameters(
+  auto dim_metadata__ = dim_metadata ? _fbb.CreateVector<flatbuffers::Offset<tflite_micro::DimensionMetadata>>(*dim_metadata) : 0;
+  return tflite_micro::CreateSparsityParameters(
       _fbb,
       traversal_order__,
       block_map__,
@@ -5650,7 +5650,7 @@ flatbuffers::Offset<SparsityParameters> CreateSparsityParameters(flatbuffers::Fl
 struct VariantSubTypeT : public flatbuffers::NativeTable {
   typedef VariantSubType TableType;
   std::vector<int32_t> shape{};
-  tflite::TensorType type = tflite::TensorType_FLOAT32;
+  tflite_micro::TensorType type = tflite_micro::TensorType_FLOAT32;
   bool has_rank = false;
 };
 
@@ -5665,8 +5665,8 @@ struct VariantSubType FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   const flatbuffers::Vector<int32_t> *shape() const {
     return GetPointer<const flatbuffers::Vector<int32_t> *>(VT_SHAPE);
   }
-  tflite::TensorType type() const {
-    return static_cast<tflite::TensorType>(GetField<int8_t>(VT_TYPE, 0));
+  tflite_micro::TensorType type() const {
+    return static_cast<tflite_micro::TensorType>(GetField<int8_t>(VT_TYPE, 0));
   }
   bool has_rank() const {
     return GetField<uint8_t>(VT_HAS_RANK, 0) != 0;
@@ -5691,7 +5691,7 @@ struct VariantSubTypeBuilder {
   void add_shape(flatbuffers::Offset<flatbuffers::Vector<int32_t>> shape) {
     fbb_.AddOffset(VariantSubType::VT_SHAPE, shape);
   }
-  void add_type(tflite::TensorType type) {
+  void add_type(tflite_micro::TensorType type) {
     fbb_.AddElement<int8_t>(VariantSubType::VT_TYPE, static_cast<int8_t>(type), 0);
   }
   void add_has_rank(bool has_rank) {
@@ -5711,7 +5711,7 @@ struct VariantSubTypeBuilder {
 inline flatbuffers::Offset<VariantSubType> CreateVariantSubType(
     flatbuffers::FlatBufferBuilder &_fbb,
     flatbuffers::Offset<flatbuffers::Vector<int32_t>> shape = 0,
-    tflite::TensorType type = tflite::TensorType_FLOAT32,
+    tflite_micro::TensorType type = tflite_micro::TensorType_FLOAT32,
     bool has_rank = false) {
   VariantSubTypeBuilder builder_(_fbb);
   builder_.add_shape(shape);
@@ -5723,10 +5723,10 @@ inline flatbuffers::Offset<VariantSubType> CreateVariantSubType(
 inline flatbuffers::Offset<VariantSubType> CreateVariantSubTypeDirect(
     flatbuffers::FlatBufferBuilder &_fbb,
     const std::vector<int32_t> *shape = nullptr,
-    tflite::TensorType type = tflite::TensorType_FLOAT32,
+    tflite_micro::TensorType type = tflite_micro::TensorType_FLOAT32,
     bool has_rank = false) {
   auto shape__ = shape ? _fbb.CreateVector<int32_t>(*shape) : 0;
-  return tflite::CreateVariantSubType(
+  return tflite_micro::CreateVariantSubType(
       _fbb,
       shape__,
       type,
@@ -5738,15 +5738,15 @@ flatbuffers::Offset<VariantSubType> CreateVariantSubType(flatbuffers::FlatBuffer
 struct TensorT : public flatbuffers::NativeTable {
   typedef Tensor TableType;
   std::vector<int32_t> shape{};
-  tflite::TensorType type = tflite::TensorType_FLOAT32;
+  tflite_micro::TensorType type = tflite_micro::TensorType_FLOAT32;
   uint32_t buffer = 0;
   std::string name{};
-  std::unique_ptr<tflite::QuantizationParametersT> quantization{};
+  std::unique_ptr<tflite_micro::QuantizationParametersT> quantization{};
   bool is_variable = false;
-  std::unique_ptr<tflite::SparsityParametersT> sparsity{};
+  std::unique_ptr<tflite_micro::SparsityParametersT> sparsity{};
   std::vector<int32_t> shape_signature{};
   bool has_rank = false;
-  std::vector<std::unique_ptr<tflite::VariantSubTypeT>> variant_tensors{};
+  std::vector<std::unique_ptr<tflite_micro::VariantSubTypeT>> variant_tensors{};
   TensorT() = default;
   TensorT(const TensorT &o);
   TensorT(TensorT&&) FLATBUFFERS_NOEXCEPT = default;
@@ -5771,8 +5771,8 @@ struct Tensor FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   const flatbuffers::Vector<int32_t> *shape() const {
     return GetPointer<const flatbuffers::Vector<int32_t> *>(VT_SHAPE);
   }
-  tflite::TensorType type() const {
-    return static_cast<tflite::TensorType>(GetField<int8_t>(VT_TYPE, 0));
+  tflite_micro::TensorType type() const {
+    return static_cast<tflite_micro::TensorType>(GetField<int8_t>(VT_TYPE, 0));
   }
   uint32_t buffer() const {
     return GetField<uint32_t>(VT_BUFFER, 0);
@@ -5780,14 +5780,14 @@ struct Tensor FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   const flatbuffers::String *name() const {
     return GetPointer<const flatbuffers::String *>(VT_NAME);
   }
-  const tflite::QuantizationParameters *quantization() const {
-    return GetPointer<const tflite::QuantizationParameters *>(VT_QUANTIZATION);
+  const tflite_micro::QuantizationParameters *quantization() const {
+    return GetPointer<const tflite_micro::QuantizationParameters *>(VT_QUANTIZATION);
   }
   bool is_variable() const {
     return GetField<uint8_t>(VT_IS_VARIABLE, 0) != 0;
   }
-  const tflite::SparsityParameters *sparsity() const {
-    return GetPointer<const tflite::SparsityParameters *>(VT_SPARSITY);
+  const tflite_micro::SparsityParameters *sparsity() const {
+    return GetPointer<const tflite_micro::SparsityParameters *>(VT_SPARSITY);
   }
   const flatbuffers::Vector<int32_t> *shape_signature() const {
     return GetPointer<const flatbuffers::Vector<int32_t> *>(VT_SHAPE_SIGNATURE);
@@ -5795,8 +5795,8 @@ struct Tensor FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   bool has_rank() const {
     return GetField<uint8_t>(VT_HAS_RANK, 0) != 0;
   }
-  const flatbuffers::Vector<flatbuffers::Offset<tflite::VariantSubType>> *variant_tensors() const {
-    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite::VariantSubType>> *>(VT_VARIANT_TENSORS);
+  const flatbuffers::Vector<flatbuffers::Offset<tflite_micro::VariantSubType>> *variant_tensors() const {
+    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite_micro::VariantSubType>> *>(VT_VARIANT_TENSORS);
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
@@ -5831,7 +5831,7 @@ struct TensorBuilder {
   void add_shape(flatbuffers::Offset<flatbuffers::Vector<int32_t>> shape) {
     fbb_.AddOffset(Tensor::VT_SHAPE, shape);
   }
-  void add_type(tflite::TensorType type) {
+  void add_type(tflite_micro::TensorType type) {
     fbb_.AddElement<int8_t>(Tensor::VT_TYPE, static_cast<int8_t>(type), 0);
   }
   void add_buffer(uint32_t buffer) {
@@ -5840,13 +5840,13 @@ struct TensorBuilder {
   void add_name(flatbuffers::Offset<flatbuffers::String> name) {
     fbb_.AddOffset(Tensor::VT_NAME, name);
   }
-  void add_quantization(flatbuffers::Offset<tflite::QuantizationParameters> quantization) {
+  void add_quantization(flatbuffers::Offset<tflite_micro::QuantizationParameters> quantization) {
     fbb_.AddOffset(Tensor::VT_QUANTIZATION, quantization);
   }
   void add_is_variable(bool is_variable) {
     fbb_.AddElement<uint8_t>(Tensor::VT_IS_VARIABLE, static_cast<uint8_t>(is_variable), 0);
   }
-  void add_sparsity(flatbuffers::Offset<tflite::SparsityParameters> sparsity) {
+  void add_sparsity(flatbuffers::Offset<tflite_micro::SparsityParameters> sparsity) {
     fbb_.AddOffset(Tensor::VT_SPARSITY, sparsity);
   }
   void add_shape_signature(flatbuffers::Offset<flatbuffers::Vector<int32_t>> shape_signature) {
@@ -5855,7 +5855,7 @@ struct TensorBuilder {
   void add_has_rank(bool has_rank) {
     fbb_.AddElement<uint8_t>(Tensor::VT_HAS_RANK, static_cast<uint8_t>(has_rank), 0);
   }
-  void add_variant_tensors(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::VariantSubType>>> variant_tensors) {
+  void add_variant_tensors(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite_micro::VariantSubType>>> variant_tensors) {
     fbb_.AddOffset(Tensor::VT_VARIANT_TENSORS, variant_tensors);
   }
   explicit TensorBuilder(flatbuffers::FlatBufferBuilder &_fbb)
@@ -5872,15 +5872,15 @@ struct TensorBuilder {
 inline flatbuffers::Offset<Tensor> CreateTensor(
     flatbuffers::FlatBufferBuilder &_fbb,
     flatbuffers::Offset<flatbuffers::Vector<int32_t>> shape = 0,
-    tflite::TensorType type = tflite::TensorType_FLOAT32,
+    tflite_micro::TensorType type = tflite_micro::TensorType_FLOAT32,
     uint32_t buffer = 0,
     flatbuffers::Offset<flatbuffers::String> name = 0,
-    flatbuffers::Offset<tflite::QuantizationParameters> quantization = 0,
+    flatbuffers::Offset<tflite_micro::QuantizationParameters> quantization = 0,
     bool is_variable = false,
-    flatbuffers::Offset<tflite::SparsityParameters> sparsity = 0,
+    flatbuffers::Offset<tflite_micro::SparsityParameters> sparsity = 0,
     flatbuffers::Offset<flatbuffers::Vector<int32_t>> shape_signature = 0,
     bool has_rank = false,
-    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::VariantSubType>>> variant_tensors = 0) {
+    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite_micro::VariantSubType>>> variant_tensors = 0) {
   TensorBuilder builder_(_fbb);
   builder_.add_variant_tensors(variant_tensors);
   builder_.add_shape_signature(shape_signature);
@@ -5898,20 +5898,20 @@ inline flatbuffers::Offset<Tensor> CreateTensor(
 inline flatbuffers::Offset<Tensor> CreateTensorDirect(
     flatbuffers::FlatBufferBuilder &_fbb,
     const std::vector<int32_t> *shape = nullptr,
-    tflite::TensorType type = tflite::TensorType_FLOAT32,
+    tflite_micro::TensorType type = tflite_micro::TensorType_FLOAT32,
     uint32_t buffer = 0,
     const char *name = nullptr,
-    flatbuffers::Offset<tflite::QuantizationParameters> quantization = 0,
+    flatbuffers::Offset<tflite_micro::QuantizationParameters> quantization = 0,
     bool is_variable = false,
-    flatbuffers::Offset<tflite::SparsityParameters> sparsity = 0,
+    flatbuffers::Offset<tflite_micro::SparsityParameters> sparsity = 0,
     const std::vector<int32_t> *shape_signature = nullptr,
     bool has_rank = false,
-    const std::vector<flatbuffers::Offset<tflite::VariantSubType>> *variant_tensors = nullptr) {
+    const std::vector<flatbuffers::Offset<tflite_micro::VariantSubType>> *variant_tensors = nullptr) {
   auto shape__ = shape ? _fbb.CreateVector<int32_t>(*shape) : 0;
   auto name__ = name ? _fbb.CreateString(name) : 0;
   auto shape_signature__ = shape_signature ? _fbb.CreateVector<int32_t>(*shape_signature) : 0;
-  auto variant_tensors__ = variant_tensors ? _fbb.CreateVector<flatbuffers::Offset<tflite::VariantSubType>>(*variant_tensors) : 0;
-  return tflite::CreateTensor(
+  auto variant_tensors__ = variant_tensors ? _fbb.CreateVector<flatbuffers::Offset<tflite_micro::VariantSubType>>(*variant_tensors) : 0;
+  return tflite_micro::CreateTensor(
       _fbb,
       shape__,
       type,
@@ -6048,7 +6048,7 @@ inline flatbuffers::Offset<StablehloGatherOptions> CreateStablehloGatherOptionsD
   auto collapsed_slice_dims__ = collapsed_slice_dims ? _fbb.CreateVector<int64_t>(*collapsed_slice_dims) : 0;
   auto start_index_map__ = start_index_map ? _fbb.CreateVector<int64_t>(*start_index_map) : 0;
   auto slice_sizes__ = slice_sizes ? _fbb.CreateVector<int64_t>(*slice_sizes) : 0;
-  return tflite::CreateStablehloGatherOptions(
+  return tflite_micro::CreateStablehloGatherOptions(
       _fbb,
       offset_dims__,
       collapsed_slice_dims__,
@@ -6115,7 +6115,7 @@ inline flatbuffers::Offset<StablehloTransposeOptions> CreateStablehloTransposeOp
     flatbuffers::FlatBufferBuilder &_fbb,
     const std::vector<int64_t> *permutation = nullptr) {
   auto permutation__ = permutation ? _fbb.CreateVector<int64_t>(*permutation) : 0;
-  return tflite::CreateStablehloTransposeOptions(
+  return tflite_micro::CreateStablehloTransposeOptions(
       _fbb,
       permutation__);
 }
@@ -6128,7 +6128,7 @@ struct StablehloDotGeneralOptionsT : public flatbuffers::NativeTable {
   std::vector<int64_t> rhs_batching_dimensions{};
   std::vector<int64_t> lhs_contracting_dimensions{};
   std::vector<int64_t> rhs_contracting_dimensions{};
-  std::vector<tflite::StablehloPrecisionConfig> precision_config{};
+  std::vector<tflite_micro::StablehloPrecisionConfig> precision_config{};
 };
 
 struct StablehloDotGeneralOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -6233,7 +6233,7 @@ inline flatbuffers::Offset<StablehloDotGeneralOptions> CreateStablehloDotGeneral
   auto lhs_contracting_dimensions__ = lhs_contracting_dimensions ? _fbb.CreateVector<int64_t>(*lhs_contracting_dimensions) : 0;
   auto rhs_contracting_dimensions__ = rhs_contracting_dimensions ? _fbb.CreateVector<int64_t>(*rhs_contracting_dimensions) : 0;
   auto precision_config__ = precision_config ? _fbb.CreateVector<uint32_t>(*precision_config) : 0;
-  return tflite::CreateStablehloDotGeneralOptions(
+  return tflite_micro::CreateStablehloDotGeneralOptions(
       _fbb,
       lhs_batching_dimensions__,
       rhs_batching_dimensions__,
@@ -6367,7 +6367,7 @@ inline flatbuffers::Offset<StablehloReduceWindowOptions> CreateStablehloReduceWi
   auto base_dilations__ = base_dilations ? _fbb.CreateVector<int64_t>(*base_dilations) : 0;
   auto window_dilations__ = window_dilations ? _fbb.CreateVector<int64_t>(*window_dilations) : 0;
   auto padding__ = padding ? _fbb.CreateVector<int64_t>(*padding) : 0;
-  return tflite::CreateStablehloReduceWindowOptions(
+  return tflite_micro::CreateStablehloReduceWindowOptions(
       _fbb,
       window_dimensions__,
       window_strides__,
@@ -6623,7 +6623,7 @@ inline flatbuffers::Offset<StablehloBroadcastInDimOptions> CreateStablehloBroadc
     flatbuffers::FlatBufferBuilder &_fbb,
     const std::vector<int64_t> *broadcast_dimensions = nullptr) {
   auto broadcast_dimensions__ = broadcast_dimensions ? _fbb.CreateVector<int64_t>(*broadcast_dimensions) : 0;
-  return tflite::CreateStablehloBroadcastInDimOptions(
+  return tflite_micro::CreateStablehloBroadcastInDimOptions(
       _fbb,
       broadcast_dimensions__);
 }
@@ -6632,8 +6632,8 @@ flatbuffers::Offset<StablehloBroadcastInDimOptions> CreateStablehloBroadcastInDi
 
 struct StablehloCompareOptionsT : public flatbuffers::NativeTable {
   typedef StablehloCompareOptions TableType;
-  tflite::StablehloComparisonDirection comparison_direction = tflite::StablehloComparisonDirection_STABLEHLO_COMPARISON_DIRECTION_EQ;
-  tflite::StablehloComparisonType compare_type = tflite::StablehloComparisonType_STABLEHLO_COMPARISON_TYPE_NOTYPE;
+  tflite_micro::StablehloComparisonDirection comparison_direction = tflite_micro::StablehloComparisonDirection_STABLEHLO_COMPARISON_DIRECTION_EQ;
+  tflite_micro::StablehloComparisonType compare_type = tflite_micro::StablehloComparisonType_STABLEHLO_COMPARISON_TYPE_NOTYPE;
 };
 
 struct StablehloCompareOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -6643,11 +6643,11 @@ struct StablehloCompareOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Ta
     VT_COMPARISON_DIRECTION = 4,
     VT_COMPARE_TYPE = 6
   };
-  tflite::StablehloComparisonDirection comparison_direction() const {
-    return static_cast<tflite::StablehloComparisonDirection>(GetField<uint32_t>(VT_COMPARISON_DIRECTION, 0));
+  tflite_micro::StablehloComparisonDirection comparison_direction() const {
+    return static_cast<tflite_micro::StablehloComparisonDirection>(GetField<uint32_t>(VT_COMPARISON_DIRECTION, 0));
   }
-  tflite::StablehloComparisonType compare_type() const {
-    return static_cast<tflite::StablehloComparisonType>(GetField<uint32_t>(VT_COMPARE_TYPE, 0));
+  tflite_micro::StablehloComparisonType compare_type() const {
+    return static_cast<tflite_micro::StablehloComparisonType>(GetField<uint32_t>(VT_COMPARE_TYPE, 0));
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
@@ -6664,10 +6664,10 @@ struct StablehloCompareOptionsBuilder {
   typedef StablehloCompareOptions Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_comparison_direction(tflite::StablehloComparisonDirection comparison_direction) {
+  void add_comparison_direction(tflite_micro::StablehloComparisonDirection comparison_direction) {
     fbb_.AddElement<uint32_t>(StablehloCompareOptions::VT_COMPARISON_DIRECTION, static_cast<uint32_t>(comparison_direction), 0);
   }
-  void add_compare_type(tflite::StablehloComparisonType compare_type) {
+  void add_compare_type(tflite_micro::StablehloComparisonType compare_type) {
     fbb_.AddElement<uint32_t>(StablehloCompareOptions::VT_COMPARE_TYPE, static_cast<uint32_t>(compare_type), 0);
   }
   explicit StablehloCompareOptionsBuilder(flatbuffers::FlatBufferBuilder &_fbb)
@@ -6683,8 +6683,8 @@ struct StablehloCompareOptionsBuilder {
 
 inline flatbuffers::Offset<StablehloCompareOptions> CreateStablehloCompareOptions(
     flatbuffers::FlatBufferBuilder &_fbb,
-    tflite::StablehloComparisonDirection comparison_direction = tflite::StablehloComparisonDirection_STABLEHLO_COMPARISON_DIRECTION_EQ,
-    tflite::StablehloComparisonType compare_type = tflite::StablehloComparisonType_STABLEHLO_COMPARISON_TYPE_NOTYPE) {
+    tflite_micro::StablehloComparisonDirection comparison_direction = tflite_micro::StablehloComparisonDirection_STABLEHLO_COMPARISON_DIRECTION_EQ,
+    tflite_micro::StablehloComparisonType compare_type = tflite_micro::StablehloComparisonType_STABLEHLO_COMPARISON_TYPE_NOTYPE) {
   StablehloCompareOptionsBuilder builder_(_fbb);
   builder_.add_compare_type(compare_type);
   builder_.add_comparison_direction(comparison_direction);
@@ -6748,7 +6748,7 @@ inline flatbuffers::Offset<StablehloDynamicSliceOptions> CreateStablehloDynamicS
     flatbuffers::FlatBufferBuilder &_fbb,
     const std::vector<int64_t> *slice_sizes = nullptr) {
   auto slice_sizes__ = slice_sizes ? _fbb.CreateVector<int64_t>(*slice_sizes) : 0;
-  return tflite::CreateStablehloDynamicSliceOptions(
+  return tflite_micro::CreateStablehloDynamicSliceOptions(
       _fbb,
       slice_sizes__);
 }
@@ -6838,7 +6838,7 @@ inline flatbuffers::Offset<StablehloPadOptions> CreateStablehloPadOptionsDirect(
   auto edge_padding_low__ = edge_padding_low ? _fbb.CreateVector<int64_t>(*edge_padding_low) : 0;
   auto edge_padding_high__ = edge_padding_high ? _fbb.CreateVector<int64_t>(*edge_padding_high) : 0;
   auto interior_padding__ = interior_padding ? _fbb.CreateVector<int64_t>(*interior_padding) : 0;
-  return tflite::CreateStablehloPadOptions(
+  return tflite_micro::CreateStablehloPadOptions(
       _fbb,
       edge_padding_low__,
       edge_padding_high__,
@@ -7020,7 +7020,7 @@ inline flatbuffers::Offset<StablehloCustomCallOptions> CreateStablehloCustomCall
   auto backend_config__ = backend_config ? _fbb.CreateString(backend_config) : 0;
   auto called_computations__ = called_computations ? _fbb.CreateVector<int32_t>(*called_computations) : 0;
   auto custom_attributes__ = custom_attributes ? _fbb.CreateVector<uint8_t>(*custom_attributes) : 0;
-  return tflite::CreateStablehloCustomCallOptions(
+  return tflite_micro::CreateStablehloCustomCallOptions(
       _fbb,
       call_target_name__,
       has_side_effect,
@@ -7099,7 +7099,7 @@ inline flatbuffers::Offset<StablehloReduceOptions> CreateStablehloReduceOptionsD
     const std::vector<int64_t> *dimensions = nullptr,
     int32_t body_subgraph_index = 0) {
   auto dimensions__ = dimensions ? _fbb.CreateVector<int64_t>(*dimensions) : 0;
-  return tflite::CreateStablehloReduceOptions(
+  return tflite_micro::CreateStablehloReduceOptions(
       _fbb,
       dimensions__,
       body_subgraph_index);
@@ -7190,7 +7190,7 @@ inline flatbuffers::Offset<StablehloSliceOptions> CreateStablehloSliceOptionsDir
   auto start_indices__ = start_indices ? _fbb.CreateVector<int64_t>(*start_indices) : 0;
   auto limit_indices__ = limit_indices ? _fbb.CreateVector<int64_t>(*limit_indices) : 0;
   auto strides__ = strides ? _fbb.CreateVector<int64_t>(*strides) : 0;
-  return tflite::CreateStablehloSliceOptions(
+  return tflite_micro::CreateStablehloSliceOptions(
       _fbb,
       start_indices__,
       limit_indices__,
@@ -7217,7 +7217,7 @@ struct StablehloConvolutionOptionsT : public flatbuffers::NativeTable {
   std::vector<int64_t> output_spatial_dimensions{};
   int64_t feature_group_count = 0;
   int64_t batch_group_count = 0;
-  std::vector<tflite::StablehloPrecisionConfig> precision_config{};
+  std::vector<tflite_micro::StablehloPrecisionConfig> precision_config{};
 };
 
 struct StablehloConvolutionOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -7462,7 +7462,7 @@ inline flatbuffers::Offset<StablehloConvolutionOptions> CreateStablehloConvoluti
   auto kernel_spatial_dimensions__ = kernel_spatial_dimensions ? _fbb.CreateVector<int64_t>(*kernel_spatial_dimensions) : 0;
   auto output_spatial_dimensions__ = output_spatial_dimensions ? _fbb.CreateVector<int64_t>(*output_spatial_dimensions) : 0;
   auto precision_config__ = precision_config ? _fbb.CreateVector<uint32_t>(*precision_config) : 0;
-  return tflite::CreateStablehloConvolutionOptions(
+  return tflite_micro::CreateStablehloConvolutionOptions(
       _fbb,
       window_strides__,
       padding__,
@@ -7616,7 +7616,7 @@ inline flatbuffers::Offset<StablehloScatterOptions> CreateStablehloScatterOption
   auto update_window_dims__ = update_window_dims ? _fbb.CreateVector<int64_t>(*update_window_dims) : 0;
   auto inserted_window_dims__ = inserted_window_dims ? _fbb.CreateVector<int64_t>(*inserted_window_dims) : 0;
   auto scatter_dims_to_operand_dims__ = scatter_dims_to_operand_dims ? _fbb.CreateVector<int64_t>(*scatter_dims_to_operand_dims) : 0;
-  return tflite::CreateStablehloScatterOptions(
+  return tflite_micro::CreateStablehloScatterOptions(
       _fbb,
       indices_are_sorted,
       update_window_dims__,
@@ -7631,7 +7631,7 @@ flatbuffers::Offset<StablehloScatterOptions> CreateStablehloScatterOptions(flatb
 
 struct StablehloRngBitGeneratorOptionsT : public flatbuffers::NativeTable {
   typedef StablehloRngBitGeneratorOptions TableType;
-  tflite::RngAlgorithm algorithm = tflite::RngAlgorithm_DEFAULT;
+  tflite_micro::RngAlgorithm algorithm = tflite_micro::RngAlgorithm_DEFAULT;
 };
 
 struct StablehloRngBitGeneratorOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -7640,8 +7640,8 @@ struct StablehloRngBitGeneratorOptions FLATBUFFERS_FINAL_CLASS : private flatbuf
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_ALGORITHM = 4
   };
-  tflite::RngAlgorithm algorithm() const {
-    return static_cast<tflite::RngAlgorithm>(GetField<int8_t>(VT_ALGORITHM, 0));
+  tflite_micro::RngAlgorithm algorithm() const {
+    return static_cast<tflite_micro::RngAlgorithm>(GetField<int8_t>(VT_ALGORITHM, 0));
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
@@ -7657,7 +7657,7 @@ struct StablehloRngBitGeneratorOptionsBuilder {
   typedef StablehloRngBitGeneratorOptions Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_algorithm(tflite::RngAlgorithm algorithm) {
+  void add_algorithm(tflite_micro::RngAlgorithm algorithm) {
     fbb_.AddElement<int8_t>(StablehloRngBitGeneratorOptions::VT_ALGORITHM, static_cast<int8_t>(algorithm), 0);
   }
   explicit StablehloRngBitGeneratorOptionsBuilder(flatbuffers::FlatBufferBuilder &_fbb)
@@ -7673,7 +7673,7 @@ struct StablehloRngBitGeneratorOptionsBuilder {
 
 inline flatbuffers::Offset<StablehloRngBitGeneratorOptions> CreateStablehloRngBitGeneratorOptions(
     flatbuffers::FlatBufferBuilder &_fbb,
-    tflite::RngAlgorithm algorithm = tflite::RngAlgorithm_DEFAULT) {
+    tflite_micro::RngAlgorithm algorithm = tflite_micro::RngAlgorithm_DEFAULT) {
   StablehloRngBitGeneratorOptionsBuilder builder_(_fbb);
   builder_.add_algorithm(algorithm);
   return builder_.Finish();
@@ -7683,13 +7683,13 @@ flatbuffers::Offset<StablehloRngBitGeneratorOptions> CreateStablehloRngBitGenera
 
 struct Conv2DOptionsT : public flatbuffers::NativeTable {
   typedef Conv2DOptions TableType;
-  tflite::Padding padding = tflite::Padding_SAME;
+  tflite_micro::Padding padding = tflite_micro::Padding_SAME;
   int32_t stride_w = 0;
   int32_t stride_h = 0;
-  tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE;
+  tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE;
   int32_t dilation_w_factor = 1;
   int32_t dilation_h_factor = 1;
-  tflite::TensorType quantized_bias_type = tflite::TensorType_FLOAT32;
+  tflite_micro::TensorType quantized_bias_type = tflite_micro::TensorType_FLOAT32;
 };
 
 struct Conv2DOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -7704,8 +7704,8 @@ struct Conv2DOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
     VT_DILATION_H_FACTOR = 14,
     VT_QUANTIZED_BIAS_TYPE = 16
   };
-  tflite::Padding padding() const {
-    return static_cast<tflite::Padding>(GetField<int8_t>(VT_PADDING, 0));
+  tflite_micro::Padding padding() const {
+    return static_cast<tflite_micro::Padding>(GetField<int8_t>(VT_PADDING, 0));
   }
   int32_t stride_w() const {
     return GetField<int32_t>(VT_STRIDE_W, 0);
@@ -7713,8 +7713,8 @@ struct Conv2DOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   int32_t stride_h() const {
     return GetField<int32_t>(VT_STRIDE_H, 0);
   }
-  tflite::ActivationFunctionType fused_activation_function() const {
-    return static_cast<tflite::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
+  tflite_micro::ActivationFunctionType fused_activation_function() const {
+    return static_cast<tflite_micro::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   }
   int32_t dilation_w_factor() const {
     return GetField<int32_t>(VT_DILATION_W_FACTOR, 1);
@@ -7722,8 +7722,8 @@ struct Conv2DOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   int32_t dilation_h_factor() const {
     return GetField<int32_t>(VT_DILATION_H_FACTOR, 1);
   }
-  tflite::TensorType quantized_bias_type() const {
-    return static_cast<tflite::TensorType>(GetField<int8_t>(VT_QUANTIZED_BIAS_TYPE, 0));
+  tflite_micro::TensorType quantized_bias_type() const {
+    return static_cast<tflite_micro::TensorType>(GetField<int8_t>(VT_QUANTIZED_BIAS_TYPE, 0));
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
@@ -7745,7 +7745,7 @@ struct Conv2DOptionsBuilder {
   typedef Conv2DOptions Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_padding(tflite::Padding padding) {
+  void add_padding(tflite_micro::Padding padding) {
     fbb_.AddElement<int8_t>(Conv2DOptions::VT_PADDING, static_cast<int8_t>(padding), 0);
   }
   void add_stride_w(int32_t stride_w) {
@@ -7754,7 +7754,7 @@ struct Conv2DOptionsBuilder {
   void add_stride_h(int32_t stride_h) {
     fbb_.AddElement<int32_t>(Conv2DOptions::VT_STRIDE_H, stride_h, 0);
   }
-  void add_fused_activation_function(tflite::ActivationFunctionType fused_activation_function) {
+  void add_fused_activation_function(tflite_micro::ActivationFunctionType fused_activation_function) {
     fbb_.AddElement<int8_t>(Conv2DOptions::VT_FUSED_ACTIVATION_FUNCTION, static_cast<int8_t>(fused_activation_function), 0);
   }
   void add_dilation_w_factor(int32_t dilation_w_factor) {
@@ -7763,7 +7763,7 @@ struct Conv2DOptionsBuilder {
   void add_dilation_h_factor(int32_t dilation_h_factor) {
     fbb_.AddElement<int32_t>(Conv2DOptions::VT_DILATION_H_FACTOR, dilation_h_factor, 1);
   }
-  void add_quantized_bias_type(tflite::TensorType quantized_bias_type) {
+  void add_quantized_bias_type(tflite_micro::TensorType quantized_bias_type) {
     fbb_.AddElement<int8_t>(Conv2DOptions::VT_QUANTIZED_BIAS_TYPE, static_cast<int8_t>(quantized_bias_type), 0);
   }
   explicit Conv2DOptionsBuilder(flatbuffers::FlatBufferBuilder &_fbb)
@@ -7779,13 +7779,13 @@ struct Conv2DOptionsBuilder {
 
 inline flatbuffers::Offset<Conv2DOptions> CreateConv2DOptions(
     flatbuffers::FlatBufferBuilder &_fbb,
-    tflite::Padding padding = tflite::Padding_SAME,
+    tflite_micro::Padding padding = tflite_micro::Padding_SAME,
     int32_t stride_w = 0,
     int32_t stride_h = 0,
-    tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE,
+    tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE,
     int32_t dilation_w_factor = 1,
     int32_t dilation_h_factor = 1,
-    tflite::TensorType quantized_bias_type = tflite::TensorType_FLOAT32) {
+    tflite_micro::TensorType quantized_bias_type = tflite_micro::TensorType_FLOAT32) {
   Conv2DOptionsBuilder builder_(_fbb);
   builder_.add_dilation_h_factor(dilation_h_factor);
   builder_.add_dilation_w_factor(dilation_w_factor);
@@ -7801,11 +7801,11 @@ flatbuffers::Offset<Conv2DOptions> CreateConv2DOptions(flatbuffers::FlatBufferBu
 
 struct Conv3DOptionsT : public flatbuffers::NativeTable {
   typedef Conv3DOptions TableType;
-  tflite::Padding padding = tflite::Padding_SAME;
+  tflite_micro::Padding padding = tflite_micro::Padding_SAME;
   int32_t stride_d = 0;
   int32_t stride_w = 0;
   int32_t stride_h = 0;
-  tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE;
+  tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE;
   int32_t dilation_d_factor = 1;
   int32_t dilation_w_factor = 1;
   int32_t dilation_h_factor = 1;
@@ -7824,8 +7824,8 @@ struct Conv3DOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
     VT_DILATION_W_FACTOR = 16,
     VT_DILATION_H_FACTOR = 18
   };
-  tflite::Padding padding() const {
-    return static_cast<tflite::Padding>(GetField<int8_t>(VT_PADDING, 0));
+  tflite_micro::Padding padding() const {
+    return static_cast<tflite_micro::Padding>(GetField<int8_t>(VT_PADDING, 0));
   }
   int32_t stride_d() const {
     return GetField<int32_t>(VT_STRIDE_D, 0);
@@ -7836,8 +7836,8 @@ struct Conv3DOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   int32_t stride_h() const {
     return GetField<int32_t>(VT_STRIDE_H, 0);
   }
-  tflite::ActivationFunctionType fused_activation_function() const {
-    return static_cast<tflite::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
+  tflite_micro::ActivationFunctionType fused_activation_function() const {
+    return static_cast<tflite_micro::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   }
   int32_t dilation_d_factor() const {
     return GetField<int32_t>(VT_DILATION_D_FACTOR, 1);
@@ -7869,7 +7869,7 @@ struct Conv3DOptionsBuilder {
   typedef Conv3DOptions Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_padding(tflite::Padding padding) {
+  void add_padding(tflite_micro::Padding padding) {
     fbb_.AddElement<int8_t>(Conv3DOptions::VT_PADDING, static_cast<int8_t>(padding), 0);
   }
   void add_stride_d(int32_t stride_d) {
@@ -7881,7 +7881,7 @@ struct Conv3DOptionsBuilder {
   void add_stride_h(int32_t stride_h) {
     fbb_.AddElement<int32_t>(Conv3DOptions::VT_STRIDE_H, stride_h, 0);
   }
-  void add_fused_activation_function(tflite::ActivationFunctionType fused_activation_function) {
+  void add_fused_activation_function(tflite_micro::ActivationFunctionType fused_activation_function) {
     fbb_.AddElement<int8_t>(Conv3DOptions::VT_FUSED_ACTIVATION_FUNCTION, static_cast<int8_t>(fused_activation_function), 0);
   }
   void add_dilation_d_factor(int32_t dilation_d_factor) {
@@ -7906,11 +7906,11 @@ struct Conv3DOptionsBuilder {
 
 inline flatbuffers::Offset<Conv3DOptions> CreateConv3DOptions(
     flatbuffers::FlatBufferBuilder &_fbb,
-    tflite::Padding padding = tflite::Padding_SAME,
+    tflite_micro::Padding padding = tflite_micro::Padding_SAME,
     int32_t stride_d = 0,
     int32_t stride_w = 0,
     int32_t stride_h = 0,
-    tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE,
+    tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE,
     int32_t dilation_d_factor = 1,
     int32_t dilation_w_factor = 1,
     int32_t dilation_h_factor = 1) {
@@ -7930,12 +7930,12 @@ flatbuffers::Offset<Conv3DOptions> CreateConv3DOptions(flatbuffers::FlatBufferBu
 
 struct Pool2DOptionsT : public flatbuffers::NativeTable {
   typedef Pool2DOptions TableType;
-  tflite::Padding padding = tflite::Padding_SAME;
+  tflite_micro::Padding padding = tflite_micro::Padding_SAME;
   int32_t stride_w = 0;
   int32_t stride_h = 0;
   int32_t filter_width = 0;
   int32_t filter_height = 0;
-  tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE;
+  tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE;
 };
 
 struct Pool2DOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -7949,8 +7949,8 @@ struct Pool2DOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
     VT_FILTER_HEIGHT = 12,
     VT_FUSED_ACTIVATION_FUNCTION = 14
   };
-  tflite::Padding padding() const {
-    return static_cast<tflite::Padding>(GetField<int8_t>(VT_PADDING, 0));
+  tflite_micro::Padding padding() const {
+    return static_cast<tflite_micro::Padding>(GetField<int8_t>(VT_PADDING, 0));
   }
   int32_t stride_w() const {
     return GetField<int32_t>(VT_STRIDE_W, 0);
@@ -7964,8 +7964,8 @@ struct Pool2DOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   int32_t filter_height() const {
     return GetField<int32_t>(VT_FILTER_HEIGHT, 0);
   }
-  tflite::ActivationFunctionType fused_activation_function() const {
-    return static_cast<tflite::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
+  tflite_micro::ActivationFunctionType fused_activation_function() const {
+    return static_cast<tflite_micro::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
@@ -7986,7 +7986,7 @@ struct Pool2DOptionsBuilder {
   typedef Pool2DOptions Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_padding(tflite::Padding padding) {
+  void add_padding(tflite_micro::Padding padding) {
     fbb_.AddElement<int8_t>(Pool2DOptions::VT_PADDING, static_cast<int8_t>(padding), 0);
   }
   void add_stride_w(int32_t stride_w) {
@@ -8001,7 +8001,7 @@ struct Pool2DOptionsBuilder {
   void add_filter_height(int32_t filter_height) {
     fbb_.AddElement<int32_t>(Pool2DOptions::VT_FILTER_HEIGHT, filter_height, 0);
   }
-  void add_fused_activation_function(tflite::ActivationFunctionType fused_activation_function) {
+  void add_fused_activation_function(tflite_micro::ActivationFunctionType fused_activation_function) {
     fbb_.AddElement<int8_t>(Pool2DOptions::VT_FUSED_ACTIVATION_FUNCTION, static_cast<int8_t>(fused_activation_function), 0);
   }
   explicit Pool2DOptionsBuilder(flatbuffers::FlatBufferBuilder &_fbb)
@@ -8017,12 +8017,12 @@ struct Pool2DOptionsBuilder {
 
 inline flatbuffers::Offset<Pool2DOptions> CreatePool2DOptions(
     flatbuffers::FlatBufferBuilder &_fbb,
-    tflite::Padding padding = tflite::Padding_SAME,
+    tflite_micro::Padding padding = tflite_micro::Padding_SAME,
     int32_t stride_w = 0,
     int32_t stride_h = 0,
     int32_t filter_width = 0,
     int32_t filter_height = 0,
-    tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE) {
+    tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE) {
   Pool2DOptionsBuilder builder_(_fbb);
   builder_.add_filter_height(filter_height);
   builder_.add_filter_width(filter_width);
@@ -8037,11 +8037,11 @@ flatbuffers::Offset<Pool2DOptions> CreatePool2DOptions(flatbuffers::FlatBufferBu
 
 struct DepthwiseConv2DOptionsT : public flatbuffers::NativeTable {
   typedef DepthwiseConv2DOptions TableType;
-  tflite::Padding padding = tflite::Padding_SAME;
+  tflite_micro::Padding padding = tflite_micro::Padding_SAME;
   int32_t stride_w = 0;
   int32_t stride_h = 0;
   int32_t depth_multiplier = 0;
-  tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE;
+  tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE;
   int32_t dilation_w_factor = 1;
   int32_t dilation_h_factor = 1;
 };
@@ -8058,8 +8058,8 @@ struct DepthwiseConv2DOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Tab
     VT_DILATION_W_FACTOR = 14,
     VT_DILATION_H_FACTOR = 16
   };
-  tflite::Padding padding() const {
-    return static_cast<tflite::Padding>(GetField<int8_t>(VT_PADDING, 0));
+  tflite_micro::Padding padding() const {
+    return static_cast<tflite_micro::Padding>(GetField<int8_t>(VT_PADDING, 0));
   }
   int32_t stride_w() const {
     return GetField<int32_t>(VT_STRIDE_W, 0);
@@ -8070,8 +8070,8 @@ struct DepthwiseConv2DOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Tab
   int32_t depth_multiplier() const {
     return GetField<int32_t>(VT_DEPTH_MULTIPLIER, 0);
   }
-  tflite::ActivationFunctionType fused_activation_function() const {
-    return static_cast<tflite::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
+  tflite_micro::ActivationFunctionType fused_activation_function() const {
+    return static_cast<tflite_micro::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   }
   int32_t dilation_w_factor() const {
     return GetField<int32_t>(VT_DILATION_W_FACTOR, 1);
@@ -8099,7 +8099,7 @@ struct DepthwiseConv2DOptionsBuilder {
   typedef DepthwiseConv2DOptions Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_padding(tflite::Padding padding) {
+  void add_padding(tflite_micro::Padding padding) {
     fbb_.AddElement<int8_t>(DepthwiseConv2DOptions::VT_PADDING, static_cast<int8_t>(padding), 0);
   }
   void add_stride_w(int32_t stride_w) {
@@ -8111,7 +8111,7 @@ struct DepthwiseConv2DOptionsBuilder {
   void add_depth_multiplier(int32_t depth_multiplier) {
     fbb_.AddElement<int32_t>(DepthwiseConv2DOptions::VT_DEPTH_MULTIPLIER, depth_multiplier, 0);
   }
-  void add_fused_activation_function(tflite::ActivationFunctionType fused_activation_function) {
+  void add_fused_activation_function(tflite_micro::ActivationFunctionType fused_activation_function) {
     fbb_.AddElement<int8_t>(DepthwiseConv2DOptions::VT_FUSED_ACTIVATION_FUNCTION, static_cast<int8_t>(fused_activation_function), 0);
   }
   void add_dilation_w_factor(int32_t dilation_w_factor) {
@@ -8133,11 +8133,11 @@ struct DepthwiseConv2DOptionsBuilder {
 
 inline flatbuffers::Offset<DepthwiseConv2DOptions> CreateDepthwiseConv2DOptions(
     flatbuffers::FlatBufferBuilder &_fbb,
-    tflite::Padding padding = tflite::Padding_SAME,
+    tflite_micro::Padding padding = tflite_micro::Padding_SAME,
     int32_t stride_w = 0,
     int32_t stride_h = 0,
     int32_t depth_multiplier = 0,
-    tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE,
+    tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE,
     int32_t dilation_w_factor = 1,
     int32_t dilation_h_factor = 1) {
   DepthwiseConv2DOptionsBuilder builder_(_fbb);
@@ -8234,7 +8234,7 @@ inline flatbuffers::Offset<ConcatEmbeddingsOptions> CreateConcatEmbeddingsOption
     const std::vector<int32_t> *embedding_dim_per_channel = nullptr) {
   auto num_columns_per_channel__ = num_columns_per_channel ? _fbb.CreateVector<int32_t>(*num_columns_per_channel) : 0;
   auto embedding_dim_per_channel__ = embedding_dim_per_channel ? _fbb.CreateVector<int32_t>(*embedding_dim_per_channel) : 0;
-  return tflite::CreateConcatEmbeddingsOptions(
+  return tflite_micro::CreateConcatEmbeddingsOptions(
       _fbb,
       num_channels,
       num_columns_per_channel__,
@@ -8245,7 +8245,7 @@ flatbuffers::Offset<ConcatEmbeddingsOptions> CreateConcatEmbeddingsOptions(flatb
 
 struct LSHProjectionOptionsT : public flatbuffers::NativeTable {
   typedef LSHProjectionOptions TableType;
-  tflite::LSHProjectionType type = tflite::LSHProjectionType_UNKNOWN;
+  tflite_micro::LSHProjectionType type = tflite_micro::LSHProjectionType_UNKNOWN;
 };
 
 struct LSHProjectionOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -8254,8 +8254,8 @@ struct LSHProjectionOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_TYPE = 4
   };
-  tflite::LSHProjectionType type() const {
-    return static_cast<tflite::LSHProjectionType>(GetField<int8_t>(VT_TYPE, 0));
+  tflite_micro::LSHProjectionType type() const {
+    return static_cast<tflite_micro::LSHProjectionType>(GetField<int8_t>(VT_TYPE, 0));
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
@@ -8271,7 +8271,7 @@ struct LSHProjectionOptionsBuilder {
   typedef LSHProjectionOptions Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_type(tflite::LSHProjectionType type) {
+  void add_type(tflite_micro::LSHProjectionType type) {
     fbb_.AddElement<int8_t>(LSHProjectionOptions::VT_TYPE, static_cast<int8_t>(type), 0);
   }
   explicit LSHProjectionOptionsBuilder(flatbuffers::FlatBufferBuilder &_fbb)
@@ -8287,7 +8287,7 @@ struct LSHProjectionOptionsBuilder {
 
 inline flatbuffers::Offset<LSHProjectionOptions> CreateLSHProjectionOptions(
     flatbuffers::FlatBufferBuilder &_fbb,
-    tflite::LSHProjectionType type = tflite::LSHProjectionType_UNKNOWN) {
+    tflite_micro::LSHProjectionType type = tflite_micro::LSHProjectionType_UNKNOWN) {
   LSHProjectionOptionsBuilder builder_(_fbb);
   builder_.add_type(type);
   return builder_.Finish();
@@ -8298,7 +8298,7 @@ flatbuffers::Offset<LSHProjectionOptions> CreateLSHProjectionOptions(flatbuffers
 struct SVDFOptionsT : public flatbuffers::NativeTable {
   typedef SVDFOptions TableType;
   int32_t rank = 0;
-  tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE;
+  tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE;
   bool asymmetric_quantize_inputs = false;
 };
 
@@ -8313,8 +8313,8 @@ struct SVDFOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   int32_t rank() const {
     return GetField<int32_t>(VT_RANK, 0);
   }
-  tflite::ActivationFunctionType fused_activation_function() const {
-    return static_cast<tflite::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
+  tflite_micro::ActivationFunctionType fused_activation_function() const {
+    return static_cast<tflite_micro::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   }
   bool asymmetric_quantize_inputs() const {
     return GetField<uint8_t>(VT_ASYMMETRIC_QUANTIZE_INPUTS, 0) != 0;
@@ -8338,7 +8338,7 @@ struct SVDFOptionsBuilder {
   void add_rank(int32_t rank) {
     fbb_.AddElement<int32_t>(SVDFOptions::VT_RANK, rank, 0);
   }
-  void add_fused_activation_function(tflite::ActivationFunctionType fused_activation_function) {
+  void add_fused_activation_function(tflite_micro::ActivationFunctionType fused_activation_function) {
     fbb_.AddElement<int8_t>(SVDFOptions::VT_FUSED_ACTIVATION_FUNCTION, static_cast<int8_t>(fused_activation_function), 0);
   }
   void add_asymmetric_quantize_inputs(bool asymmetric_quantize_inputs) {
@@ -8358,7 +8358,7 @@ struct SVDFOptionsBuilder {
 inline flatbuffers::Offset<SVDFOptions> CreateSVDFOptions(
     flatbuffers::FlatBufferBuilder &_fbb,
     int32_t rank = 0,
-    tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE,
+    tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE,
     bool asymmetric_quantize_inputs = false) {
   SVDFOptionsBuilder builder_(_fbb);
   builder_.add_rank(rank);
@@ -8371,7 +8371,7 @@ flatbuffers::Offset<SVDFOptions> CreateSVDFOptions(flatbuffers::FlatBufferBuilde
 
 struct RNNOptionsT : public flatbuffers::NativeTable {
   typedef RNNOptions TableType;
-  tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE;
+  tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE;
   bool asymmetric_quantize_inputs = false;
 };
 
@@ -8382,8 +8382,8 @@ struct RNNOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
     VT_FUSED_ACTIVATION_FUNCTION = 4,
     VT_ASYMMETRIC_QUANTIZE_INPUTS = 6
   };
-  tflite::ActivationFunctionType fused_activation_function() const {
-    return static_cast<tflite::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
+  tflite_micro::ActivationFunctionType fused_activation_function() const {
+    return static_cast<tflite_micro::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   }
   bool asymmetric_quantize_inputs() const {
     return GetField<uint8_t>(VT_ASYMMETRIC_QUANTIZE_INPUTS, 0) != 0;
@@ -8403,7 +8403,7 @@ struct RNNOptionsBuilder {
   typedef RNNOptions Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_fused_activation_function(tflite::ActivationFunctionType fused_activation_function) {
+  void add_fused_activation_function(tflite_micro::ActivationFunctionType fused_activation_function) {
     fbb_.AddElement<int8_t>(RNNOptions::VT_FUSED_ACTIVATION_FUNCTION, static_cast<int8_t>(fused_activation_function), 0);
   }
   void add_asymmetric_quantize_inputs(bool asymmetric_quantize_inputs) {
@@ -8422,7 +8422,7 @@ struct RNNOptionsBuilder {
 
 inline flatbuffers::Offset<RNNOptions> CreateRNNOptions(
     flatbuffers::FlatBufferBuilder &_fbb,
-    tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE,
+    tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE,
     bool asymmetric_quantize_inputs = false) {
   RNNOptionsBuilder builder_(_fbb);
   builder_.add_asymmetric_quantize_inputs(asymmetric_quantize_inputs);
@@ -8435,7 +8435,7 @@ flatbuffers::Offset<RNNOptions> CreateRNNOptions(flatbuffers::FlatBufferBuilder
 struct SequenceRNNOptionsT : public flatbuffers::NativeTable {
   typedef SequenceRNNOptions TableType;
   bool time_major = false;
-  tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE;
+  tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE;
   bool asymmetric_quantize_inputs = false;
 };
 
@@ -8450,8 +8450,8 @@ struct SequenceRNNOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   bool time_major() const {
     return GetField<uint8_t>(VT_TIME_MAJOR, 0) != 0;
   }
-  tflite::ActivationFunctionType fused_activation_function() const {
-    return static_cast<tflite::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
+  tflite_micro::ActivationFunctionType fused_activation_function() const {
+    return static_cast<tflite_micro::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   }
   bool asymmetric_quantize_inputs() const {
     return GetField<uint8_t>(VT_ASYMMETRIC_QUANTIZE_INPUTS, 0) != 0;
@@ -8475,7 +8475,7 @@ struct SequenceRNNOptionsBuilder {
   void add_time_major(bool time_major) {
     fbb_.AddElement<uint8_t>(SequenceRNNOptions::VT_TIME_MAJOR, static_cast<uint8_t>(time_major), 0);
   }
-  void add_fused_activation_function(tflite::ActivationFunctionType fused_activation_function) {
+  void add_fused_activation_function(tflite_micro::ActivationFunctionType fused_activation_function) {
     fbb_.AddElement<int8_t>(SequenceRNNOptions::VT_FUSED_ACTIVATION_FUNCTION, static_cast<int8_t>(fused_activation_function), 0);
   }
   void add_asymmetric_quantize_inputs(bool asymmetric_quantize_inputs) {
@@ -8495,7 +8495,7 @@ struct SequenceRNNOptionsBuilder {
 inline flatbuffers::Offset<SequenceRNNOptions> CreateSequenceRNNOptions(
     flatbuffers::FlatBufferBuilder &_fbb,
     bool time_major = false,
-    tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE,
+    tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE,
     bool asymmetric_quantize_inputs = false) {
   SequenceRNNOptionsBuilder builder_(_fbb);
   builder_.add_asymmetric_quantize_inputs(asymmetric_quantize_inputs);
@@ -8509,7 +8509,7 @@ flatbuffers::Offset<SequenceRNNOptions> CreateSequenceRNNOptions(flatbuffers::Fl
 struct BidirectionalSequenceRNNOptionsT : public flatbuffers::NativeTable {
   typedef BidirectionalSequenceRNNOptions TableType;
   bool time_major = false;
-  tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE;
+  tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE;
   bool merge_outputs = false;
   bool asymmetric_quantize_inputs = false;
 };
@@ -8526,8 +8526,8 @@ struct BidirectionalSequenceRNNOptions FLATBUFFERS_FINAL_CLASS : private flatbuf
   bool time_major() const {
     return GetField<uint8_t>(VT_TIME_MAJOR, 0) != 0;
   }
-  tflite::ActivationFunctionType fused_activation_function() const {
-    return static_cast<tflite::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
+  tflite_micro::ActivationFunctionType fused_activation_function() const {
+    return static_cast<tflite_micro::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   }
   bool merge_outputs() const {
     return GetField<uint8_t>(VT_MERGE_OUTPUTS, 0) != 0;
@@ -8555,7 +8555,7 @@ struct BidirectionalSequenceRNNOptionsBuilder {
   void add_time_major(bool time_major) {
     fbb_.AddElement<uint8_t>(BidirectionalSequenceRNNOptions::VT_TIME_MAJOR, static_cast<uint8_t>(time_major), 0);
   }
-  void add_fused_activation_function(tflite::ActivationFunctionType fused_activation_function) {
+  void add_fused_activation_function(tflite_micro::ActivationFunctionType fused_activation_function) {
     fbb_.AddElement<int8_t>(BidirectionalSequenceRNNOptions::VT_FUSED_ACTIVATION_FUNCTION, static_cast<int8_t>(fused_activation_function), 0);
   }
   void add_merge_outputs(bool merge_outputs) {
@@ -8578,7 +8578,7 @@ struct BidirectionalSequenceRNNOptionsBuilder {
 inline flatbuffers::Offset<BidirectionalSequenceRNNOptions> CreateBidirectionalSequenceRNNOptions(
     flatbuffers::FlatBufferBuilder &_fbb,
     bool time_major = false,
-    tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE,
+    tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE,
     bool merge_outputs = false,
     bool asymmetric_quantize_inputs = false) {
   BidirectionalSequenceRNNOptionsBuilder builder_(_fbb);
@@ -8593,11 +8593,11 @@ flatbuffers::Offset<BidirectionalSequenceRNNOptions> CreateBidirectionalSequence
 
 struct FullyConnectedOptionsT : public flatbuffers::NativeTable {
   typedef FullyConnectedOptions TableType;
-  tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE;
-  tflite::FullyConnectedOptionsWeightsFormat weights_format = tflite::FullyConnectedOptionsWeightsFormat_DEFAULT;
+  tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE;
+  tflite_micro::FullyConnectedOptionsWeightsFormat weights_format = tflite_micro::FullyConnectedOptionsWeightsFormat_DEFAULT;
   bool keep_num_dims = false;
   bool asymmetric_quantize_inputs = false;
-  tflite::TensorType quantized_bias_type = tflite::TensorType_FLOAT32;
+  tflite_micro::TensorType quantized_bias_type = tflite_micro::TensorType_FLOAT32;
 };
 
 struct FullyConnectedOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -8610,11 +8610,11 @@ struct FullyConnectedOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Tabl
     VT_ASYMMETRIC_QUANTIZE_INPUTS = 10,
     VT_QUANTIZED_BIAS_TYPE = 12
   };
-  tflite::ActivationFunctionType fused_activation_function() const {
-    return static_cast<tflite::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
+  tflite_micro::ActivationFunctionType fused_activation_function() const {
+    return static_cast<tflite_micro::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   }
-  tflite::FullyConnectedOptionsWeightsFormat weights_format() const {
-    return static_cast<tflite::FullyConnectedOptionsWeightsFormat>(GetField<int8_t>(VT_WEIGHTS_FORMAT, 0));
+  tflite_micro::FullyConnectedOptionsWeightsFormat weights_format() const {
+    return static_cast<tflite_micro::FullyConnectedOptionsWeightsFormat>(GetField<int8_t>(VT_WEIGHTS_FORMAT, 0));
   }
   bool keep_num_dims() const {
     return GetField<uint8_t>(VT_KEEP_NUM_DIMS, 0) != 0;
@@ -8622,8 +8622,8 @@ struct FullyConnectedOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Tabl
   bool asymmetric_quantize_inputs() const {
     return GetField<uint8_t>(VT_ASYMMETRIC_QUANTIZE_INPUTS, 0) != 0;
   }
-  tflite::TensorType quantized_bias_type() const {
-    return static_cast<tflite::TensorType>(GetField<int8_t>(VT_QUANTIZED_BIAS_TYPE, 0));
+  tflite_micro::TensorType quantized_bias_type() const {
+    return static_cast<tflite_micro::TensorType>(GetField<int8_t>(VT_QUANTIZED_BIAS_TYPE, 0));
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
@@ -8643,10 +8643,10 @@ struct FullyConnectedOptionsBuilder {
   typedef FullyConnectedOptions Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_fused_activation_function(tflite::ActivationFunctionType fused_activation_function) {
+  void add_fused_activation_function(tflite_micro::ActivationFunctionType fused_activation_function) {
     fbb_.AddElement<int8_t>(FullyConnectedOptions::VT_FUSED_ACTIVATION_FUNCTION, static_cast<int8_t>(fused_activation_function), 0);
   }
-  void add_weights_format(tflite::FullyConnectedOptionsWeightsFormat weights_format) {
+  void add_weights_format(tflite_micro::FullyConnectedOptionsWeightsFormat weights_format) {
     fbb_.AddElement<int8_t>(FullyConnectedOptions::VT_WEIGHTS_FORMAT, static_cast<int8_t>(weights_format), 0);
   }
   void add_keep_num_dims(bool keep_num_dims) {
@@ -8655,7 +8655,7 @@ struct FullyConnectedOptionsBuilder {
   void add_asymmetric_quantize_inputs(bool asymmetric_quantize_inputs) {
     fbb_.AddElement<uint8_t>(FullyConnectedOptions::VT_ASYMMETRIC_QUANTIZE_INPUTS, static_cast<uint8_t>(asymmetric_quantize_inputs), 0);
   }
-  void add_quantized_bias_type(tflite::TensorType quantized_bias_type) {
+  void add_quantized_bias_type(tflite_micro::TensorType quantized_bias_type) {
     fbb_.AddElement<int8_t>(FullyConnectedOptions::VT_QUANTIZED_BIAS_TYPE, static_cast<int8_t>(quantized_bias_type), 0);
   }
   explicit FullyConnectedOptionsBuilder(flatbuffers::FlatBufferBuilder &_fbb)
@@ -8671,11 +8671,11 @@ struct FullyConnectedOptionsBuilder {
 
 inline flatbuffers::Offset<FullyConnectedOptions> CreateFullyConnectedOptions(
     flatbuffers::FlatBufferBuilder &_fbb,
-    tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE,
-    tflite::FullyConnectedOptionsWeightsFormat weights_format = tflite::FullyConnectedOptionsWeightsFormat_DEFAULT,
+    tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE,
+    tflite_micro::FullyConnectedOptionsWeightsFormat weights_format = tflite_micro::FullyConnectedOptionsWeightsFormat_DEFAULT,
     bool keep_num_dims = false,
     bool asymmetric_quantize_inputs = false,
-    tflite::TensorType quantized_bias_type = tflite::TensorType_FLOAT32) {
+    tflite_micro::TensorType quantized_bias_type = tflite_micro::TensorType_FLOAT32) {
   FullyConnectedOptionsBuilder builder_(_fbb);
   builder_.add_quantized_bias_type(quantized_bias_type);
   builder_.add_asymmetric_quantize_inputs(asymmetric_quantize_inputs);
@@ -8742,7 +8742,7 @@ flatbuffers::Offset<SoftmaxOptions> CreateSoftmaxOptions(flatbuffers::FlatBuffer
 struct ConcatenationOptionsT : public flatbuffers::NativeTable {
   typedef ConcatenationOptions TableType;
   int32_t axis = 0;
-  tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE;
+  tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE;
 };
 
 struct ConcatenationOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -8755,8 +8755,8 @@ struct ConcatenationOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table
   int32_t axis() const {
     return GetField<int32_t>(VT_AXIS, 0);
   }
-  tflite::ActivationFunctionType fused_activation_function() const {
-    return static_cast<tflite::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
+  tflite_micro::ActivationFunctionType fused_activation_function() const {
+    return static_cast<tflite_micro::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
@@ -8776,7 +8776,7 @@ struct ConcatenationOptionsBuilder {
   void add_axis(int32_t axis) {
     fbb_.AddElement<int32_t>(ConcatenationOptions::VT_AXIS, axis, 0);
   }
-  void add_fused_activation_function(tflite::ActivationFunctionType fused_activation_function) {
+  void add_fused_activation_function(tflite_micro::ActivationFunctionType fused_activation_function) {
     fbb_.AddElement<int8_t>(ConcatenationOptions::VT_FUSED_ACTIVATION_FUNCTION, static_cast<int8_t>(fused_activation_function), 0);
   }
   explicit ConcatenationOptionsBuilder(flatbuffers::FlatBufferBuilder &_fbb)
@@ -8793,7 +8793,7 @@ struct ConcatenationOptionsBuilder {
 inline flatbuffers::Offset<ConcatenationOptions> CreateConcatenationOptions(
     flatbuffers::FlatBufferBuilder &_fbb,
     int32_t axis = 0,
-    tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE) {
+    tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE) {
   ConcatenationOptionsBuilder builder_(_fbb);
   builder_.add_axis(axis);
   builder_.add_fused_activation_function(fused_activation_function);
@@ -8804,7 +8804,7 @@ flatbuffers::Offset<ConcatenationOptions> CreateConcatenationOptions(flatbuffers
 
 struct AddOptionsT : public flatbuffers::NativeTable {
   typedef AddOptions TableType;
-  tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE;
+  tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE;
   bool pot_scale_int16 = true;
 };
 
@@ -8815,8 +8815,8 @@ struct AddOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
     VT_FUSED_ACTIVATION_FUNCTION = 4,
     VT_POT_SCALE_INT16 = 6
   };
-  tflite::ActivationFunctionType fused_activation_function() const {
-    return static_cast<tflite::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
+  tflite_micro::ActivationFunctionType fused_activation_function() const {
+    return static_cast<tflite_micro::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   }
   bool pot_scale_int16() const {
     return GetField<uint8_t>(VT_POT_SCALE_INT16, 1) != 0;
@@ -8836,7 +8836,7 @@ struct AddOptionsBuilder {
   typedef AddOptions Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_fused_activation_function(tflite::ActivationFunctionType fused_activation_function) {
+  void add_fused_activation_function(tflite_micro::ActivationFunctionType fused_activation_function) {
     fbb_.AddElement<int8_t>(AddOptions::VT_FUSED_ACTIVATION_FUNCTION, static_cast<int8_t>(fused_activation_function), 0);
   }
   void add_pot_scale_int16(bool pot_scale_int16) {
@@ -8855,7 +8855,7 @@ struct AddOptionsBuilder {
 
 inline flatbuffers::Offset<AddOptions> CreateAddOptions(
     flatbuffers::FlatBufferBuilder &_fbb,
-    tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE,
+    tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE,
     bool pot_scale_int16 = true) {
   AddOptionsBuilder builder_(_fbb);
   builder_.add_pot_scale_int16(pot_scale_int16);
@@ -8867,7 +8867,7 @@ flatbuffers::Offset<AddOptions> CreateAddOptions(flatbuffers::FlatBufferBuilder
 
 struct MulOptionsT : public flatbuffers::NativeTable {
   typedef MulOptions TableType;
-  tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE;
+  tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE;
 };
 
 struct MulOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -8876,8 +8876,8 @@ struct MulOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_FUSED_ACTIVATION_FUNCTION = 4
   };
-  tflite::ActivationFunctionType fused_activation_function() const {
-    return static_cast<tflite::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
+  tflite_micro::ActivationFunctionType fused_activation_function() const {
+    return static_cast<tflite_micro::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
@@ -8893,7 +8893,7 @@ struct MulOptionsBuilder {
   typedef MulOptions Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_fused_activation_function(tflite::ActivationFunctionType fused_activation_function) {
+  void add_fused_activation_function(tflite_micro::ActivationFunctionType fused_activation_function) {
     fbb_.AddElement<int8_t>(MulOptions::VT_FUSED_ACTIVATION_FUNCTION, static_cast<int8_t>(fused_activation_function), 0);
   }
   explicit MulOptionsBuilder(flatbuffers::FlatBufferBuilder &_fbb)
@@ -8909,7 +8909,7 @@ struct MulOptionsBuilder {
 
 inline flatbuffers::Offset<MulOptions> CreateMulOptions(
     flatbuffers::FlatBufferBuilder &_fbb,
-    tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE) {
+    tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE) {
   MulOptionsBuilder builder_(_fbb);
   builder_.add_fused_activation_function(fused_activation_function);
   return builder_.Finish();
@@ -8919,7 +8919,7 @@ flatbuffers::Offset<MulOptions> CreateMulOptions(flatbuffers::FlatBufferBuilder
 
 struct L2NormOptionsT : public flatbuffers::NativeTable {
   typedef L2NormOptions TableType;
-  tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE;
+  tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE;
 };
 
 struct L2NormOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -8928,8 +8928,8 @@ struct L2NormOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_FUSED_ACTIVATION_FUNCTION = 4
   };
-  tflite::ActivationFunctionType fused_activation_function() const {
-    return static_cast<tflite::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
+  tflite_micro::ActivationFunctionType fused_activation_function() const {
+    return static_cast<tflite_micro::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
@@ -8945,7 +8945,7 @@ struct L2NormOptionsBuilder {
   typedef L2NormOptions Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_fused_activation_function(tflite::ActivationFunctionType fused_activation_function) {
+  void add_fused_activation_function(tflite_micro::ActivationFunctionType fused_activation_function) {
     fbb_.AddElement<int8_t>(L2NormOptions::VT_FUSED_ACTIVATION_FUNCTION, static_cast<int8_t>(fused_activation_function), 0);
   }
   explicit L2NormOptionsBuilder(flatbuffers::FlatBufferBuilder &_fbb)
@@ -8961,7 +8961,7 @@ struct L2NormOptionsBuilder {
 
 inline flatbuffers::Offset<L2NormOptions> CreateL2NormOptions(
     flatbuffers::FlatBufferBuilder &_fbb,
-    tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE) {
+    tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE) {
   L2NormOptionsBuilder builder_(_fbb);
   builder_.add_fused_activation_function(fused_activation_function);
   return builder_.Finish();
@@ -9056,10 +9056,10 @@ flatbuffers::Offset<LocalResponseNormalizationOptions> CreateLocalResponseNormal
 
 struct LSTMOptionsT : public flatbuffers::NativeTable {
   typedef LSTMOptions TableType;
-  tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE;
+  tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE;
   float cell_clip = 0.0f;
   float proj_clip = 0.0f;
-  tflite::LSTMKernelType kernel_type = tflite::LSTMKernelType_FULL;
+  tflite_micro::LSTMKernelType kernel_type = tflite_micro::LSTMKernelType_FULL;
   bool asymmetric_quantize_inputs = false;
 };
 
@@ -9073,8 +9073,8 @@ struct LSTMOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
     VT_KERNEL_TYPE = 10,
     VT_ASYMMETRIC_QUANTIZE_INPUTS = 12
   };
-  tflite::ActivationFunctionType fused_activation_function() const {
-    return static_cast<tflite::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
+  tflite_micro::ActivationFunctionType fused_activation_function() const {
+    return static_cast<tflite_micro::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   }
   float cell_clip() const {
     return GetField<float>(VT_CELL_CLIP, 0.0f);
@@ -9082,8 +9082,8 @@ struct LSTMOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   float proj_clip() const {
     return GetField<float>(VT_PROJ_CLIP, 0.0f);
   }
-  tflite::LSTMKernelType kernel_type() const {
-    return static_cast<tflite::LSTMKernelType>(GetField<int8_t>(VT_KERNEL_TYPE, 0));
+  tflite_micro::LSTMKernelType kernel_type() const {
+    return static_cast<tflite_micro::LSTMKernelType>(GetField<int8_t>(VT_KERNEL_TYPE, 0));
   }
   bool asymmetric_quantize_inputs() const {
     return GetField<uint8_t>(VT_ASYMMETRIC_QUANTIZE_INPUTS, 0) != 0;
@@ -9106,7 +9106,7 @@ struct LSTMOptionsBuilder {
   typedef LSTMOptions Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_fused_activation_function(tflite::ActivationFunctionType fused_activation_function) {
+  void add_fused_activation_function(tflite_micro::ActivationFunctionType fused_activation_function) {
     fbb_.AddElement<int8_t>(LSTMOptions::VT_FUSED_ACTIVATION_FUNCTION, static_cast<int8_t>(fused_activation_function), 0);
   }
   void add_cell_clip(float cell_clip) {
@@ -9115,7 +9115,7 @@ struct LSTMOptionsBuilder {
   void add_proj_clip(float proj_clip) {
     fbb_.AddElement<float>(LSTMOptions::VT_PROJ_CLIP, proj_clip, 0.0f);
   }
-  void add_kernel_type(tflite::LSTMKernelType kernel_type) {
+  void add_kernel_type(tflite_micro::LSTMKernelType kernel_type) {
     fbb_.AddElement<int8_t>(LSTMOptions::VT_KERNEL_TYPE, static_cast<int8_t>(kernel_type), 0);
   }
   void add_asymmetric_quantize_inputs(bool asymmetric_quantize_inputs) {
@@ -9134,10 +9134,10 @@ struct LSTMOptionsBuilder {
 
 inline flatbuffers::Offset<LSTMOptions> CreateLSTMOptions(
     flatbuffers::FlatBufferBuilder &_fbb,
-    tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE,
+    tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE,
     float cell_clip = 0.0f,
     float proj_clip = 0.0f,
-    tflite::LSTMKernelType kernel_type = tflite::LSTMKernelType_FULL,
+    tflite_micro::LSTMKernelType kernel_type = tflite_micro::LSTMKernelType_FULL,
     bool asymmetric_quantize_inputs = false) {
   LSTMOptionsBuilder builder_(_fbb);
   builder_.add_proj_clip(proj_clip);
@@ -9152,7 +9152,7 @@ flatbuffers::Offset<LSTMOptions> CreateLSTMOptions(flatbuffers::FlatBufferBuilde
 
 struct UnidirectionalSequenceLSTMOptionsT : public flatbuffers::NativeTable {
   typedef UnidirectionalSequenceLSTMOptions TableType;
-  tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE;
+  tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE;
   float cell_clip = 0.0f;
   float proj_clip = 0.0f;
   bool time_major = false;
@@ -9171,8 +9171,8 @@ struct UnidirectionalSequenceLSTMOptions FLATBUFFERS_FINAL_CLASS : private flatb
     VT_ASYMMETRIC_QUANTIZE_INPUTS = 12,
     VT_DIAGONAL_RECURRENT_TENSORS = 14
   };
-  tflite::ActivationFunctionType fused_activation_function() const {
-    return static_cast<tflite::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
+  tflite_micro::ActivationFunctionType fused_activation_function() const {
+    return static_cast<tflite_micro::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   }
   float cell_clip() const {
     return GetField<float>(VT_CELL_CLIP, 0.0f);
@@ -9208,7 +9208,7 @@ struct UnidirectionalSequenceLSTMOptionsBuilder {
   typedef UnidirectionalSequenceLSTMOptions Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_fused_activation_function(tflite::ActivationFunctionType fused_activation_function) {
+  void add_fused_activation_function(tflite_micro::ActivationFunctionType fused_activation_function) {
     fbb_.AddElement<int8_t>(UnidirectionalSequenceLSTMOptions::VT_FUSED_ACTIVATION_FUNCTION, static_cast<int8_t>(fused_activation_function), 0);
   }
   void add_cell_clip(float cell_clip) {
@@ -9239,7 +9239,7 @@ struct UnidirectionalSequenceLSTMOptionsBuilder {
 
 inline flatbuffers::Offset<UnidirectionalSequenceLSTMOptions> CreateUnidirectionalSequenceLSTMOptions(
     flatbuffers::FlatBufferBuilder &_fbb,
-    tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE,
+    tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE,
     float cell_clip = 0.0f,
     float proj_clip = 0.0f,
     bool time_major = false,
@@ -9259,7 +9259,7 @@ flatbuffers::Offset<UnidirectionalSequenceLSTMOptions> CreateUnidirectionalSeque
 
 struct BidirectionalSequenceLSTMOptionsT : public flatbuffers::NativeTable {
   typedef BidirectionalSequenceLSTMOptions TableType;
-  tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE;
+  tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE;
   float cell_clip = 0.0f;
   float proj_clip = 0.0f;
   bool merge_outputs = false;
@@ -9278,8 +9278,8 @@ struct BidirectionalSequenceLSTMOptions FLATBUFFERS_FINAL_CLASS : private flatbu
     VT_TIME_MAJOR = 12,
     VT_ASYMMETRIC_QUANTIZE_INPUTS = 14
   };
-  tflite::ActivationFunctionType fused_activation_function() const {
-    return static_cast<tflite::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
+  tflite_micro::ActivationFunctionType fused_activation_function() const {
+    return static_cast<tflite_micro::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   }
   float cell_clip() const {
     return GetField<float>(VT_CELL_CLIP, 0.0f);
@@ -9315,7 +9315,7 @@ struct BidirectionalSequenceLSTMOptionsBuilder {
   typedef BidirectionalSequenceLSTMOptions Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_fused_activation_function(tflite::ActivationFunctionType fused_activation_function) {
+  void add_fused_activation_function(tflite_micro::ActivationFunctionType fused_activation_function) {
     fbb_.AddElement<int8_t>(BidirectionalSequenceLSTMOptions::VT_FUSED_ACTIVATION_FUNCTION, static_cast<int8_t>(fused_activation_function), 0);
   }
   void add_cell_clip(float cell_clip) {
@@ -9346,7 +9346,7 @@ struct BidirectionalSequenceLSTMOptionsBuilder {
 
 inline flatbuffers::Offset<BidirectionalSequenceLSTMOptions> CreateBidirectionalSequenceLSTMOptions(
     flatbuffers::FlatBufferBuilder &_fbb,
-    tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE,
+    tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE,
     float cell_clip = 0.0f,
     float proj_clip = 0.0f,
     bool merge_outputs = false,
@@ -9675,7 +9675,7 @@ inline flatbuffers::Offset<ReshapeOptions> CreateReshapeOptionsDirect(
     flatbuffers::FlatBufferBuilder &_fbb,
     const std::vector<int32_t> *new_shape = nullptr) {
   auto new_shape__ = new_shape ? _fbb.CreateVector<int32_t>(*new_shape) : 0;
-  return tflite::CreateReshapeOptions(
+  return tflite_micro::CreateReshapeOptions(
       _fbb,
       new_shape__);
 }
@@ -9940,7 +9940,7 @@ flatbuffers::Offset<DepthToSpaceOptions> CreateDepthToSpaceOptions(flatbuffers::
 
 struct SubOptionsT : public flatbuffers::NativeTable {
   typedef SubOptions TableType;
-  tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE;
+  tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE;
   bool pot_scale_int16 = true;
 };
 
@@ -9951,8 +9951,8 @@ struct SubOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
     VT_FUSED_ACTIVATION_FUNCTION = 4,
     VT_POT_SCALE_INT16 = 6
   };
-  tflite::ActivationFunctionType fused_activation_function() const {
-    return static_cast<tflite::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
+  tflite_micro::ActivationFunctionType fused_activation_function() const {
+    return static_cast<tflite_micro::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   }
   bool pot_scale_int16() const {
     return GetField<uint8_t>(VT_POT_SCALE_INT16, 1) != 0;
@@ -9972,7 +9972,7 @@ struct SubOptionsBuilder {
   typedef SubOptions Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_fused_activation_function(tflite::ActivationFunctionType fused_activation_function) {
+  void add_fused_activation_function(tflite_micro::ActivationFunctionType fused_activation_function) {
     fbb_.AddElement<int8_t>(SubOptions::VT_FUSED_ACTIVATION_FUNCTION, static_cast<int8_t>(fused_activation_function), 0);
   }
   void add_pot_scale_int16(bool pot_scale_int16) {
@@ -9991,7 +9991,7 @@ struct SubOptionsBuilder {
 
 inline flatbuffers::Offset<SubOptions> CreateSubOptions(
     flatbuffers::FlatBufferBuilder &_fbb,
-    tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE,
+    tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE,
     bool pot_scale_int16 = true) {
   SubOptionsBuilder builder_(_fbb);
   builder_.add_pot_scale_int16(pot_scale_int16);
@@ -10003,7 +10003,7 @@ flatbuffers::Offset<SubOptions> CreateSubOptions(flatbuffers::FlatBufferBuilder
 
 struct DivOptionsT : public flatbuffers::NativeTable {
   typedef DivOptions TableType;
-  tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE;
+  tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE;
 };
 
 struct DivOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -10012,8 +10012,8 @@ struct DivOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_FUSED_ACTIVATION_FUNCTION = 4
   };
-  tflite::ActivationFunctionType fused_activation_function() const {
-    return static_cast<tflite::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
+  tflite_micro::ActivationFunctionType fused_activation_function() const {
+    return static_cast<tflite_micro::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
@@ -10029,7 +10029,7 @@ struct DivOptionsBuilder {
   typedef DivOptions Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_fused_activation_function(tflite::ActivationFunctionType fused_activation_function) {
+  void add_fused_activation_function(tflite_micro::ActivationFunctionType fused_activation_function) {
     fbb_.AddElement<int8_t>(DivOptions::VT_FUSED_ACTIVATION_FUNCTION, static_cast<int8_t>(fused_activation_function), 0);
   }
   explicit DivOptionsBuilder(flatbuffers::FlatBufferBuilder &_fbb)
@@ -10045,7 +10045,7 @@ struct DivOptionsBuilder {
 
 inline flatbuffers::Offset<DivOptions> CreateDivOptions(
     flatbuffers::FlatBufferBuilder &_fbb,
-    tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE) {
+    tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE) {
   DivOptionsBuilder builder_(_fbb);
   builder_.add_fused_activation_function(fused_activation_function);
   return builder_.Finish();
@@ -10094,7 +10094,7 @@ flatbuffers::Offset<TopKV2Options> CreateTopKV2Options(flatbuffers::FlatBufferBu
 
 struct EmbeddingLookupSparseOptionsT : public flatbuffers::NativeTable {
   typedef EmbeddingLookupSparseOptions TableType;
-  tflite::CombinerType combiner = tflite::CombinerType_SUM;
+  tflite_micro::CombinerType combiner = tflite_micro::CombinerType_SUM;
 };
 
 struct EmbeddingLookupSparseOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -10103,8 +10103,8 @@ struct EmbeddingLookupSparseOptions FLATBUFFERS_FINAL_CLASS : private flatbuffer
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_COMBINER = 4
   };
-  tflite::CombinerType combiner() const {
-    return static_cast<tflite::CombinerType>(GetField<int8_t>(VT_COMBINER, 0));
+  tflite_micro::CombinerType combiner() const {
+    return static_cast<tflite_micro::CombinerType>(GetField<int8_t>(VT_COMBINER, 0));
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
@@ -10120,7 +10120,7 @@ struct EmbeddingLookupSparseOptionsBuilder {
   typedef EmbeddingLookupSparseOptions Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_combiner(tflite::CombinerType combiner) {
+  void add_combiner(tflite_micro::CombinerType combiner) {
     fbb_.AddElement<int8_t>(EmbeddingLookupSparseOptions::VT_COMBINER, static_cast<int8_t>(combiner), 0);
   }
   explicit EmbeddingLookupSparseOptionsBuilder(flatbuffers::FlatBufferBuilder &_fbb)
@@ -10136,7 +10136,7 @@ struct EmbeddingLookupSparseOptionsBuilder {
 
 inline flatbuffers::Offset<EmbeddingLookupSparseOptions> CreateEmbeddingLookupSparseOptions(
     flatbuffers::FlatBufferBuilder &_fbb,
-    tflite::CombinerType combiner = tflite::CombinerType_SUM) {
+    tflite_micro::CombinerType combiner = tflite_micro::CombinerType_SUM) {
   EmbeddingLookupSparseOptionsBuilder builder_(_fbb);
   builder_.add_combiner(combiner);
   return builder_.Finish();
@@ -10431,7 +10431,7 @@ inline flatbuffers::Offset<SqueezeOptions> CreateSqueezeOptionsDirect(
     flatbuffers::FlatBufferBuilder &_fbb,
     const std::vector<int32_t> *squeeze_dims = nullptr) {
   auto squeeze_dims__ = squeeze_dims ? _fbb.CreateVector<int32_t>(*squeeze_dims) : 0;
-  return tflite::CreateSqueezeOptions(
+  return tflite_micro::CreateSqueezeOptions(
       _fbb,
       squeeze_dims__);
 }
@@ -10690,8 +10690,8 @@ flatbuffers::Offset<LogSoftmaxOptions> CreateLogSoftmaxOptions(flatbuffers::Flat
 
 struct CastOptionsT : public flatbuffers::NativeTable {
   typedef CastOptions TableType;
-  tflite::TensorType in_data_type = tflite::TensorType_FLOAT32;
-  tflite::TensorType out_data_type = tflite::TensorType_FLOAT32;
+  tflite_micro::TensorType in_data_type = tflite_micro::TensorType_FLOAT32;
+  tflite_micro::TensorType out_data_type = tflite_micro::TensorType_FLOAT32;
 };
 
 struct CastOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -10701,11 +10701,11 @@ struct CastOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
     VT_IN_DATA_TYPE = 4,
     VT_OUT_DATA_TYPE = 6
   };
-  tflite::TensorType in_data_type() const {
-    return static_cast<tflite::TensorType>(GetField<int8_t>(VT_IN_DATA_TYPE, 0));
+  tflite_micro::TensorType in_data_type() const {
+    return static_cast<tflite_micro::TensorType>(GetField<int8_t>(VT_IN_DATA_TYPE, 0));
   }
-  tflite::TensorType out_data_type() const {
-    return static_cast<tflite::TensorType>(GetField<int8_t>(VT_OUT_DATA_TYPE, 0));
+  tflite_micro::TensorType out_data_type() const {
+    return static_cast<tflite_micro::TensorType>(GetField<int8_t>(VT_OUT_DATA_TYPE, 0));
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
@@ -10722,10 +10722,10 @@ struct CastOptionsBuilder {
   typedef CastOptions Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_in_data_type(tflite::TensorType in_data_type) {
+  void add_in_data_type(tflite_micro::TensorType in_data_type) {
     fbb_.AddElement<int8_t>(CastOptions::VT_IN_DATA_TYPE, static_cast<int8_t>(in_data_type), 0);
   }
-  void add_out_data_type(tflite::TensorType out_data_type) {
+  void add_out_data_type(tflite_micro::TensorType out_data_type) {
     fbb_.AddElement<int8_t>(CastOptions::VT_OUT_DATA_TYPE, static_cast<int8_t>(out_data_type), 0);
   }
   explicit CastOptionsBuilder(flatbuffers::FlatBufferBuilder &_fbb)
@@ -10741,8 +10741,8 @@ struct CastOptionsBuilder {
 
 inline flatbuffers::Offset<CastOptions> CreateCastOptions(
     flatbuffers::FlatBufferBuilder &_fbb,
-    tflite::TensorType in_data_type = tflite::TensorType_FLOAT32,
-    tflite::TensorType out_data_type = tflite::TensorType_FLOAT32) {
+    tflite_micro::TensorType in_data_type = tflite_micro::TensorType_FLOAT32,
+    tflite_micro::TensorType out_data_type = tflite_micro::TensorType_FLOAT32) {
   CastOptionsBuilder builder_(_fbb);
   builder_.add_out_data_type(out_data_type);
   builder_.add_in_data_type(in_data_type);
@@ -10870,7 +10870,7 @@ flatbuffers::Offset<TileOptions> CreateTileOptions(flatbuffers::FlatBufferBuilde
 
 struct ArgMaxOptionsT : public flatbuffers::NativeTable {
   typedef ArgMaxOptions TableType;
-  tflite::TensorType output_type = tflite::TensorType_FLOAT32;
+  tflite_micro::TensorType output_type = tflite_micro::TensorType_FLOAT32;
 };
 
 struct ArgMaxOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -10879,8 +10879,8 @@ struct ArgMaxOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_OUTPUT_TYPE = 4
   };
-  tflite::TensorType output_type() const {
-    return static_cast<tflite::TensorType>(GetField<int8_t>(VT_OUTPUT_TYPE, 0));
+  tflite_micro::TensorType output_type() const {
+    return static_cast<tflite_micro::TensorType>(GetField<int8_t>(VT_OUTPUT_TYPE, 0));
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
@@ -10896,7 +10896,7 @@ struct ArgMaxOptionsBuilder {
   typedef ArgMaxOptions Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_output_type(tflite::TensorType output_type) {
+  void add_output_type(tflite_micro::TensorType output_type) {
     fbb_.AddElement<int8_t>(ArgMaxOptions::VT_OUTPUT_TYPE, static_cast<int8_t>(output_type), 0);
   }
   explicit ArgMaxOptionsBuilder(flatbuffers::FlatBufferBuilder &_fbb)
@@ -10912,7 +10912,7 @@ struct ArgMaxOptionsBuilder {
 
 inline flatbuffers::Offset<ArgMaxOptions> CreateArgMaxOptions(
     flatbuffers::FlatBufferBuilder &_fbb,
-    tflite::TensorType output_type = tflite::TensorType_FLOAT32) {
+    tflite_micro::TensorType output_type = tflite_micro::TensorType_FLOAT32) {
   ArgMaxOptionsBuilder builder_(_fbb);
   builder_.add_output_type(output_type);
   return builder_.Finish();
@@ -10922,7 +10922,7 @@ flatbuffers::Offset<ArgMaxOptions> CreateArgMaxOptions(flatbuffers::FlatBufferBu
 
 struct ArgMinOptionsT : public flatbuffers::NativeTable {
   typedef ArgMinOptions TableType;
-  tflite::TensorType output_type = tflite::TensorType_FLOAT32;
+  tflite_micro::TensorType output_type = tflite_micro::TensorType_FLOAT32;
 };
 
 struct ArgMinOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -10931,8 +10931,8 @@ struct ArgMinOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_OUTPUT_TYPE = 4
   };
-  tflite::TensorType output_type() const {
-    return static_cast<tflite::TensorType>(GetField<int8_t>(VT_OUTPUT_TYPE, 0));
+  tflite_micro::TensorType output_type() const {
+    return static_cast<tflite_micro::TensorType>(GetField<int8_t>(VT_OUTPUT_TYPE, 0));
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
@@ -10948,7 +10948,7 @@ struct ArgMinOptionsBuilder {
   typedef ArgMinOptions Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_output_type(tflite::TensorType output_type) {
+  void add_output_type(tflite_micro::TensorType output_type) {
     fbb_.AddElement<int8_t>(ArgMinOptions::VT_OUTPUT_TYPE, static_cast<int8_t>(output_type), 0);
   }
   explicit ArgMinOptionsBuilder(flatbuffers::FlatBufferBuilder &_fbb)
@@ -10964,7 +10964,7 @@ struct ArgMinOptionsBuilder {
 
 inline flatbuffers::Offset<ArgMinOptions> CreateArgMinOptions(
     flatbuffers::FlatBufferBuilder &_fbb,
-    tflite::TensorType output_type = tflite::TensorType_FLOAT32) {
+    tflite_micro::TensorType output_type = tflite_micro::TensorType_FLOAT32) {
   ArgMinOptionsBuilder builder_(_fbb);
   builder_.add_output_type(output_type);
   return builder_.Finish();
@@ -11247,11 +11247,11 @@ flatbuffers::Offset<SliceOptions> CreateSliceOptions(flatbuffers::FlatBufferBuil
 
 struct TransposeConvOptionsT : public flatbuffers::NativeTable {
   typedef TransposeConvOptions TableType;
-  tflite::Padding padding = tflite::Padding_SAME;
+  tflite_micro::Padding padding = tflite_micro::Padding_SAME;
   int32_t stride_w = 0;
   int32_t stride_h = 0;
-  tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE;
-  tflite::TensorType quantized_bias_type = tflite::TensorType_FLOAT32;
+  tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE;
+  tflite_micro::TensorType quantized_bias_type = tflite_micro::TensorType_FLOAT32;
 };
 
 struct TransposeConvOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -11264,8 +11264,8 @@ struct TransposeConvOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table
     VT_FUSED_ACTIVATION_FUNCTION = 10,
     VT_QUANTIZED_BIAS_TYPE = 12
   };
-  tflite::Padding padding() const {
-    return static_cast<tflite::Padding>(GetField<int8_t>(VT_PADDING, 0));
+  tflite_micro::Padding padding() const {
+    return static_cast<tflite_micro::Padding>(GetField<int8_t>(VT_PADDING, 0));
   }
   int32_t stride_w() const {
     return GetField<int32_t>(VT_STRIDE_W, 0);
@@ -11273,11 +11273,11 @@ struct TransposeConvOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table
   int32_t stride_h() const {
     return GetField<int32_t>(VT_STRIDE_H, 0);
   }
-  tflite::ActivationFunctionType fused_activation_function() const {
-    return static_cast<tflite::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
+  tflite_micro::ActivationFunctionType fused_activation_function() const {
+    return static_cast<tflite_micro::ActivationFunctionType>(GetField<int8_t>(VT_FUSED_ACTIVATION_FUNCTION, 0));
   }
-  tflite::TensorType quantized_bias_type() const {
-    return static_cast<tflite::TensorType>(GetField<int8_t>(VT_QUANTIZED_BIAS_TYPE, 0));
+  tflite_micro::TensorType quantized_bias_type() const {
+    return static_cast<tflite_micro::TensorType>(GetField<int8_t>(VT_QUANTIZED_BIAS_TYPE, 0));
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
@@ -11297,7 +11297,7 @@ struct TransposeConvOptionsBuilder {
   typedef TransposeConvOptions Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_padding(tflite::Padding padding) {
+  void add_padding(tflite_micro::Padding padding) {
     fbb_.AddElement<int8_t>(TransposeConvOptions::VT_PADDING, static_cast<int8_t>(padding), 0);
   }
   void add_stride_w(int32_t stride_w) {
@@ -11306,10 +11306,10 @@ struct TransposeConvOptionsBuilder {
   void add_stride_h(int32_t stride_h) {
     fbb_.AddElement<int32_t>(TransposeConvOptions::VT_STRIDE_H, stride_h, 0);
   }
-  void add_fused_activation_function(tflite::ActivationFunctionType fused_activation_function) {
+  void add_fused_activation_function(tflite_micro::ActivationFunctionType fused_activation_function) {
     fbb_.AddElement<int8_t>(TransposeConvOptions::VT_FUSED_ACTIVATION_FUNCTION, static_cast<int8_t>(fused_activation_function), 0);
   }
-  void add_quantized_bias_type(tflite::TensorType quantized_bias_type) {
+  void add_quantized_bias_type(tflite_micro::TensorType quantized_bias_type) {
     fbb_.AddElement<int8_t>(TransposeConvOptions::VT_QUANTIZED_BIAS_TYPE, static_cast<int8_t>(quantized_bias_type), 0);
   }
   explicit TransposeConvOptionsBuilder(flatbuffers::FlatBufferBuilder &_fbb)
@@ -11325,11 +11325,11 @@ struct TransposeConvOptionsBuilder {
 
 inline flatbuffers::Offset<TransposeConvOptions> CreateTransposeConvOptions(
     flatbuffers::FlatBufferBuilder &_fbb,
-    tflite::Padding padding = tflite::Padding_SAME,
+    tflite_micro::Padding padding = tflite_micro::Padding_SAME,
     int32_t stride_w = 0,
     int32_t stride_h = 0,
-    tflite::ActivationFunctionType fused_activation_function = tflite::ActivationFunctionType_NONE,
-    tflite::TensorType quantized_bias_type = tflite::TensorType_FLOAT32) {
+    tflite_micro::ActivationFunctionType fused_activation_function = tflite_micro::ActivationFunctionType_NONE,
+    tflite_micro::TensorType quantized_bias_type = tflite_micro::TensorType_FLOAT32) {
   TransposeConvOptionsBuilder builder_(_fbb);
   builder_.add_stride_h(stride_h);
   builder_.add_stride_w(stride_w);
@@ -11512,7 +11512,7 @@ flatbuffers::Offset<NotEqualOptions> CreateNotEqualOptions(flatbuffers::FlatBuff
 
 struct ShapeOptionsT : public flatbuffers::NativeTable {
   typedef ShapeOptions TableType;
-  tflite::TensorType out_type = tflite::TensorType_FLOAT32;
+  tflite_micro::TensorType out_type = tflite_micro::TensorType_FLOAT32;
 };
 
 struct ShapeOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -11521,8 +11521,8 @@ struct ShapeOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_OUT_TYPE = 4
   };
-  tflite::TensorType out_type() const {
-    return static_cast<tflite::TensorType>(GetField<int8_t>(VT_OUT_TYPE, 0));
+  tflite_micro::TensorType out_type() const {
+    return static_cast<tflite_micro::TensorType>(GetField<int8_t>(VT_OUT_TYPE, 0));
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
@@ -11538,7 +11538,7 @@ struct ShapeOptionsBuilder {
   typedef ShapeOptions Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_out_type(tflite::TensorType out_type) {
+  void add_out_type(tflite_micro::TensorType out_type) {
     fbb_.AddElement<int8_t>(ShapeOptions::VT_OUT_TYPE, static_cast<int8_t>(out_type), 0);
   }
   explicit ShapeOptionsBuilder(flatbuffers::FlatBufferBuilder &_fbb)
@@ -11554,7 +11554,7 @@ struct ShapeOptionsBuilder {
 
 inline flatbuffers::Offset<ShapeOptions> CreateShapeOptions(
     flatbuffers::FlatBufferBuilder &_fbb,
-    tflite::TensorType out_type = tflite::TensorType_FLOAT32) {
+    tflite_micro::TensorType out_type = tflite_micro::TensorType_FLOAT32) {
   ShapeOptionsBuilder builder_(_fbb);
   builder_.add_out_type(out_type);
   return builder_.Finish();
@@ -12425,7 +12425,7 @@ flatbuffers::Offset<SquaredDifferenceOptions> CreateSquaredDifferenceOptions(fla
 
 struct MirrorPadOptionsT : public flatbuffers::NativeTable {
   typedef MirrorPadOptions TableType;
-  tflite::MirrorPadMode mode = tflite::MirrorPadMode_REFLECT;
+  tflite_micro::MirrorPadMode mode = tflite_micro::MirrorPadMode_REFLECT;
 };
 
 struct MirrorPadOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -12434,8 +12434,8 @@ struct MirrorPadOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_MODE = 4
   };
-  tflite::MirrorPadMode mode() const {
-    return static_cast<tflite::MirrorPadMode>(GetField<int8_t>(VT_MODE, 0));
+  tflite_micro::MirrorPadMode mode() const {
+    return static_cast<tflite_micro::MirrorPadMode>(GetField<int8_t>(VT_MODE, 0));
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
@@ -12451,7 +12451,7 @@ struct MirrorPadOptionsBuilder {
   typedef MirrorPadOptions Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_mode(tflite::MirrorPadMode mode) {
+  void add_mode(tflite_micro::MirrorPadMode mode) {
     fbb_.AddElement<int8_t>(MirrorPadOptions::VT_MODE, static_cast<int8_t>(mode), 0);
   }
   explicit MirrorPadOptionsBuilder(flatbuffers::FlatBufferBuilder &_fbb)
@@ -12467,7 +12467,7 @@ struct MirrorPadOptionsBuilder {
 
 inline flatbuffers::Offset<MirrorPadOptions> CreateMirrorPadOptions(
     flatbuffers::FlatBufferBuilder &_fbb,
-    tflite::MirrorPadMode mode = tflite::MirrorPadMode_REFLECT) {
+    tflite_micro::MirrorPadMode mode = tflite_micro::MirrorPadMode_REFLECT) {
   MirrorPadOptionsBuilder builder_(_fbb);
   builder_.add_mode(mode);
   return builder_.Finish();
@@ -12477,7 +12477,7 @@ flatbuffers::Offset<MirrorPadOptions> CreateMirrorPadOptions(flatbuffers::FlatBu
 
 struct UniqueOptionsT : public flatbuffers::NativeTable {
   typedef UniqueOptions TableType;
-  tflite::TensorType idx_out_type = tflite::TensorType_INT32;
+  tflite_micro::TensorType idx_out_type = tflite_micro::TensorType_INT32;
 };
 
 struct UniqueOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -12486,8 +12486,8 @@ struct UniqueOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_IDX_OUT_TYPE = 4
   };
-  tflite::TensorType idx_out_type() const {
-    return static_cast<tflite::TensorType>(GetField<int8_t>(VT_IDX_OUT_TYPE, 2));
+  tflite_micro::TensorType idx_out_type() const {
+    return static_cast<tflite_micro::TensorType>(GetField<int8_t>(VT_IDX_OUT_TYPE, 2));
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
@@ -12503,7 +12503,7 @@ struct UniqueOptionsBuilder {
   typedef UniqueOptions Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_idx_out_type(tflite::TensorType idx_out_type) {
+  void add_idx_out_type(tflite_micro::TensorType idx_out_type) {
     fbb_.AddElement<int8_t>(UniqueOptions::VT_IDX_OUT_TYPE, static_cast<int8_t>(idx_out_type), 2);
   }
   explicit UniqueOptionsBuilder(flatbuffers::FlatBufferBuilder &_fbb)
@@ -12519,7 +12519,7 @@ struct UniqueOptionsBuilder {
 
 inline flatbuffers::Offset<UniqueOptions> CreateUniqueOptions(
     flatbuffers::FlatBufferBuilder &_fbb,
-    tflite::TensorType idx_out_type = tflite::TensorType_INT32) {
+    tflite_micro::TensorType idx_out_type = tflite_micro::TensorType_INT32) {
   UniqueOptionsBuilder builder_(_fbb);
   builder_.add_idx_out_type(idx_out_type);
   return builder_.Finish();
@@ -13493,8 +13493,8 @@ flatbuffers::Offset<Rfft2dOptions> CreateRfft2dOptions(flatbuffers::FlatBufferBu
 struct HashtableOptionsT : public flatbuffers::NativeTable {
   typedef HashtableOptions TableType;
   int32_t table_id = 0;
-  tflite::TensorType key_dtype = tflite::TensorType_FLOAT32;
-  tflite::TensorType value_dtype = tflite::TensorType_FLOAT32;
+  tflite_micro::TensorType key_dtype = tflite_micro::TensorType_FLOAT32;
+  tflite_micro::TensorType value_dtype = tflite_micro::TensorType_FLOAT32;
 };
 
 struct HashtableOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -13508,11 +13508,11 @@ struct HashtableOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   int32_t table_id() const {
     return GetField<int32_t>(VT_TABLE_ID, 0);
   }
-  tflite::TensorType key_dtype() const {
-    return static_cast<tflite::TensorType>(GetField<int8_t>(VT_KEY_DTYPE, 0));
+  tflite_micro::TensorType key_dtype() const {
+    return static_cast<tflite_micro::TensorType>(GetField<int8_t>(VT_KEY_DTYPE, 0));
   }
-  tflite::TensorType value_dtype() const {
-    return static_cast<tflite::TensorType>(GetField<int8_t>(VT_VALUE_DTYPE, 0));
+  tflite_micro::TensorType value_dtype() const {
+    return static_cast<tflite_micro::TensorType>(GetField<int8_t>(VT_VALUE_DTYPE, 0));
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
@@ -13533,10 +13533,10 @@ struct HashtableOptionsBuilder {
   void add_table_id(int32_t table_id) {
     fbb_.AddElement<int32_t>(HashtableOptions::VT_TABLE_ID, table_id, 0);
   }
-  void add_key_dtype(tflite::TensorType key_dtype) {
+  void add_key_dtype(tflite_micro::TensorType key_dtype) {
     fbb_.AddElement<int8_t>(HashtableOptions::VT_KEY_DTYPE, static_cast<int8_t>(key_dtype), 0);
   }
-  void add_value_dtype(tflite::TensorType value_dtype) {
+  void add_value_dtype(tflite_micro::TensorType value_dtype) {
     fbb_.AddElement<int8_t>(HashtableOptions::VT_VALUE_DTYPE, static_cast<int8_t>(value_dtype), 0);
   }
   explicit HashtableOptionsBuilder(flatbuffers::FlatBufferBuilder &_fbb)
@@ -13553,8 +13553,8 @@ struct HashtableOptionsBuilder {
 inline flatbuffers::Offset<HashtableOptions> CreateHashtableOptions(
     flatbuffers::FlatBufferBuilder &_fbb,
     int32_t table_id = 0,
-    tflite::TensorType key_dtype = tflite::TensorType_FLOAT32,
-    tflite::TensorType value_dtype = tflite::TensorType_FLOAT32) {
+    tflite_micro::TensorType key_dtype = tflite_micro::TensorType_FLOAT32,
+    tflite_micro::TensorType value_dtype = tflite_micro::TensorType_FLOAT32) {
   HashtableOptionsBuilder builder_(_fbb);
   builder_.add_table_id(table_id);
   builder_.add_value_dtype(value_dtype);
@@ -13750,7 +13750,7 @@ inline flatbuffers::Offset<VarHandleOptions> CreateVarHandleOptionsDirect(
     const char *shared_name = nullptr) {
   auto container__ = container ? _fbb.CreateString(container) : 0;
   auto shared_name__ = shared_name ? _fbb.CreateString(shared_name) : 0;
-  return tflite::CreateVarHandleOptions(
+  return tflite_micro::CreateVarHandleOptions(
       _fbb,
       container__,
       shared_name__);
@@ -13954,7 +13954,7 @@ inline flatbuffers::Offset<BucketizeOptions> CreateBucketizeOptionsDirect(
     flatbuffers::FlatBufferBuilder &_fbb,
     const std::vector<float> *boundaries = nullptr) {
   auto boundaries__ = boundaries ? _fbb.CreateVector<float>(*boundaries) : 0;
-  return tflite::CreateBucketizeOptions(
+  return tflite_micro::CreateBucketizeOptions(
       _fbb,
       boundaries__);
 }
@@ -14444,7 +14444,7 @@ flatbuffers::Offset<DilateOptions> CreateDilateOptions(flatbuffers::FlatBufferBu
 
 struct ReduceWindowOptionsT : public flatbuffers::NativeTable {
   typedef ReduceWindowOptions TableType;
-  tflite::ReduceWindowFunction reduce_function = tflite::ReduceWindowFunction_UNSUPPORTED;
+  tflite_micro::ReduceWindowFunction reduce_function = tflite_micro::ReduceWindowFunction_UNSUPPORTED;
 };
 
 struct ReduceWindowOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -14453,8 +14453,8 @@ struct ReduceWindowOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_REDUCE_FUNCTION = 4
   };
-  tflite::ReduceWindowFunction reduce_function() const {
-    return static_cast<tflite::ReduceWindowFunction>(GetField<int32_t>(VT_REDUCE_FUNCTION, 0));
+  tflite_micro::ReduceWindowFunction reduce_function() const {
+    return static_cast<tflite_micro::ReduceWindowFunction>(GetField<int32_t>(VT_REDUCE_FUNCTION, 0));
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
@@ -14470,7 +14470,7 @@ struct ReduceWindowOptionsBuilder {
   typedef ReduceWindowOptions Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_reduce_function(tflite::ReduceWindowFunction reduce_function) {
+  void add_reduce_function(tflite_micro::ReduceWindowFunction reduce_function) {
     fbb_.AddElement<int32_t>(ReduceWindowOptions::VT_REDUCE_FUNCTION, static_cast<int32_t>(reduce_function), 0);
   }
   explicit ReduceWindowOptionsBuilder(flatbuffers::FlatBufferBuilder &_fbb)
@@ -14486,7 +14486,7 @@ struct ReduceWindowOptionsBuilder {
 
 inline flatbuffers::Offset<ReduceWindowOptions> CreateReduceWindowOptions(
     flatbuffers::FlatBufferBuilder &_fbb,
-    tflite::ReduceWindowFunction reduce_function = tflite::ReduceWindowFunction_UNSUPPORTED) {
+    tflite_micro::ReduceWindowFunction reduce_function = tflite_micro::ReduceWindowFunction_UNSUPPORTED) {
   ReduceWindowOptionsBuilder builder_(_fbb);
   builder_.add_reduce_function(reduce_function);
   return builder_.Finish();
@@ -14499,7 +14499,7 @@ struct OperatorCodeT : public flatbuffers::NativeTable {
   int8_t deprecated_builtin_code = 0;
   std::string custom_code{};
   int32_t version = 1;
-  tflite::BuiltinOperator builtin_code = tflite::BuiltinOperator_ADD;
+  tflite_micro::BuiltinOperator builtin_code = tflite_micro::BuiltinOperator_ADD;
 };
 
 struct OperatorCode FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -14520,8 +14520,8 @@ struct OperatorCode FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   int32_t version() const {
     return GetField<int32_t>(VT_VERSION, 1);
   }
-  tflite::BuiltinOperator builtin_code() const {
-    return static_cast<tflite::BuiltinOperator>(GetField<int32_t>(VT_BUILTIN_CODE, 0));
+  tflite_micro::BuiltinOperator builtin_code() const {
+    return static_cast<tflite_micro::BuiltinOperator>(GetField<int32_t>(VT_BUILTIN_CODE, 0));
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
@@ -14550,7 +14550,7 @@ struct OperatorCodeBuilder {
   void add_version(int32_t version) {
     fbb_.AddElement<int32_t>(OperatorCode::VT_VERSION, version, 1);
   }
-  void add_builtin_code(tflite::BuiltinOperator builtin_code) {
+  void add_builtin_code(tflite_micro::BuiltinOperator builtin_code) {
     fbb_.AddElement<int32_t>(OperatorCode::VT_BUILTIN_CODE, static_cast<int32_t>(builtin_code), 0);
   }
   explicit OperatorCodeBuilder(flatbuffers::FlatBufferBuilder &_fbb)
@@ -14569,7 +14569,7 @@ inline flatbuffers::Offset<OperatorCode> CreateOperatorCode(
     int8_t deprecated_builtin_code = 0,
     flatbuffers::Offset<flatbuffers::String> custom_code = 0,
     int32_t version = 1,
-    tflite::BuiltinOperator builtin_code = tflite::BuiltinOperator_ADD) {
+    tflite_micro::BuiltinOperator builtin_code = tflite_micro::BuiltinOperator_ADD) {
   OperatorCodeBuilder builder_(_fbb);
   builder_.add_builtin_code(builtin_code);
   builder_.add_version(version);
@@ -14583,9 +14583,9 @@ inline flatbuffers::Offset<OperatorCode> CreateOperatorCodeDirect(
     int8_t deprecated_builtin_code = 0,
     const char *custom_code = nullptr,
     int32_t version = 1,
-    tflite::BuiltinOperator builtin_code = tflite::BuiltinOperator_ADD) {
+    tflite_micro::BuiltinOperator builtin_code = tflite_micro::BuiltinOperator_ADD) {
   auto custom_code__ = custom_code ? _fbb.CreateString(custom_code) : 0;
-  return tflite::CreateOperatorCode(
+  return tflite_micro::CreateOperatorCode(
       _fbb,
       deprecated_builtin_code,
       custom_code__,
@@ -14600,14 +14600,14 @@ struct OperatorT : public flatbuffers::NativeTable {
   uint32_t opcode_index = 0;
   std::vector<int32_t> inputs{};
   std::vector<int32_t> outputs{};
-  tflite::BuiltinOptionsUnion builtin_options{};
+  tflite_micro::BuiltinOptionsUnion builtin_options{};
   std::vector<uint8_t> custom_options{};
-  tflite::CustomOptionsFormat custom_options_format = tflite::CustomOptionsFormat_FLEXBUFFERS;
+  tflite_micro::CustomOptionsFormat custom_options_format = tflite_micro::CustomOptionsFormat_FLEXBUFFERS;
   std::vector<bool> mutating_variable_inputs{};
   std::vector<int32_t> intermediates{};
   uint64_t large_custom_options_offset = 0;
   uint64_t large_custom_options_size = 0;
-  tflite::BuiltinOptions2Union builtin_options_2{};
+  tflite_micro::BuiltinOptions2Union builtin_options_2{};
 };
 
 struct Operator FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -14637,396 +14637,396 @@ struct Operator FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   const flatbuffers::Vector<int32_t> *outputs() const {
     return GetPointer<const flatbuffers::Vector<int32_t> *>(VT_OUTPUTS);
   }
-  tflite::BuiltinOptions builtin_options_type() const {
-    return static_cast<tflite::BuiltinOptions>(GetField<uint8_t>(VT_BUILTIN_OPTIONS_TYPE, 0));
+  tflite_micro::BuiltinOptions builtin_options_type() const {
+    return static_cast<tflite_micro::BuiltinOptions>(GetField<uint8_t>(VT_BUILTIN_OPTIONS_TYPE, 0));
   }
   const void *builtin_options() const {
     return GetPointer<const void *>(VT_BUILTIN_OPTIONS);
   }
   template<typename T> const T *builtin_options_as() const;
-  const tflite::Conv2DOptions *builtin_options_as_Conv2DOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_Conv2DOptions ? static_cast<const tflite::Conv2DOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::Conv2DOptions *builtin_options_as_Conv2DOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_Conv2DOptions ? static_cast<const tflite_micro::Conv2DOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::DepthwiseConv2DOptions *builtin_options_as_DepthwiseConv2DOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_DepthwiseConv2DOptions ? static_cast<const tflite::DepthwiseConv2DOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::DepthwiseConv2DOptions *builtin_options_as_DepthwiseConv2DOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_DepthwiseConv2DOptions ? static_cast<const tflite_micro::DepthwiseConv2DOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::ConcatEmbeddingsOptions *builtin_options_as_ConcatEmbeddingsOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_ConcatEmbeddingsOptions ? static_cast<const tflite::ConcatEmbeddingsOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::ConcatEmbeddingsOptions *builtin_options_as_ConcatEmbeddingsOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_ConcatEmbeddingsOptions ? static_cast<const tflite_micro::ConcatEmbeddingsOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::LSHProjectionOptions *builtin_options_as_LSHProjectionOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_LSHProjectionOptions ? static_cast<const tflite::LSHProjectionOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::LSHProjectionOptions *builtin_options_as_LSHProjectionOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_LSHProjectionOptions ? static_cast<const tflite_micro::LSHProjectionOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::Pool2DOptions *builtin_options_as_Pool2DOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_Pool2DOptions ? static_cast<const tflite::Pool2DOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::Pool2DOptions *builtin_options_as_Pool2DOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_Pool2DOptions ? static_cast<const tflite_micro::Pool2DOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::SVDFOptions *builtin_options_as_SVDFOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_SVDFOptions ? static_cast<const tflite::SVDFOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::SVDFOptions *builtin_options_as_SVDFOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_SVDFOptions ? static_cast<const tflite_micro::SVDFOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::RNNOptions *builtin_options_as_RNNOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_RNNOptions ? static_cast<const tflite::RNNOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::RNNOptions *builtin_options_as_RNNOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_RNNOptions ? static_cast<const tflite_micro::RNNOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::FullyConnectedOptions *builtin_options_as_FullyConnectedOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_FullyConnectedOptions ? static_cast<const tflite::FullyConnectedOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::FullyConnectedOptions *builtin_options_as_FullyConnectedOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_FullyConnectedOptions ? static_cast<const tflite_micro::FullyConnectedOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::SoftmaxOptions *builtin_options_as_SoftmaxOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_SoftmaxOptions ? static_cast<const tflite::SoftmaxOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::SoftmaxOptions *builtin_options_as_SoftmaxOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_SoftmaxOptions ? static_cast<const tflite_micro::SoftmaxOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::ConcatenationOptions *builtin_options_as_ConcatenationOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_ConcatenationOptions ? static_cast<const tflite::ConcatenationOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::ConcatenationOptions *builtin_options_as_ConcatenationOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_ConcatenationOptions ? static_cast<const tflite_micro::ConcatenationOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::AddOptions *builtin_options_as_AddOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_AddOptions ? static_cast<const tflite::AddOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::AddOptions *builtin_options_as_AddOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_AddOptions ? static_cast<const tflite_micro::AddOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::L2NormOptions *builtin_options_as_L2NormOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_L2NormOptions ? static_cast<const tflite::L2NormOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::L2NormOptions *builtin_options_as_L2NormOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_L2NormOptions ? static_cast<const tflite_micro::L2NormOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::LocalResponseNormalizationOptions *builtin_options_as_LocalResponseNormalizationOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_LocalResponseNormalizationOptions ? static_cast<const tflite::LocalResponseNormalizationOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::LocalResponseNormalizationOptions *builtin_options_as_LocalResponseNormalizationOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_LocalResponseNormalizationOptions ? static_cast<const tflite_micro::LocalResponseNormalizationOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::LSTMOptions *builtin_options_as_LSTMOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_LSTMOptions ? static_cast<const tflite::LSTMOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::LSTMOptions *builtin_options_as_LSTMOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_LSTMOptions ? static_cast<const tflite_micro::LSTMOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::ResizeBilinearOptions *builtin_options_as_ResizeBilinearOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_ResizeBilinearOptions ? static_cast<const tflite::ResizeBilinearOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::ResizeBilinearOptions *builtin_options_as_ResizeBilinearOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_ResizeBilinearOptions ? static_cast<const tflite_micro::ResizeBilinearOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::CallOptions *builtin_options_as_CallOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_CallOptions ? static_cast<const tflite::CallOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::CallOptions *builtin_options_as_CallOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_CallOptions ? static_cast<const tflite_micro::CallOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::ReshapeOptions *builtin_options_as_ReshapeOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_ReshapeOptions ? static_cast<const tflite::ReshapeOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::ReshapeOptions *builtin_options_as_ReshapeOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_ReshapeOptions ? static_cast<const tflite_micro::ReshapeOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::SkipGramOptions *builtin_options_as_SkipGramOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_SkipGramOptions ? static_cast<const tflite::SkipGramOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::SkipGramOptions *builtin_options_as_SkipGramOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_SkipGramOptions ? static_cast<const tflite_micro::SkipGramOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::SpaceToDepthOptions *builtin_options_as_SpaceToDepthOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_SpaceToDepthOptions ? static_cast<const tflite::SpaceToDepthOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::SpaceToDepthOptions *builtin_options_as_SpaceToDepthOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_SpaceToDepthOptions ? static_cast<const tflite_micro::SpaceToDepthOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::EmbeddingLookupSparseOptions *builtin_options_as_EmbeddingLookupSparseOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_EmbeddingLookupSparseOptions ? static_cast<const tflite::EmbeddingLookupSparseOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::EmbeddingLookupSparseOptions *builtin_options_as_EmbeddingLookupSparseOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_EmbeddingLookupSparseOptions ? static_cast<const tflite_micro::EmbeddingLookupSparseOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::MulOptions *builtin_options_as_MulOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_MulOptions ? static_cast<const tflite::MulOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::MulOptions *builtin_options_as_MulOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_MulOptions ? static_cast<const tflite_micro::MulOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::PadOptions *builtin_options_as_PadOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_PadOptions ? static_cast<const tflite::PadOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::PadOptions *builtin_options_as_PadOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_PadOptions ? static_cast<const tflite_micro::PadOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::GatherOptions *builtin_options_as_GatherOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_GatherOptions ? static_cast<const tflite::GatherOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::GatherOptions *builtin_options_as_GatherOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_GatherOptions ? static_cast<const tflite_micro::GatherOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::BatchToSpaceNDOptions *builtin_options_as_BatchToSpaceNDOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_BatchToSpaceNDOptions ? static_cast<const tflite::BatchToSpaceNDOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::BatchToSpaceNDOptions *builtin_options_as_BatchToSpaceNDOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_BatchToSpaceNDOptions ? static_cast<const tflite_micro::BatchToSpaceNDOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::SpaceToBatchNDOptions *builtin_options_as_SpaceToBatchNDOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_SpaceToBatchNDOptions ? static_cast<const tflite::SpaceToBatchNDOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::SpaceToBatchNDOptions *builtin_options_as_SpaceToBatchNDOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_SpaceToBatchNDOptions ? static_cast<const tflite_micro::SpaceToBatchNDOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::TransposeOptions *builtin_options_as_TransposeOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_TransposeOptions ? static_cast<const tflite::TransposeOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::TransposeOptions *builtin_options_as_TransposeOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_TransposeOptions ? static_cast<const tflite_micro::TransposeOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::ReducerOptions *builtin_options_as_ReducerOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_ReducerOptions ? static_cast<const tflite::ReducerOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::ReducerOptions *builtin_options_as_ReducerOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_ReducerOptions ? static_cast<const tflite_micro::ReducerOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::SubOptions *builtin_options_as_SubOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_SubOptions ? static_cast<const tflite::SubOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::SubOptions *builtin_options_as_SubOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_SubOptions ? static_cast<const tflite_micro::SubOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::DivOptions *builtin_options_as_DivOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_DivOptions ? static_cast<const tflite::DivOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::DivOptions *builtin_options_as_DivOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_DivOptions ? static_cast<const tflite_micro::DivOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::SqueezeOptions *builtin_options_as_SqueezeOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_SqueezeOptions ? static_cast<const tflite::SqueezeOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::SqueezeOptions *builtin_options_as_SqueezeOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_SqueezeOptions ? static_cast<const tflite_micro::SqueezeOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::SequenceRNNOptions *builtin_options_as_SequenceRNNOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_SequenceRNNOptions ? static_cast<const tflite::SequenceRNNOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::SequenceRNNOptions *builtin_options_as_SequenceRNNOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_SequenceRNNOptions ? static_cast<const tflite_micro::SequenceRNNOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::StridedSliceOptions *builtin_options_as_StridedSliceOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_StridedSliceOptions ? static_cast<const tflite::StridedSliceOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::StridedSliceOptions *builtin_options_as_StridedSliceOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_StridedSliceOptions ? static_cast<const tflite_micro::StridedSliceOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::ExpOptions *builtin_options_as_ExpOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_ExpOptions ? static_cast<const tflite::ExpOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::ExpOptions *builtin_options_as_ExpOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_ExpOptions ? static_cast<const tflite_micro::ExpOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::TopKV2Options *builtin_options_as_TopKV2Options() const {
-    return builtin_options_type() == tflite::BuiltinOptions_TopKV2Options ? static_cast<const tflite::TopKV2Options *>(builtin_options()) : nullptr;
+  const tflite_micro::TopKV2Options *builtin_options_as_TopKV2Options() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_TopKV2Options ? static_cast<const tflite_micro::TopKV2Options *>(builtin_options()) : nullptr;
   }
-  const tflite::SplitOptions *builtin_options_as_SplitOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_SplitOptions ? static_cast<const tflite::SplitOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::SplitOptions *builtin_options_as_SplitOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_SplitOptions ? static_cast<const tflite_micro::SplitOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::LogSoftmaxOptions *builtin_options_as_LogSoftmaxOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_LogSoftmaxOptions ? static_cast<const tflite::LogSoftmaxOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::LogSoftmaxOptions *builtin_options_as_LogSoftmaxOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_LogSoftmaxOptions ? static_cast<const tflite_micro::LogSoftmaxOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::CastOptions *builtin_options_as_CastOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_CastOptions ? static_cast<const tflite::CastOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::CastOptions *builtin_options_as_CastOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_CastOptions ? static_cast<const tflite_micro::CastOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::DequantizeOptions *builtin_options_as_DequantizeOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_DequantizeOptions ? static_cast<const tflite::DequantizeOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::DequantizeOptions *builtin_options_as_DequantizeOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_DequantizeOptions ? static_cast<const tflite_micro::DequantizeOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::MaximumMinimumOptions *builtin_options_as_MaximumMinimumOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_MaximumMinimumOptions ? static_cast<const tflite::MaximumMinimumOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::MaximumMinimumOptions *builtin_options_as_MaximumMinimumOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_MaximumMinimumOptions ? static_cast<const tflite_micro::MaximumMinimumOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::ArgMaxOptions *builtin_options_as_ArgMaxOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_ArgMaxOptions ? static_cast<const tflite::ArgMaxOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::ArgMaxOptions *builtin_options_as_ArgMaxOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_ArgMaxOptions ? static_cast<const tflite_micro::ArgMaxOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::LessOptions *builtin_options_as_LessOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_LessOptions ? static_cast<const tflite::LessOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::LessOptions *builtin_options_as_LessOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_LessOptions ? static_cast<const tflite_micro::LessOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::NegOptions *builtin_options_as_NegOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_NegOptions ? static_cast<const tflite::NegOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::NegOptions *builtin_options_as_NegOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_NegOptions ? static_cast<const tflite_micro::NegOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::PadV2Options *builtin_options_as_PadV2Options() const {
-    return builtin_options_type() == tflite::BuiltinOptions_PadV2Options ? static_cast<const tflite::PadV2Options *>(builtin_options()) : nullptr;
+  const tflite_micro::PadV2Options *builtin_options_as_PadV2Options() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_PadV2Options ? static_cast<const tflite_micro::PadV2Options *>(builtin_options()) : nullptr;
   }
-  const tflite::GreaterOptions *builtin_options_as_GreaterOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_GreaterOptions ? static_cast<const tflite::GreaterOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::GreaterOptions *builtin_options_as_GreaterOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_GreaterOptions ? static_cast<const tflite_micro::GreaterOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::GreaterEqualOptions *builtin_options_as_GreaterEqualOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_GreaterEqualOptions ? static_cast<const tflite::GreaterEqualOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::GreaterEqualOptions *builtin_options_as_GreaterEqualOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_GreaterEqualOptions ? static_cast<const tflite_micro::GreaterEqualOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::LessEqualOptions *builtin_options_as_LessEqualOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_LessEqualOptions ? static_cast<const tflite::LessEqualOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::LessEqualOptions *builtin_options_as_LessEqualOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_LessEqualOptions ? static_cast<const tflite_micro::LessEqualOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::SelectOptions *builtin_options_as_SelectOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_SelectOptions ? static_cast<const tflite::SelectOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::SelectOptions *builtin_options_as_SelectOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_SelectOptions ? static_cast<const tflite_micro::SelectOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::SliceOptions *builtin_options_as_SliceOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_SliceOptions ? static_cast<const tflite::SliceOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::SliceOptions *builtin_options_as_SliceOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_SliceOptions ? static_cast<const tflite_micro::SliceOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::TransposeConvOptions *builtin_options_as_TransposeConvOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_TransposeConvOptions ? static_cast<const tflite::TransposeConvOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::TransposeConvOptions *builtin_options_as_TransposeConvOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_TransposeConvOptions ? static_cast<const tflite_micro::TransposeConvOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::SparseToDenseOptions *builtin_options_as_SparseToDenseOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_SparseToDenseOptions ? static_cast<const tflite::SparseToDenseOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::SparseToDenseOptions *builtin_options_as_SparseToDenseOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_SparseToDenseOptions ? static_cast<const tflite_micro::SparseToDenseOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::TileOptions *builtin_options_as_TileOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_TileOptions ? static_cast<const tflite::TileOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::TileOptions *builtin_options_as_TileOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_TileOptions ? static_cast<const tflite_micro::TileOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::ExpandDimsOptions *builtin_options_as_ExpandDimsOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_ExpandDimsOptions ? static_cast<const tflite::ExpandDimsOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::ExpandDimsOptions *builtin_options_as_ExpandDimsOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_ExpandDimsOptions ? static_cast<const tflite_micro::ExpandDimsOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::EqualOptions *builtin_options_as_EqualOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_EqualOptions ? static_cast<const tflite::EqualOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::EqualOptions *builtin_options_as_EqualOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_EqualOptions ? static_cast<const tflite_micro::EqualOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::NotEqualOptions *builtin_options_as_NotEqualOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_NotEqualOptions ? static_cast<const tflite::NotEqualOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::NotEqualOptions *builtin_options_as_NotEqualOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_NotEqualOptions ? static_cast<const tflite_micro::NotEqualOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::ShapeOptions *builtin_options_as_ShapeOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_ShapeOptions ? static_cast<const tflite::ShapeOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::ShapeOptions *builtin_options_as_ShapeOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_ShapeOptions ? static_cast<const tflite_micro::ShapeOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::PowOptions *builtin_options_as_PowOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_PowOptions ? static_cast<const tflite::PowOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::PowOptions *builtin_options_as_PowOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_PowOptions ? static_cast<const tflite_micro::PowOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::ArgMinOptions *builtin_options_as_ArgMinOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_ArgMinOptions ? static_cast<const tflite::ArgMinOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::ArgMinOptions *builtin_options_as_ArgMinOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_ArgMinOptions ? static_cast<const tflite_micro::ArgMinOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::FakeQuantOptions *builtin_options_as_FakeQuantOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_FakeQuantOptions ? static_cast<const tflite::FakeQuantOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::FakeQuantOptions *builtin_options_as_FakeQuantOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_FakeQuantOptions ? static_cast<const tflite_micro::FakeQuantOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::PackOptions *builtin_options_as_PackOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_PackOptions ? static_cast<const tflite::PackOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::PackOptions *builtin_options_as_PackOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_PackOptions ? static_cast<const tflite_micro::PackOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::LogicalOrOptions *builtin_options_as_LogicalOrOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_LogicalOrOptions ? static_cast<const tflite::LogicalOrOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::LogicalOrOptions *builtin_options_as_LogicalOrOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_LogicalOrOptions ? static_cast<const tflite_micro::LogicalOrOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::OneHotOptions *builtin_options_as_OneHotOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_OneHotOptions ? static_cast<const tflite::OneHotOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::OneHotOptions *builtin_options_as_OneHotOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_OneHotOptions ? static_cast<const tflite_micro::OneHotOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::LogicalAndOptions *builtin_options_as_LogicalAndOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_LogicalAndOptions ? static_cast<const tflite::LogicalAndOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::LogicalAndOptions *builtin_options_as_LogicalAndOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_LogicalAndOptions ? static_cast<const tflite_micro::LogicalAndOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::LogicalNotOptions *builtin_options_as_LogicalNotOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_LogicalNotOptions ? static_cast<const tflite::LogicalNotOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::LogicalNotOptions *builtin_options_as_LogicalNotOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_LogicalNotOptions ? static_cast<const tflite_micro::LogicalNotOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::UnpackOptions *builtin_options_as_UnpackOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_UnpackOptions ? static_cast<const tflite::UnpackOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::UnpackOptions *builtin_options_as_UnpackOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_UnpackOptions ? static_cast<const tflite_micro::UnpackOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::FloorDivOptions *builtin_options_as_FloorDivOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_FloorDivOptions ? static_cast<const tflite::FloorDivOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::FloorDivOptions *builtin_options_as_FloorDivOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_FloorDivOptions ? static_cast<const tflite_micro::FloorDivOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::SquareOptions *builtin_options_as_SquareOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_SquareOptions ? static_cast<const tflite::SquareOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::SquareOptions *builtin_options_as_SquareOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_SquareOptions ? static_cast<const tflite_micro::SquareOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::ZerosLikeOptions *builtin_options_as_ZerosLikeOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_ZerosLikeOptions ? static_cast<const tflite::ZerosLikeOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::ZerosLikeOptions *builtin_options_as_ZerosLikeOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_ZerosLikeOptions ? static_cast<const tflite_micro::ZerosLikeOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::FillOptions *builtin_options_as_FillOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_FillOptions ? static_cast<const tflite::FillOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::FillOptions *builtin_options_as_FillOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_FillOptions ? static_cast<const tflite_micro::FillOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::BidirectionalSequenceLSTMOptions *builtin_options_as_BidirectionalSequenceLSTMOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_BidirectionalSequenceLSTMOptions ? static_cast<const tflite::BidirectionalSequenceLSTMOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::BidirectionalSequenceLSTMOptions *builtin_options_as_BidirectionalSequenceLSTMOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_BidirectionalSequenceLSTMOptions ? static_cast<const tflite_micro::BidirectionalSequenceLSTMOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::BidirectionalSequenceRNNOptions *builtin_options_as_BidirectionalSequenceRNNOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_BidirectionalSequenceRNNOptions ? static_cast<const tflite::BidirectionalSequenceRNNOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::BidirectionalSequenceRNNOptions *builtin_options_as_BidirectionalSequenceRNNOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_BidirectionalSequenceRNNOptions ? static_cast<const tflite_micro::BidirectionalSequenceRNNOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::UnidirectionalSequenceLSTMOptions *builtin_options_as_UnidirectionalSequenceLSTMOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_UnidirectionalSequenceLSTMOptions ? static_cast<const tflite::UnidirectionalSequenceLSTMOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::UnidirectionalSequenceLSTMOptions *builtin_options_as_UnidirectionalSequenceLSTMOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_UnidirectionalSequenceLSTMOptions ? static_cast<const tflite_micro::UnidirectionalSequenceLSTMOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::FloorModOptions *builtin_options_as_FloorModOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_FloorModOptions ? static_cast<const tflite::FloorModOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::FloorModOptions *builtin_options_as_FloorModOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_FloorModOptions ? static_cast<const tflite_micro::FloorModOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::RangeOptions *builtin_options_as_RangeOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_RangeOptions ? static_cast<const tflite::RangeOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::RangeOptions *builtin_options_as_RangeOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_RangeOptions ? static_cast<const tflite_micro::RangeOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::ResizeNearestNeighborOptions *builtin_options_as_ResizeNearestNeighborOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_ResizeNearestNeighborOptions ? static_cast<const tflite::ResizeNearestNeighborOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::ResizeNearestNeighborOptions *builtin_options_as_ResizeNearestNeighborOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_ResizeNearestNeighborOptions ? static_cast<const tflite_micro::ResizeNearestNeighborOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::LeakyReluOptions *builtin_options_as_LeakyReluOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_LeakyReluOptions ? static_cast<const tflite::LeakyReluOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::LeakyReluOptions *builtin_options_as_LeakyReluOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_LeakyReluOptions ? static_cast<const tflite_micro::LeakyReluOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::SquaredDifferenceOptions *builtin_options_as_SquaredDifferenceOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_SquaredDifferenceOptions ? static_cast<const tflite::SquaredDifferenceOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::SquaredDifferenceOptions *builtin_options_as_SquaredDifferenceOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_SquaredDifferenceOptions ? static_cast<const tflite_micro::SquaredDifferenceOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::MirrorPadOptions *builtin_options_as_MirrorPadOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_MirrorPadOptions ? static_cast<const tflite::MirrorPadOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::MirrorPadOptions *builtin_options_as_MirrorPadOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_MirrorPadOptions ? static_cast<const tflite_micro::MirrorPadOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::AbsOptions *builtin_options_as_AbsOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_AbsOptions ? static_cast<const tflite::AbsOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::AbsOptions *builtin_options_as_AbsOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_AbsOptions ? static_cast<const tflite_micro::AbsOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::SplitVOptions *builtin_options_as_SplitVOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_SplitVOptions ? static_cast<const tflite::SplitVOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::SplitVOptions *builtin_options_as_SplitVOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_SplitVOptions ? static_cast<const tflite_micro::SplitVOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::UniqueOptions *builtin_options_as_UniqueOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_UniqueOptions ? static_cast<const tflite::UniqueOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::UniqueOptions *builtin_options_as_UniqueOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_UniqueOptions ? static_cast<const tflite_micro::UniqueOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::ReverseV2Options *builtin_options_as_ReverseV2Options() const {
-    return builtin_options_type() == tflite::BuiltinOptions_ReverseV2Options ? static_cast<const tflite::ReverseV2Options *>(builtin_options()) : nullptr;
+  const tflite_micro::ReverseV2Options *builtin_options_as_ReverseV2Options() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_ReverseV2Options ? static_cast<const tflite_micro::ReverseV2Options *>(builtin_options()) : nullptr;
   }
-  const tflite::AddNOptions *builtin_options_as_AddNOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_AddNOptions ? static_cast<const tflite::AddNOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::AddNOptions *builtin_options_as_AddNOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_AddNOptions ? static_cast<const tflite_micro::AddNOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::GatherNdOptions *builtin_options_as_GatherNdOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_GatherNdOptions ? static_cast<const tflite::GatherNdOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::GatherNdOptions *builtin_options_as_GatherNdOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_GatherNdOptions ? static_cast<const tflite_micro::GatherNdOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::CosOptions *builtin_options_as_CosOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_CosOptions ? static_cast<const tflite::CosOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::CosOptions *builtin_options_as_CosOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_CosOptions ? static_cast<const tflite_micro::CosOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::WhereOptions *builtin_options_as_WhereOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_WhereOptions ? static_cast<const tflite::WhereOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::WhereOptions *builtin_options_as_WhereOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_WhereOptions ? static_cast<const tflite_micro::WhereOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::RankOptions *builtin_options_as_RankOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_RankOptions ? static_cast<const tflite::RankOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::RankOptions *builtin_options_as_RankOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_RankOptions ? static_cast<const tflite_micro::RankOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::ReverseSequenceOptions *builtin_options_as_ReverseSequenceOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_ReverseSequenceOptions ? static_cast<const tflite::ReverseSequenceOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::ReverseSequenceOptions *builtin_options_as_ReverseSequenceOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_ReverseSequenceOptions ? static_cast<const tflite_micro::ReverseSequenceOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::MatrixDiagOptions *builtin_options_as_MatrixDiagOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_MatrixDiagOptions ? static_cast<const tflite::MatrixDiagOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::MatrixDiagOptions *builtin_options_as_MatrixDiagOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_MatrixDiagOptions ? static_cast<const tflite_micro::MatrixDiagOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::QuantizeOptions *builtin_options_as_QuantizeOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_QuantizeOptions ? static_cast<const tflite::QuantizeOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::QuantizeOptions *builtin_options_as_QuantizeOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_QuantizeOptions ? static_cast<const tflite_micro::QuantizeOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::MatrixSetDiagOptions *builtin_options_as_MatrixSetDiagOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_MatrixSetDiagOptions ? static_cast<const tflite::MatrixSetDiagOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::MatrixSetDiagOptions *builtin_options_as_MatrixSetDiagOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_MatrixSetDiagOptions ? static_cast<const tflite_micro::MatrixSetDiagOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::HardSwishOptions *builtin_options_as_HardSwishOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_HardSwishOptions ? static_cast<const tflite::HardSwishOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::HardSwishOptions *builtin_options_as_HardSwishOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_HardSwishOptions ? static_cast<const tflite_micro::HardSwishOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::IfOptions *builtin_options_as_IfOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_IfOptions ? static_cast<const tflite::IfOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::IfOptions *builtin_options_as_IfOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_IfOptions ? static_cast<const tflite_micro::IfOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::WhileOptions *builtin_options_as_WhileOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_WhileOptions ? static_cast<const tflite::WhileOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::WhileOptions *builtin_options_as_WhileOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_WhileOptions ? static_cast<const tflite_micro::WhileOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::DepthToSpaceOptions *builtin_options_as_DepthToSpaceOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_DepthToSpaceOptions ? static_cast<const tflite::DepthToSpaceOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::DepthToSpaceOptions *builtin_options_as_DepthToSpaceOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_DepthToSpaceOptions ? static_cast<const tflite_micro::DepthToSpaceOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::NonMaxSuppressionV4Options *builtin_options_as_NonMaxSuppressionV4Options() const {
-    return builtin_options_type() == tflite::BuiltinOptions_NonMaxSuppressionV4Options ? static_cast<const tflite::NonMaxSuppressionV4Options *>(builtin_options()) : nullptr;
+  const tflite_micro::NonMaxSuppressionV4Options *builtin_options_as_NonMaxSuppressionV4Options() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_NonMaxSuppressionV4Options ? static_cast<const tflite_micro::NonMaxSuppressionV4Options *>(builtin_options()) : nullptr;
   }
-  const tflite::NonMaxSuppressionV5Options *builtin_options_as_NonMaxSuppressionV5Options() const {
-    return builtin_options_type() == tflite::BuiltinOptions_NonMaxSuppressionV5Options ? static_cast<const tflite::NonMaxSuppressionV5Options *>(builtin_options()) : nullptr;
+  const tflite_micro::NonMaxSuppressionV5Options *builtin_options_as_NonMaxSuppressionV5Options() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_NonMaxSuppressionV5Options ? static_cast<const tflite_micro::NonMaxSuppressionV5Options *>(builtin_options()) : nullptr;
   }
-  const tflite::ScatterNdOptions *builtin_options_as_ScatterNdOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_ScatterNdOptions ? static_cast<const tflite::ScatterNdOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::ScatterNdOptions *builtin_options_as_ScatterNdOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_ScatterNdOptions ? static_cast<const tflite_micro::ScatterNdOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::SelectV2Options *builtin_options_as_SelectV2Options() const {
-    return builtin_options_type() == tflite::BuiltinOptions_SelectV2Options ? static_cast<const tflite::SelectV2Options *>(builtin_options()) : nullptr;
+  const tflite_micro::SelectV2Options *builtin_options_as_SelectV2Options() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_SelectV2Options ? static_cast<const tflite_micro::SelectV2Options *>(builtin_options()) : nullptr;
   }
-  const tflite::DensifyOptions *builtin_options_as_DensifyOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_DensifyOptions ? static_cast<const tflite::DensifyOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::DensifyOptions *builtin_options_as_DensifyOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_DensifyOptions ? static_cast<const tflite_micro::DensifyOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::SegmentSumOptions *builtin_options_as_SegmentSumOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_SegmentSumOptions ? static_cast<const tflite::SegmentSumOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::SegmentSumOptions *builtin_options_as_SegmentSumOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_SegmentSumOptions ? static_cast<const tflite_micro::SegmentSumOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::BatchMatMulOptions *builtin_options_as_BatchMatMulOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_BatchMatMulOptions ? static_cast<const tflite::BatchMatMulOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::BatchMatMulOptions *builtin_options_as_BatchMatMulOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_BatchMatMulOptions ? static_cast<const tflite_micro::BatchMatMulOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::CumsumOptions *builtin_options_as_CumsumOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_CumsumOptions ? static_cast<const tflite::CumsumOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::CumsumOptions *builtin_options_as_CumsumOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_CumsumOptions ? static_cast<const tflite_micro::CumsumOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::CallOnceOptions *builtin_options_as_CallOnceOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_CallOnceOptions ? static_cast<const tflite::CallOnceOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::CallOnceOptions *builtin_options_as_CallOnceOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_CallOnceOptions ? static_cast<const tflite_micro::CallOnceOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::BroadcastToOptions *builtin_options_as_BroadcastToOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_BroadcastToOptions ? static_cast<const tflite::BroadcastToOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::BroadcastToOptions *builtin_options_as_BroadcastToOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_BroadcastToOptions ? static_cast<const tflite_micro::BroadcastToOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::Rfft2dOptions *builtin_options_as_Rfft2dOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_Rfft2dOptions ? static_cast<const tflite::Rfft2dOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::Rfft2dOptions *builtin_options_as_Rfft2dOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_Rfft2dOptions ? static_cast<const tflite_micro::Rfft2dOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::Conv3DOptions *builtin_options_as_Conv3DOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_Conv3DOptions ? static_cast<const tflite::Conv3DOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::Conv3DOptions *builtin_options_as_Conv3DOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_Conv3DOptions ? static_cast<const tflite_micro::Conv3DOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::HashtableOptions *builtin_options_as_HashtableOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_HashtableOptions ? static_cast<const tflite::HashtableOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::HashtableOptions *builtin_options_as_HashtableOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_HashtableOptions ? static_cast<const tflite_micro::HashtableOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::HashtableFindOptions *builtin_options_as_HashtableFindOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_HashtableFindOptions ? static_cast<const tflite::HashtableFindOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::HashtableFindOptions *builtin_options_as_HashtableFindOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_HashtableFindOptions ? static_cast<const tflite_micro::HashtableFindOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::HashtableImportOptions *builtin_options_as_HashtableImportOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_HashtableImportOptions ? static_cast<const tflite::HashtableImportOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::HashtableImportOptions *builtin_options_as_HashtableImportOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_HashtableImportOptions ? static_cast<const tflite_micro::HashtableImportOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::HashtableSizeOptions *builtin_options_as_HashtableSizeOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_HashtableSizeOptions ? static_cast<const tflite::HashtableSizeOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::HashtableSizeOptions *builtin_options_as_HashtableSizeOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_HashtableSizeOptions ? static_cast<const tflite_micro::HashtableSizeOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::VarHandleOptions *builtin_options_as_VarHandleOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_VarHandleOptions ? static_cast<const tflite::VarHandleOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::VarHandleOptions *builtin_options_as_VarHandleOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_VarHandleOptions ? static_cast<const tflite_micro::VarHandleOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::ReadVariableOptions *builtin_options_as_ReadVariableOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_ReadVariableOptions ? static_cast<const tflite::ReadVariableOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::ReadVariableOptions *builtin_options_as_ReadVariableOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_ReadVariableOptions ? static_cast<const tflite_micro::ReadVariableOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::AssignVariableOptions *builtin_options_as_AssignVariableOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_AssignVariableOptions ? static_cast<const tflite::AssignVariableOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::AssignVariableOptions *builtin_options_as_AssignVariableOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_AssignVariableOptions ? static_cast<const tflite_micro::AssignVariableOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::RandomOptions *builtin_options_as_RandomOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_RandomOptions ? static_cast<const tflite::RandomOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::RandomOptions *builtin_options_as_RandomOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_RandomOptions ? static_cast<const tflite_micro::RandomOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::BucketizeOptions *builtin_options_as_BucketizeOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_BucketizeOptions ? static_cast<const tflite::BucketizeOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::BucketizeOptions *builtin_options_as_BucketizeOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_BucketizeOptions ? static_cast<const tflite_micro::BucketizeOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::GeluOptions *builtin_options_as_GeluOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_GeluOptions ? static_cast<const tflite::GeluOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::GeluOptions *builtin_options_as_GeluOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_GeluOptions ? static_cast<const tflite_micro::GeluOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::DynamicUpdateSliceOptions *builtin_options_as_DynamicUpdateSliceOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_DynamicUpdateSliceOptions ? static_cast<const tflite::DynamicUpdateSliceOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::DynamicUpdateSliceOptions *builtin_options_as_DynamicUpdateSliceOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_DynamicUpdateSliceOptions ? static_cast<const tflite_micro::DynamicUpdateSliceOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::UnsortedSegmentProdOptions *builtin_options_as_UnsortedSegmentProdOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_UnsortedSegmentProdOptions ? static_cast<const tflite::UnsortedSegmentProdOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::UnsortedSegmentProdOptions *builtin_options_as_UnsortedSegmentProdOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_UnsortedSegmentProdOptions ? static_cast<const tflite_micro::UnsortedSegmentProdOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::UnsortedSegmentMaxOptions *builtin_options_as_UnsortedSegmentMaxOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_UnsortedSegmentMaxOptions ? static_cast<const tflite::UnsortedSegmentMaxOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::UnsortedSegmentMaxOptions *builtin_options_as_UnsortedSegmentMaxOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_UnsortedSegmentMaxOptions ? static_cast<const tflite_micro::UnsortedSegmentMaxOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::UnsortedSegmentMinOptions *builtin_options_as_UnsortedSegmentMinOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_UnsortedSegmentMinOptions ? static_cast<const tflite::UnsortedSegmentMinOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::UnsortedSegmentMinOptions *builtin_options_as_UnsortedSegmentMinOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_UnsortedSegmentMinOptions ? static_cast<const tflite_micro::UnsortedSegmentMinOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::UnsortedSegmentSumOptions *builtin_options_as_UnsortedSegmentSumOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_UnsortedSegmentSumOptions ? static_cast<const tflite::UnsortedSegmentSumOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::UnsortedSegmentSumOptions *builtin_options_as_UnsortedSegmentSumOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_UnsortedSegmentSumOptions ? static_cast<const tflite_micro::UnsortedSegmentSumOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::ATan2Options *builtin_options_as_ATan2Options() const {
-    return builtin_options_type() == tflite::BuiltinOptions_ATan2Options ? static_cast<const tflite::ATan2Options *>(builtin_options()) : nullptr;
+  const tflite_micro::ATan2Options *builtin_options_as_ATan2Options() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_ATan2Options ? static_cast<const tflite_micro::ATan2Options *>(builtin_options()) : nullptr;
   }
-  const tflite::SignOptions *builtin_options_as_SignOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_SignOptions ? static_cast<const tflite::SignOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::SignOptions *builtin_options_as_SignOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_SignOptions ? static_cast<const tflite_micro::SignOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::BitcastOptions *builtin_options_as_BitcastOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_BitcastOptions ? static_cast<const tflite::BitcastOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::BitcastOptions *builtin_options_as_BitcastOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_BitcastOptions ? static_cast<const tflite_micro::BitcastOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::BitwiseXorOptions *builtin_options_as_BitwiseXorOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_BitwiseXorOptions ? static_cast<const tflite::BitwiseXorOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::BitwiseXorOptions *builtin_options_as_BitwiseXorOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_BitwiseXorOptions ? static_cast<const tflite_micro::BitwiseXorOptions *>(builtin_options()) : nullptr;
   }
-  const tflite::RightShiftOptions *builtin_options_as_RightShiftOptions() const {
-    return builtin_options_type() == tflite::BuiltinOptions_RightShiftOptions ? static_cast<const tflite::RightShiftOptions *>(builtin_options()) : nullptr;
+  const tflite_micro::RightShiftOptions *builtin_options_as_RightShiftOptions() const {
+    return builtin_options_type() == tflite_micro::BuiltinOptions_RightShiftOptions ? static_cast<const tflite_micro::RightShiftOptions *>(builtin_options()) : nullptr;
   }
   const flatbuffers::Vector<uint8_t> *custom_options() const {
     return GetPointer<const flatbuffers::Vector<uint8_t> *>(VT_CUSTOM_OPTIONS);
   }
-  tflite::CustomOptionsFormat custom_options_format() const {
-    return static_cast<tflite::CustomOptionsFormat>(GetField<int8_t>(VT_CUSTOM_OPTIONS_FORMAT, 0));
+  tflite_micro::CustomOptionsFormat custom_options_format() const {
+    return static_cast<tflite_micro::CustomOptionsFormat>(GetField<int8_t>(VT_CUSTOM_OPTIONS_FORMAT, 0));
   }
   const flatbuffers::Vector<uint8_t> *mutating_variable_inputs() const {
     return GetPointer<const flatbuffers::Vector<uint8_t> *>(VT_MUTATING_VARIABLE_INPUTS);
@@ -15040,72 +15040,72 @@ struct Operator FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   uint64_t large_custom_options_size() const {
     return GetField<uint64_t>(VT_LARGE_CUSTOM_OPTIONS_SIZE, 0);
   }
-  tflite::BuiltinOptions2 builtin_options_2_type() const {
-    return static_cast<tflite::BuiltinOptions2>(GetField<uint8_t>(VT_BUILTIN_OPTIONS_2_TYPE, 0));
+  tflite_micro::BuiltinOptions2 builtin_options_2_type() const {
+    return static_cast<tflite_micro::BuiltinOptions2>(GetField<uint8_t>(VT_BUILTIN_OPTIONS_2_TYPE, 0));
   }
   const void *builtin_options_2() const {
     return GetPointer<const void *>(VT_BUILTIN_OPTIONS_2);
   }
   template<typename T> const T *builtin_options_2_as() const;
-  const tflite::StablehloConcatenateOptions *builtin_options_2_as_StablehloConcatenateOptions() const {
-    return builtin_options_2_type() == tflite::BuiltinOptions2_StablehloConcatenateOptions ? static_cast<const tflite::StablehloConcatenateOptions *>(builtin_options_2()) : nullptr;
+  const tflite_micro::StablehloConcatenateOptions *builtin_options_2_as_StablehloConcatenateOptions() const {
+    return builtin_options_2_type() == tflite_micro::BuiltinOptions2_StablehloConcatenateOptions ? static_cast<const tflite_micro::StablehloConcatenateOptions *>(builtin_options_2()) : nullptr;
   }
-  const tflite::StablehloBroadcastInDimOptions *builtin_options_2_as_StablehloBroadcastInDimOptions() const {
-    return builtin_options_2_type() == tflite::BuiltinOptions2_StablehloBroadcastInDimOptions ? static_cast<const tflite::StablehloBroadcastInDimOptions *>(builtin_options_2()) : nullptr;
+  const tflite_micro::StablehloBroadcastInDimOptions *builtin_options_2_as_StablehloBroadcastInDimOptions() const {
+    return builtin_options_2_type() == tflite_micro::BuiltinOptions2_StablehloBroadcastInDimOptions ? static_cast<const tflite_micro::StablehloBroadcastInDimOptions *>(builtin_options_2()) : nullptr;
   }
-  const tflite::StablehloSliceOptions *builtin_options_2_as_StablehloSliceOptions() const {
-    return builtin_options_2_type() == tflite::BuiltinOptions2_StablehloSliceOptions ? static_cast<const tflite::StablehloSliceOptions *>(builtin_options_2()) : nullptr;
+  const tflite_micro::StablehloSliceOptions *builtin_options_2_as_StablehloSliceOptions() const {
+    return builtin_options_2_type() == tflite_micro::BuiltinOptions2_StablehloSliceOptions ? static_cast<const tflite_micro::StablehloSliceOptions *>(builtin_options_2()) : nullptr;
   }
-  const tflite::StablehloConvolutionOptions *builtin_options_2_as_StablehloConvolutionOptions() const {
-    return builtin_options_2_type() == tflite::BuiltinOptions2_StablehloConvolutionOptions ? static_cast<const tflite::StablehloConvolutionOptions *>(builtin_options_2()) : nullptr;
+  const tflite_micro::StablehloConvolutionOptions *builtin_options_2_as_StablehloConvolutionOptions() const {
+    return builtin_options_2_type() == tflite_micro::BuiltinOptions2_StablehloConvolutionOptions ? static_cast<const tflite_micro::StablehloConvolutionOptions *>(builtin_options_2()) : nullptr;
   }
-  const tflite::StablehloCustomCallOptions *builtin_options_2_as_StablehloCustomCallOptions() const {
-    return builtin_options_2_type() == tflite::BuiltinOptions2_StablehloCustomCallOptions ? static_cast<const tflite::StablehloCustomCallOptions *>(builtin_options_2()) : nullptr;
+  const tflite_micro::StablehloCustomCallOptions *builtin_options_2_as_StablehloCustomCallOptions() const {
+    return builtin_options_2_type() == tflite_micro::BuiltinOptions2_StablehloCustomCallOptions ? static_cast<const tflite_micro::StablehloCustomCallOptions *>(builtin_options_2()) : nullptr;
   }
-  const tflite::StablehloReduceOptions *builtin_options_2_as_StablehloReduceOptions() const {
-    return builtin_options_2_type() == tflite::BuiltinOptions2_StablehloReduceOptions ? static_cast<const tflite::StablehloReduceOptions *>(builtin_options_2()) : nullptr;
+  const tflite_micro::StablehloReduceOptions *builtin_options_2_as_StablehloReduceOptions() const {
+    return builtin_options_2_type() == tflite_micro::BuiltinOptions2_StablehloReduceOptions ? static_cast<const tflite_micro::StablehloReduceOptions *>(builtin_options_2()) : nullptr;
   }
-  const tflite::StablehloScatterOptions *builtin_options_2_as_StablehloScatterOptions() const {
-    return builtin_options_2_type() == tflite::BuiltinOptions2_StablehloScatterOptions ? static_cast<const tflite::StablehloScatterOptions *>(builtin_options_2()) : nullptr;
+  const tflite_micro::StablehloScatterOptions *builtin_options_2_as_StablehloScatterOptions() const {
+    return builtin_options_2_type() == tflite_micro::BuiltinOptions2_StablehloScatterOptions ? static_cast<const tflite_micro::StablehloScatterOptions *>(builtin_options_2()) : nullptr;
   }
-  const tflite::StablehloCompareOptions *builtin_options_2_as_StablehloCompareOptions() const {
-    return builtin_options_2_type() == tflite::BuiltinOptions2_StablehloCompareOptions ? static_cast<const tflite::StablehloCompareOptions *>(builtin_options_2()) : nullptr;
+  const tflite_micro::StablehloCompareOptions *builtin_options_2_as_StablehloCompareOptions() const {
+    return builtin_options_2_type() == tflite_micro::BuiltinOptions2_StablehloCompareOptions ? static_cast<const tflite_micro::StablehloCompareOptions *>(builtin_options_2()) : nullptr;
   }
-  const tflite::StablehloDynamicSliceOptions *builtin_options_2_as_StablehloDynamicSliceOptions() const {
-    return builtin_options_2_type() == tflite::BuiltinOptions2_StablehloDynamicSliceOptions ? static_cast<const tflite::StablehloDynamicSliceOptions *>(builtin_options_2()) : nullptr;
+  const tflite_micro::StablehloDynamicSliceOptions *builtin_options_2_as_StablehloDynamicSliceOptions() const {
+    return builtin_options_2_type() == tflite_micro::BuiltinOptions2_StablehloDynamicSliceOptions ? static_cast<const tflite_micro::StablehloDynamicSliceOptions *>(builtin_options_2()) : nullptr;
   }
-  const tflite::StablehloPadOptions *builtin_options_2_as_StablehloPadOptions() const {
-    return builtin_options_2_type() == tflite::BuiltinOptions2_StablehloPadOptions ? static_cast<const tflite::StablehloPadOptions *>(builtin_options_2()) : nullptr;
+  const tflite_micro::StablehloPadOptions *builtin_options_2_as_StablehloPadOptions() const {
+    return builtin_options_2_type() == tflite_micro::BuiltinOptions2_StablehloPadOptions ? static_cast<const tflite_micro::StablehloPadOptions *>(builtin_options_2()) : nullptr;
   }
-  const tflite::StablehloIotaOptions *builtin_options_2_as_StablehloIotaOptions() const {
-    return builtin_options_2_type() == tflite::BuiltinOptions2_StablehloIotaOptions ? static_cast<const tflite::StablehloIotaOptions *>(builtin_options_2()) : nullptr;
+  const tflite_micro::StablehloIotaOptions *builtin_options_2_as_StablehloIotaOptions() const {
+    return builtin_options_2_type() == tflite_micro::BuiltinOptions2_StablehloIotaOptions ? static_cast<const tflite_micro::StablehloIotaOptions *>(builtin_options_2()) : nullptr;
   }
-  const tflite::StablehloDotGeneralOptions *builtin_options_2_as_StablehloDotGeneralOptions() const {
-    return builtin_options_2_type() == tflite::BuiltinOptions2_StablehloDotGeneralOptions ? static_cast<const tflite::StablehloDotGeneralOptions *>(builtin_options_2()) : nullptr;
+  const tflite_micro::StablehloDotGeneralOptions *builtin_options_2_as_StablehloDotGeneralOptions() const {
+    return builtin_options_2_type() == tflite_micro::BuiltinOptions2_StablehloDotGeneralOptions ? static_cast<const tflite_micro::StablehloDotGeneralOptions *>(builtin_options_2()) : nullptr;
   }
-  const tflite::StablehloReduceWindowOptions *builtin_options_2_as_StablehloReduceWindowOptions() const {
-    return builtin_options_2_type() == tflite::BuiltinOptions2_StablehloReduceWindowOptions ? static_cast<const tflite::StablehloReduceWindowOptions *>(builtin_options_2()) : nullptr;
+  const tflite_micro::StablehloReduceWindowOptions *builtin_options_2_as_StablehloReduceWindowOptions() const {
+    return builtin_options_2_type() == tflite_micro::BuiltinOptions2_StablehloReduceWindowOptions ? static_cast<const tflite_micro::StablehloReduceWindowOptions *>(builtin_options_2()) : nullptr;
   }
-  const tflite::StablehloSortOptions *builtin_options_2_as_StablehloSortOptions() const {
-    return builtin_options_2_type() == tflite::BuiltinOptions2_StablehloSortOptions ? static_cast<const tflite::StablehloSortOptions *>(builtin_options_2()) : nullptr;
+  const tflite_micro::StablehloSortOptions *builtin_options_2_as_StablehloSortOptions() const {
+    return builtin_options_2_type() == tflite_micro::BuiltinOptions2_StablehloSortOptions ? static_cast<const tflite_micro::StablehloSortOptions *>(builtin_options_2()) : nullptr;
   }
-  const tflite::StablehloWhileOptions *builtin_options_2_as_StablehloWhileOptions() const {
-    return builtin_options_2_type() == tflite::BuiltinOptions2_StablehloWhileOptions ? static_cast<const tflite::StablehloWhileOptions *>(builtin_options_2()) : nullptr;
+  const tflite_micro::StablehloWhileOptions *builtin_options_2_as_StablehloWhileOptions() const {
+    return builtin_options_2_type() == tflite_micro::BuiltinOptions2_StablehloWhileOptions ? static_cast<const tflite_micro::StablehloWhileOptions *>(builtin_options_2()) : nullptr;
   }
-  const tflite::StablehloGatherOptions *builtin_options_2_as_StablehloGatherOptions() const {
-    return builtin_options_2_type() == tflite::BuiltinOptions2_StablehloGatherOptions ? static_cast<const tflite::StablehloGatherOptions *>(builtin_options_2()) : nullptr;
+  const tflite_micro::StablehloGatherOptions *builtin_options_2_as_StablehloGatherOptions() const {
+    return builtin_options_2_type() == tflite_micro::BuiltinOptions2_StablehloGatherOptions ? static_cast<const tflite_micro::StablehloGatherOptions *>(builtin_options_2()) : nullptr;
   }
-  const tflite::StablehloTransposeOptions *builtin_options_2_as_StablehloTransposeOptions() const {
-    return builtin_options_2_type() == tflite::BuiltinOptions2_StablehloTransposeOptions ? static_cast<const tflite::StablehloTransposeOptions *>(builtin_options_2()) : nullptr;
+  const tflite_micro::StablehloTransposeOptions *builtin_options_2_as_StablehloTransposeOptions() const {
+    return builtin_options_2_type() == tflite_micro::BuiltinOptions2_StablehloTransposeOptions ? static_cast<const tflite_micro::StablehloTransposeOptions *>(builtin_options_2()) : nullptr;
   }
-  const tflite::DilateOptions *builtin_options_2_as_DilateOptions() const {
-    return builtin_options_2_type() == tflite::BuiltinOptions2_DilateOptions ? static_cast<const tflite::DilateOptions *>(builtin_options_2()) : nullptr;
+  const tflite_micro::DilateOptions *builtin_options_2_as_DilateOptions() const {
+    return builtin_options_2_type() == tflite_micro::BuiltinOptions2_DilateOptions ? static_cast<const tflite_micro::DilateOptions *>(builtin_options_2()) : nullptr;
   }
-  const tflite::StablehloRngBitGeneratorOptions *builtin_options_2_as_StablehloRngBitGeneratorOptions() const {
-    return builtin_options_2_type() == tflite::BuiltinOptions2_StablehloRngBitGeneratorOptions ? static_cast<const tflite::StablehloRngBitGeneratorOptions *>(builtin_options_2()) : nullptr;
+  const tflite_micro::StablehloRngBitGeneratorOptions *builtin_options_2_as_StablehloRngBitGeneratorOptions() const {
+    return builtin_options_2_type() == tflite_micro::BuiltinOptions2_StablehloRngBitGeneratorOptions ? static_cast<const tflite_micro::StablehloRngBitGeneratorOptions *>(builtin_options_2()) : nullptr;
   }
-  const tflite::ReduceWindowOptions *builtin_options_2_as_ReduceWindowOptions() const {
-    return builtin_options_2_type() == tflite::BuiltinOptions2_ReduceWindowOptions ? static_cast<const tflite::ReduceWindowOptions *>(builtin_options_2()) : nullptr;
+  const tflite_micro::ReduceWindowOptions *builtin_options_2_as_ReduceWindowOptions() const {
+    return builtin_options_2_type() == tflite_micro::BuiltinOptions2_ReduceWindowOptions ? static_cast<const tflite_micro::ReduceWindowOptions *>(builtin_options_2()) : nullptr;
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
@@ -15136,587 +15136,587 @@ struct Operator FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   static flatbuffers::Offset<Operator> Pack(flatbuffers::FlatBufferBuilder &_fbb, const OperatorT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
 };
 
-template<> inline const tflite::Conv2DOptions *Operator::builtin_options_as<tflite::Conv2DOptions>() const {
+template<> inline const tflite_micro::Conv2DOptions *Operator::builtin_options_as<tflite_micro::Conv2DOptions>() const {
   return builtin_options_as_Conv2DOptions();
 }
 
-template<> inline const tflite::DepthwiseConv2DOptions *Operator::builtin_options_as<tflite::DepthwiseConv2DOptions>() const {
+template<> inline const tflite_micro::DepthwiseConv2DOptions *Operator::builtin_options_as<tflite_micro::DepthwiseConv2DOptions>() const {
   return builtin_options_as_DepthwiseConv2DOptions();
 }
 
-template<> inline const tflite::ConcatEmbeddingsOptions *Operator::builtin_options_as<tflite::ConcatEmbeddingsOptions>() const {
+template<> inline const tflite_micro::ConcatEmbeddingsOptions *Operator::builtin_options_as<tflite_micro::ConcatEmbeddingsOptions>() const {
   return builtin_options_as_ConcatEmbeddingsOptions();
 }
 
-template<> inline const tflite::LSHProjectionOptions *Operator::builtin_options_as<tflite::LSHProjectionOptions>() const {
+template<> inline const tflite_micro::LSHProjectionOptions *Operator::builtin_options_as<tflite_micro::LSHProjectionOptions>() const {
   return builtin_options_as_LSHProjectionOptions();
 }
 
-template<> inline const tflite::Pool2DOptions *Operator::builtin_options_as<tflite::Pool2DOptions>() const {
+template<> inline const tflite_micro::Pool2DOptions *Operator::builtin_options_as<tflite_micro::Pool2DOptions>() const {
   return builtin_options_as_Pool2DOptions();
 }
 
-template<> inline const tflite::SVDFOptions *Operator::builtin_options_as<tflite::SVDFOptions>() const {
+template<> inline const tflite_micro::SVDFOptions *Operator::builtin_options_as<tflite_micro::SVDFOptions>() const {
   return builtin_options_as_SVDFOptions();
 }
 
-template<> inline const tflite::RNNOptions *Operator::builtin_options_as<tflite::RNNOptions>() const {
+template<> inline const tflite_micro::RNNOptions *Operator::builtin_options_as<tflite_micro::RNNOptions>() const {
   return builtin_options_as_RNNOptions();
 }
 
-template<> inline const tflite::FullyConnectedOptions *Operator::builtin_options_as<tflite::FullyConnectedOptions>() const {
+template<> inline const tflite_micro::FullyConnectedOptions *Operator::builtin_options_as<tflite_micro::FullyConnectedOptions>() const {
   return builtin_options_as_FullyConnectedOptions();
 }
 
-template<> inline const tflite::SoftmaxOptions *Operator::builtin_options_as<tflite::SoftmaxOptions>() const {
+template<> inline const tflite_micro::SoftmaxOptions *Operator::builtin_options_as<tflite_micro::SoftmaxOptions>() const {
   return builtin_options_as_SoftmaxOptions();
 }
 
-template<> inline const tflite::ConcatenationOptions *Operator::builtin_options_as<tflite::ConcatenationOptions>() const {
+template<> inline const tflite_micro::ConcatenationOptions *Operator::builtin_options_as<tflite_micro::ConcatenationOptions>() const {
   return builtin_options_as_ConcatenationOptions();
 }
 
-template<> inline const tflite::AddOptions *Operator::builtin_options_as<tflite::AddOptions>() const {
+template<> inline const tflite_micro::AddOptions *Operator::builtin_options_as<tflite_micro::AddOptions>() const {
   return builtin_options_as_AddOptions();
 }
 
-template<> inline const tflite::L2NormOptions *Operator::builtin_options_as<tflite::L2NormOptions>() const {
+template<> inline const tflite_micro::L2NormOptions *Operator::builtin_options_as<tflite_micro::L2NormOptions>() const {
   return builtin_options_as_L2NormOptions();
 }
 
-template<> inline const tflite::LocalResponseNormalizationOptions *Operator::builtin_options_as<tflite::LocalResponseNormalizationOptions>() const {
+template<> inline const tflite_micro::LocalResponseNormalizationOptions *Operator::builtin_options_as<tflite_micro::LocalResponseNormalizationOptions>() const {
   return builtin_options_as_LocalResponseNormalizationOptions();
 }
 
-template<> inline const tflite::LSTMOptions *Operator::builtin_options_as<tflite::LSTMOptions>() const {
+template<> inline const tflite_micro::LSTMOptions *Operator::builtin_options_as<tflite_micro::LSTMOptions>() const {
   return builtin_options_as_LSTMOptions();
 }
 
-template<> inline const tflite::ResizeBilinearOptions *Operator::builtin_options_as<tflite::ResizeBilinearOptions>() const {
+template<> inline const tflite_micro::ResizeBilinearOptions *Operator::builtin_options_as<tflite_micro::ResizeBilinearOptions>() const {
   return builtin_options_as_ResizeBilinearOptions();
 }
 
-template<> inline const tflite::CallOptions *Operator::builtin_options_as<tflite::CallOptions>() const {
+template<> inline const tflite_micro::CallOptions *Operator::builtin_options_as<tflite_micro::CallOptions>() const {
   return builtin_options_as_CallOptions();
 }
 
-template<> inline const tflite::ReshapeOptions *Operator::builtin_options_as<tflite::ReshapeOptions>() const {
+template<> inline const tflite_micro::ReshapeOptions *Operator::builtin_options_as<tflite_micro::ReshapeOptions>() const {
   return builtin_options_as_ReshapeOptions();
 }
 
-template<> inline const tflite::SkipGramOptions *Operator::builtin_options_as<tflite::SkipGramOptions>() const {
+template<> inline const tflite_micro::SkipGramOptions *Operator::builtin_options_as<tflite_micro::SkipGramOptions>() const {
   return builtin_options_as_SkipGramOptions();
 }
 
-template<> inline const tflite::SpaceToDepthOptions *Operator::builtin_options_as<tflite::SpaceToDepthOptions>() const {
+template<> inline const tflite_micro::SpaceToDepthOptions *Operator::builtin_options_as<tflite_micro::SpaceToDepthOptions>() const {
   return builtin_options_as_SpaceToDepthOptions();
 }
 
-template<> inline const tflite::EmbeddingLookupSparseOptions *Operator::builtin_options_as<tflite::EmbeddingLookupSparseOptions>() const {
+template<> inline const tflite_micro::EmbeddingLookupSparseOptions *Operator::builtin_options_as<tflite_micro::EmbeddingLookupSparseOptions>() const {
   return builtin_options_as_EmbeddingLookupSparseOptions();
 }
 
-template<> inline const tflite::MulOptions *Operator::builtin_options_as<tflite::MulOptions>() const {
+template<> inline const tflite_micro::MulOptions *Operator::builtin_options_as<tflite_micro::MulOptions>() const {
   return builtin_options_as_MulOptions();
 }
 
-template<> inline const tflite::PadOptions *Operator::builtin_options_as<tflite::PadOptions>() const {
+template<> inline const tflite_micro::PadOptions *Operator::builtin_options_as<tflite_micro::PadOptions>() const {
   return builtin_options_as_PadOptions();
 }
 
-template<> inline const tflite::GatherOptions *Operator::builtin_options_as<tflite::GatherOptions>() const {
+template<> inline const tflite_micro::GatherOptions *Operator::builtin_options_as<tflite_micro::GatherOptions>() const {
   return builtin_options_as_GatherOptions();
 }
 
-template<> inline const tflite::BatchToSpaceNDOptions *Operator::builtin_options_as<tflite::BatchToSpaceNDOptions>() const {
+template<> inline const tflite_micro::BatchToSpaceNDOptions *Operator::builtin_options_as<tflite_micro::BatchToSpaceNDOptions>() const {
   return builtin_options_as_BatchToSpaceNDOptions();
 }
 
-template<> inline const tflite::SpaceToBatchNDOptions *Operator::builtin_options_as<tflite::SpaceToBatchNDOptions>() const {
+template<> inline const tflite_micro::SpaceToBatchNDOptions *Operator::builtin_options_as<tflite_micro::SpaceToBatchNDOptions>() const {
   return builtin_options_as_SpaceToBatchNDOptions();
 }
 
-template<> inline const tflite::TransposeOptions *Operator::builtin_options_as<tflite::TransposeOptions>() const {
+template<> inline const tflite_micro::TransposeOptions *Operator::builtin_options_as<tflite_micro::TransposeOptions>() const {
   return builtin_options_as_TransposeOptions();
 }
 
-template<> inline const tflite::ReducerOptions *Operator::builtin_options_as<tflite::ReducerOptions>() const {
+template<> inline const tflite_micro::ReducerOptions *Operator::builtin_options_as<tflite_micro::ReducerOptions>() const {
   return builtin_options_as_ReducerOptions();
 }
 
-template<> inline const tflite::SubOptions *Operator::builtin_options_as<tflite::SubOptions>() const {
+template<> inline const tflite_micro::SubOptions *Operator::builtin_options_as<tflite_micro::SubOptions>() const {
   return builtin_options_as_SubOptions();
 }
 
-template<> inline const tflite::DivOptions *Operator::builtin_options_as<tflite::DivOptions>() const {
+template<> inline const tflite_micro::DivOptions *Operator::builtin_options_as<tflite_micro::DivOptions>() const {
   return builtin_options_as_DivOptions();
 }
 
-template<> inline const tflite::SqueezeOptions *Operator::builtin_options_as<tflite::SqueezeOptions>() const {
+template<> inline const tflite_micro::SqueezeOptions *Operator::builtin_options_as<tflite_micro::SqueezeOptions>() const {
   return builtin_options_as_SqueezeOptions();
 }
 
-template<> inline const tflite::SequenceRNNOptions *Operator::builtin_options_as<tflite::SequenceRNNOptions>() const {
+template<> inline const tflite_micro::SequenceRNNOptions *Operator::builtin_options_as<tflite_micro::SequenceRNNOptions>() const {
   return builtin_options_as_SequenceRNNOptions();
 }
 
-template<> inline const tflite::StridedSliceOptions *Operator::builtin_options_as<tflite::StridedSliceOptions>() const {
+template<> inline const tflite_micro::StridedSliceOptions *Operator::builtin_options_as<tflite_micro::StridedSliceOptions>() const {
   return builtin_options_as_StridedSliceOptions();
 }
 
-template<> inline const tflite::ExpOptions *Operator::builtin_options_as<tflite::ExpOptions>() const {
+template<> inline const tflite_micro::ExpOptions *Operator::builtin_options_as<tflite_micro::ExpOptions>() const {
   return builtin_options_as_ExpOptions();
 }
 
-template<> inline const tflite::TopKV2Options *Operator::builtin_options_as<tflite::TopKV2Options>() const {
+template<> inline const tflite_micro::TopKV2Options *Operator::builtin_options_as<tflite_micro::TopKV2Options>() const {
   return builtin_options_as_TopKV2Options();
 }
 
-template<> inline const tflite::SplitOptions *Operator::builtin_options_as<tflite::SplitOptions>() const {
+template<> inline const tflite_micro::SplitOptions *Operator::builtin_options_as<tflite_micro::SplitOptions>() const {
   return builtin_options_as_SplitOptions();
 }
 
-template<> inline const tflite::LogSoftmaxOptions *Operator::builtin_options_as<tflite::LogSoftmaxOptions>() const {
+template<> inline const tflite_micro::LogSoftmaxOptions *Operator::builtin_options_as<tflite_micro::LogSoftmaxOptions>() const {
   return builtin_options_as_LogSoftmaxOptions();
 }
 
-template<> inline const tflite::CastOptions *Operator::builtin_options_as<tflite::CastOptions>() const {
+template<> inline const tflite_micro::CastOptions *Operator::builtin_options_as<tflite_micro::CastOptions>() const {
   return builtin_options_as_CastOptions();
 }
 
-template<> inline const tflite::DequantizeOptions *Operator::builtin_options_as<tflite::DequantizeOptions>() const {
+template<> inline const tflite_micro::DequantizeOptions *Operator::builtin_options_as<tflite_micro::DequantizeOptions>() const {
   return builtin_options_as_DequantizeOptions();
 }
 
-template<> inline const tflite::MaximumMinimumOptions *Operator::builtin_options_as<tflite::MaximumMinimumOptions>() const {
+template<> inline const tflite_micro::MaximumMinimumOptions *Operator::builtin_options_as<tflite_micro::MaximumMinimumOptions>() const {
   return builtin_options_as_MaximumMinimumOptions();
 }
 
-template<> inline const tflite::ArgMaxOptions *Operator::builtin_options_as<tflite::ArgMaxOptions>() const {
+template<> inline const tflite_micro::ArgMaxOptions *Operator::builtin_options_as<tflite_micro::ArgMaxOptions>() const {
   return builtin_options_as_ArgMaxOptions();
 }
 
-template<> inline const tflite::LessOptions *Operator::builtin_options_as<tflite::LessOptions>() const {
+template<> inline const tflite_micro::LessOptions *Operator::builtin_options_as<tflite_micro::LessOptions>() const {
   return builtin_options_as_LessOptions();
 }
 
-template<> inline const tflite::NegOptions *Operator::builtin_options_as<tflite::NegOptions>() const {
+template<> inline const tflite_micro::NegOptions *Operator::builtin_options_as<tflite_micro::NegOptions>() const {
   return builtin_options_as_NegOptions();
 }
 
-template<> inline const tflite::PadV2Options *Operator::builtin_options_as<tflite::PadV2Options>() const {
+template<> inline const tflite_micro::PadV2Options *Operator::builtin_options_as<tflite_micro::PadV2Options>() const {
   return builtin_options_as_PadV2Options();
 }
 
-template<> inline const tflite::GreaterOptions *Operator::builtin_options_as<tflite::GreaterOptions>() const {
+template<> inline const tflite_micro::GreaterOptions *Operator::builtin_options_as<tflite_micro::GreaterOptions>() const {
   return builtin_options_as_GreaterOptions();
 }
 
-template<> inline const tflite::GreaterEqualOptions *Operator::builtin_options_as<tflite::GreaterEqualOptions>() const {
+template<> inline const tflite_micro::GreaterEqualOptions *Operator::builtin_options_as<tflite_micro::GreaterEqualOptions>() const {
   return builtin_options_as_GreaterEqualOptions();
 }
 
-template<> inline const tflite::LessEqualOptions *Operator::builtin_options_as<tflite::LessEqualOptions>() const {
+template<> inline const tflite_micro::LessEqualOptions *Operator::builtin_options_as<tflite_micro::LessEqualOptions>() const {
   return builtin_options_as_LessEqualOptions();
 }
 
-template<> inline const tflite::SelectOptions *Operator::builtin_options_as<tflite::SelectOptions>() const {
+template<> inline const tflite_micro::SelectOptions *Operator::builtin_options_as<tflite_micro::SelectOptions>() const {
   return builtin_options_as_SelectOptions();
 }
 
-template<> inline const tflite::SliceOptions *Operator::builtin_options_as<tflite::SliceOptions>() const {
+template<> inline const tflite_micro::SliceOptions *Operator::builtin_options_as<tflite_micro::SliceOptions>() const {
   return builtin_options_as_SliceOptions();
 }
 
-template<> inline const tflite::TransposeConvOptions *Operator::builtin_options_as<tflite::TransposeConvOptions>() const {
+template<> inline const tflite_micro::TransposeConvOptions *Operator::builtin_options_as<tflite_micro::TransposeConvOptions>() const {
   return builtin_options_as_TransposeConvOptions();
 }
 
-template<> inline const tflite::SparseToDenseOptions *Operator::builtin_options_as<tflite::SparseToDenseOptions>() const {
+template<> inline const tflite_micro::SparseToDenseOptions *Operator::builtin_options_as<tflite_micro::SparseToDenseOptions>() const {
   return builtin_options_as_SparseToDenseOptions();
 }
 
-template<> inline const tflite::TileOptions *Operator::builtin_options_as<tflite::TileOptions>() const {
+template<> inline const tflite_micro::TileOptions *Operator::builtin_options_as<tflite_micro::TileOptions>() const {
   return builtin_options_as_TileOptions();
 }
 
-template<> inline const tflite::ExpandDimsOptions *Operator::builtin_options_as<tflite::ExpandDimsOptions>() const {
+template<> inline const tflite_micro::ExpandDimsOptions *Operator::builtin_options_as<tflite_micro::ExpandDimsOptions>() const {
   return builtin_options_as_ExpandDimsOptions();
 }
 
-template<> inline const tflite::EqualOptions *Operator::builtin_options_as<tflite::EqualOptions>() const {
+template<> inline const tflite_micro::EqualOptions *Operator::builtin_options_as<tflite_micro::EqualOptions>() const {
   return builtin_options_as_EqualOptions();
 }
 
-template<> inline const tflite::NotEqualOptions *Operator::builtin_options_as<tflite::NotEqualOptions>() const {
+template<> inline const tflite_micro::NotEqualOptions *Operator::builtin_options_as<tflite_micro::NotEqualOptions>() const {
   return builtin_options_as_NotEqualOptions();
 }
 
-template<> inline const tflite::ShapeOptions *Operator::builtin_options_as<tflite::ShapeOptions>() const {
+template<> inline const tflite_micro::ShapeOptions *Operator::builtin_options_as<tflite_micro::ShapeOptions>() const {
   return builtin_options_as_ShapeOptions();
 }
 
-template<> inline const tflite::PowOptions *Operator::builtin_options_as<tflite::PowOptions>() const {
+template<> inline const tflite_micro::PowOptions *Operator::builtin_options_as<tflite_micro::PowOptions>() const {
   return builtin_options_as_PowOptions();
 }
 
-template<> inline const tflite::ArgMinOptions *Operator::builtin_options_as<tflite::ArgMinOptions>() const {
+template<> inline const tflite_micro::ArgMinOptions *Operator::builtin_options_as<tflite_micro::ArgMinOptions>() const {
   return builtin_options_as_ArgMinOptions();
 }
 
-template<> inline const tflite::FakeQuantOptions *Operator::builtin_options_as<tflite::FakeQuantOptions>() const {
+template<> inline const tflite_micro::FakeQuantOptions *Operator::builtin_options_as<tflite_micro::FakeQuantOptions>() const {
   return builtin_options_as_FakeQuantOptions();
 }
 
-template<> inline const tflite::PackOptions *Operator::builtin_options_as<tflite::PackOptions>() const {
+template<> inline const tflite_micro::PackOptions *Operator::builtin_options_as<tflite_micro::PackOptions>() const {
   return builtin_options_as_PackOptions();
 }
 
-template<> inline const tflite::LogicalOrOptions *Operator::builtin_options_as<tflite::LogicalOrOptions>() const {
+template<> inline const tflite_micro::LogicalOrOptions *Operator::builtin_options_as<tflite_micro::LogicalOrOptions>() const {
   return builtin_options_as_LogicalOrOptions();
 }
 
-template<> inline const tflite::OneHotOptions *Operator::builtin_options_as<tflite::OneHotOptions>() const {
+template<> inline const tflite_micro::OneHotOptions *Operator::builtin_options_as<tflite_micro::OneHotOptions>() const {
   return builtin_options_as_OneHotOptions();
 }
 
-template<> inline const tflite::LogicalAndOptions *Operator::builtin_options_as<tflite::LogicalAndOptions>() const {
+template<> inline const tflite_micro::LogicalAndOptions *Operator::builtin_options_as<tflite_micro::LogicalAndOptions>() const {
   return builtin_options_as_LogicalAndOptions();
 }
 
-template<> inline const tflite::LogicalNotOptions *Operator::builtin_options_as<tflite::LogicalNotOptions>() const {
+template<> inline const tflite_micro::LogicalNotOptions *Operator::builtin_options_as<tflite_micro::LogicalNotOptions>() const {
   return builtin_options_as_LogicalNotOptions();
 }
 
-template<> inline const tflite::UnpackOptions *Operator::builtin_options_as<tflite::UnpackOptions>() const {
+template<> inline const tflite_micro::UnpackOptions *Operator::builtin_options_as<tflite_micro::UnpackOptions>() const {
   return builtin_options_as_UnpackOptions();
 }
 
-template<> inline const tflite::FloorDivOptions *Operator::builtin_options_as<tflite::FloorDivOptions>() const {
+template<> inline const tflite_micro::FloorDivOptions *Operator::builtin_options_as<tflite_micro::FloorDivOptions>() const {
   return builtin_options_as_FloorDivOptions();
 }
 
-template<> inline const tflite::SquareOptions *Operator::builtin_options_as<tflite::SquareOptions>() const {
+template<> inline const tflite_micro::SquareOptions *Operator::builtin_options_as<tflite_micro::SquareOptions>() const {
   return builtin_options_as_SquareOptions();
 }
 
-template<> inline const tflite::ZerosLikeOptions *Operator::builtin_options_as<tflite::ZerosLikeOptions>() const {
+template<> inline const tflite_micro::ZerosLikeOptions *Operator::builtin_options_as<tflite_micro::ZerosLikeOptions>() const {
   return builtin_options_as_ZerosLikeOptions();
 }
 
-template<> inline const tflite::FillOptions *Operator::builtin_options_as<tflite::FillOptions>() const {
+template<> inline const tflite_micro::FillOptions *Operator::builtin_options_as<tflite_micro::FillOptions>() const {
   return builtin_options_as_FillOptions();
 }
 
-template<> inline const tflite::BidirectionalSequenceLSTMOptions *Operator::builtin_options_as<tflite::BidirectionalSequenceLSTMOptions>() const {
+template<> inline const tflite_micro::BidirectionalSequenceLSTMOptions *Operator::builtin_options_as<tflite_micro::BidirectionalSequenceLSTMOptions>() const {
   return builtin_options_as_BidirectionalSequenceLSTMOptions();
 }
 
-template<> inline const tflite::BidirectionalSequenceRNNOptions *Operator::builtin_options_as<tflite::BidirectionalSequenceRNNOptions>() const {
+template<> inline const tflite_micro::BidirectionalSequenceRNNOptions *Operator::builtin_options_as<tflite_micro::BidirectionalSequenceRNNOptions>() const {
   return builtin_options_as_BidirectionalSequenceRNNOptions();
 }
 
-template<> inline const tflite::UnidirectionalSequenceLSTMOptions *Operator::builtin_options_as<tflite::UnidirectionalSequenceLSTMOptions>() const {
+template<> inline const tflite_micro::UnidirectionalSequenceLSTMOptions *Operator::builtin_options_as<tflite_micro::UnidirectionalSequenceLSTMOptions>() const {
   return builtin_options_as_UnidirectionalSequenceLSTMOptions();
 }
 
-template<> inline const tflite::FloorModOptions *Operator::builtin_options_as<tflite::FloorModOptions>() const {
+template<> inline const tflite_micro::FloorModOptions *Operator::builtin_options_as<tflite_micro::FloorModOptions>() const {
   return builtin_options_as_FloorModOptions();
 }
 
-template<> inline const tflite::RangeOptions *Operator::builtin_options_as<tflite::RangeOptions>() const {
+template<> inline const tflite_micro::RangeOptions *Operator::builtin_options_as<tflite_micro::RangeOptions>() const {
   return builtin_options_as_RangeOptions();
 }
 
-template<> inline const tflite::ResizeNearestNeighborOptions *Operator::builtin_options_as<tflite::ResizeNearestNeighborOptions>() const {
+template<> inline const tflite_micro::ResizeNearestNeighborOptions *Operator::builtin_options_as<tflite_micro::ResizeNearestNeighborOptions>() const {
   return builtin_options_as_ResizeNearestNeighborOptions();
 }
 
-template<> inline const tflite::LeakyReluOptions *Operator::builtin_options_as<tflite::LeakyReluOptions>() const {
+template<> inline const tflite_micro::LeakyReluOptions *Operator::builtin_options_as<tflite_micro::LeakyReluOptions>() const {
   return builtin_options_as_LeakyReluOptions();
 }
 
-template<> inline const tflite::SquaredDifferenceOptions *Operator::builtin_options_as<tflite::SquaredDifferenceOptions>() const {
+template<> inline const tflite_micro::SquaredDifferenceOptions *Operator::builtin_options_as<tflite_micro::SquaredDifferenceOptions>() const {
   return builtin_options_as_SquaredDifferenceOptions();
 }
 
-template<> inline const tflite::MirrorPadOptions *Operator::builtin_options_as<tflite::MirrorPadOptions>() const {
+template<> inline const tflite_micro::MirrorPadOptions *Operator::builtin_options_as<tflite_micro::MirrorPadOptions>() const {
   return builtin_options_as_MirrorPadOptions();
 }
 
-template<> inline const tflite::AbsOptions *Operator::builtin_options_as<tflite::AbsOptions>() const {
+template<> inline const tflite_micro::AbsOptions *Operator::builtin_options_as<tflite_micro::AbsOptions>() const {
   return builtin_options_as_AbsOptions();
 }
 
-template<> inline const tflite::SplitVOptions *Operator::builtin_options_as<tflite::SplitVOptions>() const {
+template<> inline const tflite_micro::SplitVOptions *Operator::builtin_options_as<tflite_micro::SplitVOptions>() const {
   return builtin_options_as_SplitVOptions();
 }
 
-template<> inline const tflite::UniqueOptions *Operator::builtin_options_as<tflite::UniqueOptions>() const {
+template<> inline const tflite_micro::UniqueOptions *Operator::builtin_options_as<tflite_micro::UniqueOptions>() const {
   return builtin_options_as_UniqueOptions();
 }
 
-template<> inline const tflite::ReverseV2Options *Operator::builtin_options_as<tflite::ReverseV2Options>() const {
+template<> inline const tflite_micro::ReverseV2Options *Operator::builtin_options_as<tflite_micro::ReverseV2Options>() const {
   return builtin_options_as_ReverseV2Options();
 }
 
-template<> inline const tflite::AddNOptions *Operator::builtin_options_as<tflite::AddNOptions>() const {
+template<> inline const tflite_micro::AddNOptions *Operator::builtin_options_as<tflite_micro::AddNOptions>() const {
   return builtin_options_as_AddNOptions();
 }
 
-template<> inline const tflite::GatherNdOptions *Operator::builtin_options_as<tflite::GatherNdOptions>() const {
+template<> inline const tflite_micro::GatherNdOptions *Operator::builtin_options_as<tflite_micro::GatherNdOptions>() const {
   return builtin_options_as_GatherNdOptions();
 }
 
-template<> inline const tflite::CosOptions *Operator::builtin_options_as<tflite::CosOptions>() const {
+template<> inline const tflite_micro::CosOptions *Operator::builtin_options_as<tflite_micro::CosOptions>() const {
   return builtin_options_as_CosOptions();
 }
 
-template<> inline const tflite::WhereOptions *Operator::builtin_options_as<tflite::WhereOptions>() const {
+template<> inline const tflite_micro::WhereOptions *Operator::builtin_options_as<tflite_micro::WhereOptions>() const {
   return builtin_options_as_WhereOptions();
 }
 
-template<> inline const tflite::RankOptions *Operator::builtin_options_as<tflite::RankOptions>() const {
+template<> inline const tflite_micro::RankOptions *Operator::builtin_options_as<tflite_micro::RankOptions>() const {
   return builtin_options_as_RankOptions();
 }
 
-template<> inline const tflite::ReverseSequenceOptions *Operator::builtin_options_as<tflite::ReverseSequenceOptions>() const {
+template<> inline const tflite_micro::ReverseSequenceOptions *Operator::builtin_options_as<tflite_micro::ReverseSequenceOptions>() const {
   return builtin_options_as_ReverseSequenceOptions();
 }
 
-template<> inline const tflite::MatrixDiagOptions *Operator::builtin_options_as<tflite::MatrixDiagOptions>() const {
+template<> inline const tflite_micro::MatrixDiagOptions *Operator::builtin_options_as<tflite_micro::MatrixDiagOptions>() const {
   return builtin_options_as_MatrixDiagOptions();
 }
 
-template<> inline const tflite::QuantizeOptions *Operator::builtin_options_as<tflite::QuantizeOptions>() const {
+template<> inline const tflite_micro::QuantizeOptions *Operator::builtin_options_as<tflite_micro::QuantizeOptions>() const {
   return builtin_options_as_QuantizeOptions();
 }
 
-template<> inline const tflite::MatrixSetDiagOptions *Operator::builtin_options_as<tflite::MatrixSetDiagOptions>() const {
+template<> inline const tflite_micro::MatrixSetDiagOptions *Operator::builtin_options_as<tflite_micro::MatrixSetDiagOptions>() const {
   return builtin_options_as_MatrixSetDiagOptions();
 }
 
-template<> inline const tflite::HardSwishOptions *Operator::builtin_options_as<tflite::HardSwishOptions>() const {
+template<> inline const tflite_micro::HardSwishOptions *Operator::builtin_options_as<tflite_micro::HardSwishOptions>() const {
   return builtin_options_as_HardSwishOptions();
 }
 
-template<> inline const tflite::IfOptions *Operator::builtin_options_as<tflite::IfOptions>() const {
+template<> inline const tflite_micro::IfOptions *Operator::builtin_options_as<tflite_micro::IfOptions>() const {
   return builtin_options_as_IfOptions();
 }
 
-template<> inline const tflite::WhileOptions *Operator::builtin_options_as<tflite::WhileOptions>() const {
+template<> inline const tflite_micro::WhileOptions *Operator::builtin_options_as<tflite_micro::WhileOptions>() const {
   return builtin_options_as_WhileOptions();
 }
 
-template<> inline const tflite::DepthToSpaceOptions *Operator::builtin_options_as<tflite::DepthToSpaceOptions>() const {
+template<> inline const tflite_micro::DepthToSpaceOptions *Operator::builtin_options_as<tflite_micro::DepthToSpaceOptions>() const {
   return builtin_options_as_DepthToSpaceOptions();
 }
 
-template<> inline const tflite::NonMaxSuppressionV4Options *Operator::builtin_options_as<tflite::NonMaxSuppressionV4Options>() const {
+template<> inline const tflite_micro::NonMaxSuppressionV4Options *Operator::builtin_options_as<tflite_micro::NonMaxSuppressionV4Options>() const {
   return builtin_options_as_NonMaxSuppressionV4Options();
 }
 
-template<> inline const tflite::NonMaxSuppressionV5Options *Operator::builtin_options_as<tflite::NonMaxSuppressionV5Options>() const {
+template<> inline const tflite_micro::NonMaxSuppressionV5Options *Operator::builtin_options_as<tflite_micro::NonMaxSuppressionV5Options>() const {
   return builtin_options_as_NonMaxSuppressionV5Options();
 }
 
-template<> inline const tflite::ScatterNdOptions *Operator::builtin_options_as<tflite::ScatterNdOptions>() const {
+template<> inline const tflite_micro::ScatterNdOptions *Operator::builtin_options_as<tflite_micro::ScatterNdOptions>() const {
   return builtin_options_as_ScatterNdOptions();
 }
 
-template<> inline const tflite::SelectV2Options *Operator::builtin_options_as<tflite::SelectV2Options>() const {
+template<> inline const tflite_micro::SelectV2Options *Operator::builtin_options_as<tflite_micro::SelectV2Options>() const {
   return builtin_options_as_SelectV2Options();
 }
 
-template<> inline const tflite::DensifyOptions *Operator::builtin_options_as<tflite::DensifyOptions>() const {
+template<> inline const tflite_micro::DensifyOptions *Operator::builtin_options_as<tflite_micro::DensifyOptions>() const {
   return builtin_options_as_DensifyOptions();
 }
 
-template<> inline const tflite::SegmentSumOptions *Operator::builtin_options_as<tflite::SegmentSumOptions>() const {
+template<> inline const tflite_micro::SegmentSumOptions *Operator::builtin_options_as<tflite_micro::SegmentSumOptions>() const {
   return builtin_options_as_SegmentSumOptions();
 }
 
-template<> inline const tflite::BatchMatMulOptions *Operator::builtin_options_as<tflite::BatchMatMulOptions>() const {
+template<> inline const tflite_micro::BatchMatMulOptions *Operator::builtin_options_as<tflite_micro::BatchMatMulOptions>() const {
   return builtin_options_as_BatchMatMulOptions();
 }
 
-template<> inline const tflite::CumsumOptions *Operator::builtin_options_as<tflite::CumsumOptions>() const {
+template<> inline const tflite_micro::CumsumOptions *Operator::builtin_options_as<tflite_micro::CumsumOptions>() const {
   return builtin_options_as_CumsumOptions();
 }
 
-template<> inline const tflite::CallOnceOptions *Operator::builtin_options_as<tflite::CallOnceOptions>() const {
+template<> inline const tflite_micro::CallOnceOptions *Operator::builtin_options_as<tflite_micro::CallOnceOptions>() const {
   return builtin_options_as_CallOnceOptions();
 }
 
-template<> inline const tflite::BroadcastToOptions *Operator::builtin_options_as<tflite::BroadcastToOptions>() const {
+template<> inline const tflite_micro::BroadcastToOptions *Operator::builtin_options_as<tflite_micro::BroadcastToOptions>() const {
   return builtin_options_as_BroadcastToOptions();
 }
 
-template<> inline const tflite::Rfft2dOptions *Operator::builtin_options_as<tflite::Rfft2dOptions>() const {
+template<> inline const tflite_micro::Rfft2dOptions *Operator::builtin_options_as<tflite_micro::Rfft2dOptions>() const {
   return builtin_options_as_Rfft2dOptions();
 }
 
-template<> inline const tflite::Conv3DOptions *Operator::builtin_options_as<tflite::Conv3DOptions>() const {
+template<> inline const tflite_micro::Conv3DOptions *Operator::builtin_options_as<tflite_micro::Conv3DOptions>() const {
   return builtin_options_as_Conv3DOptions();
 }
 
-template<> inline const tflite::HashtableOptions *Operator::builtin_options_as<tflite::HashtableOptions>() const {
+template<> inline const tflite_micro::HashtableOptions *Operator::builtin_options_as<tflite_micro::HashtableOptions>() const {
   return builtin_options_as_HashtableOptions();
 }
 
-template<> inline const tflite::HashtableFindOptions *Operator::builtin_options_as<tflite::HashtableFindOptions>() const {
+template<> inline const tflite_micro::HashtableFindOptions *Operator::builtin_options_as<tflite_micro::HashtableFindOptions>() const {
   return builtin_options_as_HashtableFindOptions();
 }
 
-template<> inline const tflite::HashtableImportOptions *Operator::builtin_options_as<tflite::HashtableImportOptions>() const {
+template<> inline const tflite_micro::HashtableImportOptions *Operator::builtin_options_as<tflite_micro::HashtableImportOptions>() const {
   return builtin_options_as_HashtableImportOptions();
 }
 
-template<> inline const tflite::HashtableSizeOptions *Operator::builtin_options_as<tflite::HashtableSizeOptions>() const {
+template<> inline const tflite_micro::HashtableSizeOptions *Operator::builtin_options_as<tflite_micro::HashtableSizeOptions>() const {
   return builtin_options_as_HashtableSizeOptions();
 }
 
-template<> inline const tflite::VarHandleOptions *Operator::builtin_options_as<tflite::VarHandleOptions>() const {
+template<> inline const tflite_micro::VarHandleOptions *Operator::builtin_options_as<tflite_micro::VarHandleOptions>() const {
   return builtin_options_as_VarHandleOptions();
 }
 
-template<> inline const tflite::ReadVariableOptions *Operator::builtin_options_as<tflite::ReadVariableOptions>() const {
+template<> inline const tflite_micro::ReadVariableOptions *Operator::builtin_options_as<tflite_micro::ReadVariableOptions>() const {
   return builtin_options_as_ReadVariableOptions();
 }
 
-template<> inline const tflite::AssignVariableOptions *Operator::builtin_options_as<tflite::AssignVariableOptions>() const {
+template<> inline const tflite_micro::AssignVariableOptions *Operator::builtin_options_as<tflite_micro::AssignVariableOptions>() const {
   return builtin_options_as_AssignVariableOptions();
 }
 
-template<> inline const tflite::RandomOptions *Operator::builtin_options_as<tflite::RandomOptions>() const {
+template<> inline const tflite_micro::RandomOptions *Operator::builtin_options_as<tflite_micro::RandomOptions>() const {
   return builtin_options_as_RandomOptions();
 }
 
-template<> inline const tflite::BucketizeOptions *Operator::builtin_options_as<tflite::BucketizeOptions>() const {
+template<> inline const tflite_micro::BucketizeOptions *Operator::builtin_options_as<tflite_micro::BucketizeOptions>() const {
   return builtin_options_as_BucketizeOptions();
 }
 
-template<> inline const tflite::GeluOptions *Operator::builtin_options_as<tflite::GeluOptions>() const {
+template<> inline const tflite_micro::GeluOptions *Operator::builtin_options_as<tflite_micro::GeluOptions>() const {
   return builtin_options_as_GeluOptions();
 }
 
-template<> inline const tflite::DynamicUpdateSliceOptions *Operator::builtin_options_as<tflite::DynamicUpdateSliceOptions>() const {
+template<> inline const tflite_micro::DynamicUpdateSliceOptions *Operator::builtin_options_as<tflite_micro::DynamicUpdateSliceOptions>() const {
   return builtin_options_as_DynamicUpdateSliceOptions();
 }
 
-template<> inline const tflite::UnsortedSegmentProdOptions *Operator::builtin_options_as<tflite::UnsortedSegmentProdOptions>() const {
+template<> inline const tflite_micro::UnsortedSegmentProdOptions *Operator::builtin_options_as<tflite_micro::UnsortedSegmentProdOptions>() const {
   return builtin_options_as_UnsortedSegmentProdOptions();
 }
 
-template<> inline const tflite::UnsortedSegmentMaxOptions *Operator::builtin_options_as<tflite::UnsortedSegmentMaxOptions>() const {
+template<> inline const tflite_micro::UnsortedSegmentMaxOptions *Operator::builtin_options_as<tflite_micro::UnsortedSegmentMaxOptions>() const {
   return builtin_options_as_UnsortedSegmentMaxOptions();
 }
 
-template<> inline const tflite::UnsortedSegmentMinOptions *Operator::builtin_options_as<tflite::UnsortedSegmentMinOptions>() const {
+template<> inline const tflite_micro::UnsortedSegmentMinOptions *Operator::builtin_options_as<tflite_micro::UnsortedSegmentMinOptions>() const {
   return builtin_options_as_UnsortedSegmentMinOptions();
 }
 
-template<> inline const tflite::UnsortedSegmentSumOptions *Operator::builtin_options_as<tflite::UnsortedSegmentSumOptions>() const {
+template<> inline const tflite_micro::UnsortedSegmentSumOptions *Operator::builtin_options_as<tflite_micro::UnsortedSegmentSumOptions>() const {
   return builtin_options_as_UnsortedSegmentSumOptions();
 }
 
-template<> inline const tflite::ATan2Options *Operator::builtin_options_as<tflite::ATan2Options>() const {
+template<> inline const tflite_micro::ATan2Options *Operator::builtin_options_as<tflite_micro::ATan2Options>() const {
   return builtin_options_as_ATan2Options();
 }
 
-template<> inline const tflite::SignOptions *Operator::builtin_options_as<tflite::SignOptions>() const {
+template<> inline const tflite_micro::SignOptions *Operator::builtin_options_as<tflite_micro::SignOptions>() const {
   return builtin_options_as_SignOptions();
 }
 
-template<> inline const tflite::BitcastOptions *Operator::builtin_options_as<tflite::BitcastOptions>() const {
+template<> inline const tflite_micro::BitcastOptions *Operator::builtin_options_as<tflite_micro::BitcastOptions>() const {
   return builtin_options_as_BitcastOptions();
 }
 
-template<> inline const tflite::BitwiseXorOptions *Operator::builtin_options_as<tflite::BitwiseXorOptions>() const {
+template<> inline const tflite_micro::BitwiseXorOptions *Operator::builtin_options_as<tflite_micro::BitwiseXorOptions>() const {
   return builtin_options_as_BitwiseXorOptions();
 }
 
-template<> inline const tflite::RightShiftOptions *Operator::builtin_options_as<tflite::RightShiftOptions>() const {
+template<> inline const tflite_micro::RightShiftOptions *Operator::builtin_options_as<tflite_micro::RightShiftOptions>() const {
   return builtin_options_as_RightShiftOptions();
 }
 
-template<> inline const tflite::StablehloConcatenateOptions *Operator::builtin_options_2_as<tflite::StablehloConcatenateOptions>() const {
+template<> inline const tflite_micro::StablehloConcatenateOptions *Operator::builtin_options_2_as<tflite_micro::StablehloConcatenateOptions>() const {
   return builtin_options_2_as_StablehloConcatenateOptions();
 }
 
-template<> inline const tflite::StablehloBroadcastInDimOptions *Operator::builtin_options_2_as<tflite::StablehloBroadcastInDimOptions>() const {
+template<> inline const tflite_micro::StablehloBroadcastInDimOptions *Operator::builtin_options_2_as<tflite_micro::StablehloBroadcastInDimOptions>() const {
   return builtin_options_2_as_StablehloBroadcastInDimOptions();
 }
 
-template<> inline const tflite::StablehloSliceOptions *Operator::builtin_options_2_as<tflite::StablehloSliceOptions>() const {
+template<> inline const tflite_micro::StablehloSliceOptions *Operator::builtin_options_2_as<tflite_micro::StablehloSliceOptions>() const {
   return builtin_options_2_as_StablehloSliceOptions();
 }
 
-template<> inline const tflite::StablehloConvolutionOptions *Operator::builtin_options_2_as<tflite::StablehloConvolutionOptions>() const {
+template<> inline const tflite_micro::StablehloConvolutionOptions *Operator::builtin_options_2_as<tflite_micro::StablehloConvolutionOptions>() const {
   return builtin_options_2_as_StablehloConvolutionOptions();
 }
 
-template<> inline const tflite::StablehloCustomCallOptions *Operator::builtin_options_2_as<tflite::StablehloCustomCallOptions>() const {
+template<> inline const tflite_micro::StablehloCustomCallOptions *Operator::builtin_options_2_as<tflite_micro::StablehloCustomCallOptions>() const {
   return builtin_options_2_as_StablehloCustomCallOptions();
 }
 
-template<> inline const tflite::StablehloReduceOptions *Operator::builtin_options_2_as<tflite::StablehloReduceOptions>() const {
+template<> inline const tflite_micro::StablehloReduceOptions *Operator::builtin_options_2_as<tflite_micro::StablehloReduceOptions>() const {
   return builtin_options_2_as_StablehloReduceOptions();
 }
 
-template<> inline const tflite::StablehloScatterOptions *Operator::builtin_options_2_as<tflite::StablehloScatterOptions>() const {
+template<> inline const tflite_micro::StablehloScatterOptions *Operator::builtin_options_2_as<tflite_micro::StablehloScatterOptions>() const {
   return builtin_options_2_as_StablehloScatterOptions();
 }
 
-template<> inline const tflite::StablehloCompareOptions *Operator::builtin_options_2_as<tflite::StablehloCompareOptions>() const {
+template<> inline const tflite_micro::StablehloCompareOptions *Operator::builtin_options_2_as<tflite_micro::StablehloCompareOptions>() const {
   return builtin_options_2_as_StablehloCompareOptions();
 }
 
-template<> inline const tflite::StablehloDynamicSliceOptions *Operator::builtin_options_2_as<tflite::StablehloDynamicSliceOptions>() const {
+template<> inline const tflite_micro::StablehloDynamicSliceOptions *Operator::builtin_options_2_as<tflite_micro::StablehloDynamicSliceOptions>() const {
   return builtin_options_2_as_StablehloDynamicSliceOptions();
 }
 
-template<> inline const tflite::StablehloPadOptions *Operator::builtin_options_2_as<tflite::StablehloPadOptions>() const {
+template<> inline const tflite_micro::StablehloPadOptions *Operator::builtin_options_2_as<tflite_micro::StablehloPadOptions>() const {
   return builtin_options_2_as_StablehloPadOptions();
 }
 
-template<> inline const tflite::StablehloIotaOptions *Operator::builtin_options_2_as<tflite::StablehloIotaOptions>() const {
+template<> inline const tflite_micro::StablehloIotaOptions *Operator::builtin_options_2_as<tflite_micro::StablehloIotaOptions>() const {
   return builtin_options_2_as_StablehloIotaOptions();
 }
 
-template<> inline const tflite::StablehloDotGeneralOptions *Operator::builtin_options_2_as<tflite::StablehloDotGeneralOptions>() const {
+template<> inline const tflite_micro::StablehloDotGeneralOptions *Operator::builtin_options_2_as<tflite_micro::StablehloDotGeneralOptions>() const {
   return builtin_options_2_as_StablehloDotGeneralOptions();
 }
 
-template<> inline const tflite::StablehloReduceWindowOptions *Operator::builtin_options_2_as<tflite::StablehloReduceWindowOptions>() const {
+template<> inline const tflite_micro::StablehloReduceWindowOptions *Operator::builtin_options_2_as<tflite_micro::StablehloReduceWindowOptions>() const {
   return builtin_options_2_as_StablehloReduceWindowOptions();
 }
 
-template<> inline const tflite::StablehloSortOptions *Operator::builtin_options_2_as<tflite::StablehloSortOptions>() const {
+template<> inline const tflite_micro::StablehloSortOptions *Operator::builtin_options_2_as<tflite_micro::StablehloSortOptions>() const {
   return builtin_options_2_as_StablehloSortOptions();
 }
 
-template<> inline const tflite::StablehloWhileOptions *Operator::builtin_options_2_as<tflite::StablehloWhileOptions>() const {
+template<> inline const tflite_micro::StablehloWhileOptions *Operator::builtin_options_2_as<tflite_micro::StablehloWhileOptions>() const {
   return builtin_options_2_as_StablehloWhileOptions();
 }
 
-template<> inline const tflite::StablehloGatherOptions *Operator::builtin_options_2_as<tflite::StablehloGatherOptions>() const {
+template<> inline const tflite_micro::StablehloGatherOptions *Operator::builtin_options_2_as<tflite_micro::StablehloGatherOptions>() const {
   return builtin_options_2_as_StablehloGatherOptions();
 }
 
-template<> inline const tflite::StablehloTransposeOptions *Operator::builtin_options_2_as<tflite::StablehloTransposeOptions>() const {
+template<> inline const tflite_micro::StablehloTransposeOptions *Operator::builtin_options_2_as<tflite_micro::StablehloTransposeOptions>() const {
   return builtin_options_2_as_StablehloTransposeOptions();
 }
 
-template<> inline const tflite::DilateOptions *Operator::builtin_options_2_as<tflite::DilateOptions>() const {
+template<> inline const tflite_micro::DilateOptions *Operator::builtin_options_2_as<tflite_micro::DilateOptions>() const {
   return builtin_options_2_as_DilateOptions();
 }
 
-template<> inline const tflite::StablehloRngBitGeneratorOptions *Operator::builtin_options_2_as<tflite::StablehloRngBitGeneratorOptions>() const {
+template<> inline const tflite_micro::StablehloRngBitGeneratorOptions *Operator::builtin_options_2_as<tflite_micro::StablehloRngBitGeneratorOptions>() const {
   return builtin_options_2_as_StablehloRngBitGeneratorOptions();
 }
 
-template<> inline const tflite::ReduceWindowOptions *Operator::builtin_options_2_as<tflite::ReduceWindowOptions>() const {
+template<> inline const tflite_micro::ReduceWindowOptions *Operator::builtin_options_2_as<tflite_micro::ReduceWindowOptions>() const {
   return builtin_options_2_as_ReduceWindowOptions();
 }
 
@@ -15733,7 +15733,7 @@ struct OperatorBuilder {
   void add_outputs(flatbuffers::Offset<flatbuffers::Vector<int32_t>> outputs) {
     fbb_.AddOffset(Operator::VT_OUTPUTS, outputs);
   }
-  void add_builtin_options_type(tflite::BuiltinOptions builtin_options_type) {
+  void add_builtin_options_type(tflite_micro::BuiltinOptions builtin_options_type) {
     fbb_.AddElement<uint8_t>(Operator::VT_BUILTIN_OPTIONS_TYPE, static_cast<uint8_t>(builtin_options_type), 0);
   }
   void add_builtin_options(flatbuffers::Offset<void> builtin_options) {
@@ -15742,7 +15742,7 @@ struct OperatorBuilder {
   void add_custom_options(flatbuffers::Offset<flatbuffers::Vector<uint8_t>> custom_options) {
     fbb_.AddOffset(Operator::VT_CUSTOM_OPTIONS, custom_options);
   }
-  void add_custom_options_format(tflite::CustomOptionsFormat custom_options_format) {
+  void add_custom_options_format(tflite_micro::CustomOptionsFormat custom_options_format) {
     fbb_.AddElement<int8_t>(Operator::VT_CUSTOM_OPTIONS_FORMAT, static_cast<int8_t>(custom_options_format), 0);
   }
   void add_mutating_variable_inputs(flatbuffers::Offset<flatbuffers::Vector<uint8_t>> mutating_variable_inputs) {
@@ -15757,7 +15757,7 @@ struct OperatorBuilder {
   void add_large_custom_options_size(uint64_t large_custom_options_size) {
     fbb_.AddElement<uint64_t>(Operator::VT_LARGE_CUSTOM_OPTIONS_SIZE, large_custom_options_size, 0);
   }
-  void add_builtin_options_2_type(tflite::BuiltinOptions2 builtin_options_2_type) {
+  void add_builtin_options_2_type(tflite_micro::BuiltinOptions2 builtin_options_2_type) {
     fbb_.AddElement<uint8_t>(Operator::VT_BUILTIN_OPTIONS_2_TYPE, static_cast<uint8_t>(builtin_options_2_type), 0);
   }
   void add_builtin_options_2(flatbuffers::Offset<void> builtin_options_2) {
@@ -15779,15 +15779,15 @@ inline flatbuffers::Offset<Operator> CreateOperator(
     uint32_t opcode_index = 0,
     flatbuffers::Offset<flatbuffers::Vector<int32_t>> inputs = 0,
     flatbuffers::Offset<flatbuffers::Vector<int32_t>> outputs = 0,
-    tflite::BuiltinOptions builtin_options_type = tflite::BuiltinOptions_NONE,
+    tflite_micro::BuiltinOptions builtin_options_type = tflite_micro::BuiltinOptions_NONE,
     flatbuffers::Offset<void> builtin_options = 0,
     flatbuffers::Offset<flatbuffers::Vector<uint8_t>> custom_options = 0,
-    tflite::CustomOptionsFormat custom_options_format = tflite::CustomOptionsFormat_FLEXBUFFERS,
+    tflite_micro::CustomOptionsFormat custom_options_format = tflite_micro::CustomOptionsFormat_FLEXBUFFERS,
     flatbuffers::Offset<flatbuffers::Vector<uint8_t>> mutating_variable_inputs = 0,
     flatbuffers::Offset<flatbuffers::Vector<int32_t>> intermediates = 0,
     uint64_t large_custom_options_offset = 0,
     uint64_t large_custom_options_size = 0,
-    tflite::BuiltinOptions2 builtin_options_2_type = tflite::BuiltinOptions2_NONE,
+    tflite_micro::BuiltinOptions2 builtin_options_2_type = tflite_micro::BuiltinOptions2_NONE,
     flatbuffers::Offset<void> builtin_options_2 = 0) {
   OperatorBuilder builder_(_fbb);
   builder_.add_large_custom_options_size(large_custom_options_size);
@@ -15811,22 +15811,22 @@ inline flatbuffers::Offset<Operator> CreateOperatorDirect(
     uint32_t opcode_index = 0,
     const std::vector<int32_t> *inputs = nullptr,
     const std::vector<int32_t> *outputs = nullptr,
-    tflite::BuiltinOptions builtin_options_type = tflite::BuiltinOptions_NONE,
+    tflite_micro::BuiltinOptions builtin_options_type = tflite_micro::BuiltinOptions_NONE,
     flatbuffers::Offset<void> builtin_options = 0,
     const std::vector<uint8_t> *custom_options = nullptr,
-    tflite::CustomOptionsFormat custom_options_format = tflite::CustomOptionsFormat_FLEXBUFFERS,
+    tflite_micro::CustomOptionsFormat custom_options_format = tflite_micro::CustomOptionsFormat_FLEXBUFFERS,
     const std::vector<uint8_t> *mutating_variable_inputs = nullptr,
     const std::vector<int32_t> *intermediates = nullptr,
     uint64_t large_custom_options_offset = 0,
     uint64_t large_custom_options_size = 0,
-    tflite::BuiltinOptions2 builtin_options_2_type = tflite::BuiltinOptions2_NONE,
+    tflite_micro::BuiltinOptions2 builtin_options_2_type = tflite_micro::BuiltinOptions2_NONE,
     flatbuffers::Offset<void> builtin_options_2 = 0) {
   auto inputs__ = inputs ? _fbb.CreateVector<int32_t>(*inputs) : 0;
   auto outputs__ = outputs ? _fbb.CreateVector<int32_t>(*outputs) : 0;
   auto custom_options__ = custom_options ? _fbb.CreateVector<uint8_t>(*custom_options) : 0;
   auto mutating_variable_inputs__ = mutating_variable_inputs ? _fbb.CreateVector<uint8_t>(*mutating_variable_inputs) : 0;
   auto intermediates__ = intermediates ? _fbb.CreateVector<int32_t>(*intermediates) : 0;
-  return tflite::CreateOperator(
+  return tflite_micro::CreateOperator(
       _fbb,
       opcode_index,
       inputs__,
@@ -15847,10 +15847,10 @@ flatbuffers::Offset<Operator> CreateOperator(flatbuffers::FlatBufferBuilder &_fb
 
 struct SubGraphT : public flatbuffers::NativeTable {
   typedef SubGraph TableType;
-  std::vector<std::unique_ptr<tflite::TensorT>> tensors{};
+  std::vector<std::unique_ptr<tflite_micro::TensorT>> tensors{};
   std::vector<int32_t> inputs{};
   std::vector<int32_t> outputs{};
-  std::vector<std::unique_ptr<tflite::OperatorT>> operators{};
+  std::vector<std::unique_ptr<tflite_micro::OperatorT>> operators{};
   std::string name{};
   SubGraphT() = default;
   SubGraphT(const SubGraphT &o);
@@ -15868,8 +15868,8 @@ struct SubGraph FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
     VT_OPERATORS = 10,
     VT_NAME = 12
   };
-  const flatbuffers::Vector<flatbuffers::Offset<tflite::Tensor>> *tensors() const {
-    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite::Tensor>> *>(VT_TENSORS);
+  const flatbuffers::Vector<flatbuffers::Offset<tflite_micro::Tensor>> *tensors() const {
+    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite_micro::Tensor>> *>(VT_TENSORS);
   }
   const flatbuffers::Vector<int32_t> *inputs() const {
     return GetPointer<const flatbuffers::Vector<int32_t> *>(VT_INPUTS);
@@ -15877,8 +15877,8 @@ struct SubGraph FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   const flatbuffers::Vector<int32_t> *outputs() const {
     return GetPointer<const flatbuffers::Vector<int32_t> *>(VT_OUTPUTS);
   }
-  const flatbuffers::Vector<flatbuffers::Offset<tflite::Operator>> *operators() const {
-    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite::Operator>> *>(VT_OPERATORS);
+  const flatbuffers::Vector<flatbuffers::Offset<tflite_micro::Operator>> *operators() const {
+    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite_micro::Operator>> *>(VT_OPERATORS);
   }
   const flatbuffers::String *name() const {
     return GetPointer<const flatbuffers::String *>(VT_NAME);
@@ -15908,7 +15908,7 @@ struct SubGraphBuilder {
   typedef SubGraph Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_tensors(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::Tensor>>> tensors) {
+  void add_tensors(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite_micro::Tensor>>> tensors) {
     fbb_.AddOffset(SubGraph::VT_TENSORS, tensors);
   }
   void add_inputs(flatbuffers::Offset<flatbuffers::Vector<int32_t>> inputs) {
@@ -15917,7 +15917,7 @@ struct SubGraphBuilder {
   void add_outputs(flatbuffers::Offset<flatbuffers::Vector<int32_t>> outputs) {
     fbb_.AddOffset(SubGraph::VT_OUTPUTS, outputs);
   }
-  void add_operators(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::Operator>>> operators) {
+  void add_operators(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite_micro::Operator>>> operators) {
     fbb_.AddOffset(SubGraph::VT_OPERATORS, operators);
   }
   void add_name(flatbuffers::Offset<flatbuffers::String> name) {
@@ -15936,10 +15936,10 @@ struct SubGraphBuilder {
 
 inline flatbuffers::Offset<SubGraph> CreateSubGraph(
     flatbuffers::FlatBufferBuilder &_fbb,
-    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::Tensor>>> tensors = 0,
+    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite_micro::Tensor>>> tensors = 0,
     flatbuffers::Offset<flatbuffers::Vector<int32_t>> inputs = 0,
     flatbuffers::Offset<flatbuffers::Vector<int32_t>> outputs = 0,
-    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::Operator>>> operators = 0,
+    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite_micro::Operator>>> operators = 0,
     flatbuffers::Offset<flatbuffers::String> name = 0) {
   SubGraphBuilder builder_(_fbb);
   builder_.add_name(name);
@@ -15952,17 +15952,17 @@ inline flatbuffers::Offset<SubGraph> CreateSubGraph(
 
 inline flatbuffers::Offset<SubGraph> CreateSubGraphDirect(
     flatbuffers::FlatBufferBuilder &_fbb,
-    const std::vector<flatbuffers::Offset<tflite::Tensor>> *tensors = nullptr,
+    const std::vector<flatbuffers::Offset<tflite_micro::Tensor>> *tensors = nullptr,
     const std::vector<int32_t> *inputs = nullptr,
     const std::vector<int32_t> *outputs = nullptr,
-    const std::vector<flatbuffers::Offset<tflite::Operator>> *operators = nullptr,
+    const std::vector<flatbuffers::Offset<tflite_micro::Operator>> *operators = nullptr,
     const char *name = nullptr) {
-  auto tensors__ = tensors ? _fbb.CreateVector<flatbuffers::Offset<tflite::Tensor>>(*tensors) : 0;
+  auto tensors__ = tensors ? _fbb.CreateVector<flatbuffers::Offset<tflite_micro::Tensor>>(*tensors) : 0;
   auto inputs__ = inputs ? _fbb.CreateVector<int32_t>(*inputs) : 0;
   auto outputs__ = outputs ? _fbb.CreateVector<int32_t>(*outputs) : 0;
-  auto operators__ = operators ? _fbb.CreateVector<flatbuffers::Offset<tflite::Operator>>(*operators) : 0;
+  auto operators__ = operators ? _fbb.CreateVector<flatbuffers::Offset<tflite_micro::Operator>>(*operators) : 0;
   auto name__ = name ? _fbb.CreateString(name) : 0;
-  return tflite::CreateSubGraph(
+  return tflite_micro::CreateSubGraph(
       _fbb,
       tensors__,
       inputs__,
@@ -16053,7 +16053,7 @@ inline flatbuffers::Offset<Buffer> CreateBufferDirect(
     uint64_t size = 0) {
   if (data) { _fbb.ForceVectorAlignment(data->size(), sizeof(uint8_t), 16); }
   auto data__ = data ? _fbb.CreateVector<uint8_t>(*data) : 0;
-  return tflite::CreateBuffer(
+  return tflite_micro::CreateBuffer(
       _fbb,
       data__,
       offset,
@@ -16129,7 +16129,7 @@ inline flatbuffers::Offset<Metadata> CreateMetadataDirect(
     const char *name = nullptr,
     uint32_t buffer = 0) {
   auto name__ = name ? _fbb.CreateString(name) : 0;
-  return tflite::CreateMetadata(
+  return tflite_micro::CreateMetadata(
       _fbb,
       name__,
       buffer);
@@ -16204,7 +16204,7 @@ inline flatbuffers::Offset<TensorMap> CreateTensorMapDirect(
     const char *name = nullptr,
     uint32_t tensor_index = 0) {
   auto name__ = name ? _fbb.CreateString(name) : 0;
-  return tflite::CreateTensorMap(
+  return tflite_micro::CreateTensorMap(
       _fbb,
       name__,
       tensor_index);
@@ -16214,8 +16214,8 @@ flatbuffers::Offset<TensorMap> CreateTensorMap(flatbuffers::FlatBufferBuilder &_
 
 struct SignatureDefT : public flatbuffers::NativeTable {
   typedef SignatureDef TableType;
-  std::vector<std::unique_ptr<tflite::TensorMapT>> inputs{};
-  std::vector<std::unique_ptr<tflite::TensorMapT>> outputs{};
+  std::vector<std::unique_ptr<tflite_micro::TensorMapT>> inputs{};
+  std::vector<std::unique_ptr<tflite_micro::TensorMapT>> outputs{};
   std::string signature_key{};
   uint32_t subgraph_index = 0;
   SignatureDefT() = default;
@@ -16233,11 +16233,11 @@ struct SignatureDef FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
     VT_SIGNATURE_KEY = 8,
     VT_SUBGRAPH_INDEX = 12
   };
-  const flatbuffers::Vector<flatbuffers::Offset<tflite::TensorMap>> *inputs() const {
-    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite::TensorMap>> *>(VT_INPUTS);
+  const flatbuffers::Vector<flatbuffers::Offset<tflite_micro::TensorMap>> *inputs() const {
+    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite_micro::TensorMap>> *>(VT_INPUTS);
   }
-  const flatbuffers::Vector<flatbuffers::Offset<tflite::TensorMap>> *outputs() const {
-    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite::TensorMap>> *>(VT_OUTPUTS);
+  const flatbuffers::Vector<flatbuffers::Offset<tflite_micro::TensorMap>> *outputs() const {
+    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite_micro::TensorMap>> *>(VT_OUTPUTS);
   }
   const flatbuffers::String *signature_key() const {
     return GetPointer<const flatbuffers::String *>(VT_SIGNATURE_KEY);
@@ -16267,10 +16267,10 @@ struct SignatureDefBuilder {
   typedef SignatureDef Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_inputs(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::TensorMap>>> inputs) {
+  void add_inputs(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite_micro::TensorMap>>> inputs) {
     fbb_.AddOffset(SignatureDef::VT_INPUTS, inputs);
   }
-  void add_outputs(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::TensorMap>>> outputs) {
+  void add_outputs(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite_micro::TensorMap>>> outputs) {
     fbb_.AddOffset(SignatureDef::VT_OUTPUTS, outputs);
   }
   void add_signature_key(flatbuffers::Offset<flatbuffers::String> signature_key) {
@@ -16292,8 +16292,8 @@ struct SignatureDefBuilder {
 
 inline flatbuffers::Offset<SignatureDef> CreateSignatureDef(
     flatbuffers::FlatBufferBuilder &_fbb,
-    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::TensorMap>>> inputs = 0,
-    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::TensorMap>>> outputs = 0,
+    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite_micro::TensorMap>>> inputs = 0,
+    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite_micro::TensorMap>>> outputs = 0,
     flatbuffers::Offset<flatbuffers::String> signature_key = 0,
     uint32_t subgraph_index = 0) {
   SignatureDefBuilder builder_(_fbb);
@@ -16306,14 +16306,14 @@ inline flatbuffers::Offset<SignatureDef> CreateSignatureDef(
 
 inline flatbuffers::Offset<SignatureDef> CreateSignatureDefDirect(
     flatbuffers::FlatBufferBuilder &_fbb,
-    const std::vector<flatbuffers::Offset<tflite::TensorMap>> *inputs = nullptr,
-    const std::vector<flatbuffers::Offset<tflite::TensorMap>> *outputs = nullptr,
+    const std::vector<flatbuffers::Offset<tflite_micro::TensorMap>> *inputs = nullptr,
+    const std::vector<flatbuffers::Offset<tflite_micro::TensorMap>> *outputs = nullptr,
     const char *signature_key = nullptr,
     uint32_t subgraph_index = 0) {
-  auto inputs__ = inputs ? _fbb.CreateVector<flatbuffers::Offset<tflite::TensorMap>>(*inputs) : 0;
-  auto outputs__ = outputs ? _fbb.CreateVector<flatbuffers::Offset<tflite::TensorMap>>(*outputs) : 0;
+  auto inputs__ = inputs ? _fbb.CreateVector<flatbuffers::Offset<tflite_micro::TensorMap>>(*inputs) : 0;
+  auto outputs__ = outputs ? _fbb.CreateVector<flatbuffers::Offset<tflite_micro::TensorMap>>(*outputs) : 0;
   auto signature_key__ = signature_key ? _fbb.CreateString(signature_key) : 0;
-  return tflite::CreateSignatureDef(
+  return tflite_micro::CreateSignatureDef(
       _fbb,
       inputs__,
       outputs__,
@@ -16326,13 +16326,13 @@ flatbuffers::Offset<SignatureDef> CreateSignatureDef(flatbuffers::FlatBufferBuil
 struct ModelT : public flatbuffers::NativeTable {
   typedef Model TableType;
   uint32_t version = 0;
-  std::vector<std::unique_ptr<tflite::OperatorCodeT>> operator_codes{};
-  std::vector<std::unique_ptr<tflite::SubGraphT>> subgraphs{};
+  std::vector<std::unique_ptr<tflite_micro::OperatorCodeT>> operator_codes{};
+  std::vector<std::unique_ptr<tflite_micro::SubGraphT>> subgraphs{};
   std::string description{};
-  std::vector<std::unique_ptr<tflite::BufferT>> buffers{};
+  std::vector<std::unique_ptr<tflite_micro::BufferT>> buffers{};
   std::vector<int32_t> metadata_buffer{};
-  std::vector<std::unique_ptr<tflite::MetadataT>> metadata{};
-  std::vector<std::unique_ptr<tflite::SignatureDefT>> signature_defs{};
+  std::vector<std::unique_ptr<tflite_micro::MetadataT>> metadata{};
+  std::vector<std::unique_ptr<tflite_micro::SignatureDefT>> signature_defs{};
   ModelT() = default;
   ModelT(const ModelT &o);
   ModelT(ModelT&&) FLATBUFFERS_NOEXCEPT = default;
@@ -16355,26 +16355,26 @@ struct Model FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   uint32_t version() const {
     return GetField<uint32_t>(VT_VERSION, 0);
   }
-  const flatbuffers::Vector<flatbuffers::Offset<tflite::OperatorCode>> *operator_codes() const {
-    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite::OperatorCode>> *>(VT_OPERATOR_CODES);
+  const flatbuffers::Vector<flatbuffers::Offset<tflite_micro::OperatorCode>> *operator_codes() const {
+    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite_micro::OperatorCode>> *>(VT_OPERATOR_CODES);
   }
-  const flatbuffers::Vector<flatbuffers::Offset<tflite::SubGraph>> *subgraphs() const {
-    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite::SubGraph>> *>(VT_SUBGRAPHS);
+  const flatbuffers::Vector<flatbuffers::Offset<tflite_micro::SubGraph>> *subgraphs() const {
+    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite_micro::SubGraph>> *>(VT_SUBGRAPHS);
   }
   const flatbuffers::String *description() const {
     return GetPointer<const flatbuffers::String *>(VT_DESCRIPTION);
   }
-  const flatbuffers::Vector<flatbuffers::Offset<tflite::Buffer>> *buffers() const {
-    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite::Buffer>> *>(VT_BUFFERS);
+  const flatbuffers::Vector<flatbuffers::Offset<tflite_micro::Buffer>> *buffers() const {
+    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite_micro::Buffer>> *>(VT_BUFFERS);
   }
   const flatbuffers::Vector<int32_t> *metadata_buffer() const {
     return GetPointer<const flatbuffers::Vector<int32_t> *>(VT_METADATA_BUFFER);
   }
-  const flatbuffers::Vector<flatbuffers::Offset<tflite::Metadata>> *metadata() const {
-    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite::Metadata>> *>(VT_METADATA);
+  const flatbuffers::Vector<flatbuffers::Offset<tflite_micro::Metadata>> *metadata() const {
+    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite_micro::Metadata>> *>(VT_METADATA);
   }
-  const flatbuffers::Vector<flatbuffers::Offset<tflite::SignatureDef>> *signature_defs() const {
-    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite::SignatureDef>> *>(VT_SIGNATURE_DEFS);
+  const flatbuffers::Vector<flatbuffers::Offset<tflite_micro::SignatureDef>> *signature_defs() const {
+    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite_micro::SignatureDef>> *>(VT_SIGNATURE_DEFS);
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
@@ -16412,25 +16412,25 @@ struct ModelBuilder {
   void add_version(uint32_t version) {
     fbb_.AddElement<uint32_t>(Model::VT_VERSION, version, 0);
   }
-  void add_operator_codes(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::OperatorCode>>> operator_codes) {
+  void add_operator_codes(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite_micro::OperatorCode>>> operator_codes) {
     fbb_.AddOffset(Model::VT_OPERATOR_CODES, operator_codes);
   }
-  void add_subgraphs(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::SubGraph>>> subgraphs) {
+  void add_subgraphs(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite_micro::SubGraph>>> subgraphs) {
     fbb_.AddOffset(Model::VT_SUBGRAPHS, subgraphs);
   }
   void add_description(flatbuffers::Offset<flatbuffers::String> description) {
     fbb_.AddOffset(Model::VT_DESCRIPTION, description);
   }
-  void add_buffers(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::Buffer>>> buffers) {
+  void add_buffers(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite_micro::Buffer>>> buffers) {
     fbb_.AddOffset(Model::VT_BUFFERS, buffers);
   }
   void add_metadata_buffer(flatbuffers::Offset<flatbuffers::Vector<int32_t>> metadata_buffer) {
     fbb_.AddOffset(Model::VT_METADATA_BUFFER, metadata_buffer);
   }
-  void add_metadata(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::Metadata>>> metadata) {
+  void add_metadata(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite_micro::Metadata>>> metadata) {
     fbb_.AddOffset(Model::VT_METADATA, metadata);
   }
-  void add_signature_defs(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::SignatureDef>>> signature_defs) {
+  void add_signature_defs(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite_micro::SignatureDef>>> signature_defs) {
     fbb_.AddOffset(Model::VT_SIGNATURE_DEFS, signature_defs);
   }
   explicit ModelBuilder(flatbuffers::FlatBufferBuilder &_fbb)
@@ -16447,13 +16447,13 @@ struct ModelBuilder {
 inline flatbuffers::Offset<Model> CreateModel(
     flatbuffers::FlatBufferBuilder &_fbb,
     uint32_t version = 0,
-    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::OperatorCode>>> operator_codes = 0,
-    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::SubGraph>>> subgraphs = 0,
+    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite_micro::OperatorCode>>> operator_codes = 0,
+    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite_micro::SubGraph>>> subgraphs = 0,
     flatbuffers::Offset<flatbuffers::String> description = 0,
-    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::Buffer>>> buffers = 0,
+    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite_micro::Buffer>>> buffers = 0,
     flatbuffers::Offset<flatbuffers::Vector<int32_t>> metadata_buffer = 0,
-    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::Metadata>>> metadata = 0,
-    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::SignatureDef>>> signature_defs = 0) {
+    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite_micro::Metadata>>> metadata = 0,
+    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite_micro::SignatureDef>>> signature_defs = 0) {
   ModelBuilder builder_(_fbb);
   builder_.add_signature_defs(signature_defs);
   builder_.add_metadata(metadata);
@@ -16469,21 +16469,21 @@ inline flatbuffers::Offset<Model> CreateModel(
 inline flatbuffers::Offset<Model> CreateModelDirect(
     flatbuffers::FlatBufferBuilder &_fbb,
     uint32_t version = 0,
-    const std::vector<flatbuffers::Offset<tflite::OperatorCode>> *operator_codes = nullptr,
-    const std::vector<flatbuffers::Offset<tflite::SubGraph>> *subgraphs = nullptr,
+    const std::vector<flatbuffers::Offset<tflite_micro::OperatorCode>> *operator_codes = nullptr,
+    const std::vector<flatbuffers::Offset<tflite_micro::SubGraph>> *subgraphs = nullptr,
     const char *description = nullptr,
-    const std::vector<flatbuffers::Offset<tflite::Buffer>> *buffers = nullptr,
+    const std::vector<flatbuffers::Offset<tflite_micro::Buffer>> *buffers = nullptr,
     const std::vector<int32_t> *metadata_buffer = nullptr,
-    const std::vector<flatbuffers::Offset<tflite::Metadata>> *metadata = nullptr,
-    const std::vector<flatbuffers::Offset<tflite::SignatureDef>> *signature_defs = nullptr) {
-  auto operator_codes__ = operator_codes ? _fbb.CreateVector<flatbuffers::Offset<tflite::OperatorCode>>(*operator_codes) : 0;
-  auto subgraphs__ = subgraphs ? _fbb.CreateVector<flatbuffers::Offset<tflite::SubGraph>>(*subgraphs) : 0;
+    const std::vector<flatbuffers::Offset<tflite_micro::Metadata>> *metadata = nullptr,
+    const std::vector<flatbuffers::Offset<tflite_micro::SignatureDef>> *signature_defs = nullptr) {
+  auto operator_codes__ = operator_codes ? _fbb.CreateVector<flatbuffers::Offset<tflite_micro::OperatorCode>>(*operator_codes) : 0;
+  auto subgraphs__ = subgraphs ? _fbb.CreateVector<flatbuffers::Offset<tflite_micro::SubGraph>>(*subgraphs) : 0;
   auto description__ = description ? _fbb.CreateString(description) : 0;
-  auto buffers__ = buffers ? _fbb.CreateVector<flatbuffers::Offset<tflite::Buffer>>(*buffers) : 0;
+  auto buffers__ = buffers ? _fbb.CreateVector<flatbuffers::Offset<tflite_micro::Buffer>>(*buffers) : 0;
   auto metadata_buffer__ = metadata_buffer ? _fbb.CreateVector<int32_t>(*metadata_buffer) : 0;
-  auto metadata__ = metadata ? _fbb.CreateVector<flatbuffers::Offset<tflite::Metadata>>(*metadata) : 0;
-  auto signature_defs__ = signature_defs ? _fbb.CreateVector<flatbuffers::Offset<tflite::SignatureDef>>(*signature_defs) : 0;
-  return tflite::CreateModel(
+  auto metadata__ = metadata ? _fbb.CreateVector<flatbuffers::Offset<tflite_micro::Metadata>>(*metadata) : 0;
+  auto signature_defs__ = signature_defs ? _fbb.CreateVector<flatbuffers::Offset<tflite_micro::SignatureDef>>(*signature_defs) : 0;
+  return tflite_micro::CreateModel(
       _fbb,
       version,
       operator_codes__,
@@ -16519,7 +16519,7 @@ inline flatbuffers::Offset<CustomQuantization> CreateCustomQuantization(flatbuff
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const CustomQuantizationT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   _fbb.ForceVectorAlignment(_o->custom.size(), sizeof(uint8_t), 16);
   auto _custom = _o->custom.size() ? _fbb.CreateVector(_o->custom) : 0;
-  return tflite::CreateCustomQuantization(
+  return tflite_micro::CreateCustomQuantization(
       _fbb,
       _custom);
 }
@@ -16538,7 +16538,7 @@ inline void QuantizationParameters::UnPackTo(QuantizationParametersT *_o, const
   { auto _e = scale(); if (_e) { _o->scale.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->scale[_i] = _e->Get(_i); } } }
   { auto _e = zero_point(); if (_e) { _o->zero_point.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->zero_point[_i] = _e->Get(_i); } } }
   { auto _e = details_type(); _o->details.type = _e; }
-  { auto _e = details(); if (_e) _o->details.value = tflite::QuantizationDetailsUnion::UnPack(_e, details_type(), _resolver); }
+  { auto _e = details(); if (_e) _o->details.value = tflite_micro::QuantizationDetailsUnion::UnPack(_e, details_type(), _resolver); }
   { auto _e = quantized_dimension(); _o->quantized_dimension = _e; }
 }
 
@@ -16557,7 +16557,7 @@ inline flatbuffers::Offset<QuantizationParameters> CreateQuantizationParameters(
   auto _details_type = _o->details.type;
   auto _details = _o->details.Pack(_fbb);
   auto _quantized_dimension = _o->quantized_dimension;
-  return tflite::CreateQuantizationParameters(
+  return tflite_micro::CreateQuantizationParameters(
       _fbb,
       _min,
       _max,
@@ -16589,7 +16589,7 @@ inline flatbuffers::Offset<Int32Vector> CreateInt32Vector(flatbuffers::FlatBuffe
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const Int32VectorT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _values = _o->values.size() ? _fbb.CreateVector(_o->values) : 0;
-  return tflite::CreateInt32Vector(
+  return tflite_micro::CreateInt32Vector(
       _fbb,
       _values);
 }
@@ -16616,7 +16616,7 @@ inline flatbuffers::Offset<Uint16Vector> CreateUint16Vector(flatbuffers::FlatBuf
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const Uint16VectorT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   _fbb.ForceVectorAlignment(_o->values.size(), sizeof(uint16_t), 4);
   auto _values = _o->values.size() ? _fbb.CreateVector(_o->values) : 0;
-  return tflite::CreateUint16Vector(
+  return tflite_micro::CreateUint16Vector(
       _fbb,
       _values);
 }
@@ -16643,7 +16643,7 @@ inline flatbuffers::Offset<Uint8Vector> CreateUint8Vector(flatbuffers::FlatBuffe
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const Uint8VectorT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   _fbb.ForceVectorAlignment(_o->values.size(), sizeof(uint8_t), 4);
   auto _values = _o->values.size() ? _fbb.CreateVector(_o->values) : 0;
-  return tflite::CreateUint8Vector(
+  return tflite_micro::CreateUint8Vector(
       _fbb,
       _values);
 }
@@ -16660,9 +16660,9 @@ inline void DimensionMetadata::UnPackTo(DimensionMetadataT *_o, const flatbuffer
   { auto _e = format(); _o->format = _e; }
   { auto _e = dense_size(); _o->dense_size = _e; }
   { auto _e = array_segments_type(); _o->array_segments.type = _e; }
-  { auto _e = array_segments(); if (_e) _o->array_segments.value = tflite::SparseIndexVectorUnion::UnPack(_e, array_segments_type(), _resolver); }
+  { auto _e = array_segments(); if (_e) _o->array_segments.value = tflite_micro::SparseIndexVectorUnion::UnPack(_e, array_segments_type(), _resolver); }
   { auto _e = array_indices_type(); _o->array_indices.type = _e; }
-  { auto _e = array_indices(); if (_e) _o->array_indices.value = tflite::SparseIndexVectorUnion::UnPack(_e, array_indices_type(), _resolver); }
+  { auto _e = array_indices(); if (_e) _o->array_indices.value = tflite_micro::SparseIndexVectorUnion::UnPack(_e, array_indices_type(), _resolver); }
 }
 
 inline flatbuffers::Offset<DimensionMetadata> DimensionMetadata::Pack(flatbuffers::FlatBufferBuilder &_fbb, const DimensionMetadataT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
@@ -16679,7 +16679,7 @@ inline flatbuffers::Offset<DimensionMetadata> CreateDimensionMetadata(flatbuffer
   auto _array_segments = _o->array_segments.Pack(_fbb);
   auto _array_indices_type = _o->array_indices.type;
   auto _array_indices = _o->array_indices.Pack(_fbb);
-  return tflite::CreateDimensionMetadata(
+  return tflite_micro::CreateDimensionMetadata(
       _fbb,
       _format,
       _dense_size,
@@ -16693,7 +16693,7 @@ inline SparsityParametersT::SparsityParametersT(const SparsityParametersT &o)
       : traversal_order(o.traversal_order),
         block_map(o.block_map) {
   dim_metadata.reserve(o.dim_metadata.size());
-  for (const auto &dim_metadata_ : o.dim_metadata) { dim_metadata.emplace_back((dim_metadata_) ? new tflite::DimensionMetadataT(*dim_metadata_) : nullptr); }
+  for (const auto &dim_metadata_ : o.dim_metadata) { dim_metadata.emplace_back((dim_metadata_) ? new tflite_micro::DimensionMetadataT(*dim_metadata_) : nullptr); }
 }
 
 inline SparsityParametersT &SparsityParametersT::operator=(SparsityParametersT o) FLATBUFFERS_NOEXCEPT {
@@ -16714,7 +16714,7 @@ inline void SparsityParameters::UnPackTo(SparsityParametersT *_o, const flatbuff
   (void)_resolver;
   { auto _e = traversal_order(); if (_e) { _o->traversal_order.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->traversal_order[_i] = _e->Get(_i); } } }
   { auto _e = block_map(); if (_e) { _o->block_map.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->block_map[_i] = _e->Get(_i); } } }
-  { auto _e = dim_metadata(); if (_e) { _o->dim_metadata.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->dim_metadata[_i]) { _e->Get(_i)->UnPackTo(_o->dim_metadata[_i].get(), _resolver); } else { _o->dim_metadata[_i] = std::unique_ptr<tflite::DimensionMetadataT>(_e->Get(_i)->UnPack(_resolver)); }; } } }
+  { auto _e = dim_metadata(); if (_e) { _o->dim_metadata.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->dim_metadata[_i]) { _e->Get(_i)->UnPackTo(_o->dim_metadata[_i].get(), _resolver); } else { _o->dim_metadata[_i] = std::unique_ptr<tflite_micro::DimensionMetadataT>(_e->Get(_i)->UnPack(_resolver)); }; } } }
 }
 
 inline flatbuffers::Offset<SparsityParameters> SparsityParameters::Pack(flatbuffers::FlatBufferBuilder &_fbb, const SparsityParametersT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
@@ -16727,8 +16727,8 @@ inline flatbuffers::Offset<SparsityParameters> CreateSparsityParameters(flatbuff
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const SparsityParametersT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _traversal_order = _o->traversal_order.size() ? _fbb.CreateVector(_o->traversal_order) : 0;
   auto _block_map = _o->block_map.size() ? _fbb.CreateVector(_o->block_map) : 0;
-  auto _dim_metadata = _o->dim_metadata.size() ? _fbb.CreateVector<flatbuffers::Offset<tflite::DimensionMetadata>> (_o->dim_metadata.size(), [](size_t i, _VectorArgs *__va) { return CreateDimensionMetadata(*__va->__fbb, __va->__o->dim_metadata[i].get(), __va->__rehasher); }, &_va ) : 0;
-  return tflite::CreateSparsityParameters(
+  auto _dim_metadata = _o->dim_metadata.size() ? _fbb.CreateVector<flatbuffers::Offset<tflite_micro::DimensionMetadata>> (_o->dim_metadata.size(), [](size_t i, _VectorArgs *__va) { return CreateDimensionMetadata(*__va->__fbb, __va->__o->dim_metadata[i].get(), __va->__rehasher); }, &_va ) : 0;
+  return tflite_micro::CreateSparsityParameters(
       _fbb,
       _traversal_order,
       _block_map,
@@ -16760,7 +16760,7 @@ inline flatbuffers::Offset<VariantSubType> CreateVariantSubType(flatbuffers::Fla
   auto _shape = _o->shape.size() ? _fbb.CreateVector(_o->shape) : 0;
   auto _type = _o->type;
   auto _has_rank = _o->has_rank;
-  return tflite::CreateVariantSubType(
+  return tflite_micro::CreateVariantSubType(
       _fbb,
       _shape,
       _type,
@@ -16772,13 +16772,13 @@ inline TensorT::TensorT(const TensorT &o)
         type(o.type),
         buffer(o.buffer),
         name(o.name),
-        quantization((o.quantization) ? new tflite::QuantizationParametersT(*o.quantization) : nullptr),
+        quantization((o.quantization) ? new tflite_micro::QuantizationParametersT(*o.quantization) : nullptr),
         is_variable(o.is_variable),
-        sparsity((o.sparsity) ? new tflite::SparsityParametersT(*o.sparsity) : nullptr),
+        sparsity((o.sparsity) ? new tflite_micro::SparsityParametersT(*o.sparsity) : nullptr),
         shape_signature(o.shape_signature),
         has_rank(o.has_rank) {
   variant_tensors.reserve(o.variant_tensors.size());
-  for (const auto &variant_tensors_ : o.variant_tensors) { variant_tensors.emplace_back((variant_tensors_) ? new tflite::VariantSubTypeT(*variant_tensors_) : nullptr); }
+  for (const auto &variant_tensors_ : o.variant_tensors) { variant_tensors.emplace_back((variant_tensors_) ? new tflite_micro::VariantSubTypeT(*variant_tensors_) : nullptr); }
 }
 
 inline TensorT &TensorT::operator=(TensorT o) FLATBUFFERS_NOEXCEPT {
@@ -16808,12 +16808,12 @@ inline void Tensor::UnPackTo(TensorT *_o, const flatbuffers::resolver_function_t
   { auto _e = type(); _o->type = _e; }
   { auto _e = buffer(); _o->buffer = _e; }
   { auto _e = name(); if (_e) _o->name = _e->str(); }
-  { auto _e = quantization(); if (_e) { if(_o->quantization) { _e->UnPackTo(_o->quantization.get(), _resolver); } else { _o->quantization = std::unique_ptr<tflite::QuantizationParametersT>(_e->UnPack(_resolver)); } } }
+  { auto _e = quantization(); if (_e) { if(_o->quantization) { _e->UnPackTo(_o->quantization.get(), _resolver); } else { _o->quantization = std::unique_ptr<tflite_micro::QuantizationParametersT>(_e->UnPack(_resolver)); } } }
   { auto _e = is_variable(); _o->is_variable = _e; }
-  { auto _e = sparsity(); if (_e) { if(_o->sparsity) { _e->UnPackTo(_o->sparsity.get(), _resolver); } else { _o->sparsity = std::unique_ptr<tflite::SparsityParametersT>(_e->UnPack(_resolver)); } } }
+  { auto _e = sparsity(); if (_e) { if(_o->sparsity) { _e->UnPackTo(_o->sparsity.get(), _resolver); } else { _o->sparsity = std::unique_ptr<tflite_micro::SparsityParametersT>(_e->UnPack(_resolver)); } } }
   { auto _e = shape_signature(); if (_e) { _o->shape_signature.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->shape_signature[_i] = _e->Get(_i); } } }
   { auto _e = has_rank(); _o->has_rank = _e; }
-  { auto _e = variant_tensors(); if (_e) { _o->variant_tensors.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->variant_tensors[_i]) { _e->Get(_i)->UnPackTo(_o->variant_tensors[_i].get(), _resolver); } else { _o->variant_tensors[_i] = std::unique_ptr<tflite::VariantSubTypeT>(_e->Get(_i)->UnPack(_resolver)); }; } } }
+  { auto _e = variant_tensors(); if (_e) { _o->variant_tensors.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->variant_tensors[_i]) { _e->Get(_i)->UnPackTo(_o->variant_tensors[_i].get(), _resolver); } else { _o->variant_tensors[_i] = std::unique_ptr<tflite_micro::VariantSubTypeT>(_e->Get(_i)->UnPack(_resolver)); }; } } }
 }
 
 inline flatbuffers::Offset<Tensor> Tensor::Pack(flatbuffers::FlatBufferBuilder &_fbb, const TensorT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
@@ -16833,8 +16833,8 @@ inline flatbuffers::Offset<Tensor> CreateTensor(flatbuffers::FlatBufferBuilder &
   auto _sparsity = _o->sparsity ? CreateSparsityParameters(_fbb, _o->sparsity.get(), _rehasher) : 0;
   auto _shape_signature = _o->shape_signature.size() ? _fbb.CreateVector(_o->shape_signature) : 0;
   auto _has_rank = _o->has_rank;
-  auto _variant_tensors = _o->variant_tensors.size() ? _fbb.CreateVector<flatbuffers::Offset<tflite::VariantSubType>> (_o->variant_tensors.size(), [](size_t i, _VectorArgs *__va) { return CreateVariantSubType(*__va->__fbb, __va->__o->variant_tensors[i].get(), __va->__rehasher); }, &_va ) : 0;
-  return tflite::CreateTensor(
+  auto _variant_tensors = _o->variant_tensors.size() ? _fbb.CreateVector<flatbuffers::Offset<tflite_micro::VariantSubType>> (_o->variant_tensors.size(), [](size_t i, _VectorArgs *__va) { return CreateVariantSubType(*__va->__fbb, __va->__o->variant_tensors[i].get(), __va->__rehasher); }, &_va ) : 0;
+  return tflite_micro::CreateTensor(
       _fbb,
       _shape,
       _type,
@@ -16879,7 +16879,7 @@ inline flatbuffers::Offset<StablehloGatherOptions> CreateStablehloGatherOptions(
   auto _index_vector_dim = _o->index_vector_dim;
   auto _slice_sizes = _o->slice_sizes.size() ? _fbb.CreateVector(_o->slice_sizes) : 0;
   auto _indices_are_sorted = _o->indices_are_sorted;
-  return tflite::CreateStablehloGatherOptions(
+  return tflite_micro::CreateStablehloGatherOptions(
       _fbb,
       _offset_dims,
       _collapsed_slice_dims,
@@ -16910,7 +16910,7 @@ inline flatbuffers::Offset<StablehloTransposeOptions> CreateStablehloTransposeOp
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const StablehloTransposeOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _permutation = _o->permutation.size() ? _fbb.CreateVector(_o->permutation) : 0;
-  return tflite::CreateStablehloTransposeOptions(
+  return tflite_micro::CreateStablehloTransposeOptions(
       _fbb,
       _permutation);
 }
@@ -16928,7 +16928,7 @@ inline void StablehloDotGeneralOptions::UnPackTo(StablehloDotGeneralOptionsT *_o
   { auto _e = rhs_batching_dimensions(); if (_e) { _o->rhs_batching_dimensions.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->rhs_batching_dimensions[_i] = _e->Get(_i); } } }
   { auto _e = lhs_contracting_dimensions(); if (_e) { _o->lhs_contracting_dimensions.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->lhs_contracting_dimensions[_i] = _e->Get(_i); } } }
   { auto _e = rhs_contracting_dimensions(); if (_e) { _o->rhs_contracting_dimensions.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->rhs_contracting_dimensions[_i] = _e->Get(_i); } } }
-  { auto _e = precision_config(); if (_e) { _o->precision_config.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->precision_config[_i] = static_cast<tflite::StablehloPrecisionConfig>(_e->Get(_i)); } } }
+  { auto _e = precision_config(); if (_e) { _o->precision_config.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->precision_config[_i] = static_cast<tflite_micro::StablehloPrecisionConfig>(_e->Get(_i)); } } }
 }
 
 inline flatbuffers::Offset<StablehloDotGeneralOptions> StablehloDotGeneralOptions::Pack(flatbuffers::FlatBufferBuilder &_fbb, const StablehloDotGeneralOptionsT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
@@ -16944,7 +16944,7 @@ inline flatbuffers::Offset<StablehloDotGeneralOptions> CreateStablehloDotGeneral
   auto _lhs_contracting_dimensions = _o->lhs_contracting_dimensions.size() ? _fbb.CreateVector(_o->lhs_contracting_dimensions) : 0;
   auto _rhs_contracting_dimensions = _o->rhs_contracting_dimensions.size() ? _fbb.CreateVector(_o->rhs_contracting_dimensions) : 0;
   auto _precision_config = _o->precision_config.size() ? _fbb.CreateVectorScalarCast<uint32_t>(flatbuffers::data(_o->precision_config), _o->precision_config.size()) : 0;
-  return tflite::CreateStablehloDotGeneralOptions(
+  return tflite_micro::CreateStablehloDotGeneralOptions(
       _fbb,
       _lhs_batching_dimensions,
       _rhs_batching_dimensions,
@@ -16984,7 +16984,7 @@ inline flatbuffers::Offset<StablehloReduceWindowOptions> CreateStablehloReduceWi
   auto _window_dilations = _o->window_dilations.size() ? _fbb.CreateVector(_o->window_dilations) : 0;
   auto _padding = _o->padding.size() ? _fbb.CreateVector(_o->padding) : 0;
   auto _body_subgraph_index = _o->body_subgraph_index;
-  return tflite::CreateStablehloReduceWindowOptions(
+  return tflite_micro::CreateStablehloReduceWindowOptions(
       _fbb,
       _window_dimensions,
       _window_strides,
@@ -17017,7 +17017,7 @@ inline flatbuffers::Offset<StablehloWhileOptions> CreateStablehloWhileOptions(fl
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const StablehloWhileOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _cond_subgraph_index = _o->cond_subgraph_index;
   auto _body_subgraph_index = _o->body_subgraph_index;
-  return tflite::CreateStablehloWhileOptions(
+  return tflite_micro::CreateStablehloWhileOptions(
       _fbb,
       _cond_subgraph_index,
       _body_subgraph_index);
@@ -17048,7 +17048,7 @@ inline flatbuffers::Offset<StablehloSortOptions> CreateStablehloSortOptions(flat
   auto _dimension = _o->dimension;
   auto _is_stable = _o->is_stable;
   auto _comparator_subgraph_index = _o->comparator_subgraph_index;
-  return tflite::CreateStablehloSortOptions(
+  return tflite_micro::CreateStablehloSortOptions(
       _fbb,
       _dimension,
       _is_stable,
@@ -17076,7 +17076,7 @@ inline flatbuffers::Offset<StablehloConcatenateOptions> CreateStablehloConcatena
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const StablehloConcatenateOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _dimension = _o->dimension;
-  return tflite::CreateStablehloConcatenateOptions(
+  return tflite_micro::CreateStablehloConcatenateOptions(
       _fbb,
       _dimension);
 }
@@ -17102,7 +17102,7 @@ inline flatbuffers::Offset<StablehloBroadcastInDimOptions> CreateStablehloBroadc
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const StablehloBroadcastInDimOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _broadcast_dimensions = _o->broadcast_dimensions.size() ? _fbb.CreateVector(_o->broadcast_dimensions) : 0;
-  return tflite::CreateStablehloBroadcastInDimOptions(
+  return tflite_micro::CreateStablehloBroadcastInDimOptions(
       _fbb,
       _broadcast_dimensions);
 }
@@ -17130,7 +17130,7 @@ inline flatbuffers::Offset<StablehloCompareOptions> CreateStablehloCompareOption
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const StablehloCompareOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _comparison_direction = _o->comparison_direction;
   auto _compare_type = _o->compare_type;
-  return tflite::CreateStablehloCompareOptions(
+  return tflite_micro::CreateStablehloCompareOptions(
       _fbb,
       _comparison_direction,
       _compare_type);
@@ -17157,7 +17157,7 @@ inline flatbuffers::Offset<StablehloDynamicSliceOptions> CreateStablehloDynamicS
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const StablehloDynamicSliceOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _slice_sizes = _o->slice_sizes.size() ? _fbb.CreateVector(_o->slice_sizes) : 0;
-  return tflite::CreateStablehloDynamicSliceOptions(
+  return tflite_micro::CreateStablehloDynamicSliceOptions(
       _fbb,
       _slice_sizes);
 }
@@ -17187,7 +17187,7 @@ inline flatbuffers::Offset<StablehloPadOptions> CreateStablehloPadOptions(flatbu
   auto _edge_padding_low = _o->edge_padding_low.size() ? _fbb.CreateVector(_o->edge_padding_low) : 0;
   auto _edge_padding_high = _o->edge_padding_high.size() ? _fbb.CreateVector(_o->edge_padding_high) : 0;
   auto _interior_padding = _o->interior_padding.size() ? _fbb.CreateVector(_o->interior_padding) : 0;
-  return tflite::CreateStablehloPadOptions(
+  return tflite_micro::CreateStablehloPadOptions(
       _fbb,
       _edge_padding_low,
       _edge_padding_high,
@@ -17215,7 +17215,7 @@ inline flatbuffers::Offset<StablehloIotaOptions> CreateStablehloIotaOptions(flat
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const StablehloIotaOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _iota_dimension = _o->iota_dimension;
-  return tflite::CreateStablehloIotaOptions(
+  return tflite_micro::CreateStablehloIotaOptions(
       _fbb,
       _iota_dimension);
 }
@@ -17251,7 +17251,7 @@ inline flatbuffers::Offset<StablehloCustomCallOptions> CreateStablehloCustomCall
   auto _api_version = _o->api_version;
   auto _called_computations = _o->called_computations.size() ? _fbb.CreateVector(_o->called_computations) : 0;
   auto _custom_attributes = _o->custom_attributes.size() ? _fbb.CreateVector(_o->custom_attributes) : 0;
-  return tflite::CreateStablehloCustomCallOptions(
+  return tflite_micro::CreateStablehloCustomCallOptions(
       _fbb,
       _call_target_name,
       _has_side_effect,
@@ -17284,7 +17284,7 @@ inline flatbuffers::Offset<StablehloReduceOptions> CreateStablehloReduceOptions(
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const StablehloReduceOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _dimensions = _o->dimensions.size() ? _fbb.CreateVector(_o->dimensions) : 0;
   auto _body_subgraph_index = _o->body_subgraph_index;
-  return tflite::CreateStablehloReduceOptions(
+  return tflite_micro::CreateStablehloReduceOptions(
       _fbb,
       _dimensions,
       _body_subgraph_index);
@@ -17315,7 +17315,7 @@ inline flatbuffers::Offset<StablehloSliceOptions> CreateStablehloSliceOptions(fl
   auto _start_indices = _o->start_indices.size() ? _fbb.CreateVector(_o->start_indices) : 0;
   auto _limit_indices = _o->limit_indices.size() ? _fbb.CreateVector(_o->limit_indices) : 0;
   auto _strides = _o->strides.size() ? _fbb.CreateVector(_o->strides) : 0;
-  return tflite::CreateStablehloSliceOptions(
+  return tflite_micro::CreateStablehloSliceOptions(
       _fbb,
       _start_indices,
       _limit_indices,
@@ -17347,7 +17347,7 @@ inline void StablehloConvolutionOptions::UnPackTo(StablehloConvolutionOptionsT *
   { auto _e = output_spatial_dimensions(); if (_e) { _o->output_spatial_dimensions.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->output_spatial_dimensions[_i] = _e->Get(_i); } } }
   { auto _e = feature_group_count(); _o->feature_group_count = _e; }
   { auto _e = batch_group_count(); _o->batch_group_count = _e; }
-  { auto _e = precision_config(); if (_e) { _o->precision_config.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->precision_config[_i] = static_cast<tflite::StablehloPrecisionConfig>(_e->Get(_i)); } } }
+  { auto _e = precision_config(); if (_e) { _o->precision_config.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->precision_config[_i] = static_cast<tflite_micro::StablehloPrecisionConfig>(_e->Get(_i)); } } }
 }
 
 inline flatbuffers::Offset<StablehloConvolutionOptions> StablehloConvolutionOptions::Pack(flatbuffers::FlatBufferBuilder &_fbb, const StablehloConvolutionOptionsT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
@@ -17375,7 +17375,7 @@ inline flatbuffers::Offset<StablehloConvolutionOptions> CreateStablehloConvoluti
   auto _feature_group_count = _o->feature_group_count;
   auto _batch_group_count = _o->batch_group_count;
   auto _precision_config = _o->precision_config.size() ? _fbb.CreateVectorScalarCast<uint32_t>(flatbuffers::data(_o->precision_config), _o->precision_config.size()) : 0;
-  return tflite::CreateStablehloConvolutionOptions(
+  return tflite_micro::CreateStablehloConvolutionOptions(
       _fbb,
       _window_strides,
       _padding,
@@ -17429,7 +17429,7 @@ inline flatbuffers::Offset<StablehloScatterOptions> CreateStablehloScatterOption
   auto _index_vector_dim = _o->index_vector_dim;
   auto _unique_indices = _o->unique_indices;
   auto _update_computation_subgraph_index = _o->update_computation_subgraph_index;
-  return tflite::CreateStablehloScatterOptions(
+  return tflite_micro::CreateStablehloScatterOptions(
       _fbb,
       _indices_are_sorted,
       _update_window_dims,
@@ -17461,7 +17461,7 @@ inline flatbuffers::Offset<StablehloRngBitGeneratorOptions> CreateStablehloRngBi
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const StablehloRngBitGeneratorOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _algorithm = _o->algorithm;
-  return tflite::CreateStablehloRngBitGeneratorOptions(
+  return tflite_micro::CreateStablehloRngBitGeneratorOptions(
       _fbb,
       _algorithm);
 }
@@ -17499,7 +17499,7 @@ inline flatbuffers::Offset<Conv2DOptions> CreateConv2DOptions(flatbuffers::FlatB
   auto _dilation_w_factor = _o->dilation_w_factor;
   auto _dilation_h_factor = _o->dilation_h_factor;
   auto _quantized_bias_type = _o->quantized_bias_type;
-  return tflite::CreateConv2DOptions(
+  return tflite_micro::CreateConv2DOptions(
       _fbb,
       _padding,
       _stride_w,
@@ -17545,7 +17545,7 @@ inline flatbuffers::Offset<Conv3DOptions> CreateConv3DOptions(flatbuffers::FlatB
   auto _dilation_d_factor = _o->dilation_d_factor;
   auto _dilation_w_factor = _o->dilation_w_factor;
   auto _dilation_h_factor = _o->dilation_h_factor;
-  return tflite::CreateConv3DOptions(
+  return tflite_micro::CreateConv3DOptions(
       _fbb,
       _padding,
       _stride_d,
@@ -17588,7 +17588,7 @@ inline flatbuffers::Offset<Pool2DOptions> CreatePool2DOptions(flatbuffers::FlatB
   auto _filter_width = _o->filter_width;
   auto _filter_height = _o->filter_height;
   auto _fused_activation_function = _o->fused_activation_function;
-  return tflite::CreatePool2DOptions(
+  return tflite_micro::CreatePool2DOptions(
       _fbb,
       _padding,
       _stride_w,
@@ -17631,7 +17631,7 @@ inline flatbuffers::Offset<DepthwiseConv2DOptions> CreateDepthwiseConv2DOptions(
   auto _fused_activation_function = _o->fused_activation_function;
   auto _dilation_w_factor = _o->dilation_w_factor;
   auto _dilation_h_factor = _o->dilation_h_factor;
-  return tflite::CreateDepthwiseConv2DOptions(
+  return tflite_micro::CreateDepthwiseConv2DOptions(
       _fbb,
       _padding,
       _stride_w,
@@ -17667,7 +17667,7 @@ inline flatbuffers::Offset<ConcatEmbeddingsOptions> CreateConcatEmbeddingsOption
   auto _num_channels = _o->num_channels;
   auto _num_columns_per_channel = _o->num_columns_per_channel.size() ? _fbb.CreateVector(_o->num_columns_per_channel) : 0;
   auto _embedding_dim_per_channel = _o->embedding_dim_per_channel.size() ? _fbb.CreateVector(_o->embedding_dim_per_channel) : 0;
-  return tflite::CreateConcatEmbeddingsOptions(
+  return tflite_micro::CreateConcatEmbeddingsOptions(
       _fbb,
       _num_channels,
       _num_columns_per_channel,
@@ -17695,7 +17695,7 @@ inline flatbuffers::Offset<LSHProjectionOptions> CreateLSHProjectionOptions(flat
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const LSHProjectionOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _type = _o->type;
-  return tflite::CreateLSHProjectionOptions(
+  return tflite_micro::CreateLSHProjectionOptions(
       _fbb,
       _type);
 }
@@ -17725,7 +17725,7 @@ inline flatbuffers::Offset<SVDFOptions> CreateSVDFOptions(flatbuffers::FlatBuffe
   auto _rank = _o->rank;
   auto _fused_activation_function = _o->fused_activation_function;
   auto _asymmetric_quantize_inputs = _o->asymmetric_quantize_inputs;
-  return tflite::CreateSVDFOptions(
+  return tflite_micro::CreateSVDFOptions(
       _fbb,
       _rank,
       _fused_activation_function,
@@ -17755,7 +17755,7 @@ inline flatbuffers::Offset<RNNOptions> CreateRNNOptions(flatbuffers::FlatBufferB
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const RNNOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _fused_activation_function = _o->fused_activation_function;
   auto _asymmetric_quantize_inputs = _o->asymmetric_quantize_inputs;
-  return tflite::CreateRNNOptions(
+  return tflite_micro::CreateRNNOptions(
       _fbb,
       _fused_activation_function,
       _asymmetric_quantize_inputs);
@@ -17786,7 +17786,7 @@ inline flatbuffers::Offset<SequenceRNNOptions> CreateSequenceRNNOptions(flatbuff
   auto _time_major = _o->time_major;
   auto _fused_activation_function = _o->fused_activation_function;
   auto _asymmetric_quantize_inputs = _o->asymmetric_quantize_inputs;
-  return tflite::CreateSequenceRNNOptions(
+  return tflite_micro::CreateSequenceRNNOptions(
       _fbb,
       _time_major,
       _fused_activation_function,
@@ -17820,7 +17820,7 @@ inline flatbuffers::Offset<BidirectionalSequenceRNNOptions> CreateBidirectionalS
   auto _fused_activation_function = _o->fused_activation_function;
   auto _merge_outputs = _o->merge_outputs;
   auto _asymmetric_quantize_inputs = _o->asymmetric_quantize_inputs;
-  return tflite::CreateBidirectionalSequenceRNNOptions(
+  return tflite_micro::CreateBidirectionalSequenceRNNOptions(
       _fbb,
       _time_major,
       _fused_activation_function,
@@ -17857,7 +17857,7 @@ inline flatbuffers::Offset<FullyConnectedOptions> CreateFullyConnectedOptions(fl
   auto _keep_num_dims = _o->keep_num_dims;
   auto _asymmetric_quantize_inputs = _o->asymmetric_quantize_inputs;
   auto _quantized_bias_type = _o->quantized_bias_type;
-  return tflite::CreateFullyConnectedOptions(
+  return tflite_micro::CreateFullyConnectedOptions(
       _fbb,
       _fused_activation_function,
       _weights_format,
@@ -17887,7 +17887,7 @@ inline flatbuffers::Offset<SoftmaxOptions> CreateSoftmaxOptions(flatbuffers::Fla
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const SoftmaxOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _beta = _o->beta;
-  return tflite::CreateSoftmaxOptions(
+  return tflite_micro::CreateSoftmaxOptions(
       _fbb,
       _beta);
 }
@@ -17915,7 +17915,7 @@ inline flatbuffers::Offset<ConcatenationOptions> CreateConcatenationOptions(flat
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const ConcatenationOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _axis = _o->axis;
   auto _fused_activation_function = _o->fused_activation_function;
-  return tflite::CreateConcatenationOptions(
+  return tflite_micro::CreateConcatenationOptions(
       _fbb,
       _axis,
       _fused_activation_function);
@@ -17944,7 +17944,7 @@ inline flatbuffers::Offset<AddOptions> CreateAddOptions(flatbuffers::FlatBufferB
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const AddOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _fused_activation_function = _o->fused_activation_function;
   auto _pot_scale_int16 = _o->pot_scale_int16;
-  return tflite::CreateAddOptions(
+  return tflite_micro::CreateAddOptions(
       _fbb,
       _fused_activation_function,
       _pot_scale_int16);
@@ -17971,7 +17971,7 @@ inline flatbuffers::Offset<MulOptions> CreateMulOptions(flatbuffers::FlatBufferB
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const MulOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _fused_activation_function = _o->fused_activation_function;
-  return tflite::CreateMulOptions(
+  return tflite_micro::CreateMulOptions(
       _fbb,
       _fused_activation_function);
 }
@@ -17997,7 +17997,7 @@ inline flatbuffers::Offset<L2NormOptions> CreateL2NormOptions(flatbuffers::FlatB
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const L2NormOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _fused_activation_function = _o->fused_activation_function;
-  return tflite::CreateL2NormOptions(
+  return tflite_micro::CreateL2NormOptions(
       _fbb,
       _fused_activation_function);
 }
@@ -18029,7 +18029,7 @@ inline flatbuffers::Offset<LocalResponseNormalizationOptions> CreateLocalRespons
   auto _bias = _o->bias;
   auto _alpha = _o->alpha;
   auto _beta = _o->beta;
-  return tflite::CreateLocalResponseNormalizationOptions(
+  return tflite_micro::CreateLocalResponseNormalizationOptions(
       _fbb,
       _radius,
       _bias,
@@ -18066,7 +18066,7 @@ inline flatbuffers::Offset<LSTMOptions> CreateLSTMOptions(flatbuffers::FlatBuffe
   auto _proj_clip = _o->proj_clip;
   auto _kernel_type = _o->kernel_type;
   auto _asymmetric_quantize_inputs = _o->asymmetric_quantize_inputs;
-  return tflite::CreateLSTMOptions(
+  return tflite_micro::CreateLSTMOptions(
       _fbb,
       _fused_activation_function,
       _cell_clip,
@@ -18106,7 +18106,7 @@ inline flatbuffers::Offset<UnidirectionalSequenceLSTMOptions> CreateUnidirection
   auto _time_major = _o->time_major;
   auto _asymmetric_quantize_inputs = _o->asymmetric_quantize_inputs;
   auto _diagonal_recurrent_tensors = _o->diagonal_recurrent_tensors;
-  return tflite::CreateUnidirectionalSequenceLSTMOptions(
+  return tflite_micro::CreateUnidirectionalSequenceLSTMOptions(
       _fbb,
       _fused_activation_function,
       _cell_clip,
@@ -18147,7 +18147,7 @@ inline flatbuffers::Offset<BidirectionalSequenceLSTMOptions> CreateBidirectional
   auto _merge_outputs = _o->merge_outputs;
   auto _time_major = _o->time_major;
   auto _asymmetric_quantize_inputs = _o->asymmetric_quantize_inputs;
-  return tflite::CreateBidirectionalSequenceLSTMOptions(
+  return tflite_micro::CreateBidirectionalSequenceLSTMOptions(
       _fbb,
       _fused_activation_function,
       _cell_clip,
@@ -18180,7 +18180,7 @@ inline flatbuffers::Offset<ResizeBilinearOptions> CreateResizeBilinearOptions(fl
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const ResizeBilinearOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _align_corners = _o->align_corners;
   auto _half_pixel_centers = _o->half_pixel_centers;
-  return tflite::CreateResizeBilinearOptions(
+  return tflite_micro::CreateResizeBilinearOptions(
       _fbb,
       _align_corners,
       _half_pixel_centers);
@@ -18209,7 +18209,7 @@ inline flatbuffers::Offset<ResizeNearestNeighborOptions> CreateResizeNearestNeig
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const ResizeNearestNeighborOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _align_corners = _o->align_corners;
   auto _half_pixel_centers = _o->half_pixel_centers;
-  return tflite::CreateResizeNearestNeighborOptions(
+  return tflite_micro::CreateResizeNearestNeighborOptions(
       _fbb,
       _align_corners,
       _half_pixel_centers);
@@ -18236,7 +18236,7 @@ inline flatbuffers::Offset<CallOptions> CreateCallOptions(flatbuffers::FlatBuffe
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const CallOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _subgraph = _o->subgraph;
-  return tflite::CreateCallOptions(
+  return tflite_micro::CreateCallOptions(
       _fbb,
       _subgraph);
 }
@@ -18260,7 +18260,7 @@ inline flatbuffers::Offset<PadOptions> CreatePadOptions(flatbuffers::FlatBufferB
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const PadOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreatePadOptions(
+  return tflite_micro::CreatePadOptions(
       _fbb);
 }
 
@@ -18283,7 +18283,7 @@ inline flatbuffers::Offset<PadV2Options> CreatePadV2Options(flatbuffers::FlatBuf
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const PadV2OptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreatePadV2Options(
+  return tflite_micro::CreatePadV2Options(
       _fbb);
 }
 
@@ -18308,7 +18308,7 @@ inline flatbuffers::Offset<ReshapeOptions> CreateReshapeOptions(flatbuffers::Fla
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const ReshapeOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _new_shape = _o->new_shape.size() ? _fbb.CreateVector(_o->new_shape) : 0;
-  return tflite::CreateReshapeOptions(
+  return tflite_micro::CreateReshapeOptions(
       _fbb,
       _new_shape);
 }
@@ -18332,7 +18332,7 @@ inline flatbuffers::Offset<SpaceToBatchNDOptions> CreateSpaceToBatchNDOptions(fl
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const SpaceToBatchNDOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateSpaceToBatchNDOptions(
+  return tflite_micro::CreateSpaceToBatchNDOptions(
       _fbb);
 }
 
@@ -18355,7 +18355,7 @@ inline flatbuffers::Offset<BatchToSpaceNDOptions> CreateBatchToSpaceNDOptions(fl
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const BatchToSpaceNDOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateBatchToSpaceNDOptions(
+  return tflite_micro::CreateBatchToSpaceNDOptions(
       _fbb);
 }
 
@@ -18384,7 +18384,7 @@ inline flatbuffers::Offset<SkipGramOptions> CreateSkipGramOptions(flatbuffers::F
   auto _ngram_size = _o->ngram_size;
   auto _max_skip_size = _o->max_skip_size;
   auto _include_all_ngrams = _o->include_all_ngrams;
-  return tflite::CreateSkipGramOptions(
+  return tflite_micro::CreateSkipGramOptions(
       _fbb,
       _ngram_size,
       _max_skip_size,
@@ -18412,7 +18412,7 @@ inline flatbuffers::Offset<SpaceToDepthOptions> CreateSpaceToDepthOptions(flatbu
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const SpaceToDepthOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _block_size = _o->block_size;
-  return tflite::CreateSpaceToDepthOptions(
+  return tflite_micro::CreateSpaceToDepthOptions(
       _fbb,
       _block_size);
 }
@@ -18438,7 +18438,7 @@ inline flatbuffers::Offset<DepthToSpaceOptions> CreateDepthToSpaceOptions(flatbu
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const DepthToSpaceOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _block_size = _o->block_size;
-  return tflite::CreateDepthToSpaceOptions(
+  return tflite_micro::CreateDepthToSpaceOptions(
       _fbb,
       _block_size);
 }
@@ -18466,7 +18466,7 @@ inline flatbuffers::Offset<SubOptions> CreateSubOptions(flatbuffers::FlatBufferB
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const SubOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _fused_activation_function = _o->fused_activation_function;
   auto _pot_scale_int16 = _o->pot_scale_int16;
-  return tflite::CreateSubOptions(
+  return tflite_micro::CreateSubOptions(
       _fbb,
       _fused_activation_function,
       _pot_scale_int16);
@@ -18493,7 +18493,7 @@ inline flatbuffers::Offset<DivOptions> CreateDivOptions(flatbuffers::FlatBufferB
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const DivOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _fused_activation_function = _o->fused_activation_function;
-  return tflite::CreateDivOptions(
+  return tflite_micro::CreateDivOptions(
       _fbb,
       _fused_activation_function);
 }
@@ -18517,7 +18517,7 @@ inline flatbuffers::Offset<TopKV2Options> CreateTopKV2Options(flatbuffers::FlatB
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const TopKV2OptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateTopKV2Options(
+  return tflite_micro::CreateTopKV2Options(
       _fbb);
 }
 
@@ -18542,7 +18542,7 @@ inline flatbuffers::Offset<EmbeddingLookupSparseOptions> CreateEmbeddingLookupSp
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const EmbeddingLookupSparseOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _combiner = _o->combiner;
-  return tflite::CreateEmbeddingLookupSparseOptions(
+  return tflite_micro::CreateEmbeddingLookupSparseOptions(
       _fbb,
       _combiner);
 }
@@ -18570,7 +18570,7 @@ inline flatbuffers::Offset<GatherOptions> CreateGatherOptions(flatbuffers::FlatB
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const GatherOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _axis = _o->axis;
   auto _batch_dims = _o->batch_dims;
-  return tflite::CreateGatherOptions(
+  return tflite_micro::CreateGatherOptions(
       _fbb,
       _axis,
       _batch_dims);
@@ -18595,7 +18595,7 @@ inline flatbuffers::Offset<TransposeOptions> CreateTransposeOptions(flatbuffers:
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const TransposeOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateTransposeOptions(
+  return tflite_micro::CreateTransposeOptions(
       _fbb);
 }
 
@@ -18618,7 +18618,7 @@ inline flatbuffers::Offset<ExpOptions> CreateExpOptions(flatbuffers::FlatBufferB
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const ExpOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateExpOptions(
+  return tflite_micro::CreateExpOptions(
       _fbb);
 }
 
@@ -18641,7 +18641,7 @@ inline flatbuffers::Offset<CosOptions> CreateCosOptions(flatbuffers::FlatBufferB
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const CosOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateCosOptions(
+  return tflite_micro::CreateCosOptions(
       _fbb);
 }
 
@@ -18666,7 +18666,7 @@ inline flatbuffers::Offset<ReducerOptions> CreateReducerOptions(flatbuffers::Fla
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const ReducerOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _keep_dims = _o->keep_dims;
-  return tflite::CreateReducerOptions(
+  return tflite_micro::CreateReducerOptions(
       _fbb,
       _keep_dims);
 }
@@ -18692,7 +18692,7 @@ inline flatbuffers::Offset<SqueezeOptions> CreateSqueezeOptions(flatbuffers::Fla
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const SqueezeOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _squeeze_dims = _o->squeeze_dims.size() ? _fbb.CreateVector(_o->squeeze_dims) : 0;
-  return tflite::CreateSqueezeOptions(
+  return tflite_micro::CreateSqueezeOptions(
       _fbb,
       _squeeze_dims);
 }
@@ -18718,7 +18718,7 @@ inline flatbuffers::Offset<SplitOptions> CreateSplitOptions(flatbuffers::FlatBuf
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const SplitOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _num_splits = _o->num_splits;
-  return tflite::CreateSplitOptions(
+  return tflite_micro::CreateSplitOptions(
       _fbb,
       _num_splits);
 }
@@ -18744,7 +18744,7 @@ inline flatbuffers::Offset<SplitVOptions> CreateSplitVOptions(flatbuffers::FlatB
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const SplitVOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _num_splits = _o->num_splits;
-  return tflite::CreateSplitVOptions(
+  return tflite_micro::CreateSplitVOptions(
       _fbb,
       _num_splits);
 }
@@ -18780,7 +18780,7 @@ inline flatbuffers::Offset<StridedSliceOptions> CreateStridedSliceOptions(flatbu
   auto _new_axis_mask = _o->new_axis_mask;
   auto _shrink_axis_mask = _o->shrink_axis_mask;
   auto _offset = _o->offset;
-  return tflite::CreateStridedSliceOptions(
+  return tflite_micro::CreateStridedSliceOptions(
       _fbb,
       _begin_mask,
       _end_mask,
@@ -18809,7 +18809,7 @@ inline flatbuffers::Offset<LogSoftmaxOptions> CreateLogSoftmaxOptions(flatbuffer
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const LogSoftmaxOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateLogSoftmaxOptions(
+  return tflite_micro::CreateLogSoftmaxOptions(
       _fbb);
 }
 
@@ -18836,7 +18836,7 @@ inline flatbuffers::Offset<CastOptions> CreateCastOptions(flatbuffers::FlatBuffe
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const CastOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _in_data_type = _o->in_data_type;
   auto _out_data_type = _o->out_data_type;
-  return tflite::CreateCastOptions(
+  return tflite_micro::CreateCastOptions(
       _fbb,
       _in_data_type,
       _out_data_type);
@@ -18861,7 +18861,7 @@ inline flatbuffers::Offset<DequantizeOptions> CreateDequantizeOptions(flatbuffer
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const DequantizeOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateDequantizeOptions(
+  return tflite_micro::CreateDequantizeOptions(
       _fbb);
 }
 
@@ -18884,7 +18884,7 @@ inline flatbuffers::Offset<MaximumMinimumOptions> CreateMaximumMinimumOptions(fl
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const MaximumMinimumOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateMaximumMinimumOptions(
+  return tflite_micro::CreateMaximumMinimumOptions(
       _fbb);
 }
 
@@ -18907,7 +18907,7 @@ inline flatbuffers::Offset<TileOptions> CreateTileOptions(flatbuffers::FlatBuffe
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const TileOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateTileOptions(
+  return tflite_micro::CreateTileOptions(
       _fbb);
 }
 
@@ -18932,7 +18932,7 @@ inline flatbuffers::Offset<ArgMaxOptions> CreateArgMaxOptions(flatbuffers::FlatB
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const ArgMaxOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _output_type = _o->output_type;
-  return tflite::CreateArgMaxOptions(
+  return tflite_micro::CreateArgMaxOptions(
       _fbb,
       _output_type);
 }
@@ -18958,7 +18958,7 @@ inline flatbuffers::Offset<ArgMinOptions> CreateArgMinOptions(flatbuffers::FlatB
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const ArgMinOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _output_type = _o->output_type;
-  return tflite::CreateArgMinOptions(
+  return tflite_micro::CreateArgMinOptions(
       _fbb,
       _output_type);
 }
@@ -18982,7 +18982,7 @@ inline flatbuffers::Offset<GreaterOptions> CreateGreaterOptions(flatbuffers::Fla
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const GreaterOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateGreaterOptions(
+  return tflite_micro::CreateGreaterOptions(
       _fbb);
 }
 
@@ -19005,7 +19005,7 @@ inline flatbuffers::Offset<GreaterEqualOptions> CreateGreaterEqualOptions(flatbu
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const GreaterEqualOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateGreaterEqualOptions(
+  return tflite_micro::CreateGreaterEqualOptions(
       _fbb);
 }
 
@@ -19028,7 +19028,7 @@ inline flatbuffers::Offset<LessOptions> CreateLessOptions(flatbuffers::FlatBuffe
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const LessOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateLessOptions(
+  return tflite_micro::CreateLessOptions(
       _fbb);
 }
 
@@ -19051,7 +19051,7 @@ inline flatbuffers::Offset<LessEqualOptions> CreateLessEqualOptions(flatbuffers:
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const LessEqualOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateLessEqualOptions(
+  return tflite_micro::CreateLessEqualOptions(
       _fbb);
 }
 
@@ -19074,7 +19074,7 @@ inline flatbuffers::Offset<NegOptions> CreateNegOptions(flatbuffers::FlatBufferB
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const NegOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateNegOptions(
+  return tflite_micro::CreateNegOptions(
       _fbb);
 }
 
@@ -19097,7 +19097,7 @@ inline flatbuffers::Offset<SelectOptions> CreateSelectOptions(flatbuffers::FlatB
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const SelectOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateSelectOptions(
+  return tflite_micro::CreateSelectOptions(
       _fbb);
 }
 
@@ -19120,7 +19120,7 @@ inline flatbuffers::Offset<SliceOptions> CreateSliceOptions(flatbuffers::FlatBuf
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const SliceOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateSliceOptions(
+  return tflite_micro::CreateSliceOptions(
       _fbb);
 }
 
@@ -19153,7 +19153,7 @@ inline flatbuffers::Offset<TransposeConvOptions> CreateTransposeConvOptions(flat
   auto _stride_h = _o->stride_h;
   auto _fused_activation_function = _o->fused_activation_function;
   auto _quantized_bias_type = _o->quantized_bias_type;
-  return tflite::CreateTransposeConvOptions(
+  return tflite_micro::CreateTransposeConvOptions(
       _fbb,
       _padding,
       _stride_w,
@@ -19181,7 +19181,7 @@ inline flatbuffers::Offset<ExpandDimsOptions> CreateExpandDimsOptions(flatbuffer
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const ExpandDimsOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateExpandDimsOptions(
+  return tflite_micro::CreateExpandDimsOptions(
       _fbb);
 }
 
@@ -19206,7 +19206,7 @@ inline flatbuffers::Offset<SparseToDenseOptions> CreateSparseToDenseOptions(flat
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const SparseToDenseOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _validate_indices = _o->validate_indices;
-  return tflite::CreateSparseToDenseOptions(
+  return tflite_micro::CreateSparseToDenseOptions(
       _fbb,
       _validate_indices);
 }
@@ -19230,7 +19230,7 @@ inline flatbuffers::Offset<EqualOptions> CreateEqualOptions(flatbuffers::FlatBuf
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const EqualOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateEqualOptions(
+  return tflite_micro::CreateEqualOptions(
       _fbb);
 }
 
@@ -19253,7 +19253,7 @@ inline flatbuffers::Offset<NotEqualOptions> CreateNotEqualOptions(flatbuffers::F
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const NotEqualOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateNotEqualOptions(
+  return tflite_micro::CreateNotEqualOptions(
       _fbb);
 }
 
@@ -19278,7 +19278,7 @@ inline flatbuffers::Offset<ShapeOptions> CreateShapeOptions(flatbuffers::FlatBuf
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const ShapeOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _out_type = _o->out_type;
-  return tflite::CreateShapeOptions(
+  return tflite_micro::CreateShapeOptions(
       _fbb,
       _out_type);
 }
@@ -19302,7 +19302,7 @@ inline flatbuffers::Offset<RankOptions> CreateRankOptions(flatbuffers::FlatBuffe
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const RankOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateRankOptions(
+  return tflite_micro::CreateRankOptions(
       _fbb);
 }
 
@@ -19325,7 +19325,7 @@ inline flatbuffers::Offset<PowOptions> CreatePowOptions(flatbuffers::FlatBufferB
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const PowOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreatePowOptions(
+  return tflite_micro::CreatePowOptions(
       _fbb);
 }
 
@@ -19356,7 +19356,7 @@ inline flatbuffers::Offset<FakeQuantOptions> CreateFakeQuantOptions(flatbuffers:
   auto _max = _o->max;
   auto _num_bits = _o->num_bits;
   auto _narrow_range = _o->narrow_range;
-  return tflite::CreateFakeQuantOptions(
+  return tflite_micro::CreateFakeQuantOptions(
       _fbb,
       _min,
       _max,
@@ -19387,7 +19387,7 @@ inline flatbuffers::Offset<PackOptions> CreatePackOptions(flatbuffers::FlatBuffe
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const PackOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _values_count = _o->values_count;
   auto _axis = _o->axis;
-  return tflite::CreatePackOptions(
+  return tflite_micro::CreatePackOptions(
       _fbb,
       _values_count,
       _axis);
@@ -19412,7 +19412,7 @@ inline flatbuffers::Offset<LogicalOrOptions> CreateLogicalOrOptions(flatbuffers:
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const LogicalOrOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateLogicalOrOptions(
+  return tflite_micro::CreateLogicalOrOptions(
       _fbb);
 }
 
@@ -19437,7 +19437,7 @@ inline flatbuffers::Offset<OneHotOptions> CreateOneHotOptions(flatbuffers::FlatB
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const OneHotOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _axis = _o->axis;
-  return tflite::CreateOneHotOptions(
+  return tflite_micro::CreateOneHotOptions(
       _fbb,
       _axis);
 }
@@ -19461,7 +19461,7 @@ inline flatbuffers::Offset<AbsOptions> CreateAbsOptions(flatbuffers::FlatBufferB
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const AbsOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateAbsOptions(
+  return tflite_micro::CreateAbsOptions(
       _fbb);
 }
 
@@ -19484,7 +19484,7 @@ inline flatbuffers::Offset<HardSwishOptions> CreateHardSwishOptions(flatbuffers:
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const HardSwishOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateHardSwishOptions(
+  return tflite_micro::CreateHardSwishOptions(
       _fbb);
 }
 
@@ -19507,7 +19507,7 @@ inline flatbuffers::Offset<LogicalAndOptions> CreateLogicalAndOptions(flatbuffer
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const LogicalAndOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateLogicalAndOptions(
+  return tflite_micro::CreateLogicalAndOptions(
       _fbb);
 }
 
@@ -19530,7 +19530,7 @@ inline flatbuffers::Offset<LogicalNotOptions> CreateLogicalNotOptions(flatbuffer
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const LogicalNotOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateLogicalNotOptions(
+  return tflite_micro::CreateLogicalNotOptions(
       _fbb);
 }
 
@@ -19557,7 +19557,7 @@ inline flatbuffers::Offset<UnpackOptions> CreateUnpackOptions(flatbuffers::FlatB
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const UnpackOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _num = _o->num;
   auto _axis = _o->axis;
-  return tflite::CreateUnpackOptions(
+  return tflite_micro::CreateUnpackOptions(
       _fbb,
       _num,
       _axis);
@@ -19582,7 +19582,7 @@ inline flatbuffers::Offset<FloorDivOptions> CreateFloorDivOptions(flatbuffers::F
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const FloorDivOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateFloorDivOptions(
+  return tflite_micro::CreateFloorDivOptions(
       _fbb);
 }
 
@@ -19605,7 +19605,7 @@ inline flatbuffers::Offset<SquareOptions> CreateSquareOptions(flatbuffers::FlatB
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const SquareOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateSquareOptions(
+  return tflite_micro::CreateSquareOptions(
       _fbb);
 }
 
@@ -19628,7 +19628,7 @@ inline flatbuffers::Offset<ZerosLikeOptions> CreateZerosLikeOptions(flatbuffers:
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const ZerosLikeOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateZerosLikeOptions(
+  return tflite_micro::CreateZerosLikeOptions(
       _fbb);
 }
 
@@ -19651,7 +19651,7 @@ inline flatbuffers::Offset<FillOptions> CreateFillOptions(flatbuffers::FlatBuffe
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const FillOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateFillOptions(
+  return tflite_micro::CreateFillOptions(
       _fbb);
 }
 
@@ -19674,7 +19674,7 @@ inline flatbuffers::Offset<FloorModOptions> CreateFloorModOptions(flatbuffers::F
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const FloorModOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateFloorModOptions(
+  return tflite_micro::CreateFloorModOptions(
       _fbb);
 }
 
@@ -19697,7 +19697,7 @@ inline flatbuffers::Offset<RangeOptions> CreateRangeOptions(flatbuffers::FlatBuf
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const RangeOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateRangeOptions(
+  return tflite_micro::CreateRangeOptions(
       _fbb);
 }
 
@@ -19722,7 +19722,7 @@ inline flatbuffers::Offset<LeakyReluOptions> CreateLeakyReluOptions(flatbuffers:
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const LeakyReluOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _alpha = _o->alpha;
-  return tflite::CreateLeakyReluOptions(
+  return tflite_micro::CreateLeakyReluOptions(
       _fbb,
       _alpha);
 }
@@ -19746,7 +19746,7 @@ inline flatbuffers::Offset<SquaredDifferenceOptions> CreateSquaredDifferenceOpti
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const SquaredDifferenceOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateSquaredDifferenceOptions(
+  return tflite_micro::CreateSquaredDifferenceOptions(
       _fbb);
 }
 
@@ -19771,7 +19771,7 @@ inline flatbuffers::Offset<MirrorPadOptions> CreateMirrorPadOptions(flatbuffers:
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const MirrorPadOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _mode = _o->mode;
-  return tflite::CreateMirrorPadOptions(
+  return tflite_micro::CreateMirrorPadOptions(
       _fbb,
       _mode);
 }
@@ -19797,7 +19797,7 @@ inline flatbuffers::Offset<UniqueOptions> CreateUniqueOptions(flatbuffers::FlatB
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const UniqueOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _idx_out_type = _o->idx_out_type;
-  return tflite::CreateUniqueOptions(
+  return tflite_micro::CreateUniqueOptions(
       _fbb,
       _idx_out_type);
 }
@@ -19821,7 +19821,7 @@ inline flatbuffers::Offset<ReverseV2Options> CreateReverseV2Options(flatbuffers:
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const ReverseV2OptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateReverseV2Options(
+  return tflite_micro::CreateReverseV2Options(
       _fbb);
 }
 
@@ -19844,7 +19844,7 @@ inline flatbuffers::Offset<AddNOptions> CreateAddNOptions(flatbuffers::FlatBuffe
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const AddNOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateAddNOptions(
+  return tflite_micro::CreateAddNOptions(
       _fbb);
 }
 
@@ -19867,7 +19867,7 @@ inline flatbuffers::Offset<GatherNdOptions> CreateGatherNdOptions(flatbuffers::F
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const GatherNdOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateGatherNdOptions(
+  return tflite_micro::CreateGatherNdOptions(
       _fbb);
 }
 
@@ -19890,7 +19890,7 @@ inline flatbuffers::Offset<WhereOptions> CreateWhereOptions(flatbuffers::FlatBuf
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const WhereOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateWhereOptions(
+  return tflite_micro::CreateWhereOptions(
       _fbb);
 }
 
@@ -19917,7 +19917,7 @@ inline flatbuffers::Offset<ReverseSequenceOptions> CreateReverseSequenceOptions(
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const ReverseSequenceOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _seq_dim = _o->seq_dim;
   auto _batch_dim = _o->batch_dim;
-  return tflite::CreateReverseSequenceOptions(
+  return tflite_micro::CreateReverseSequenceOptions(
       _fbb,
       _seq_dim,
       _batch_dim);
@@ -19942,7 +19942,7 @@ inline flatbuffers::Offset<MatrixDiagOptions> CreateMatrixDiagOptions(flatbuffer
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const MatrixDiagOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateMatrixDiagOptions(
+  return tflite_micro::CreateMatrixDiagOptions(
       _fbb);
 }
 
@@ -19965,7 +19965,7 @@ inline flatbuffers::Offset<QuantizeOptions> CreateQuantizeOptions(flatbuffers::F
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const QuantizeOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateQuantizeOptions(
+  return tflite_micro::CreateQuantizeOptions(
       _fbb);
 }
 
@@ -19988,7 +19988,7 @@ inline flatbuffers::Offset<MatrixSetDiagOptions> CreateMatrixSetDiagOptions(flat
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const MatrixSetDiagOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateMatrixSetDiagOptions(
+  return tflite_micro::CreateMatrixSetDiagOptions(
       _fbb);
 }
 
@@ -20015,7 +20015,7 @@ inline flatbuffers::Offset<IfOptions> CreateIfOptions(flatbuffers::FlatBufferBui
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const IfOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _then_subgraph_index = _o->then_subgraph_index;
   auto _else_subgraph_index = _o->else_subgraph_index;
-  return tflite::CreateIfOptions(
+  return tflite_micro::CreateIfOptions(
       _fbb,
       _then_subgraph_index,
       _else_subgraph_index);
@@ -20042,7 +20042,7 @@ inline flatbuffers::Offset<CallOnceOptions> CreateCallOnceOptions(flatbuffers::F
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const CallOnceOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _init_subgraph_index = _o->init_subgraph_index;
-  return tflite::CreateCallOnceOptions(
+  return tflite_micro::CreateCallOnceOptions(
       _fbb,
       _init_subgraph_index);
 }
@@ -20070,7 +20070,7 @@ inline flatbuffers::Offset<WhileOptions> CreateWhileOptions(flatbuffers::FlatBuf
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const WhileOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _cond_subgraph_index = _o->cond_subgraph_index;
   auto _body_subgraph_index = _o->body_subgraph_index;
-  return tflite::CreateWhileOptions(
+  return tflite_micro::CreateWhileOptions(
       _fbb,
       _cond_subgraph_index,
       _body_subgraph_index);
@@ -20095,7 +20095,7 @@ inline flatbuffers::Offset<NonMaxSuppressionV4Options> CreateNonMaxSuppressionV4
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const NonMaxSuppressionV4OptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateNonMaxSuppressionV4Options(
+  return tflite_micro::CreateNonMaxSuppressionV4Options(
       _fbb);
 }
 
@@ -20118,7 +20118,7 @@ inline flatbuffers::Offset<NonMaxSuppressionV5Options> CreateNonMaxSuppressionV5
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const NonMaxSuppressionV5OptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateNonMaxSuppressionV5Options(
+  return tflite_micro::CreateNonMaxSuppressionV5Options(
       _fbb);
 }
 
@@ -20141,7 +20141,7 @@ inline flatbuffers::Offset<ScatterNdOptions> CreateScatterNdOptions(flatbuffers:
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const ScatterNdOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateScatterNdOptions(
+  return tflite_micro::CreateScatterNdOptions(
       _fbb);
 }
 
@@ -20164,7 +20164,7 @@ inline flatbuffers::Offset<SelectV2Options> CreateSelectV2Options(flatbuffers::F
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const SelectV2OptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateSelectV2Options(
+  return tflite_micro::CreateSelectV2Options(
       _fbb);
 }
 
@@ -20187,7 +20187,7 @@ inline flatbuffers::Offset<DensifyOptions> CreateDensifyOptions(flatbuffers::Fla
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const DensifyOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateDensifyOptions(
+  return tflite_micro::CreateDensifyOptions(
       _fbb);
 }
 
@@ -20210,7 +20210,7 @@ inline flatbuffers::Offset<SegmentSumOptions> CreateSegmentSumOptions(flatbuffer
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const SegmentSumOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateSegmentSumOptions(
+  return tflite_micro::CreateSegmentSumOptions(
       _fbb);
 }
 
@@ -20239,7 +20239,7 @@ inline flatbuffers::Offset<BatchMatMulOptions> CreateBatchMatMulOptions(flatbuff
   auto _adj_x = _o->adj_x;
   auto _adj_y = _o->adj_y;
   auto _asymmetric_quantize_inputs = _o->asymmetric_quantize_inputs;
-  return tflite::CreateBatchMatMulOptions(
+  return tflite_micro::CreateBatchMatMulOptions(
       _fbb,
       _adj_x,
       _adj_y,
@@ -20269,7 +20269,7 @@ inline flatbuffers::Offset<CumsumOptions> CreateCumsumOptions(flatbuffers::FlatB
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const CumsumOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _exclusive = _o->exclusive;
   auto _reverse = _o->reverse;
-  return tflite::CreateCumsumOptions(
+  return tflite_micro::CreateCumsumOptions(
       _fbb,
       _exclusive,
       _reverse);
@@ -20294,7 +20294,7 @@ inline flatbuffers::Offset<BroadcastToOptions> CreateBroadcastToOptions(flatbuff
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const BroadcastToOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateBroadcastToOptions(
+  return tflite_micro::CreateBroadcastToOptions(
       _fbb);
 }
 
@@ -20317,7 +20317,7 @@ inline flatbuffers::Offset<Rfft2dOptions> CreateRfft2dOptions(flatbuffers::FlatB
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const Rfft2dOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateRfft2dOptions(
+  return tflite_micro::CreateRfft2dOptions(
       _fbb);
 }
 
@@ -20346,7 +20346,7 @@ inline flatbuffers::Offset<HashtableOptions> CreateHashtableOptions(flatbuffers:
   auto _table_id = _o->table_id;
   auto _key_dtype = _o->key_dtype;
   auto _value_dtype = _o->value_dtype;
-  return tflite::CreateHashtableOptions(
+  return tflite_micro::CreateHashtableOptions(
       _fbb,
       _table_id,
       _key_dtype,
@@ -20372,7 +20372,7 @@ inline flatbuffers::Offset<HashtableFindOptions> CreateHashtableFindOptions(flat
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const HashtableFindOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateHashtableFindOptions(
+  return tflite_micro::CreateHashtableFindOptions(
       _fbb);
 }
 
@@ -20395,7 +20395,7 @@ inline flatbuffers::Offset<HashtableImportOptions> CreateHashtableImportOptions(
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const HashtableImportOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateHashtableImportOptions(
+  return tflite_micro::CreateHashtableImportOptions(
       _fbb);
 }
 
@@ -20418,7 +20418,7 @@ inline flatbuffers::Offset<HashtableSizeOptions> CreateHashtableSizeOptions(flat
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const HashtableSizeOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateHashtableSizeOptions(
+  return tflite_micro::CreateHashtableSizeOptions(
       _fbb);
 }
 
@@ -20445,7 +20445,7 @@ inline flatbuffers::Offset<VarHandleOptions> CreateVarHandleOptions(flatbuffers:
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const VarHandleOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _container = _o->container.empty() ? 0 : _fbb.CreateString(_o->container);
   auto _shared_name = _o->shared_name.empty() ? 0 : _fbb.CreateString(_o->shared_name);
-  return tflite::CreateVarHandleOptions(
+  return tflite_micro::CreateVarHandleOptions(
       _fbb,
       _container,
       _shared_name);
@@ -20470,7 +20470,7 @@ inline flatbuffers::Offset<ReadVariableOptions> CreateReadVariableOptions(flatbu
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const ReadVariableOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateReadVariableOptions(
+  return tflite_micro::CreateReadVariableOptions(
       _fbb);
 }
 
@@ -20493,7 +20493,7 @@ inline flatbuffers::Offset<AssignVariableOptions> CreateAssignVariableOptions(fl
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const AssignVariableOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateAssignVariableOptions(
+  return tflite_micro::CreateAssignVariableOptions(
       _fbb);
 }
 
@@ -20520,7 +20520,7 @@ inline flatbuffers::Offset<RandomOptions> CreateRandomOptions(flatbuffers::FlatB
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const RandomOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _seed = _o->seed;
   auto _seed2 = _o->seed2;
-  return tflite::CreateRandomOptions(
+  return tflite_micro::CreateRandomOptions(
       _fbb,
       _seed,
       _seed2);
@@ -20547,7 +20547,7 @@ inline flatbuffers::Offset<BucketizeOptions> CreateBucketizeOptions(flatbuffers:
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const BucketizeOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _boundaries = _o->boundaries.size() ? _fbb.CreateVector(_o->boundaries) : 0;
-  return tflite::CreateBucketizeOptions(
+  return tflite_micro::CreateBucketizeOptions(
       _fbb,
       _boundaries);
 }
@@ -20573,7 +20573,7 @@ inline flatbuffers::Offset<GeluOptions> CreateGeluOptions(flatbuffers::FlatBuffe
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const GeluOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _approximate = _o->approximate;
-  return tflite::CreateGeluOptions(
+  return tflite_micro::CreateGeluOptions(
       _fbb,
       _approximate);
 }
@@ -20597,7 +20597,7 @@ inline flatbuffers::Offset<DynamicUpdateSliceOptions> CreateDynamicUpdateSliceOp
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const DynamicUpdateSliceOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateDynamicUpdateSliceOptions(
+  return tflite_micro::CreateDynamicUpdateSliceOptions(
       _fbb);
 }
 
@@ -20620,7 +20620,7 @@ inline flatbuffers::Offset<UnsortedSegmentProdOptions> CreateUnsortedSegmentProd
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const UnsortedSegmentProdOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateUnsortedSegmentProdOptions(
+  return tflite_micro::CreateUnsortedSegmentProdOptions(
       _fbb);
 }
 
@@ -20643,7 +20643,7 @@ inline flatbuffers::Offset<UnsortedSegmentMaxOptions> CreateUnsortedSegmentMaxOp
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const UnsortedSegmentMaxOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateUnsortedSegmentMaxOptions(
+  return tflite_micro::CreateUnsortedSegmentMaxOptions(
       _fbb);
 }
 
@@ -20666,7 +20666,7 @@ inline flatbuffers::Offset<UnsortedSegmentSumOptions> CreateUnsortedSegmentSumOp
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const UnsortedSegmentSumOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateUnsortedSegmentSumOptions(
+  return tflite_micro::CreateUnsortedSegmentSumOptions(
       _fbb);
 }
 
@@ -20689,7 +20689,7 @@ inline flatbuffers::Offset<ATan2Options> CreateATan2Options(flatbuffers::FlatBuf
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const ATan2OptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateATan2Options(
+  return tflite_micro::CreateATan2Options(
       _fbb);
 }
 
@@ -20712,7 +20712,7 @@ inline flatbuffers::Offset<UnsortedSegmentMinOptions> CreateUnsortedSegmentMinOp
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const UnsortedSegmentMinOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateUnsortedSegmentMinOptions(
+  return tflite_micro::CreateUnsortedSegmentMinOptions(
       _fbb);
 }
 
@@ -20735,7 +20735,7 @@ inline flatbuffers::Offset<SignOptions> CreateSignOptions(flatbuffers::FlatBuffe
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const SignOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateSignOptions(
+  return tflite_micro::CreateSignOptions(
       _fbb);
 }
 
@@ -20758,7 +20758,7 @@ inline flatbuffers::Offset<BitcastOptions> CreateBitcastOptions(flatbuffers::Fla
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const BitcastOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateBitcastOptions(
+  return tflite_micro::CreateBitcastOptions(
       _fbb);
 }
 
@@ -20781,7 +20781,7 @@ inline flatbuffers::Offset<BitwiseXorOptions> CreateBitwiseXorOptions(flatbuffer
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const BitwiseXorOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateBitwiseXorOptions(
+  return tflite_micro::CreateBitwiseXorOptions(
       _fbb);
 }
 
@@ -20804,7 +20804,7 @@ inline flatbuffers::Offset<RightShiftOptions> CreateRightShiftOptions(flatbuffer
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const RightShiftOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateRightShiftOptions(
+  return tflite_micro::CreateRightShiftOptions(
       _fbb);
 }
 
@@ -20827,7 +20827,7 @@ inline flatbuffers::Offset<DilateOptions> CreateDilateOptions(flatbuffers::FlatB
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const DilateOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  return tflite::CreateDilateOptions(
+  return tflite_micro::CreateDilateOptions(
       _fbb);
 }
 
@@ -20852,7 +20852,7 @@ inline flatbuffers::Offset<ReduceWindowOptions> CreateReduceWindowOptions(flatbu
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const ReduceWindowOptionsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _reduce_function = _o->reduce_function;
-  return tflite::CreateReduceWindowOptions(
+  return tflite_micro::CreateReduceWindowOptions(
       _fbb,
       _reduce_function);
 }
@@ -20884,7 +20884,7 @@ inline flatbuffers::Offset<OperatorCode> CreateOperatorCode(flatbuffers::FlatBuf
   auto _custom_code = _o->custom_code.empty() ? 0 : _fbb.CreateString(_o->custom_code);
   auto _version = _o->version;
   auto _builtin_code = _o->builtin_code;
-  return tflite::CreateOperatorCode(
+  return tflite_micro::CreateOperatorCode(
       _fbb,
       _deprecated_builtin_code,
       _custom_code,
@@ -20905,7 +20905,7 @@ inline void Operator::UnPackTo(OperatorT *_o, const flatbuffers::resolver_functi
   { auto _e = inputs(); if (_e) { _o->inputs.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->inputs[_i] = _e->Get(_i); } } }
   { auto _e = outputs(); if (_e) { _o->outputs.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->outputs[_i] = _e->Get(_i); } } }
   { auto _e = builtin_options_type(); _o->builtin_options.type = _e; }
-  { auto _e = builtin_options(); if (_e) _o->builtin_options.value = tflite::BuiltinOptionsUnion::UnPack(_e, builtin_options_type(), _resolver); }
+  { auto _e = builtin_options(); if (_e) _o->builtin_options.value = tflite_micro::BuiltinOptionsUnion::UnPack(_e, builtin_options_type(), _resolver); }
   { auto _e = custom_options(); if (_e) { _o->custom_options.resize(_e->size()); std::copy(_e->begin(), _e->end(), _o->custom_options.begin()); } }
   { auto _e = custom_options_format(); _o->custom_options_format = _e; }
   { auto _e = mutating_variable_inputs(); if (_e) { _o->mutating_variable_inputs.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->mutating_variable_inputs[_i] = _e->Get(_i) != 0; } } }
@@ -20913,7 +20913,7 @@ inline void Operator::UnPackTo(OperatorT *_o, const flatbuffers::resolver_functi
   { auto _e = large_custom_options_offset(); _o->large_custom_options_offset = _e; }
   { auto _e = large_custom_options_size(); _o->large_custom_options_size = _e; }
   { auto _e = builtin_options_2_type(); _o->builtin_options_2.type = _e; }
-  { auto _e = builtin_options_2(); if (_e) _o->builtin_options_2.value = tflite::BuiltinOptions2Union::UnPack(_e, builtin_options_2_type(), _resolver); }
+  { auto _e = builtin_options_2(); if (_e) _o->builtin_options_2.value = tflite_micro::BuiltinOptions2Union::UnPack(_e, builtin_options_2_type(), _resolver); }
 }
 
 inline flatbuffers::Offset<Operator> Operator::Pack(flatbuffers::FlatBufferBuilder &_fbb, const OperatorT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
@@ -20937,7 +20937,7 @@ inline flatbuffers::Offset<Operator> CreateOperator(flatbuffers::FlatBufferBuild
   auto _large_custom_options_size = _o->large_custom_options_size;
   auto _builtin_options_2_type = _o->builtin_options_2.type;
   auto _builtin_options_2 = _o->builtin_options_2.Pack(_fbb);
-  return tflite::CreateOperator(
+  return tflite_micro::CreateOperator(
       _fbb,
       _opcode_index,
       _inputs,
@@ -20959,9 +20959,9 @@ inline SubGraphT::SubGraphT(const SubGraphT &o)
         outputs(o.outputs),
         name(o.name) {
   tensors.reserve(o.tensors.size());
-  for (const auto &tensors_ : o.tensors) { tensors.emplace_back((tensors_) ? new tflite::TensorT(*tensors_) : nullptr); }
+  for (const auto &tensors_ : o.tensors) { tensors.emplace_back((tensors_) ? new tflite_micro::TensorT(*tensors_) : nullptr); }
   operators.reserve(o.operators.size());
-  for (const auto &operators_ : o.operators) { operators.emplace_back((operators_) ? new tflite::OperatorT(*operators_) : nullptr); }
+  for (const auto &operators_ : o.operators) { operators.emplace_back((operators_) ? new tflite_micro::OperatorT(*operators_) : nullptr); }
 }
 
 inline SubGraphT &SubGraphT::operator=(SubGraphT o) FLATBUFFERS_NOEXCEPT {
@@ -20982,10 +20982,10 @@ inline SubGraphT *SubGraph::UnPack(const flatbuffers::resolver_function_t *_reso
 inline void SubGraph::UnPackTo(SubGraphT *_o, const flatbuffers::resolver_function_t *_resolver) const {
   (void)_o;
   (void)_resolver;
-  { auto _e = tensors(); if (_e) { _o->tensors.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->tensors[_i]) { _e->Get(_i)->UnPackTo(_o->tensors[_i].get(), _resolver); } else { _o->tensors[_i] = std::unique_ptr<tflite::TensorT>(_e->Get(_i)->UnPack(_resolver)); }; } } }
+  { auto _e = tensors(); if (_e) { _o->tensors.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->tensors[_i]) { _e->Get(_i)->UnPackTo(_o->tensors[_i].get(), _resolver); } else { _o->tensors[_i] = std::unique_ptr<tflite_micro::TensorT>(_e->Get(_i)->UnPack(_resolver)); }; } } }
   { auto _e = inputs(); if (_e) { _o->inputs.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->inputs[_i] = _e->Get(_i); } } }
   { auto _e = outputs(); if (_e) { _o->outputs.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->outputs[_i] = _e->Get(_i); } } }
-  { auto _e = operators(); if (_e) { _o->operators.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->operators[_i]) { _e->Get(_i)->UnPackTo(_o->operators[_i].get(), _resolver); } else { _o->operators[_i] = std::unique_ptr<tflite::OperatorT>(_e->Get(_i)->UnPack(_resolver)); }; } } }
+  { auto _e = operators(); if (_e) { _o->operators.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->operators[_i]) { _e->Get(_i)->UnPackTo(_o->operators[_i].get(), _resolver); } else { _o->operators[_i] = std::unique_ptr<tflite_micro::OperatorT>(_e->Get(_i)->UnPack(_resolver)); }; } } }
   { auto _e = name(); if (_e) _o->name = _e->str(); }
 }
 
@@ -20997,12 +20997,12 @@ inline flatbuffers::Offset<SubGraph> CreateSubGraph(flatbuffers::FlatBufferBuild
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const SubGraphT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  auto _tensors = _o->tensors.size() ? _fbb.CreateVector<flatbuffers::Offset<tflite::Tensor>> (_o->tensors.size(), [](size_t i, _VectorArgs *__va) { return CreateTensor(*__va->__fbb, __va->__o->tensors[i].get(), __va->__rehasher); }, &_va ) : 0;
+  auto _tensors = _o->tensors.size() ? _fbb.CreateVector<flatbuffers::Offset<tflite_micro::Tensor>> (_o->tensors.size(), [](size_t i, _VectorArgs *__va) { return CreateTensor(*__va->__fbb, __va->__o->tensors[i].get(), __va->__rehasher); }, &_va ) : 0;
   auto _inputs = _o->inputs.size() ? _fbb.CreateVector(_o->inputs) : 0;
   auto _outputs = _o->outputs.size() ? _fbb.CreateVector(_o->outputs) : 0;
-  auto _operators = _o->operators.size() ? _fbb.CreateVector<flatbuffers::Offset<tflite::Operator>> (_o->operators.size(), [](size_t i, _VectorArgs *__va) { return CreateOperator(*__va->__fbb, __va->__o->operators[i].get(), __va->__rehasher); }, &_va ) : 0;
+  auto _operators = _o->operators.size() ? _fbb.CreateVector<flatbuffers::Offset<tflite_micro::Operator>> (_o->operators.size(), [](size_t i, _VectorArgs *__va) { return CreateOperator(*__va->__fbb, __va->__o->operators[i].get(), __va->__rehasher); }, &_va ) : 0;
   auto _name = _o->name.empty() ? 0 : _fbb.CreateString(_o->name);
-  return tflite::CreateSubGraph(
+  return tflite_micro::CreateSubGraph(
       _fbb,
       _tensors,
       _inputs,
@@ -21037,7 +21037,7 @@ inline flatbuffers::Offset<Buffer> CreateBuffer(flatbuffers::FlatBufferBuilder &
   auto _data = _o->data.size() ? _fbb.CreateVector(_o->data) : 0;
   auto _offset = _o->offset;
   auto _size = _o->size;
-  return tflite::CreateBuffer(
+  return tflite_micro::CreateBuffer(
       _fbb,
       _data,
       _offset,
@@ -21067,7 +21067,7 @@ inline flatbuffers::Offset<Metadata> CreateMetadata(flatbuffers::FlatBufferBuild
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const MetadataT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _name = _o->name.empty() ? 0 : _fbb.CreateString(_o->name);
   auto _buffer = _o->buffer;
-  return tflite::CreateMetadata(
+  return tflite_micro::CreateMetadata(
       _fbb,
       _name,
       _buffer);
@@ -21096,7 +21096,7 @@ inline flatbuffers::Offset<TensorMap> CreateTensorMap(flatbuffers::FlatBufferBui
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const TensorMapT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _name = _o->name.empty() ? 0 : _fbb.CreateString(_o->name);
   auto _tensor_index = _o->tensor_index;
-  return tflite::CreateTensorMap(
+  return tflite_micro::CreateTensorMap(
       _fbb,
       _name,
       _tensor_index);
@@ -21106,9 +21106,9 @@ inline SignatureDefT::SignatureDefT(const SignatureDefT &o)
       : signature_key(o.signature_key),
         subgraph_index(o.subgraph_index) {
   inputs.reserve(o.inputs.size());
-  for (const auto &inputs_ : o.inputs) { inputs.emplace_back((inputs_) ? new tflite::TensorMapT(*inputs_) : nullptr); }
+  for (const auto &inputs_ : o.inputs) { inputs.emplace_back((inputs_) ? new tflite_micro::TensorMapT(*inputs_) : nullptr); }
   outputs.reserve(o.outputs.size());
-  for (const auto &outputs_ : o.outputs) { outputs.emplace_back((outputs_) ? new tflite::TensorMapT(*outputs_) : nullptr); }
+  for (const auto &outputs_ : o.outputs) { outputs.emplace_back((outputs_) ? new tflite_micro::TensorMapT(*outputs_) : nullptr); }
 }
 
 inline SignatureDefT &SignatureDefT::operator=(SignatureDefT o) FLATBUFFERS_NOEXCEPT {
@@ -21128,8 +21128,8 @@ inline SignatureDefT *SignatureDef::UnPack(const flatbuffers::resolver_function_
 inline void SignatureDef::UnPackTo(SignatureDefT *_o, const flatbuffers::resolver_function_t *_resolver) const {
   (void)_o;
   (void)_resolver;
-  { auto _e = inputs(); if (_e) { _o->inputs.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->inputs[_i]) { _e->Get(_i)->UnPackTo(_o->inputs[_i].get(), _resolver); } else { _o->inputs[_i] = std::unique_ptr<tflite::TensorMapT>(_e->Get(_i)->UnPack(_resolver)); }; } } }
-  { auto _e = outputs(); if (_e) { _o->outputs.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->outputs[_i]) { _e->Get(_i)->UnPackTo(_o->outputs[_i].get(), _resolver); } else { _o->outputs[_i] = std::unique_ptr<tflite::TensorMapT>(_e->Get(_i)->UnPack(_resolver)); }; } } }
+  { auto _e = inputs(); if (_e) { _o->inputs.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->inputs[_i]) { _e->Get(_i)->UnPackTo(_o->inputs[_i].get(), _resolver); } else { _o->inputs[_i] = std::unique_ptr<tflite_micro::TensorMapT>(_e->Get(_i)->UnPack(_resolver)); }; } } }
+  { auto _e = outputs(); if (_e) { _o->outputs.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->outputs[_i]) { _e->Get(_i)->UnPackTo(_o->outputs[_i].get(), _resolver); } else { _o->outputs[_i] = std::unique_ptr<tflite_micro::TensorMapT>(_e->Get(_i)->UnPack(_resolver)); }; } } }
   { auto _e = signature_key(); if (_e) _o->signature_key = _e->str(); }
   { auto _e = subgraph_index(); _o->subgraph_index = _e; }
 }
@@ -21142,11 +21142,11 @@ inline flatbuffers::Offset<SignatureDef> CreateSignatureDef(flatbuffers::FlatBuf
   (void)_rehasher;
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const SignatureDefT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
-  auto _inputs = _o->inputs.size() ? _fbb.CreateVector<flatbuffers::Offset<tflite::TensorMap>> (_o->inputs.size(), [](size_t i, _VectorArgs *__va) { return CreateTensorMap(*__va->__fbb, __va->__o->inputs[i].get(), __va->__rehasher); }, &_va ) : 0;
-  auto _outputs = _o->outputs.size() ? _fbb.CreateVector<flatbuffers::Offset<tflite::TensorMap>> (_o->outputs.size(), [](size_t i, _VectorArgs *__va) { return CreateTensorMap(*__va->__fbb, __va->__o->outputs[i].get(), __va->__rehasher); }, &_va ) : 0;
+  auto _inputs = _o->inputs.size() ? _fbb.CreateVector<flatbuffers::Offset<tflite_micro::TensorMap>> (_o->inputs.size(), [](size_t i, _VectorArgs *__va) { return CreateTensorMap(*__va->__fbb, __va->__o->inputs[i].get(), __va->__rehasher); }, &_va ) : 0;
+  auto _outputs = _o->outputs.size() ? _fbb.CreateVector<flatbuffers::Offset<tflite_micro::TensorMap>> (_o->outputs.size(), [](size_t i, _VectorArgs *__va) { return CreateTensorMap(*__va->__fbb, __va->__o->outputs[i].get(), __va->__rehasher); }, &_va ) : 0;
   auto _signature_key = _o->signature_key.empty() ? 0 : _fbb.CreateString(_o->signature_key);
   auto _subgraph_index = _o->subgraph_index;
-  return tflite::CreateSignatureDef(
+  return tflite_micro::CreateSignatureDef(
       _fbb,
       _inputs,
       _outputs,
@@ -21159,15 +21159,15 @@ inline ModelT::ModelT(const ModelT &o)
         description(o.description),
         metadata_buffer(o.metadata_buffer) {
   operator_codes.reserve(o.operator_codes.size());
-  for (const auto &operator_codes_ : o.operator_codes) { operator_codes.emplace_back((operator_codes_) ? new tflite::OperatorCodeT(*operator_codes_) : nullptr); }
+  for (const auto &operator_codes_ : o.operator_codes) { operator_codes.emplace_back((operator_codes_) ? new tflite_micro::OperatorCodeT(*operator_codes_) : nullptr); }
   subgraphs.reserve(o.subgraphs.size());
-  for (const auto &subgraphs_ : o.subgraphs) { subgraphs.emplace_back((subgraphs_) ? new tflite::SubGraphT(*subgraphs_) : nullptr); }
+  for (const auto &subgraphs_ : o.subgraphs) { subgraphs.emplace_back((subgraphs_) ? new tflite_micro::SubGraphT(*subgraphs_) : nullptr); }
   buffers.reserve(o.buffers.size());
-  for (const auto &buffers_ : o.buffers) { buffers.emplace_back((buffers_) ? new tflite::BufferT(*buffers_) : nullptr); }
+  for (const auto &buffers_ : o.buffers) { buffers.emplace_back((buffers_) ? new tflite_micro::BufferT(*buffers_) : nullptr); }
   metadata.reserve(o.metadata.size());
-  for (const auto &metadata_ : o.metadata) { metadata.emplace_back((metadata_) ? new tflite::MetadataT(*metadata_) : nullptr); }
+  for (const auto &metadata_ : o.metadata) { metadata.emplace_back((metadata_) ? new tflite_micro::MetadataT(*metadata_) : nullptr); }
   signature_defs.reserve(o.signature_defs.size());
-  for (const auto &signature_defs_ : o.signature_defs) { signature_defs.emplace_back((signature_defs_) ? new tflite::SignatureDefT(*signature_defs_) : nullptr); }
+  for (const auto &signature_defs_ : o.signature_defs) { signature_defs.emplace_back((signature_defs_) ? new tflite_micro::SignatureDefT(*signature_defs_) : nullptr); }
 }
 
 inline ModelT &ModelT::operator=(ModelT o) FLATBUFFERS_NOEXCEPT {
@@ -21192,13 +21192,13 @@ inline void Model::UnPackTo(ModelT *_o, const flatbuffers::resolver_function_t *
   (void)_o;
   (void)_resolver;
   { auto _e = version(); _o->version = _e; }
-  { auto _e = operator_codes(); if (_e) { _o->operator_codes.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->operator_codes[_i]) { _e->Get(_i)->UnPackTo(_o->operator_codes[_i].get(), _resolver); } else { _o->operator_codes[_i] = std::unique_ptr<tflite::OperatorCodeT>(_e->Get(_i)->UnPack(_resolver)); }; } } }
-  { auto _e = subgraphs(); if (_e) { _o->subgraphs.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->subgraphs[_i]) { _e->Get(_i)->UnPackTo(_o->subgraphs[_i].get(), _resolver); } else { _o->subgraphs[_i] = std::unique_ptr<tflite::SubGraphT>(_e->Get(_i)->UnPack(_resolver)); }; } } }
+  { auto _e = operator_codes(); if (_e) { _o->operator_codes.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->operator_codes[_i]) { _e->Get(_i)->UnPackTo(_o->operator_codes[_i].get(), _resolver); } else { _o->operator_codes[_i] = std::unique_ptr<tflite_micro::OperatorCodeT>(_e->Get(_i)->UnPack(_resolver)); }; } } }
+  { auto _e = subgraphs(); if (_e) { _o->subgraphs.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->subgraphs[_i]) { _e->Get(_i)->UnPackTo(_o->subgraphs[_i].get(), _resolver); } else { _o->subgraphs[_i] = std::unique_ptr<tflite_micro::SubGraphT>(_e->Get(_i)->UnPack(_resolver)); }; } } }
   { auto _e = description(); if (_e) _o->description = _e->str(); }
-  { auto _e = buffers(); if (_e) { _o->buffers.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->buffers[_i]) { _e->Get(_i)->UnPackTo(_o->buffers[_i].get(), _resolver); } else { _o->buffers[_i] = std::unique_ptr<tflite::BufferT>(_e->Get(_i)->UnPack(_resolver)); }; } } }
+  { auto _e = buffers(); if (_e) { _o->buffers.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->buffers[_i]) { _e->Get(_i)->UnPackTo(_o->buffers[_i].get(), _resolver); } else { _o->buffers[_i] = std::unique_ptr<tflite_micro::BufferT>(_e->Get(_i)->UnPack(_resolver)); }; } } }
   { auto _e = metadata_buffer(); if (_e) { _o->metadata_buffer.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->metadata_buffer[_i] = _e->Get(_i); } } }
-  { auto _e = metadata(); if (_e) { _o->metadata.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->metadata[_i]) { _e->Get(_i)->UnPackTo(_o->metadata[_i].get(), _resolver); } else { _o->metadata[_i] = std::unique_ptr<tflite::MetadataT>(_e->Get(_i)->UnPack(_resolver)); }; } } }
-  { auto _e = signature_defs(); if (_e) { _o->signature_defs.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->signature_defs[_i]) { _e->Get(_i)->UnPackTo(_o->signature_defs[_i].get(), _resolver); } else { _o->signature_defs[_i] = std::unique_ptr<tflite::SignatureDefT>(_e->Get(_i)->UnPack(_resolver)); }; } } }
+  { auto _e = metadata(); if (_e) { _o->metadata.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->metadata[_i]) { _e->Get(_i)->UnPackTo(_o->metadata[_i].get(), _resolver); } else { _o->metadata[_i] = std::unique_ptr<tflite_micro::MetadataT>(_e->Get(_i)->UnPack(_resolver)); }; } } }
+  { auto _e = signature_defs(); if (_e) { _o->signature_defs.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->signature_defs[_i]) { _e->Get(_i)->UnPackTo(_o->signature_defs[_i].get(), _resolver); } else { _o->signature_defs[_i] = std::unique_ptr<tflite_micro::SignatureDefT>(_e->Get(_i)->UnPack(_resolver)); }; } } }
 }
 
 inline flatbuffers::Offset<Model> Model::Pack(flatbuffers::FlatBufferBuilder &_fbb, const ModelT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
@@ -21210,14 +21210,14 @@ inline flatbuffers::Offset<Model> CreateModel(flatbuffers::FlatBufferBuilder &_f
   (void)_o;
   struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const ModelT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
   auto _version = _o->version;
-  auto _operator_codes = _o->operator_codes.size() ? _fbb.CreateVector<flatbuffers::Offset<tflite::OperatorCode>> (_o->operator_codes.size(), [](size_t i, _VectorArgs *__va) { return CreateOperatorCode(*__va->__fbb, __va->__o->operator_codes[i].get(), __va->__rehasher); }, &_va ) : 0;
-  auto _subgraphs = _o->subgraphs.size() ? _fbb.CreateVector<flatbuffers::Offset<tflite::SubGraph>> (_o->subgraphs.size(), [](size_t i, _VectorArgs *__va) { return CreateSubGraph(*__va->__fbb, __va->__o->subgraphs[i].get(), __va->__rehasher); }, &_va ) : 0;
+  auto _operator_codes = _o->operator_codes.size() ? _fbb.CreateVector<flatbuffers::Offset<tflite_micro::OperatorCode>> (_o->operator_codes.size(), [](size_t i, _VectorArgs *__va) { return CreateOperatorCode(*__va->__fbb, __va->__o->operator_codes[i].get(), __va->__rehasher); }, &_va ) : 0;
+  auto _subgraphs = _o->subgraphs.size() ? _fbb.CreateVector<flatbuffers::Offset<tflite_micro::SubGraph>> (_o->subgraphs.size(), [](size_t i, _VectorArgs *__va) { return CreateSubGraph(*__va->__fbb, __va->__o->subgraphs[i].get(), __va->__rehasher); }, &_va ) : 0;
   auto _description = _o->description.empty() ? 0 : _fbb.CreateString(_o->description);
-  auto _buffers = _o->buffers.size() ? _fbb.CreateVector<flatbuffers::Offset<tflite::Buffer>> (_o->buffers.size(), [](size_t i, _VectorArgs *__va) { return CreateBuffer(*__va->__fbb, __va->__o->buffers[i].get(), __va->__rehasher); }, &_va ) : 0;
+  auto _buffers = _o->buffers.size() ? _fbb.CreateVector<flatbuffers::Offset<tflite_micro::Buffer>> (_o->buffers.size(), [](size_t i, _VectorArgs *__va) { return CreateBuffer(*__va->__fbb, __va->__o->buffers[i].get(), __va->__rehasher); }, &_va ) : 0;
   auto _metadata_buffer = _o->metadata_buffer.size() ? _fbb.CreateVector(_o->metadata_buffer) : 0;
-  auto _metadata = _o->metadata.size() ? _fbb.CreateVector<flatbuffers::Offset<tflite::Metadata>> (_o->metadata.size(), [](size_t i, _VectorArgs *__va) { return CreateMetadata(*__va->__fbb, __va->__o->metadata[i].get(), __va->__rehasher); }, &_va ) : 0;
-  auto _signature_defs = _o->signature_defs.size() ? _fbb.CreateVector<flatbuffers::Offset<tflite::SignatureDef>> (_o->signature_defs.size(), [](size_t i, _VectorArgs *__va) { return CreateSignatureDef(*__va->__fbb, __va->__o->signature_defs[i].get(), __va->__rehasher); }, &_va ) : 0;
-  return tflite::CreateModel(
+  auto _metadata = _o->metadata.size() ? _fbb.CreateVector<flatbuffers::Offset<tflite_micro::Metadata>> (_o->metadata.size(), [](size_t i, _VectorArgs *__va) { return CreateMetadata(*__va->__fbb, __va->__o->metadata[i].get(), __va->__rehasher); }, &_va ) : 0;
+  auto _signature_defs = _o->signature_defs.size() ? _fbb.CreateVector<flatbuffers::Offset<tflite_micro::SignatureDef>> (_o->signature_defs.size(), [](size_t i, _VectorArgs *__va) { return CreateSignatureDef(*__va->__fbb, __va->__o->signature_defs[i].get(), __va->__rehasher); }, &_va ) : 0;
+  return tflite_micro::CreateModel(
       _fbb,
       _version,
       _operator_codes,
@@ -21235,7 +21235,7 @@ inline bool VerifyQuantizationDetails(flatbuffers::Verifier &verifier, const voi
       return true;
     }
     case QuantizationDetails_CustomQuantization: {
-      auto ptr = reinterpret_cast<const tflite::CustomQuantization *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::CustomQuantization *>(obj);
       return verifier.VerifyTable(ptr);
     }
     default: return true;
@@ -21258,7 +21258,7 @@ inline void *QuantizationDetailsUnion::UnPack(const void *obj, QuantizationDetai
   (void)resolver;
   switch (type) {
     case QuantizationDetails_CustomQuantization: {
-      auto ptr = reinterpret_cast<const tflite::CustomQuantization *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::CustomQuantization *>(obj);
       return ptr->UnPack(resolver);
     }
     default: return nullptr;
@@ -21269,7 +21269,7 @@ inline flatbuffers::Offset<void> QuantizationDetailsUnion::Pack(flatbuffers::Fla
   (void)_rehasher;
   switch (type) {
     case QuantizationDetails_CustomQuantization: {
-      auto ptr = reinterpret_cast<const tflite::CustomQuantizationT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::CustomQuantizationT *>(value);
       return CreateCustomQuantization(_fbb, ptr, _rehasher).Union();
     }
     default: return 0;
@@ -21279,7 +21279,7 @@ inline flatbuffers::Offset<void> QuantizationDetailsUnion::Pack(flatbuffers::Fla
 inline QuantizationDetailsUnion::QuantizationDetailsUnion(const QuantizationDetailsUnion &u) : type(u.type), value(nullptr) {
   switch (type) {
     case QuantizationDetails_CustomQuantization: {
-      value = new tflite::CustomQuantizationT(*reinterpret_cast<tflite::CustomQuantizationT *>(u.value));
+      value = new tflite_micro::CustomQuantizationT(*reinterpret_cast<tflite_micro::CustomQuantizationT *>(u.value));
       break;
     }
     default:
@@ -21290,7 +21290,7 @@ inline QuantizationDetailsUnion::QuantizationDetailsUnion(const QuantizationDeta
 inline void QuantizationDetailsUnion::Reset() {
   switch (type) {
     case QuantizationDetails_CustomQuantization: {
-      auto ptr = reinterpret_cast<tflite::CustomQuantizationT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::CustomQuantizationT *>(value);
       delete ptr;
       break;
     }
@@ -21306,15 +21306,15 @@ inline bool VerifySparseIndexVector(flatbuffers::Verifier &verifier, const void
       return true;
     }
     case SparseIndexVector_Int32Vector: {
-      auto ptr = reinterpret_cast<const tflite::Int32Vector *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::Int32Vector *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case SparseIndexVector_Uint16Vector: {
-      auto ptr = reinterpret_cast<const tflite::Uint16Vector *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::Uint16Vector *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case SparseIndexVector_Uint8Vector: {
-      auto ptr = reinterpret_cast<const tflite::Uint8Vector *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::Uint8Vector *>(obj);
       return verifier.VerifyTable(ptr);
     }
     default: return true;
@@ -21337,15 +21337,15 @@ inline void *SparseIndexVectorUnion::UnPack(const void *obj, SparseIndexVector t
   (void)resolver;
   switch (type) {
     case SparseIndexVector_Int32Vector: {
-      auto ptr = reinterpret_cast<const tflite::Int32Vector *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::Int32Vector *>(obj);
       return ptr->UnPack(resolver);
     }
     case SparseIndexVector_Uint16Vector: {
-      auto ptr = reinterpret_cast<const tflite::Uint16Vector *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::Uint16Vector *>(obj);
       return ptr->UnPack(resolver);
     }
     case SparseIndexVector_Uint8Vector: {
-      auto ptr = reinterpret_cast<const tflite::Uint8Vector *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::Uint8Vector *>(obj);
       return ptr->UnPack(resolver);
     }
     default: return nullptr;
@@ -21356,15 +21356,15 @@ inline flatbuffers::Offset<void> SparseIndexVectorUnion::Pack(flatbuffers::FlatB
   (void)_rehasher;
   switch (type) {
     case SparseIndexVector_Int32Vector: {
-      auto ptr = reinterpret_cast<const tflite::Int32VectorT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::Int32VectorT *>(value);
       return CreateInt32Vector(_fbb, ptr, _rehasher).Union();
     }
     case SparseIndexVector_Uint16Vector: {
-      auto ptr = reinterpret_cast<const tflite::Uint16VectorT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::Uint16VectorT *>(value);
       return CreateUint16Vector(_fbb, ptr, _rehasher).Union();
     }
     case SparseIndexVector_Uint8Vector: {
-      auto ptr = reinterpret_cast<const tflite::Uint8VectorT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::Uint8VectorT *>(value);
       return CreateUint8Vector(_fbb, ptr, _rehasher).Union();
     }
     default: return 0;
@@ -21374,15 +21374,15 @@ inline flatbuffers::Offset<void> SparseIndexVectorUnion::Pack(flatbuffers::FlatB
 inline SparseIndexVectorUnion::SparseIndexVectorUnion(const SparseIndexVectorUnion &u) : type(u.type), value(nullptr) {
   switch (type) {
     case SparseIndexVector_Int32Vector: {
-      value = new tflite::Int32VectorT(*reinterpret_cast<tflite::Int32VectorT *>(u.value));
+      value = new tflite_micro::Int32VectorT(*reinterpret_cast<tflite_micro::Int32VectorT *>(u.value));
       break;
     }
     case SparseIndexVector_Uint16Vector: {
-      value = new tflite::Uint16VectorT(*reinterpret_cast<tflite::Uint16VectorT *>(u.value));
+      value = new tflite_micro::Uint16VectorT(*reinterpret_cast<tflite_micro::Uint16VectorT *>(u.value));
       break;
     }
     case SparseIndexVector_Uint8Vector: {
-      value = new tflite::Uint8VectorT(*reinterpret_cast<tflite::Uint8VectorT *>(u.value));
+      value = new tflite_micro::Uint8VectorT(*reinterpret_cast<tflite_micro::Uint8VectorT *>(u.value));
       break;
     }
     default:
@@ -21393,17 +21393,17 @@ inline SparseIndexVectorUnion::SparseIndexVectorUnion(const SparseIndexVectorUni
 inline void SparseIndexVectorUnion::Reset() {
   switch (type) {
     case SparseIndexVector_Int32Vector: {
-      auto ptr = reinterpret_cast<tflite::Int32VectorT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::Int32VectorT *>(value);
       delete ptr;
       break;
     }
     case SparseIndexVector_Uint16Vector: {
-      auto ptr = reinterpret_cast<tflite::Uint16VectorT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::Uint16VectorT *>(value);
       delete ptr;
       break;
     }
     case SparseIndexVector_Uint8Vector: {
-      auto ptr = reinterpret_cast<tflite::Uint8VectorT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::Uint8VectorT *>(value);
       delete ptr;
       break;
     }
@@ -21419,507 +21419,507 @@ inline bool VerifyBuiltinOptions(flatbuffers::Verifier &verifier, const void *ob
       return true;
     }
     case BuiltinOptions_Conv2DOptions: {
-      auto ptr = reinterpret_cast<const tflite::Conv2DOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::Conv2DOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_DepthwiseConv2DOptions: {
-      auto ptr = reinterpret_cast<const tflite::DepthwiseConv2DOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::DepthwiseConv2DOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_ConcatEmbeddingsOptions: {
-      auto ptr = reinterpret_cast<const tflite::ConcatEmbeddingsOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ConcatEmbeddingsOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_LSHProjectionOptions: {
-      auto ptr = reinterpret_cast<const tflite::LSHProjectionOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::LSHProjectionOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_Pool2DOptions: {
-      auto ptr = reinterpret_cast<const tflite::Pool2DOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::Pool2DOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_SVDFOptions: {
-      auto ptr = reinterpret_cast<const tflite::SVDFOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SVDFOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_RNNOptions: {
-      auto ptr = reinterpret_cast<const tflite::RNNOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::RNNOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_FullyConnectedOptions: {
-      auto ptr = reinterpret_cast<const tflite::FullyConnectedOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::FullyConnectedOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_SoftmaxOptions: {
-      auto ptr = reinterpret_cast<const tflite::SoftmaxOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SoftmaxOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_ConcatenationOptions: {
-      auto ptr = reinterpret_cast<const tflite::ConcatenationOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ConcatenationOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_AddOptions: {
-      auto ptr = reinterpret_cast<const tflite::AddOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::AddOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_L2NormOptions: {
-      auto ptr = reinterpret_cast<const tflite::L2NormOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::L2NormOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_LocalResponseNormalizationOptions: {
-      auto ptr = reinterpret_cast<const tflite::LocalResponseNormalizationOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::LocalResponseNormalizationOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_LSTMOptions: {
-      auto ptr = reinterpret_cast<const tflite::LSTMOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::LSTMOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_ResizeBilinearOptions: {
-      auto ptr = reinterpret_cast<const tflite::ResizeBilinearOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ResizeBilinearOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_CallOptions: {
-      auto ptr = reinterpret_cast<const tflite::CallOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::CallOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_ReshapeOptions: {
-      auto ptr = reinterpret_cast<const tflite::ReshapeOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ReshapeOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_SkipGramOptions: {
-      auto ptr = reinterpret_cast<const tflite::SkipGramOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SkipGramOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_SpaceToDepthOptions: {
-      auto ptr = reinterpret_cast<const tflite::SpaceToDepthOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SpaceToDepthOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_EmbeddingLookupSparseOptions: {
-      auto ptr = reinterpret_cast<const tflite::EmbeddingLookupSparseOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::EmbeddingLookupSparseOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_MulOptions: {
-      auto ptr = reinterpret_cast<const tflite::MulOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::MulOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_PadOptions: {
-      auto ptr = reinterpret_cast<const tflite::PadOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::PadOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_GatherOptions: {
-      auto ptr = reinterpret_cast<const tflite::GatherOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::GatherOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_BatchToSpaceNDOptions: {
-      auto ptr = reinterpret_cast<const tflite::BatchToSpaceNDOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::BatchToSpaceNDOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_SpaceToBatchNDOptions: {
-      auto ptr = reinterpret_cast<const tflite::SpaceToBatchNDOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SpaceToBatchNDOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_TransposeOptions: {
-      auto ptr = reinterpret_cast<const tflite::TransposeOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::TransposeOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_ReducerOptions: {
-      auto ptr = reinterpret_cast<const tflite::ReducerOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ReducerOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_SubOptions: {
-      auto ptr = reinterpret_cast<const tflite::SubOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SubOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_DivOptions: {
-      auto ptr = reinterpret_cast<const tflite::DivOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::DivOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_SqueezeOptions: {
-      auto ptr = reinterpret_cast<const tflite::SqueezeOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SqueezeOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_SequenceRNNOptions: {
-      auto ptr = reinterpret_cast<const tflite::SequenceRNNOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SequenceRNNOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_StridedSliceOptions: {
-      auto ptr = reinterpret_cast<const tflite::StridedSliceOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StridedSliceOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_ExpOptions: {
-      auto ptr = reinterpret_cast<const tflite::ExpOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ExpOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_TopKV2Options: {
-      auto ptr = reinterpret_cast<const tflite::TopKV2Options *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::TopKV2Options *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_SplitOptions: {
-      auto ptr = reinterpret_cast<const tflite::SplitOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SplitOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_LogSoftmaxOptions: {
-      auto ptr = reinterpret_cast<const tflite::LogSoftmaxOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::LogSoftmaxOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_CastOptions: {
-      auto ptr = reinterpret_cast<const tflite::CastOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::CastOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_DequantizeOptions: {
-      auto ptr = reinterpret_cast<const tflite::DequantizeOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::DequantizeOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_MaximumMinimumOptions: {
-      auto ptr = reinterpret_cast<const tflite::MaximumMinimumOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::MaximumMinimumOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_ArgMaxOptions: {
-      auto ptr = reinterpret_cast<const tflite::ArgMaxOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ArgMaxOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_LessOptions: {
-      auto ptr = reinterpret_cast<const tflite::LessOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::LessOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_NegOptions: {
-      auto ptr = reinterpret_cast<const tflite::NegOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::NegOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_PadV2Options: {
-      auto ptr = reinterpret_cast<const tflite::PadV2Options *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::PadV2Options *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_GreaterOptions: {
-      auto ptr = reinterpret_cast<const tflite::GreaterOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::GreaterOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_GreaterEqualOptions: {
-      auto ptr = reinterpret_cast<const tflite::GreaterEqualOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::GreaterEqualOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_LessEqualOptions: {
-      auto ptr = reinterpret_cast<const tflite::LessEqualOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::LessEqualOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_SelectOptions: {
-      auto ptr = reinterpret_cast<const tflite::SelectOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SelectOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_SliceOptions: {
-      auto ptr = reinterpret_cast<const tflite::SliceOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SliceOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_TransposeConvOptions: {
-      auto ptr = reinterpret_cast<const tflite::TransposeConvOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::TransposeConvOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_SparseToDenseOptions: {
-      auto ptr = reinterpret_cast<const tflite::SparseToDenseOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SparseToDenseOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_TileOptions: {
-      auto ptr = reinterpret_cast<const tflite::TileOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::TileOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_ExpandDimsOptions: {
-      auto ptr = reinterpret_cast<const tflite::ExpandDimsOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ExpandDimsOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_EqualOptions: {
-      auto ptr = reinterpret_cast<const tflite::EqualOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::EqualOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_NotEqualOptions: {
-      auto ptr = reinterpret_cast<const tflite::NotEqualOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::NotEqualOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_ShapeOptions: {
-      auto ptr = reinterpret_cast<const tflite::ShapeOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ShapeOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_PowOptions: {
-      auto ptr = reinterpret_cast<const tflite::PowOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::PowOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_ArgMinOptions: {
-      auto ptr = reinterpret_cast<const tflite::ArgMinOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ArgMinOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_FakeQuantOptions: {
-      auto ptr = reinterpret_cast<const tflite::FakeQuantOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::FakeQuantOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_PackOptions: {
-      auto ptr = reinterpret_cast<const tflite::PackOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::PackOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_LogicalOrOptions: {
-      auto ptr = reinterpret_cast<const tflite::LogicalOrOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::LogicalOrOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_OneHotOptions: {
-      auto ptr = reinterpret_cast<const tflite::OneHotOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::OneHotOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_LogicalAndOptions: {
-      auto ptr = reinterpret_cast<const tflite::LogicalAndOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::LogicalAndOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_LogicalNotOptions: {
-      auto ptr = reinterpret_cast<const tflite::LogicalNotOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::LogicalNotOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_UnpackOptions: {
-      auto ptr = reinterpret_cast<const tflite::UnpackOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::UnpackOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_FloorDivOptions: {
-      auto ptr = reinterpret_cast<const tflite::FloorDivOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::FloorDivOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_SquareOptions: {
-      auto ptr = reinterpret_cast<const tflite::SquareOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SquareOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_ZerosLikeOptions: {
-      auto ptr = reinterpret_cast<const tflite::ZerosLikeOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ZerosLikeOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_FillOptions: {
-      auto ptr = reinterpret_cast<const tflite::FillOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::FillOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_BidirectionalSequenceLSTMOptions: {
-      auto ptr = reinterpret_cast<const tflite::BidirectionalSequenceLSTMOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::BidirectionalSequenceLSTMOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_BidirectionalSequenceRNNOptions: {
-      auto ptr = reinterpret_cast<const tflite::BidirectionalSequenceRNNOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::BidirectionalSequenceRNNOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_UnidirectionalSequenceLSTMOptions: {
-      auto ptr = reinterpret_cast<const tflite::UnidirectionalSequenceLSTMOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::UnidirectionalSequenceLSTMOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_FloorModOptions: {
-      auto ptr = reinterpret_cast<const tflite::FloorModOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::FloorModOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_RangeOptions: {
-      auto ptr = reinterpret_cast<const tflite::RangeOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::RangeOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_ResizeNearestNeighborOptions: {
-      auto ptr = reinterpret_cast<const tflite::ResizeNearestNeighborOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ResizeNearestNeighborOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_LeakyReluOptions: {
-      auto ptr = reinterpret_cast<const tflite::LeakyReluOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::LeakyReluOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_SquaredDifferenceOptions: {
-      auto ptr = reinterpret_cast<const tflite::SquaredDifferenceOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SquaredDifferenceOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_MirrorPadOptions: {
-      auto ptr = reinterpret_cast<const tflite::MirrorPadOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::MirrorPadOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_AbsOptions: {
-      auto ptr = reinterpret_cast<const tflite::AbsOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::AbsOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_SplitVOptions: {
-      auto ptr = reinterpret_cast<const tflite::SplitVOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SplitVOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_UniqueOptions: {
-      auto ptr = reinterpret_cast<const tflite::UniqueOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::UniqueOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_ReverseV2Options: {
-      auto ptr = reinterpret_cast<const tflite::ReverseV2Options *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ReverseV2Options *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_AddNOptions: {
-      auto ptr = reinterpret_cast<const tflite::AddNOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::AddNOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_GatherNdOptions: {
-      auto ptr = reinterpret_cast<const tflite::GatherNdOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::GatherNdOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_CosOptions: {
-      auto ptr = reinterpret_cast<const tflite::CosOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::CosOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_WhereOptions: {
-      auto ptr = reinterpret_cast<const tflite::WhereOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::WhereOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_RankOptions: {
-      auto ptr = reinterpret_cast<const tflite::RankOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::RankOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_ReverseSequenceOptions: {
-      auto ptr = reinterpret_cast<const tflite::ReverseSequenceOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ReverseSequenceOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_MatrixDiagOptions: {
-      auto ptr = reinterpret_cast<const tflite::MatrixDiagOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::MatrixDiagOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_QuantizeOptions: {
-      auto ptr = reinterpret_cast<const tflite::QuantizeOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::QuantizeOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_MatrixSetDiagOptions: {
-      auto ptr = reinterpret_cast<const tflite::MatrixSetDiagOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::MatrixSetDiagOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_HardSwishOptions: {
-      auto ptr = reinterpret_cast<const tflite::HardSwishOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::HardSwishOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_IfOptions: {
-      auto ptr = reinterpret_cast<const tflite::IfOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::IfOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_WhileOptions: {
-      auto ptr = reinterpret_cast<const tflite::WhileOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::WhileOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_DepthToSpaceOptions: {
-      auto ptr = reinterpret_cast<const tflite::DepthToSpaceOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::DepthToSpaceOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_NonMaxSuppressionV4Options: {
-      auto ptr = reinterpret_cast<const tflite::NonMaxSuppressionV4Options *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::NonMaxSuppressionV4Options *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_NonMaxSuppressionV5Options: {
-      auto ptr = reinterpret_cast<const tflite::NonMaxSuppressionV5Options *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::NonMaxSuppressionV5Options *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_ScatterNdOptions: {
-      auto ptr = reinterpret_cast<const tflite::ScatterNdOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ScatterNdOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_SelectV2Options: {
-      auto ptr = reinterpret_cast<const tflite::SelectV2Options *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SelectV2Options *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_DensifyOptions: {
-      auto ptr = reinterpret_cast<const tflite::DensifyOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::DensifyOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_SegmentSumOptions: {
-      auto ptr = reinterpret_cast<const tflite::SegmentSumOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SegmentSumOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_BatchMatMulOptions: {
-      auto ptr = reinterpret_cast<const tflite::BatchMatMulOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::BatchMatMulOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_CumsumOptions: {
-      auto ptr = reinterpret_cast<const tflite::CumsumOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::CumsumOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_CallOnceOptions: {
-      auto ptr = reinterpret_cast<const tflite::CallOnceOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::CallOnceOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_BroadcastToOptions: {
-      auto ptr = reinterpret_cast<const tflite::BroadcastToOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::BroadcastToOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_Rfft2dOptions: {
-      auto ptr = reinterpret_cast<const tflite::Rfft2dOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::Rfft2dOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_Conv3DOptions: {
-      auto ptr = reinterpret_cast<const tflite::Conv3DOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::Conv3DOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_HashtableOptions: {
-      auto ptr = reinterpret_cast<const tflite::HashtableOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::HashtableOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_HashtableFindOptions: {
-      auto ptr = reinterpret_cast<const tflite::HashtableFindOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::HashtableFindOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_HashtableImportOptions: {
-      auto ptr = reinterpret_cast<const tflite::HashtableImportOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::HashtableImportOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_HashtableSizeOptions: {
-      auto ptr = reinterpret_cast<const tflite::HashtableSizeOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::HashtableSizeOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_VarHandleOptions: {
-      auto ptr = reinterpret_cast<const tflite::VarHandleOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::VarHandleOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_ReadVariableOptions: {
-      auto ptr = reinterpret_cast<const tflite::ReadVariableOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ReadVariableOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_AssignVariableOptions: {
-      auto ptr = reinterpret_cast<const tflite::AssignVariableOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::AssignVariableOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_RandomOptions: {
-      auto ptr = reinterpret_cast<const tflite::RandomOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::RandomOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_BucketizeOptions: {
-      auto ptr = reinterpret_cast<const tflite::BucketizeOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::BucketizeOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_GeluOptions: {
-      auto ptr = reinterpret_cast<const tflite::GeluOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::GeluOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_DynamicUpdateSliceOptions: {
-      auto ptr = reinterpret_cast<const tflite::DynamicUpdateSliceOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::DynamicUpdateSliceOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_UnsortedSegmentProdOptions: {
-      auto ptr = reinterpret_cast<const tflite::UnsortedSegmentProdOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::UnsortedSegmentProdOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_UnsortedSegmentMaxOptions: {
-      auto ptr = reinterpret_cast<const tflite::UnsortedSegmentMaxOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::UnsortedSegmentMaxOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_UnsortedSegmentMinOptions: {
-      auto ptr = reinterpret_cast<const tflite::UnsortedSegmentMinOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::UnsortedSegmentMinOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_UnsortedSegmentSumOptions: {
-      auto ptr = reinterpret_cast<const tflite::UnsortedSegmentSumOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::UnsortedSegmentSumOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_ATan2Options: {
-      auto ptr = reinterpret_cast<const tflite::ATan2Options *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ATan2Options *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_SignOptions: {
-      auto ptr = reinterpret_cast<const tflite::SignOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SignOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_BitcastOptions: {
-      auto ptr = reinterpret_cast<const tflite::BitcastOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::BitcastOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_BitwiseXorOptions: {
-      auto ptr = reinterpret_cast<const tflite::BitwiseXorOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::BitwiseXorOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions_RightShiftOptions: {
-      auto ptr = reinterpret_cast<const tflite::RightShiftOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::RightShiftOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     default: return true;
@@ -21942,507 +21942,507 @@ inline void *BuiltinOptionsUnion::UnPack(const void *obj, BuiltinOptions type, c
   (void)resolver;
   switch (type) {
     case BuiltinOptions_Conv2DOptions: {
-      auto ptr = reinterpret_cast<const tflite::Conv2DOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::Conv2DOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_DepthwiseConv2DOptions: {
-      auto ptr = reinterpret_cast<const tflite::DepthwiseConv2DOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::DepthwiseConv2DOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_ConcatEmbeddingsOptions: {
-      auto ptr = reinterpret_cast<const tflite::ConcatEmbeddingsOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ConcatEmbeddingsOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_LSHProjectionOptions: {
-      auto ptr = reinterpret_cast<const tflite::LSHProjectionOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::LSHProjectionOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_Pool2DOptions: {
-      auto ptr = reinterpret_cast<const tflite::Pool2DOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::Pool2DOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_SVDFOptions: {
-      auto ptr = reinterpret_cast<const tflite::SVDFOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SVDFOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_RNNOptions: {
-      auto ptr = reinterpret_cast<const tflite::RNNOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::RNNOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_FullyConnectedOptions: {
-      auto ptr = reinterpret_cast<const tflite::FullyConnectedOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::FullyConnectedOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_SoftmaxOptions: {
-      auto ptr = reinterpret_cast<const tflite::SoftmaxOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SoftmaxOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_ConcatenationOptions: {
-      auto ptr = reinterpret_cast<const tflite::ConcatenationOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ConcatenationOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_AddOptions: {
-      auto ptr = reinterpret_cast<const tflite::AddOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::AddOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_L2NormOptions: {
-      auto ptr = reinterpret_cast<const tflite::L2NormOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::L2NormOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_LocalResponseNormalizationOptions: {
-      auto ptr = reinterpret_cast<const tflite::LocalResponseNormalizationOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::LocalResponseNormalizationOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_LSTMOptions: {
-      auto ptr = reinterpret_cast<const tflite::LSTMOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::LSTMOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_ResizeBilinearOptions: {
-      auto ptr = reinterpret_cast<const tflite::ResizeBilinearOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ResizeBilinearOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_CallOptions: {
-      auto ptr = reinterpret_cast<const tflite::CallOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::CallOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_ReshapeOptions: {
-      auto ptr = reinterpret_cast<const tflite::ReshapeOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ReshapeOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_SkipGramOptions: {
-      auto ptr = reinterpret_cast<const tflite::SkipGramOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SkipGramOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_SpaceToDepthOptions: {
-      auto ptr = reinterpret_cast<const tflite::SpaceToDepthOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SpaceToDepthOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_EmbeddingLookupSparseOptions: {
-      auto ptr = reinterpret_cast<const tflite::EmbeddingLookupSparseOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::EmbeddingLookupSparseOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_MulOptions: {
-      auto ptr = reinterpret_cast<const tflite::MulOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::MulOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_PadOptions: {
-      auto ptr = reinterpret_cast<const tflite::PadOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::PadOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_GatherOptions: {
-      auto ptr = reinterpret_cast<const tflite::GatherOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::GatherOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_BatchToSpaceNDOptions: {
-      auto ptr = reinterpret_cast<const tflite::BatchToSpaceNDOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::BatchToSpaceNDOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_SpaceToBatchNDOptions: {
-      auto ptr = reinterpret_cast<const tflite::SpaceToBatchNDOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SpaceToBatchNDOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_TransposeOptions: {
-      auto ptr = reinterpret_cast<const tflite::TransposeOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::TransposeOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_ReducerOptions: {
-      auto ptr = reinterpret_cast<const tflite::ReducerOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ReducerOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_SubOptions: {
-      auto ptr = reinterpret_cast<const tflite::SubOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SubOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_DivOptions: {
-      auto ptr = reinterpret_cast<const tflite::DivOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::DivOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_SqueezeOptions: {
-      auto ptr = reinterpret_cast<const tflite::SqueezeOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SqueezeOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_SequenceRNNOptions: {
-      auto ptr = reinterpret_cast<const tflite::SequenceRNNOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SequenceRNNOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_StridedSliceOptions: {
-      auto ptr = reinterpret_cast<const tflite::StridedSliceOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StridedSliceOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_ExpOptions: {
-      auto ptr = reinterpret_cast<const tflite::ExpOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ExpOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_TopKV2Options: {
-      auto ptr = reinterpret_cast<const tflite::TopKV2Options *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::TopKV2Options *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_SplitOptions: {
-      auto ptr = reinterpret_cast<const tflite::SplitOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SplitOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_LogSoftmaxOptions: {
-      auto ptr = reinterpret_cast<const tflite::LogSoftmaxOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::LogSoftmaxOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_CastOptions: {
-      auto ptr = reinterpret_cast<const tflite::CastOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::CastOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_DequantizeOptions: {
-      auto ptr = reinterpret_cast<const tflite::DequantizeOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::DequantizeOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_MaximumMinimumOptions: {
-      auto ptr = reinterpret_cast<const tflite::MaximumMinimumOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::MaximumMinimumOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_ArgMaxOptions: {
-      auto ptr = reinterpret_cast<const tflite::ArgMaxOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ArgMaxOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_LessOptions: {
-      auto ptr = reinterpret_cast<const tflite::LessOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::LessOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_NegOptions: {
-      auto ptr = reinterpret_cast<const tflite::NegOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::NegOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_PadV2Options: {
-      auto ptr = reinterpret_cast<const tflite::PadV2Options *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::PadV2Options *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_GreaterOptions: {
-      auto ptr = reinterpret_cast<const tflite::GreaterOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::GreaterOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_GreaterEqualOptions: {
-      auto ptr = reinterpret_cast<const tflite::GreaterEqualOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::GreaterEqualOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_LessEqualOptions: {
-      auto ptr = reinterpret_cast<const tflite::LessEqualOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::LessEqualOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_SelectOptions: {
-      auto ptr = reinterpret_cast<const tflite::SelectOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SelectOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_SliceOptions: {
-      auto ptr = reinterpret_cast<const tflite::SliceOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SliceOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_TransposeConvOptions: {
-      auto ptr = reinterpret_cast<const tflite::TransposeConvOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::TransposeConvOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_SparseToDenseOptions: {
-      auto ptr = reinterpret_cast<const tflite::SparseToDenseOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SparseToDenseOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_TileOptions: {
-      auto ptr = reinterpret_cast<const tflite::TileOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::TileOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_ExpandDimsOptions: {
-      auto ptr = reinterpret_cast<const tflite::ExpandDimsOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ExpandDimsOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_EqualOptions: {
-      auto ptr = reinterpret_cast<const tflite::EqualOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::EqualOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_NotEqualOptions: {
-      auto ptr = reinterpret_cast<const tflite::NotEqualOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::NotEqualOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_ShapeOptions: {
-      auto ptr = reinterpret_cast<const tflite::ShapeOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ShapeOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_PowOptions: {
-      auto ptr = reinterpret_cast<const tflite::PowOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::PowOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_ArgMinOptions: {
-      auto ptr = reinterpret_cast<const tflite::ArgMinOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ArgMinOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_FakeQuantOptions: {
-      auto ptr = reinterpret_cast<const tflite::FakeQuantOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::FakeQuantOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_PackOptions: {
-      auto ptr = reinterpret_cast<const tflite::PackOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::PackOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_LogicalOrOptions: {
-      auto ptr = reinterpret_cast<const tflite::LogicalOrOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::LogicalOrOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_OneHotOptions: {
-      auto ptr = reinterpret_cast<const tflite::OneHotOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::OneHotOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_LogicalAndOptions: {
-      auto ptr = reinterpret_cast<const tflite::LogicalAndOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::LogicalAndOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_LogicalNotOptions: {
-      auto ptr = reinterpret_cast<const tflite::LogicalNotOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::LogicalNotOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_UnpackOptions: {
-      auto ptr = reinterpret_cast<const tflite::UnpackOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::UnpackOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_FloorDivOptions: {
-      auto ptr = reinterpret_cast<const tflite::FloorDivOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::FloorDivOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_SquareOptions: {
-      auto ptr = reinterpret_cast<const tflite::SquareOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SquareOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_ZerosLikeOptions: {
-      auto ptr = reinterpret_cast<const tflite::ZerosLikeOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ZerosLikeOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_FillOptions: {
-      auto ptr = reinterpret_cast<const tflite::FillOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::FillOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_BidirectionalSequenceLSTMOptions: {
-      auto ptr = reinterpret_cast<const tflite::BidirectionalSequenceLSTMOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::BidirectionalSequenceLSTMOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_BidirectionalSequenceRNNOptions: {
-      auto ptr = reinterpret_cast<const tflite::BidirectionalSequenceRNNOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::BidirectionalSequenceRNNOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_UnidirectionalSequenceLSTMOptions: {
-      auto ptr = reinterpret_cast<const tflite::UnidirectionalSequenceLSTMOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::UnidirectionalSequenceLSTMOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_FloorModOptions: {
-      auto ptr = reinterpret_cast<const tflite::FloorModOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::FloorModOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_RangeOptions: {
-      auto ptr = reinterpret_cast<const tflite::RangeOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::RangeOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_ResizeNearestNeighborOptions: {
-      auto ptr = reinterpret_cast<const tflite::ResizeNearestNeighborOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ResizeNearestNeighborOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_LeakyReluOptions: {
-      auto ptr = reinterpret_cast<const tflite::LeakyReluOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::LeakyReluOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_SquaredDifferenceOptions: {
-      auto ptr = reinterpret_cast<const tflite::SquaredDifferenceOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SquaredDifferenceOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_MirrorPadOptions: {
-      auto ptr = reinterpret_cast<const tflite::MirrorPadOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::MirrorPadOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_AbsOptions: {
-      auto ptr = reinterpret_cast<const tflite::AbsOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::AbsOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_SplitVOptions: {
-      auto ptr = reinterpret_cast<const tflite::SplitVOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SplitVOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_UniqueOptions: {
-      auto ptr = reinterpret_cast<const tflite::UniqueOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::UniqueOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_ReverseV2Options: {
-      auto ptr = reinterpret_cast<const tflite::ReverseV2Options *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ReverseV2Options *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_AddNOptions: {
-      auto ptr = reinterpret_cast<const tflite::AddNOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::AddNOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_GatherNdOptions: {
-      auto ptr = reinterpret_cast<const tflite::GatherNdOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::GatherNdOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_CosOptions: {
-      auto ptr = reinterpret_cast<const tflite::CosOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::CosOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_WhereOptions: {
-      auto ptr = reinterpret_cast<const tflite::WhereOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::WhereOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_RankOptions: {
-      auto ptr = reinterpret_cast<const tflite::RankOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::RankOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_ReverseSequenceOptions: {
-      auto ptr = reinterpret_cast<const tflite::ReverseSequenceOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ReverseSequenceOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_MatrixDiagOptions: {
-      auto ptr = reinterpret_cast<const tflite::MatrixDiagOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::MatrixDiagOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_QuantizeOptions: {
-      auto ptr = reinterpret_cast<const tflite::QuantizeOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::QuantizeOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_MatrixSetDiagOptions: {
-      auto ptr = reinterpret_cast<const tflite::MatrixSetDiagOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::MatrixSetDiagOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_HardSwishOptions: {
-      auto ptr = reinterpret_cast<const tflite::HardSwishOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::HardSwishOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_IfOptions: {
-      auto ptr = reinterpret_cast<const tflite::IfOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::IfOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_WhileOptions: {
-      auto ptr = reinterpret_cast<const tflite::WhileOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::WhileOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_DepthToSpaceOptions: {
-      auto ptr = reinterpret_cast<const tflite::DepthToSpaceOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::DepthToSpaceOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_NonMaxSuppressionV4Options: {
-      auto ptr = reinterpret_cast<const tflite::NonMaxSuppressionV4Options *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::NonMaxSuppressionV4Options *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_NonMaxSuppressionV5Options: {
-      auto ptr = reinterpret_cast<const tflite::NonMaxSuppressionV5Options *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::NonMaxSuppressionV5Options *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_ScatterNdOptions: {
-      auto ptr = reinterpret_cast<const tflite::ScatterNdOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ScatterNdOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_SelectV2Options: {
-      auto ptr = reinterpret_cast<const tflite::SelectV2Options *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SelectV2Options *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_DensifyOptions: {
-      auto ptr = reinterpret_cast<const tflite::DensifyOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::DensifyOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_SegmentSumOptions: {
-      auto ptr = reinterpret_cast<const tflite::SegmentSumOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SegmentSumOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_BatchMatMulOptions: {
-      auto ptr = reinterpret_cast<const tflite::BatchMatMulOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::BatchMatMulOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_CumsumOptions: {
-      auto ptr = reinterpret_cast<const tflite::CumsumOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::CumsumOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_CallOnceOptions: {
-      auto ptr = reinterpret_cast<const tflite::CallOnceOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::CallOnceOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_BroadcastToOptions: {
-      auto ptr = reinterpret_cast<const tflite::BroadcastToOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::BroadcastToOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_Rfft2dOptions: {
-      auto ptr = reinterpret_cast<const tflite::Rfft2dOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::Rfft2dOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_Conv3DOptions: {
-      auto ptr = reinterpret_cast<const tflite::Conv3DOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::Conv3DOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_HashtableOptions: {
-      auto ptr = reinterpret_cast<const tflite::HashtableOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::HashtableOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_HashtableFindOptions: {
-      auto ptr = reinterpret_cast<const tflite::HashtableFindOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::HashtableFindOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_HashtableImportOptions: {
-      auto ptr = reinterpret_cast<const tflite::HashtableImportOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::HashtableImportOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_HashtableSizeOptions: {
-      auto ptr = reinterpret_cast<const tflite::HashtableSizeOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::HashtableSizeOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_VarHandleOptions: {
-      auto ptr = reinterpret_cast<const tflite::VarHandleOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::VarHandleOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_ReadVariableOptions: {
-      auto ptr = reinterpret_cast<const tflite::ReadVariableOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ReadVariableOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_AssignVariableOptions: {
-      auto ptr = reinterpret_cast<const tflite::AssignVariableOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::AssignVariableOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_RandomOptions: {
-      auto ptr = reinterpret_cast<const tflite::RandomOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::RandomOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_BucketizeOptions: {
-      auto ptr = reinterpret_cast<const tflite::BucketizeOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::BucketizeOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_GeluOptions: {
-      auto ptr = reinterpret_cast<const tflite::GeluOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::GeluOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_DynamicUpdateSliceOptions: {
-      auto ptr = reinterpret_cast<const tflite::DynamicUpdateSliceOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::DynamicUpdateSliceOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_UnsortedSegmentProdOptions: {
-      auto ptr = reinterpret_cast<const tflite::UnsortedSegmentProdOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::UnsortedSegmentProdOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_UnsortedSegmentMaxOptions: {
-      auto ptr = reinterpret_cast<const tflite::UnsortedSegmentMaxOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::UnsortedSegmentMaxOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_UnsortedSegmentMinOptions: {
-      auto ptr = reinterpret_cast<const tflite::UnsortedSegmentMinOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::UnsortedSegmentMinOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_UnsortedSegmentSumOptions: {
-      auto ptr = reinterpret_cast<const tflite::UnsortedSegmentSumOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::UnsortedSegmentSumOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_ATan2Options: {
-      auto ptr = reinterpret_cast<const tflite::ATan2Options *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ATan2Options *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_SignOptions: {
-      auto ptr = reinterpret_cast<const tflite::SignOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::SignOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_BitcastOptions: {
-      auto ptr = reinterpret_cast<const tflite::BitcastOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::BitcastOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_BitwiseXorOptions: {
-      auto ptr = reinterpret_cast<const tflite::BitwiseXorOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::BitwiseXorOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions_RightShiftOptions: {
-      auto ptr = reinterpret_cast<const tflite::RightShiftOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::RightShiftOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     default: return nullptr;
@@ -22453,507 +22453,507 @@ inline flatbuffers::Offset<void> BuiltinOptionsUnion::Pack(flatbuffers::FlatBuff
   (void)_rehasher;
   switch (type) {
     case BuiltinOptions_Conv2DOptions: {
-      auto ptr = reinterpret_cast<const tflite::Conv2DOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::Conv2DOptionsT *>(value);
       return CreateConv2DOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_DepthwiseConv2DOptions: {
-      auto ptr = reinterpret_cast<const tflite::DepthwiseConv2DOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::DepthwiseConv2DOptionsT *>(value);
       return CreateDepthwiseConv2DOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_ConcatEmbeddingsOptions: {
-      auto ptr = reinterpret_cast<const tflite::ConcatEmbeddingsOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::ConcatEmbeddingsOptionsT *>(value);
       return CreateConcatEmbeddingsOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_LSHProjectionOptions: {
-      auto ptr = reinterpret_cast<const tflite::LSHProjectionOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::LSHProjectionOptionsT *>(value);
       return CreateLSHProjectionOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_Pool2DOptions: {
-      auto ptr = reinterpret_cast<const tflite::Pool2DOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::Pool2DOptionsT *>(value);
       return CreatePool2DOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_SVDFOptions: {
-      auto ptr = reinterpret_cast<const tflite::SVDFOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::SVDFOptionsT *>(value);
       return CreateSVDFOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_RNNOptions: {
-      auto ptr = reinterpret_cast<const tflite::RNNOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::RNNOptionsT *>(value);
       return CreateRNNOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_FullyConnectedOptions: {
-      auto ptr = reinterpret_cast<const tflite::FullyConnectedOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::FullyConnectedOptionsT *>(value);
       return CreateFullyConnectedOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_SoftmaxOptions: {
-      auto ptr = reinterpret_cast<const tflite::SoftmaxOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::SoftmaxOptionsT *>(value);
       return CreateSoftmaxOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_ConcatenationOptions: {
-      auto ptr = reinterpret_cast<const tflite::ConcatenationOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::ConcatenationOptionsT *>(value);
       return CreateConcatenationOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_AddOptions: {
-      auto ptr = reinterpret_cast<const tflite::AddOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::AddOptionsT *>(value);
       return CreateAddOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_L2NormOptions: {
-      auto ptr = reinterpret_cast<const tflite::L2NormOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::L2NormOptionsT *>(value);
       return CreateL2NormOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_LocalResponseNormalizationOptions: {
-      auto ptr = reinterpret_cast<const tflite::LocalResponseNormalizationOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::LocalResponseNormalizationOptionsT *>(value);
       return CreateLocalResponseNormalizationOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_LSTMOptions: {
-      auto ptr = reinterpret_cast<const tflite::LSTMOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::LSTMOptionsT *>(value);
       return CreateLSTMOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_ResizeBilinearOptions: {
-      auto ptr = reinterpret_cast<const tflite::ResizeBilinearOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::ResizeBilinearOptionsT *>(value);
       return CreateResizeBilinearOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_CallOptions: {
-      auto ptr = reinterpret_cast<const tflite::CallOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::CallOptionsT *>(value);
       return CreateCallOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_ReshapeOptions: {
-      auto ptr = reinterpret_cast<const tflite::ReshapeOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::ReshapeOptionsT *>(value);
       return CreateReshapeOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_SkipGramOptions: {
-      auto ptr = reinterpret_cast<const tflite::SkipGramOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::SkipGramOptionsT *>(value);
       return CreateSkipGramOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_SpaceToDepthOptions: {
-      auto ptr = reinterpret_cast<const tflite::SpaceToDepthOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::SpaceToDepthOptionsT *>(value);
       return CreateSpaceToDepthOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_EmbeddingLookupSparseOptions: {
-      auto ptr = reinterpret_cast<const tflite::EmbeddingLookupSparseOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::EmbeddingLookupSparseOptionsT *>(value);
       return CreateEmbeddingLookupSparseOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_MulOptions: {
-      auto ptr = reinterpret_cast<const tflite::MulOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::MulOptionsT *>(value);
       return CreateMulOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_PadOptions: {
-      auto ptr = reinterpret_cast<const tflite::PadOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::PadOptionsT *>(value);
       return CreatePadOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_GatherOptions: {
-      auto ptr = reinterpret_cast<const tflite::GatherOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::GatherOptionsT *>(value);
       return CreateGatherOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_BatchToSpaceNDOptions: {
-      auto ptr = reinterpret_cast<const tflite::BatchToSpaceNDOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::BatchToSpaceNDOptionsT *>(value);
       return CreateBatchToSpaceNDOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_SpaceToBatchNDOptions: {
-      auto ptr = reinterpret_cast<const tflite::SpaceToBatchNDOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::SpaceToBatchNDOptionsT *>(value);
       return CreateSpaceToBatchNDOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_TransposeOptions: {
-      auto ptr = reinterpret_cast<const tflite::TransposeOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::TransposeOptionsT *>(value);
       return CreateTransposeOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_ReducerOptions: {
-      auto ptr = reinterpret_cast<const tflite::ReducerOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::ReducerOptionsT *>(value);
       return CreateReducerOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_SubOptions: {
-      auto ptr = reinterpret_cast<const tflite::SubOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::SubOptionsT *>(value);
       return CreateSubOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_DivOptions: {
-      auto ptr = reinterpret_cast<const tflite::DivOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::DivOptionsT *>(value);
       return CreateDivOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_SqueezeOptions: {
-      auto ptr = reinterpret_cast<const tflite::SqueezeOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::SqueezeOptionsT *>(value);
       return CreateSqueezeOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_SequenceRNNOptions: {
-      auto ptr = reinterpret_cast<const tflite::SequenceRNNOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::SequenceRNNOptionsT *>(value);
       return CreateSequenceRNNOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_StridedSliceOptions: {
-      auto ptr = reinterpret_cast<const tflite::StridedSliceOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::StridedSliceOptionsT *>(value);
       return CreateStridedSliceOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_ExpOptions: {
-      auto ptr = reinterpret_cast<const tflite::ExpOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::ExpOptionsT *>(value);
       return CreateExpOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_TopKV2Options: {
-      auto ptr = reinterpret_cast<const tflite::TopKV2OptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::TopKV2OptionsT *>(value);
       return CreateTopKV2Options(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_SplitOptions: {
-      auto ptr = reinterpret_cast<const tflite::SplitOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::SplitOptionsT *>(value);
       return CreateSplitOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_LogSoftmaxOptions: {
-      auto ptr = reinterpret_cast<const tflite::LogSoftmaxOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::LogSoftmaxOptionsT *>(value);
       return CreateLogSoftmaxOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_CastOptions: {
-      auto ptr = reinterpret_cast<const tflite::CastOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::CastOptionsT *>(value);
       return CreateCastOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_DequantizeOptions: {
-      auto ptr = reinterpret_cast<const tflite::DequantizeOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::DequantizeOptionsT *>(value);
       return CreateDequantizeOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_MaximumMinimumOptions: {
-      auto ptr = reinterpret_cast<const tflite::MaximumMinimumOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::MaximumMinimumOptionsT *>(value);
       return CreateMaximumMinimumOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_ArgMaxOptions: {
-      auto ptr = reinterpret_cast<const tflite::ArgMaxOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::ArgMaxOptionsT *>(value);
       return CreateArgMaxOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_LessOptions: {
-      auto ptr = reinterpret_cast<const tflite::LessOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::LessOptionsT *>(value);
       return CreateLessOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_NegOptions: {
-      auto ptr = reinterpret_cast<const tflite::NegOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::NegOptionsT *>(value);
       return CreateNegOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_PadV2Options: {
-      auto ptr = reinterpret_cast<const tflite::PadV2OptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::PadV2OptionsT *>(value);
       return CreatePadV2Options(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_GreaterOptions: {
-      auto ptr = reinterpret_cast<const tflite::GreaterOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::GreaterOptionsT *>(value);
       return CreateGreaterOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_GreaterEqualOptions: {
-      auto ptr = reinterpret_cast<const tflite::GreaterEqualOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::GreaterEqualOptionsT *>(value);
       return CreateGreaterEqualOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_LessEqualOptions: {
-      auto ptr = reinterpret_cast<const tflite::LessEqualOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::LessEqualOptionsT *>(value);
       return CreateLessEqualOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_SelectOptions: {
-      auto ptr = reinterpret_cast<const tflite::SelectOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::SelectOptionsT *>(value);
       return CreateSelectOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_SliceOptions: {
-      auto ptr = reinterpret_cast<const tflite::SliceOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::SliceOptionsT *>(value);
       return CreateSliceOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_TransposeConvOptions: {
-      auto ptr = reinterpret_cast<const tflite::TransposeConvOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::TransposeConvOptionsT *>(value);
       return CreateTransposeConvOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_SparseToDenseOptions: {
-      auto ptr = reinterpret_cast<const tflite::SparseToDenseOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::SparseToDenseOptionsT *>(value);
       return CreateSparseToDenseOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_TileOptions: {
-      auto ptr = reinterpret_cast<const tflite::TileOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::TileOptionsT *>(value);
       return CreateTileOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_ExpandDimsOptions: {
-      auto ptr = reinterpret_cast<const tflite::ExpandDimsOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::ExpandDimsOptionsT *>(value);
       return CreateExpandDimsOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_EqualOptions: {
-      auto ptr = reinterpret_cast<const tflite::EqualOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::EqualOptionsT *>(value);
       return CreateEqualOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_NotEqualOptions: {
-      auto ptr = reinterpret_cast<const tflite::NotEqualOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::NotEqualOptionsT *>(value);
       return CreateNotEqualOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_ShapeOptions: {
-      auto ptr = reinterpret_cast<const tflite::ShapeOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::ShapeOptionsT *>(value);
       return CreateShapeOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_PowOptions: {
-      auto ptr = reinterpret_cast<const tflite::PowOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::PowOptionsT *>(value);
       return CreatePowOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_ArgMinOptions: {
-      auto ptr = reinterpret_cast<const tflite::ArgMinOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::ArgMinOptionsT *>(value);
       return CreateArgMinOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_FakeQuantOptions: {
-      auto ptr = reinterpret_cast<const tflite::FakeQuantOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::FakeQuantOptionsT *>(value);
       return CreateFakeQuantOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_PackOptions: {
-      auto ptr = reinterpret_cast<const tflite::PackOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::PackOptionsT *>(value);
       return CreatePackOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_LogicalOrOptions: {
-      auto ptr = reinterpret_cast<const tflite::LogicalOrOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::LogicalOrOptionsT *>(value);
       return CreateLogicalOrOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_OneHotOptions: {
-      auto ptr = reinterpret_cast<const tflite::OneHotOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::OneHotOptionsT *>(value);
       return CreateOneHotOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_LogicalAndOptions: {
-      auto ptr = reinterpret_cast<const tflite::LogicalAndOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::LogicalAndOptionsT *>(value);
       return CreateLogicalAndOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_LogicalNotOptions: {
-      auto ptr = reinterpret_cast<const tflite::LogicalNotOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::LogicalNotOptionsT *>(value);
       return CreateLogicalNotOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_UnpackOptions: {
-      auto ptr = reinterpret_cast<const tflite::UnpackOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::UnpackOptionsT *>(value);
       return CreateUnpackOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_FloorDivOptions: {
-      auto ptr = reinterpret_cast<const tflite::FloorDivOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::FloorDivOptionsT *>(value);
       return CreateFloorDivOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_SquareOptions: {
-      auto ptr = reinterpret_cast<const tflite::SquareOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::SquareOptionsT *>(value);
       return CreateSquareOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_ZerosLikeOptions: {
-      auto ptr = reinterpret_cast<const tflite::ZerosLikeOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::ZerosLikeOptionsT *>(value);
       return CreateZerosLikeOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_FillOptions: {
-      auto ptr = reinterpret_cast<const tflite::FillOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::FillOptionsT *>(value);
       return CreateFillOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_BidirectionalSequenceLSTMOptions: {
-      auto ptr = reinterpret_cast<const tflite::BidirectionalSequenceLSTMOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::BidirectionalSequenceLSTMOptionsT *>(value);
       return CreateBidirectionalSequenceLSTMOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_BidirectionalSequenceRNNOptions: {
-      auto ptr = reinterpret_cast<const tflite::BidirectionalSequenceRNNOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::BidirectionalSequenceRNNOptionsT *>(value);
       return CreateBidirectionalSequenceRNNOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_UnidirectionalSequenceLSTMOptions: {
-      auto ptr = reinterpret_cast<const tflite::UnidirectionalSequenceLSTMOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::UnidirectionalSequenceLSTMOptionsT *>(value);
       return CreateUnidirectionalSequenceLSTMOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_FloorModOptions: {
-      auto ptr = reinterpret_cast<const tflite::FloorModOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::FloorModOptionsT *>(value);
       return CreateFloorModOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_RangeOptions: {
-      auto ptr = reinterpret_cast<const tflite::RangeOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::RangeOptionsT *>(value);
       return CreateRangeOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_ResizeNearestNeighborOptions: {
-      auto ptr = reinterpret_cast<const tflite::ResizeNearestNeighborOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::ResizeNearestNeighborOptionsT *>(value);
       return CreateResizeNearestNeighborOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_LeakyReluOptions: {
-      auto ptr = reinterpret_cast<const tflite::LeakyReluOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::LeakyReluOptionsT *>(value);
       return CreateLeakyReluOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_SquaredDifferenceOptions: {
-      auto ptr = reinterpret_cast<const tflite::SquaredDifferenceOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::SquaredDifferenceOptionsT *>(value);
       return CreateSquaredDifferenceOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_MirrorPadOptions: {
-      auto ptr = reinterpret_cast<const tflite::MirrorPadOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::MirrorPadOptionsT *>(value);
       return CreateMirrorPadOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_AbsOptions: {
-      auto ptr = reinterpret_cast<const tflite::AbsOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::AbsOptionsT *>(value);
       return CreateAbsOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_SplitVOptions: {
-      auto ptr = reinterpret_cast<const tflite::SplitVOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::SplitVOptionsT *>(value);
       return CreateSplitVOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_UniqueOptions: {
-      auto ptr = reinterpret_cast<const tflite::UniqueOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::UniqueOptionsT *>(value);
       return CreateUniqueOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_ReverseV2Options: {
-      auto ptr = reinterpret_cast<const tflite::ReverseV2OptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::ReverseV2OptionsT *>(value);
       return CreateReverseV2Options(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_AddNOptions: {
-      auto ptr = reinterpret_cast<const tflite::AddNOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::AddNOptionsT *>(value);
       return CreateAddNOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_GatherNdOptions: {
-      auto ptr = reinterpret_cast<const tflite::GatherNdOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::GatherNdOptionsT *>(value);
       return CreateGatherNdOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_CosOptions: {
-      auto ptr = reinterpret_cast<const tflite::CosOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::CosOptionsT *>(value);
       return CreateCosOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_WhereOptions: {
-      auto ptr = reinterpret_cast<const tflite::WhereOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::WhereOptionsT *>(value);
       return CreateWhereOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_RankOptions: {
-      auto ptr = reinterpret_cast<const tflite::RankOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::RankOptionsT *>(value);
       return CreateRankOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_ReverseSequenceOptions: {
-      auto ptr = reinterpret_cast<const tflite::ReverseSequenceOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::ReverseSequenceOptionsT *>(value);
       return CreateReverseSequenceOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_MatrixDiagOptions: {
-      auto ptr = reinterpret_cast<const tflite::MatrixDiagOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::MatrixDiagOptionsT *>(value);
       return CreateMatrixDiagOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_QuantizeOptions: {
-      auto ptr = reinterpret_cast<const tflite::QuantizeOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::QuantizeOptionsT *>(value);
       return CreateQuantizeOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_MatrixSetDiagOptions: {
-      auto ptr = reinterpret_cast<const tflite::MatrixSetDiagOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::MatrixSetDiagOptionsT *>(value);
       return CreateMatrixSetDiagOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_HardSwishOptions: {
-      auto ptr = reinterpret_cast<const tflite::HardSwishOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::HardSwishOptionsT *>(value);
       return CreateHardSwishOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_IfOptions: {
-      auto ptr = reinterpret_cast<const tflite::IfOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::IfOptionsT *>(value);
       return CreateIfOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_WhileOptions: {
-      auto ptr = reinterpret_cast<const tflite::WhileOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::WhileOptionsT *>(value);
       return CreateWhileOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_DepthToSpaceOptions: {
-      auto ptr = reinterpret_cast<const tflite::DepthToSpaceOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::DepthToSpaceOptionsT *>(value);
       return CreateDepthToSpaceOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_NonMaxSuppressionV4Options: {
-      auto ptr = reinterpret_cast<const tflite::NonMaxSuppressionV4OptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::NonMaxSuppressionV4OptionsT *>(value);
       return CreateNonMaxSuppressionV4Options(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_NonMaxSuppressionV5Options: {
-      auto ptr = reinterpret_cast<const tflite::NonMaxSuppressionV5OptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::NonMaxSuppressionV5OptionsT *>(value);
       return CreateNonMaxSuppressionV5Options(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_ScatterNdOptions: {
-      auto ptr = reinterpret_cast<const tflite::ScatterNdOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::ScatterNdOptionsT *>(value);
       return CreateScatterNdOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_SelectV2Options: {
-      auto ptr = reinterpret_cast<const tflite::SelectV2OptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::SelectV2OptionsT *>(value);
       return CreateSelectV2Options(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_DensifyOptions: {
-      auto ptr = reinterpret_cast<const tflite::DensifyOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::DensifyOptionsT *>(value);
       return CreateDensifyOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_SegmentSumOptions: {
-      auto ptr = reinterpret_cast<const tflite::SegmentSumOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::SegmentSumOptionsT *>(value);
       return CreateSegmentSumOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_BatchMatMulOptions: {
-      auto ptr = reinterpret_cast<const tflite::BatchMatMulOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::BatchMatMulOptionsT *>(value);
       return CreateBatchMatMulOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_CumsumOptions: {
-      auto ptr = reinterpret_cast<const tflite::CumsumOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::CumsumOptionsT *>(value);
       return CreateCumsumOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_CallOnceOptions: {
-      auto ptr = reinterpret_cast<const tflite::CallOnceOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::CallOnceOptionsT *>(value);
       return CreateCallOnceOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_BroadcastToOptions: {
-      auto ptr = reinterpret_cast<const tflite::BroadcastToOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::BroadcastToOptionsT *>(value);
       return CreateBroadcastToOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_Rfft2dOptions: {
-      auto ptr = reinterpret_cast<const tflite::Rfft2dOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::Rfft2dOptionsT *>(value);
       return CreateRfft2dOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_Conv3DOptions: {
-      auto ptr = reinterpret_cast<const tflite::Conv3DOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::Conv3DOptionsT *>(value);
       return CreateConv3DOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_HashtableOptions: {
-      auto ptr = reinterpret_cast<const tflite::HashtableOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::HashtableOptionsT *>(value);
       return CreateHashtableOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_HashtableFindOptions: {
-      auto ptr = reinterpret_cast<const tflite::HashtableFindOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::HashtableFindOptionsT *>(value);
       return CreateHashtableFindOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_HashtableImportOptions: {
-      auto ptr = reinterpret_cast<const tflite::HashtableImportOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::HashtableImportOptionsT *>(value);
       return CreateHashtableImportOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_HashtableSizeOptions: {
-      auto ptr = reinterpret_cast<const tflite::HashtableSizeOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::HashtableSizeOptionsT *>(value);
       return CreateHashtableSizeOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_VarHandleOptions: {
-      auto ptr = reinterpret_cast<const tflite::VarHandleOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::VarHandleOptionsT *>(value);
       return CreateVarHandleOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_ReadVariableOptions: {
-      auto ptr = reinterpret_cast<const tflite::ReadVariableOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::ReadVariableOptionsT *>(value);
       return CreateReadVariableOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_AssignVariableOptions: {
-      auto ptr = reinterpret_cast<const tflite::AssignVariableOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::AssignVariableOptionsT *>(value);
       return CreateAssignVariableOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_RandomOptions: {
-      auto ptr = reinterpret_cast<const tflite::RandomOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::RandomOptionsT *>(value);
       return CreateRandomOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_BucketizeOptions: {
-      auto ptr = reinterpret_cast<const tflite::BucketizeOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::BucketizeOptionsT *>(value);
       return CreateBucketizeOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_GeluOptions: {
-      auto ptr = reinterpret_cast<const tflite::GeluOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::GeluOptionsT *>(value);
       return CreateGeluOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_DynamicUpdateSliceOptions: {
-      auto ptr = reinterpret_cast<const tflite::DynamicUpdateSliceOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::DynamicUpdateSliceOptionsT *>(value);
       return CreateDynamicUpdateSliceOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_UnsortedSegmentProdOptions: {
-      auto ptr = reinterpret_cast<const tflite::UnsortedSegmentProdOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::UnsortedSegmentProdOptionsT *>(value);
       return CreateUnsortedSegmentProdOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_UnsortedSegmentMaxOptions: {
-      auto ptr = reinterpret_cast<const tflite::UnsortedSegmentMaxOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::UnsortedSegmentMaxOptionsT *>(value);
       return CreateUnsortedSegmentMaxOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_UnsortedSegmentMinOptions: {
-      auto ptr = reinterpret_cast<const tflite::UnsortedSegmentMinOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::UnsortedSegmentMinOptionsT *>(value);
       return CreateUnsortedSegmentMinOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_UnsortedSegmentSumOptions: {
-      auto ptr = reinterpret_cast<const tflite::UnsortedSegmentSumOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::UnsortedSegmentSumOptionsT *>(value);
       return CreateUnsortedSegmentSumOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_ATan2Options: {
-      auto ptr = reinterpret_cast<const tflite::ATan2OptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::ATan2OptionsT *>(value);
       return CreateATan2Options(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_SignOptions: {
-      auto ptr = reinterpret_cast<const tflite::SignOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::SignOptionsT *>(value);
       return CreateSignOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_BitcastOptions: {
-      auto ptr = reinterpret_cast<const tflite::BitcastOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::BitcastOptionsT *>(value);
       return CreateBitcastOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_BitwiseXorOptions: {
-      auto ptr = reinterpret_cast<const tflite::BitwiseXorOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::BitwiseXorOptionsT *>(value);
       return CreateBitwiseXorOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions_RightShiftOptions: {
-      auto ptr = reinterpret_cast<const tflite::RightShiftOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::RightShiftOptionsT *>(value);
       return CreateRightShiftOptions(_fbb, ptr, _rehasher).Union();
     }
     default: return 0;
@@ -22963,507 +22963,507 @@ inline flatbuffers::Offset<void> BuiltinOptionsUnion::Pack(flatbuffers::FlatBuff
 inline BuiltinOptionsUnion::BuiltinOptionsUnion(const BuiltinOptionsUnion &u) : type(u.type), value(nullptr) {
   switch (type) {
     case BuiltinOptions_Conv2DOptions: {
-      value = new tflite::Conv2DOptionsT(*reinterpret_cast<tflite::Conv2DOptionsT *>(u.value));
+      value = new tflite_micro::Conv2DOptionsT(*reinterpret_cast<tflite_micro::Conv2DOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_DepthwiseConv2DOptions: {
-      value = new tflite::DepthwiseConv2DOptionsT(*reinterpret_cast<tflite::DepthwiseConv2DOptionsT *>(u.value));
+      value = new tflite_micro::DepthwiseConv2DOptionsT(*reinterpret_cast<tflite_micro::DepthwiseConv2DOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_ConcatEmbeddingsOptions: {
-      value = new tflite::ConcatEmbeddingsOptionsT(*reinterpret_cast<tflite::ConcatEmbeddingsOptionsT *>(u.value));
+      value = new tflite_micro::ConcatEmbeddingsOptionsT(*reinterpret_cast<tflite_micro::ConcatEmbeddingsOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_LSHProjectionOptions: {
-      value = new tflite::LSHProjectionOptionsT(*reinterpret_cast<tflite::LSHProjectionOptionsT *>(u.value));
+      value = new tflite_micro::LSHProjectionOptionsT(*reinterpret_cast<tflite_micro::LSHProjectionOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_Pool2DOptions: {
-      value = new tflite::Pool2DOptionsT(*reinterpret_cast<tflite::Pool2DOptionsT *>(u.value));
+      value = new tflite_micro::Pool2DOptionsT(*reinterpret_cast<tflite_micro::Pool2DOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_SVDFOptions: {
-      value = new tflite::SVDFOptionsT(*reinterpret_cast<tflite::SVDFOptionsT *>(u.value));
+      value = new tflite_micro::SVDFOptionsT(*reinterpret_cast<tflite_micro::SVDFOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_RNNOptions: {
-      value = new tflite::RNNOptionsT(*reinterpret_cast<tflite::RNNOptionsT *>(u.value));
+      value = new tflite_micro::RNNOptionsT(*reinterpret_cast<tflite_micro::RNNOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_FullyConnectedOptions: {
-      value = new tflite::FullyConnectedOptionsT(*reinterpret_cast<tflite::FullyConnectedOptionsT *>(u.value));
+      value = new tflite_micro::FullyConnectedOptionsT(*reinterpret_cast<tflite_micro::FullyConnectedOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_SoftmaxOptions: {
-      value = new tflite::SoftmaxOptionsT(*reinterpret_cast<tflite::SoftmaxOptionsT *>(u.value));
+      value = new tflite_micro::SoftmaxOptionsT(*reinterpret_cast<tflite_micro::SoftmaxOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_ConcatenationOptions: {
-      value = new tflite::ConcatenationOptionsT(*reinterpret_cast<tflite::ConcatenationOptionsT *>(u.value));
+      value = new tflite_micro::ConcatenationOptionsT(*reinterpret_cast<tflite_micro::ConcatenationOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_AddOptions: {
-      value = new tflite::AddOptionsT(*reinterpret_cast<tflite::AddOptionsT *>(u.value));
+      value = new tflite_micro::AddOptionsT(*reinterpret_cast<tflite_micro::AddOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_L2NormOptions: {
-      value = new tflite::L2NormOptionsT(*reinterpret_cast<tflite::L2NormOptionsT *>(u.value));
+      value = new tflite_micro::L2NormOptionsT(*reinterpret_cast<tflite_micro::L2NormOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_LocalResponseNormalizationOptions: {
-      value = new tflite::LocalResponseNormalizationOptionsT(*reinterpret_cast<tflite::LocalResponseNormalizationOptionsT *>(u.value));
+      value = new tflite_micro::LocalResponseNormalizationOptionsT(*reinterpret_cast<tflite_micro::LocalResponseNormalizationOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_LSTMOptions: {
-      value = new tflite::LSTMOptionsT(*reinterpret_cast<tflite::LSTMOptionsT *>(u.value));
+      value = new tflite_micro::LSTMOptionsT(*reinterpret_cast<tflite_micro::LSTMOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_ResizeBilinearOptions: {
-      value = new tflite::ResizeBilinearOptionsT(*reinterpret_cast<tflite::ResizeBilinearOptionsT *>(u.value));
+      value = new tflite_micro::ResizeBilinearOptionsT(*reinterpret_cast<tflite_micro::ResizeBilinearOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_CallOptions: {
-      value = new tflite::CallOptionsT(*reinterpret_cast<tflite::CallOptionsT *>(u.value));
+      value = new tflite_micro::CallOptionsT(*reinterpret_cast<tflite_micro::CallOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_ReshapeOptions: {
-      value = new tflite::ReshapeOptionsT(*reinterpret_cast<tflite::ReshapeOptionsT *>(u.value));
+      value = new tflite_micro::ReshapeOptionsT(*reinterpret_cast<tflite_micro::ReshapeOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_SkipGramOptions: {
-      value = new tflite::SkipGramOptionsT(*reinterpret_cast<tflite::SkipGramOptionsT *>(u.value));
+      value = new tflite_micro::SkipGramOptionsT(*reinterpret_cast<tflite_micro::SkipGramOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_SpaceToDepthOptions: {
-      value = new tflite::SpaceToDepthOptionsT(*reinterpret_cast<tflite::SpaceToDepthOptionsT *>(u.value));
+      value = new tflite_micro::SpaceToDepthOptionsT(*reinterpret_cast<tflite_micro::SpaceToDepthOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_EmbeddingLookupSparseOptions: {
-      value = new tflite::EmbeddingLookupSparseOptionsT(*reinterpret_cast<tflite::EmbeddingLookupSparseOptionsT *>(u.value));
+      value = new tflite_micro::EmbeddingLookupSparseOptionsT(*reinterpret_cast<tflite_micro::EmbeddingLookupSparseOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_MulOptions: {
-      value = new tflite::MulOptionsT(*reinterpret_cast<tflite::MulOptionsT *>(u.value));
+      value = new tflite_micro::MulOptionsT(*reinterpret_cast<tflite_micro::MulOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_PadOptions: {
-      value = new tflite::PadOptionsT(*reinterpret_cast<tflite::PadOptionsT *>(u.value));
+      value = new tflite_micro::PadOptionsT(*reinterpret_cast<tflite_micro::PadOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_GatherOptions: {
-      value = new tflite::GatherOptionsT(*reinterpret_cast<tflite::GatherOptionsT *>(u.value));
+      value = new tflite_micro::GatherOptionsT(*reinterpret_cast<tflite_micro::GatherOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_BatchToSpaceNDOptions: {
-      value = new tflite::BatchToSpaceNDOptionsT(*reinterpret_cast<tflite::BatchToSpaceNDOptionsT *>(u.value));
+      value = new tflite_micro::BatchToSpaceNDOptionsT(*reinterpret_cast<tflite_micro::BatchToSpaceNDOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_SpaceToBatchNDOptions: {
-      value = new tflite::SpaceToBatchNDOptionsT(*reinterpret_cast<tflite::SpaceToBatchNDOptionsT *>(u.value));
+      value = new tflite_micro::SpaceToBatchNDOptionsT(*reinterpret_cast<tflite_micro::SpaceToBatchNDOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_TransposeOptions: {
-      value = new tflite::TransposeOptionsT(*reinterpret_cast<tflite::TransposeOptionsT *>(u.value));
+      value = new tflite_micro::TransposeOptionsT(*reinterpret_cast<tflite_micro::TransposeOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_ReducerOptions: {
-      value = new tflite::ReducerOptionsT(*reinterpret_cast<tflite::ReducerOptionsT *>(u.value));
+      value = new tflite_micro::ReducerOptionsT(*reinterpret_cast<tflite_micro::ReducerOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_SubOptions: {
-      value = new tflite::SubOptionsT(*reinterpret_cast<tflite::SubOptionsT *>(u.value));
+      value = new tflite_micro::SubOptionsT(*reinterpret_cast<tflite_micro::SubOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_DivOptions: {
-      value = new tflite::DivOptionsT(*reinterpret_cast<tflite::DivOptionsT *>(u.value));
+      value = new tflite_micro::DivOptionsT(*reinterpret_cast<tflite_micro::DivOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_SqueezeOptions: {
-      value = new tflite::SqueezeOptionsT(*reinterpret_cast<tflite::SqueezeOptionsT *>(u.value));
+      value = new tflite_micro::SqueezeOptionsT(*reinterpret_cast<tflite_micro::SqueezeOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_SequenceRNNOptions: {
-      value = new tflite::SequenceRNNOptionsT(*reinterpret_cast<tflite::SequenceRNNOptionsT *>(u.value));
+      value = new tflite_micro::SequenceRNNOptionsT(*reinterpret_cast<tflite_micro::SequenceRNNOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_StridedSliceOptions: {
-      value = new tflite::StridedSliceOptionsT(*reinterpret_cast<tflite::StridedSliceOptionsT *>(u.value));
+      value = new tflite_micro::StridedSliceOptionsT(*reinterpret_cast<tflite_micro::StridedSliceOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_ExpOptions: {
-      value = new tflite::ExpOptionsT(*reinterpret_cast<tflite::ExpOptionsT *>(u.value));
+      value = new tflite_micro::ExpOptionsT(*reinterpret_cast<tflite_micro::ExpOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_TopKV2Options: {
-      value = new tflite::TopKV2OptionsT(*reinterpret_cast<tflite::TopKV2OptionsT *>(u.value));
+      value = new tflite_micro::TopKV2OptionsT(*reinterpret_cast<tflite_micro::TopKV2OptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_SplitOptions: {
-      value = new tflite::SplitOptionsT(*reinterpret_cast<tflite::SplitOptionsT *>(u.value));
+      value = new tflite_micro::SplitOptionsT(*reinterpret_cast<tflite_micro::SplitOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_LogSoftmaxOptions: {
-      value = new tflite::LogSoftmaxOptionsT(*reinterpret_cast<tflite::LogSoftmaxOptionsT *>(u.value));
+      value = new tflite_micro::LogSoftmaxOptionsT(*reinterpret_cast<tflite_micro::LogSoftmaxOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_CastOptions: {
-      value = new tflite::CastOptionsT(*reinterpret_cast<tflite::CastOptionsT *>(u.value));
+      value = new tflite_micro::CastOptionsT(*reinterpret_cast<tflite_micro::CastOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_DequantizeOptions: {
-      value = new tflite::DequantizeOptionsT(*reinterpret_cast<tflite::DequantizeOptionsT *>(u.value));
+      value = new tflite_micro::DequantizeOptionsT(*reinterpret_cast<tflite_micro::DequantizeOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_MaximumMinimumOptions: {
-      value = new tflite::MaximumMinimumOptionsT(*reinterpret_cast<tflite::MaximumMinimumOptionsT *>(u.value));
+      value = new tflite_micro::MaximumMinimumOptionsT(*reinterpret_cast<tflite_micro::MaximumMinimumOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_ArgMaxOptions: {
-      value = new tflite::ArgMaxOptionsT(*reinterpret_cast<tflite::ArgMaxOptionsT *>(u.value));
+      value = new tflite_micro::ArgMaxOptionsT(*reinterpret_cast<tflite_micro::ArgMaxOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_LessOptions: {
-      value = new tflite::LessOptionsT(*reinterpret_cast<tflite::LessOptionsT *>(u.value));
+      value = new tflite_micro::LessOptionsT(*reinterpret_cast<tflite_micro::LessOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_NegOptions: {
-      value = new tflite::NegOptionsT(*reinterpret_cast<tflite::NegOptionsT *>(u.value));
+      value = new tflite_micro::NegOptionsT(*reinterpret_cast<tflite_micro::NegOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_PadV2Options: {
-      value = new tflite::PadV2OptionsT(*reinterpret_cast<tflite::PadV2OptionsT *>(u.value));
+      value = new tflite_micro::PadV2OptionsT(*reinterpret_cast<tflite_micro::PadV2OptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_GreaterOptions: {
-      value = new tflite::GreaterOptionsT(*reinterpret_cast<tflite::GreaterOptionsT *>(u.value));
+      value = new tflite_micro::GreaterOptionsT(*reinterpret_cast<tflite_micro::GreaterOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_GreaterEqualOptions: {
-      value = new tflite::GreaterEqualOptionsT(*reinterpret_cast<tflite::GreaterEqualOptionsT *>(u.value));
+      value = new tflite_micro::GreaterEqualOptionsT(*reinterpret_cast<tflite_micro::GreaterEqualOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_LessEqualOptions: {
-      value = new tflite::LessEqualOptionsT(*reinterpret_cast<tflite::LessEqualOptionsT *>(u.value));
+      value = new tflite_micro::LessEqualOptionsT(*reinterpret_cast<tflite_micro::LessEqualOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_SelectOptions: {
-      value = new tflite::SelectOptionsT(*reinterpret_cast<tflite::SelectOptionsT *>(u.value));
+      value = new tflite_micro::SelectOptionsT(*reinterpret_cast<tflite_micro::SelectOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_SliceOptions: {
-      value = new tflite::SliceOptionsT(*reinterpret_cast<tflite::SliceOptionsT *>(u.value));
+      value = new tflite_micro::SliceOptionsT(*reinterpret_cast<tflite_micro::SliceOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_TransposeConvOptions: {
-      value = new tflite::TransposeConvOptionsT(*reinterpret_cast<tflite::TransposeConvOptionsT *>(u.value));
+      value = new tflite_micro::TransposeConvOptionsT(*reinterpret_cast<tflite_micro::TransposeConvOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_SparseToDenseOptions: {
-      value = new tflite::SparseToDenseOptionsT(*reinterpret_cast<tflite::SparseToDenseOptionsT *>(u.value));
+      value = new tflite_micro::SparseToDenseOptionsT(*reinterpret_cast<tflite_micro::SparseToDenseOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_TileOptions: {
-      value = new tflite::TileOptionsT(*reinterpret_cast<tflite::TileOptionsT *>(u.value));
+      value = new tflite_micro::TileOptionsT(*reinterpret_cast<tflite_micro::TileOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_ExpandDimsOptions: {
-      value = new tflite::ExpandDimsOptionsT(*reinterpret_cast<tflite::ExpandDimsOptionsT *>(u.value));
+      value = new tflite_micro::ExpandDimsOptionsT(*reinterpret_cast<tflite_micro::ExpandDimsOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_EqualOptions: {
-      value = new tflite::EqualOptionsT(*reinterpret_cast<tflite::EqualOptionsT *>(u.value));
+      value = new tflite_micro::EqualOptionsT(*reinterpret_cast<tflite_micro::EqualOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_NotEqualOptions: {
-      value = new tflite::NotEqualOptionsT(*reinterpret_cast<tflite::NotEqualOptionsT *>(u.value));
+      value = new tflite_micro::NotEqualOptionsT(*reinterpret_cast<tflite_micro::NotEqualOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_ShapeOptions: {
-      value = new tflite::ShapeOptionsT(*reinterpret_cast<tflite::ShapeOptionsT *>(u.value));
+      value = new tflite_micro::ShapeOptionsT(*reinterpret_cast<tflite_micro::ShapeOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_PowOptions: {
-      value = new tflite::PowOptionsT(*reinterpret_cast<tflite::PowOptionsT *>(u.value));
+      value = new tflite_micro::PowOptionsT(*reinterpret_cast<tflite_micro::PowOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_ArgMinOptions: {
-      value = new tflite::ArgMinOptionsT(*reinterpret_cast<tflite::ArgMinOptionsT *>(u.value));
+      value = new tflite_micro::ArgMinOptionsT(*reinterpret_cast<tflite_micro::ArgMinOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_FakeQuantOptions: {
-      value = new tflite::FakeQuantOptionsT(*reinterpret_cast<tflite::FakeQuantOptionsT *>(u.value));
+      value = new tflite_micro::FakeQuantOptionsT(*reinterpret_cast<tflite_micro::FakeQuantOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_PackOptions: {
-      value = new tflite::PackOptionsT(*reinterpret_cast<tflite::PackOptionsT *>(u.value));
+      value = new tflite_micro::PackOptionsT(*reinterpret_cast<tflite_micro::PackOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_LogicalOrOptions: {
-      value = new tflite::LogicalOrOptionsT(*reinterpret_cast<tflite::LogicalOrOptionsT *>(u.value));
+      value = new tflite_micro::LogicalOrOptionsT(*reinterpret_cast<tflite_micro::LogicalOrOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_OneHotOptions: {
-      value = new tflite::OneHotOptionsT(*reinterpret_cast<tflite::OneHotOptionsT *>(u.value));
+      value = new tflite_micro::OneHotOptionsT(*reinterpret_cast<tflite_micro::OneHotOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_LogicalAndOptions: {
-      value = new tflite::LogicalAndOptionsT(*reinterpret_cast<tflite::LogicalAndOptionsT *>(u.value));
+      value = new tflite_micro::LogicalAndOptionsT(*reinterpret_cast<tflite_micro::LogicalAndOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_LogicalNotOptions: {
-      value = new tflite::LogicalNotOptionsT(*reinterpret_cast<tflite::LogicalNotOptionsT *>(u.value));
+      value = new tflite_micro::LogicalNotOptionsT(*reinterpret_cast<tflite_micro::LogicalNotOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_UnpackOptions: {
-      value = new tflite::UnpackOptionsT(*reinterpret_cast<tflite::UnpackOptionsT *>(u.value));
+      value = new tflite_micro::UnpackOptionsT(*reinterpret_cast<tflite_micro::UnpackOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_FloorDivOptions: {
-      value = new tflite::FloorDivOptionsT(*reinterpret_cast<tflite::FloorDivOptionsT *>(u.value));
+      value = new tflite_micro::FloorDivOptionsT(*reinterpret_cast<tflite_micro::FloorDivOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_SquareOptions: {
-      value = new tflite::SquareOptionsT(*reinterpret_cast<tflite::SquareOptionsT *>(u.value));
+      value = new tflite_micro::SquareOptionsT(*reinterpret_cast<tflite_micro::SquareOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_ZerosLikeOptions: {
-      value = new tflite::ZerosLikeOptionsT(*reinterpret_cast<tflite::ZerosLikeOptionsT *>(u.value));
+      value = new tflite_micro::ZerosLikeOptionsT(*reinterpret_cast<tflite_micro::ZerosLikeOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_FillOptions: {
-      value = new tflite::FillOptionsT(*reinterpret_cast<tflite::FillOptionsT *>(u.value));
+      value = new tflite_micro::FillOptionsT(*reinterpret_cast<tflite_micro::FillOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_BidirectionalSequenceLSTMOptions: {
-      value = new tflite::BidirectionalSequenceLSTMOptionsT(*reinterpret_cast<tflite::BidirectionalSequenceLSTMOptionsT *>(u.value));
+      value = new tflite_micro::BidirectionalSequenceLSTMOptionsT(*reinterpret_cast<tflite_micro::BidirectionalSequenceLSTMOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_BidirectionalSequenceRNNOptions: {
-      value = new tflite::BidirectionalSequenceRNNOptionsT(*reinterpret_cast<tflite::BidirectionalSequenceRNNOptionsT *>(u.value));
+      value = new tflite_micro::BidirectionalSequenceRNNOptionsT(*reinterpret_cast<tflite_micro::BidirectionalSequenceRNNOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_UnidirectionalSequenceLSTMOptions: {
-      value = new tflite::UnidirectionalSequenceLSTMOptionsT(*reinterpret_cast<tflite::UnidirectionalSequenceLSTMOptionsT *>(u.value));
+      value = new tflite_micro::UnidirectionalSequenceLSTMOptionsT(*reinterpret_cast<tflite_micro::UnidirectionalSequenceLSTMOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_FloorModOptions: {
-      value = new tflite::FloorModOptionsT(*reinterpret_cast<tflite::FloorModOptionsT *>(u.value));
+      value = new tflite_micro::FloorModOptionsT(*reinterpret_cast<tflite_micro::FloorModOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_RangeOptions: {
-      value = new tflite::RangeOptionsT(*reinterpret_cast<tflite::RangeOptionsT *>(u.value));
+      value = new tflite_micro::RangeOptionsT(*reinterpret_cast<tflite_micro::RangeOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_ResizeNearestNeighborOptions: {
-      value = new tflite::ResizeNearestNeighborOptionsT(*reinterpret_cast<tflite::ResizeNearestNeighborOptionsT *>(u.value));
+      value = new tflite_micro::ResizeNearestNeighborOptionsT(*reinterpret_cast<tflite_micro::ResizeNearestNeighborOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_LeakyReluOptions: {
-      value = new tflite::LeakyReluOptionsT(*reinterpret_cast<tflite::LeakyReluOptionsT *>(u.value));
+      value = new tflite_micro::LeakyReluOptionsT(*reinterpret_cast<tflite_micro::LeakyReluOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_SquaredDifferenceOptions: {
-      value = new tflite::SquaredDifferenceOptionsT(*reinterpret_cast<tflite::SquaredDifferenceOptionsT *>(u.value));
+      value = new tflite_micro::SquaredDifferenceOptionsT(*reinterpret_cast<tflite_micro::SquaredDifferenceOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_MirrorPadOptions: {
-      value = new tflite::MirrorPadOptionsT(*reinterpret_cast<tflite::MirrorPadOptionsT *>(u.value));
+      value = new tflite_micro::MirrorPadOptionsT(*reinterpret_cast<tflite_micro::MirrorPadOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_AbsOptions: {
-      value = new tflite::AbsOptionsT(*reinterpret_cast<tflite::AbsOptionsT *>(u.value));
+      value = new tflite_micro::AbsOptionsT(*reinterpret_cast<tflite_micro::AbsOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_SplitVOptions: {
-      value = new tflite::SplitVOptionsT(*reinterpret_cast<tflite::SplitVOptionsT *>(u.value));
+      value = new tflite_micro::SplitVOptionsT(*reinterpret_cast<tflite_micro::SplitVOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_UniqueOptions: {
-      value = new tflite::UniqueOptionsT(*reinterpret_cast<tflite::UniqueOptionsT *>(u.value));
+      value = new tflite_micro::UniqueOptionsT(*reinterpret_cast<tflite_micro::UniqueOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_ReverseV2Options: {
-      value = new tflite::ReverseV2OptionsT(*reinterpret_cast<tflite::ReverseV2OptionsT *>(u.value));
+      value = new tflite_micro::ReverseV2OptionsT(*reinterpret_cast<tflite_micro::ReverseV2OptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_AddNOptions: {
-      value = new tflite::AddNOptionsT(*reinterpret_cast<tflite::AddNOptionsT *>(u.value));
+      value = new tflite_micro::AddNOptionsT(*reinterpret_cast<tflite_micro::AddNOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_GatherNdOptions: {
-      value = new tflite::GatherNdOptionsT(*reinterpret_cast<tflite::GatherNdOptionsT *>(u.value));
+      value = new tflite_micro::GatherNdOptionsT(*reinterpret_cast<tflite_micro::GatherNdOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_CosOptions: {
-      value = new tflite::CosOptionsT(*reinterpret_cast<tflite::CosOptionsT *>(u.value));
+      value = new tflite_micro::CosOptionsT(*reinterpret_cast<tflite_micro::CosOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_WhereOptions: {
-      value = new tflite::WhereOptionsT(*reinterpret_cast<tflite::WhereOptionsT *>(u.value));
+      value = new tflite_micro::WhereOptionsT(*reinterpret_cast<tflite_micro::WhereOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_RankOptions: {
-      value = new tflite::RankOptionsT(*reinterpret_cast<tflite::RankOptionsT *>(u.value));
+      value = new tflite_micro::RankOptionsT(*reinterpret_cast<tflite_micro::RankOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_ReverseSequenceOptions: {
-      value = new tflite::ReverseSequenceOptionsT(*reinterpret_cast<tflite::ReverseSequenceOptionsT *>(u.value));
+      value = new tflite_micro::ReverseSequenceOptionsT(*reinterpret_cast<tflite_micro::ReverseSequenceOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_MatrixDiagOptions: {
-      value = new tflite::MatrixDiagOptionsT(*reinterpret_cast<tflite::MatrixDiagOptionsT *>(u.value));
+      value = new tflite_micro::MatrixDiagOptionsT(*reinterpret_cast<tflite_micro::MatrixDiagOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_QuantizeOptions: {
-      value = new tflite::QuantizeOptionsT(*reinterpret_cast<tflite::QuantizeOptionsT *>(u.value));
+      value = new tflite_micro::QuantizeOptionsT(*reinterpret_cast<tflite_micro::QuantizeOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_MatrixSetDiagOptions: {
-      value = new tflite::MatrixSetDiagOptionsT(*reinterpret_cast<tflite::MatrixSetDiagOptionsT *>(u.value));
+      value = new tflite_micro::MatrixSetDiagOptionsT(*reinterpret_cast<tflite_micro::MatrixSetDiagOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_HardSwishOptions: {
-      value = new tflite::HardSwishOptionsT(*reinterpret_cast<tflite::HardSwishOptionsT *>(u.value));
+      value = new tflite_micro::HardSwishOptionsT(*reinterpret_cast<tflite_micro::HardSwishOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_IfOptions: {
-      value = new tflite::IfOptionsT(*reinterpret_cast<tflite::IfOptionsT *>(u.value));
+      value = new tflite_micro::IfOptionsT(*reinterpret_cast<tflite_micro::IfOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_WhileOptions: {
-      value = new tflite::WhileOptionsT(*reinterpret_cast<tflite::WhileOptionsT *>(u.value));
+      value = new tflite_micro::WhileOptionsT(*reinterpret_cast<tflite_micro::WhileOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_DepthToSpaceOptions: {
-      value = new tflite::DepthToSpaceOptionsT(*reinterpret_cast<tflite::DepthToSpaceOptionsT *>(u.value));
+      value = new tflite_micro::DepthToSpaceOptionsT(*reinterpret_cast<tflite_micro::DepthToSpaceOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_NonMaxSuppressionV4Options: {
-      value = new tflite::NonMaxSuppressionV4OptionsT(*reinterpret_cast<tflite::NonMaxSuppressionV4OptionsT *>(u.value));
+      value = new tflite_micro::NonMaxSuppressionV4OptionsT(*reinterpret_cast<tflite_micro::NonMaxSuppressionV4OptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_NonMaxSuppressionV5Options: {
-      value = new tflite::NonMaxSuppressionV5OptionsT(*reinterpret_cast<tflite::NonMaxSuppressionV5OptionsT *>(u.value));
+      value = new tflite_micro::NonMaxSuppressionV5OptionsT(*reinterpret_cast<tflite_micro::NonMaxSuppressionV5OptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_ScatterNdOptions: {
-      value = new tflite::ScatterNdOptionsT(*reinterpret_cast<tflite::ScatterNdOptionsT *>(u.value));
+      value = new tflite_micro::ScatterNdOptionsT(*reinterpret_cast<tflite_micro::ScatterNdOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_SelectV2Options: {
-      value = new tflite::SelectV2OptionsT(*reinterpret_cast<tflite::SelectV2OptionsT *>(u.value));
+      value = new tflite_micro::SelectV2OptionsT(*reinterpret_cast<tflite_micro::SelectV2OptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_DensifyOptions: {
-      value = new tflite::DensifyOptionsT(*reinterpret_cast<tflite::DensifyOptionsT *>(u.value));
+      value = new tflite_micro::DensifyOptionsT(*reinterpret_cast<tflite_micro::DensifyOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_SegmentSumOptions: {
-      value = new tflite::SegmentSumOptionsT(*reinterpret_cast<tflite::SegmentSumOptionsT *>(u.value));
+      value = new tflite_micro::SegmentSumOptionsT(*reinterpret_cast<tflite_micro::SegmentSumOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_BatchMatMulOptions: {
-      value = new tflite::BatchMatMulOptionsT(*reinterpret_cast<tflite::BatchMatMulOptionsT *>(u.value));
+      value = new tflite_micro::BatchMatMulOptionsT(*reinterpret_cast<tflite_micro::BatchMatMulOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_CumsumOptions: {
-      value = new tflite::CumsumOptionsT(*reinterpret_cast<tflite::CumsumOptionsT *>(u.value));
+      value = new tflite_micro::CumsumOptionsT(*reinterpret_cast<tflite_micro::CumsumOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_CallOnceOptions: {
-      value = new tflite::CallOnceOptionsT(*reinterpret_cast<tflite::CallOnceOptionsT *>(u.value));
+      value = new tflite_micro::CallOnceOptionsT(*reinterpret_cast<tflite_micro::CallOnceOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_BroadcastToOptions: {
-      value = new tflite::BroadcastToOptionsT(*reinterpret_cast<tflite::BroadcastToOptionsT *>(u.value));
+      value = new tflite_micro::BroadcastToOptionsT(*reinterpret_cast<tflite_micro::BroadcastToOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_Rfft2dOptions: {
-      value = new tflite::Rfft2dOptionsT(*reinterpret_cast<tflite::Rfft2dOptionsT *>(u.value));
+      value = new tflite_micro::Rfft2dOptionsT(*reinterpret_cast<tflite_micro::Rfft2dOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_Conv3DOptions: {
-      value = new tflite::Conv3DOptionsT(*reinterpret_cast<tflite::Conv3DOptionsT *>(u.value));
+      value = new tflite_micro::Conv3DOptionsT(*reinterpret_cast<tflite_micro::Conv3DOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_HashtableOptions: {
-      value = new tflite::HashtableOptionsT(*reinterpret_cast<tflite::HashtableOptionsT *>(u.value));
+      value = new tflite_micro::HashtableOptionsT(*reinterpret_cast<tflite_micro::HashtableOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_HashtableFindOptions: {
-      value = new tflite::HashtableFindOptionsT(*reinterpret_cast<tflite::HashtableFindOptionsT *>(u.value));
+      value = new tflite_micro::HashtableFindOptionsT(*reinterpret_cast<tflite_micro::HashtableFindOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_HashtableImportOptions: {
-      value = new tflite::HashtableImportOptionsT(*reinterpret_cast<tflite::HashtableImportOptionsT *>(u.value));
+      value = new tflite_micro::HashtableImportOptionsT(*reinterpret_cast<tflite_micro::HashtableImportOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_HashtableSizeOptions: {
-      value = new tflite::HashtableSizeOptionsT(*reinterpret_cast<tflite::HashtableSizeOptionsT *>(u.value));
+      value = new tflite_micro::HashtableSizeOptionsT(*reinterpret_cast<tflite_micro::HashtableSizeOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_VarHandleOptions: {
-      value = new tflite::VarHandleOptionsT(*reinterpret_cast<tflite::VarHandleOptionsT *>(u.value));
+      value = new tflite_micro::VarHandleOptionsT(*reinterpret_cast<tflite_micro::VarHandleOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_ReadVariableOptions: {
-      value = new tflite::ReadVariableOptionsT(*reinterpret_cast<tflite::ReadVariableOptionsT *>(u.value));
+      value = new tflite_micro::ReadVariableOptionsT(*reinterpret_cast<tflite_micro::ReadVariableOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_AssignVariableOptions: {
-      value = new tflite::AssignVariableOptionsT(*reinterpret_cast<tflite::AssignVariableOptionsT *>(u.value));
+      value = new tflite_micro::AssignVariableOptionsT(*reinterpret_cast<tflite_micro::AssignVariableOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_RandomOptions: {
-      value = new tflite::RandomOptionsT(*reinterpret_cast<tflite::RandomOptionsT *>(u.value));
+      value = new tflite_micro::RandomOptionsT(*reinterpret_cast<tflite_micro::RandomOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_BucketizeOptions: {
-      value = new tflite::BucketizeOptionsT(*reinterpret_cast<tflite::BucketizeOptionsT *>(u.value));
+      value = new tflite_micro::BucketizeOptionsT(*reinterpret_cast<tflite_micro::BucketizeOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_GeluOptions: {
-      value = new tflite::GeluOptionsT(*reinterpret_cast<tflite::GeluOptionsT *>(u.value));
+      value = new tflite_micro::GeluOptionsT(*reinterpret_cast<tflite_micro::GeluOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_DynamicUpdateSliceOptions: {
-      value = new tflite::DynamicUpdateSliceOptionsT(*reinterpret_cast<tflite::DynamicUpdateSliceOptionsT *>(u.value));
+      value = new tflite_micro::DynamicUpdateSliceOptionsT(*reinterpret_cast<tflite_micro::DynamicUpdateSliceOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_UnsortedSegmentProdOptions: {
-      value = new tflite::UnsortedSegmentProdOptionsT(*reinterpret_cast<tflite::UnsortedSegmentProdOptionsT *>(u.value));
+      value = new tflite_micro::UnsortedSegmentProdOptionsT(*reinterpret_cast<tflite_micro::UnsortedSegmentProdOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_UnsortedSegmentMaxOptions: {
-      value = new tflite::UnsortedSegmentMaxOptionsT(*reinterpret_cast<tflite::UnsortedSegmentMaxOptionsT *>(u.value));
+      value = new tflite_micro::UnsortedSegmentMaxOptionsT(*reinterpret_cast<tflite_micro::UnsortedSegmentMaxOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_UnsortedSegmentMinOptions: {
-      value = new tflite::UnsortedSegmentMinOptionsT(*reinterpret_cast<tflite::UnsortedSegmentMinOptionsT *>(u.value));
+      value = new tflite_micro::UnsortedSegmentMinOptionsT(*reinterpret_cast<tflite_micro::UnsortedSegmentMinOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_UnsortedSegmentSumOptions: {
-      value = new tflite::UnsortedSegmentSumOptionsT(*reinterpret_cast<tflite::UnsortedSegmentSumOptionsT *>(u.value));
+      value = new tflite_micro::UnsortedSegmentSumOptionsT(*reinterpret_cast<tflite_micro::UnsortedSegmentSumOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_ATan2Options: {
-      value = new tflite::ATan2OptionsT(*reinterpret_cast<tflite::ATan2OptionsT *>(u.value));
+      value = new tflite_micro::ATan2OptionsT(*reinterpret_cast<tflite_micro::ATan2OptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_SignOptions: {
-      value = new tflite::SignOptionsT(*reinterpret_cast<tflite::SignOptionsT *>(u.value));
+      value = new tflite_micro::SignOptionsT(*reinterpret_cast<tflite_micro::SignOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_BitcastOptions: {
-      value = new tflite::BitcastOptionsT(*reinterpret_cast<tflite::BitcastOptionsT *>(u.value));
+      value = new tflite_micro::BitcastOptionsT(*reinterpret_cast<tflite_micro::BitcastOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_BitwiseXorOptions: {
-      value = new tflite::BitwiseXorOptionsT(*reinterpret_cast<tflite::BitwiseXorOptionsT *>(u.value));
+      value = new tflite_micro::BitwiseXorOptionsT(*reinterpret_cast<tflite_micro::BitwiseXorOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions_RightShiftOptions: {
-      value = new tflite::RightShiftOptionsT(*reinterpret_cast<tflite::RightShiftOptionsT *>(u.value));
+      value = new tflite_micro::RightShiftOptionsT(*reinterpret_cast<tflite_micro::RightShiftOptionsT *>(u.value));
       break;
     }
     default:
@@ -23474,632 +23474,632 @@ inline BuiltinOptionsUnion::BuiltinOptionsUnion(const BuiltinOptionsUnion &u) :
 inline void BuiltinOptionsUnion::Reset() {
   switch (type) {
     case BuiltinOptions_Conv2DOptions: {
-      auto ptr = reinterpret_cast<tflite::Conv2DOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::Conv2DOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_DepthwiseConv2DOptions: {
-      auto ptr = reinterpret_cast<tflite::DepthwiseConv2DOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::DepthwiseConv2DOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_ConcatEmbeddingsOptions: {
-      auto ptr = reinterpret_cast<tflite::ConcatEmbeddingsOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::ConcatEmbeddingsOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_LSHProjectionOptions: {
-      auto ptr = reinterpret_cast<tflite::LSHProjectionOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::LSHProjectionOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_Pool2DOptions: {
-      auto ptr = reinterpret_cast<tflite::Pool2DOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::Pool2DOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_SVDFOptions: {
-      auto ptr = reinterpret_cast<tflite::SVDFOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::SVDFOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_RNNOptions: {
-      auto ptr = reinterpret_cast<tflite::RNNOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::RNNOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_FullyConnectedOptions: {
-      auto ptr = reinterpret_cast<tflite::FullyConnectedOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::FullyConnectedOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_SoftmaxOptions: {
-      auto ptr = reinterpret_cast<tflite::SoftmaxOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::SoftmaxOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_ConcatenationOptions: {
-      auto ptr = reinterpret_cast<tflite::ConcatenationOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::ConcatenationOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_AddOptions: {
-      auto ptr = reinterpret_cast<tflite::AddOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::AddOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_L2NormOptions: {
-      auto ptr = reinterpret_cast<tflite::L2NormOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::L2NormOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_LocalResponseNormalizationOptions: {
-      auto ptr = reinterpret_cast<tflite::LocalResponseNormalizationOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::LocalResponseNormalizationOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_LSTMOptions: {
-      auto ptr = reinterpret_cast<tflite::LSTMOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::LSTMOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_ResizeBilinearOptions: {
-      auto ptr = reinterpret_cast<tflite::ResizeBilinearOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::ResizeBilinearOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_CallOptions: {
-      auto ptr = reinterpret_cast<tflite::CallOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::CallOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_ReshapeOptions: {
-      auto ptr = reinterpret_cast<tflite::ReshapeOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::ReshapeOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_SkipGramOptions: {
-      auto ptr = reinterpret_cast<tflite::SkipGramOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::SkipGramOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_SpaceToDepthOptions: {
-      auto ptr = reinterpret_cast<tflite::SpaceToDepthOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::SpaceToDepthOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_EmbeddingLookupSparseOptions: {
-      auto ptr = reinterpret_cast<tflite::EmbeddingLookupSparseOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::EmbeddingLookupSparseOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_MulOptions: {
-      auto ptr = reinterpret_cast<tflite::MulOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::MulOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_PadOptions: {
-      auto ptr = reinterpret_cast<tflite::PadOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::PadOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_GatherOptions: {
-      auto ptr = reinterpret_cast<tflite::GatherOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::GatherOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_BatchToSpaceNDOptions: {
-      auto ptr = reinterpret_cast<tflite::BatchToSpaceNDOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::BatchToSpaceNDOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_SpaceToBatchNDOptions: {
-      auto ptr = reinterpret_cast<tflite::SpaceToBatchNDOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::SpaceToBatchNDOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_TransposeOptions: {
-      auto ptr = reinterpret_cast<tflite::TransposeOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::TransposeOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_ReducerOptions: {
-      auto ptr = reinterpret_cast<tflite::ReducerOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::ReducerOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_SubOptions: {
-      auto ptr = reinterpret_cast<tflite::SubOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::SubOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_DivOptions: {
-      auto ptr = reinterpret_cast<tflite::DivOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::DivOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_SqueezeOptions: {
-      auto ptr = reinterpret_cast<tflite::SqueezeOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::SqueezeOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_SequenceRNNOptions: {
-      auto ptr = reinterpret_cast<tflite::SequenceRNNOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::SequenceRNNOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_StridedSliceOptions: {
-      auto ptr = reinterpret_cast<tflite::StridedSliceOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::StridedSliceOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_ExpOptions: {
-      auto ptr = reinterpret_cast<tflite::ExpOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::ExpOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_TopKV2Options: {
-      auto ptr = reinterpret_cast<tflite::TopKV2OptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::TopKV2OptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_SplitOptions: {
-      auto ptr = reinterpret_cast<tflite::SplitOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::SplitOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_LogSoftmaxOptions: {
-      auto ptr = reinterpret_cast<tflite::LogSoftmaxOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::LogSoftmaxOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_CastOptions: {
-      auto ptr = reinterpret_cast<tflite::CastOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::CastOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_DequantizeOptions: {
-      auto ptr = reinterpret_cast<tflite::DequantizeOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::DequantizeOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_MaximumMinimumOptions: {
-      auto ptr = reinterpret_cast<tflite::MaximumMinimumOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::MaximumMinimumOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_ArgMaxOptions: {
-      auto ptr = reinterpret_cast<tflite::ArgMaxOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::ArgMaxOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_LessOptions: {
-      auto ptr = reinterpret_cast<tflite::LessOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::LessOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_NegOptions: {
-      auto ptr = reinterpret_cast<tflite::NegOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::NegOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_PadV2Options: {
-      auto ptr = reinterpret_cast<tflite::PadV2OptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::PadV2OptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_GreaterOptions: {
-      auto ptr = reinterpret_cast<tflite::GreaterOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::GreaterOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_GreaterEqualOptions: {
-      auto ptr = reinterpret_cast<tflite::GreaterEqualOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::GreaterEqualOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_LessEqualOptions: {
-      auto ptr = reinterpret_cast<tflite::LessEqualOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::LessEqualOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_SelectOptions: {
-      auto ptr = reinterpret_cast<tflite::SelectOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::SelectOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_SliceOptions: {
-      auto ptr = reinterpret_cast<tflite::SliceOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::SliceOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_TransposeConvOptions: {
-      auto ptr = reinterpret_cast<tflite::TransposeConvOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::TransposeConvOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_SparseToDenseOptions: {
-      auto ptr = reinterpret_cast<tflite::SparseToDenseOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::SparseToDenseOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_TileOptions: {
-      auto ptr = reinterpret_cast<tflite::TileOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::TileOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_ExpandDimsOptions: {
-      auto ptr = reinterpret_cast<tflite::ExpandDimsOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::ExpandDimsOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_EqualOptions: {
-      auto ptr = reinterpret_cast<tflite::EqualOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::EqualOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_NotEqualOptions: {
-      auto ptr = reinterpret_cast<tflite::NotEqualOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::NotEqualOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_ShapeOptions: {
-      auto ptr = reinterpret_cast<tflite::ShapeOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::ShapeOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_PowOptions: {
-      auto ptr = reinterpret_cast<tflite::PowOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::PowOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_ArgMinOptions: {
-      auto ptr = reinterpret_cast<tflite::ArgMinOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::ArgMinOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_FakeQuantOptions: {
-      auto ptr = reinterpret_cast<tflite::FakeQuantOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::FakeQuantOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_PackOptions: {
-      auto ptr = reinterpret_cast<tflite::PackOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::PackOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_LogicalOrOptions: {
-      auto ptr = reinterpret_cast<tflite::LogicalOrOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::LogicalOrOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_OneHotOptions: {
-      auto ptr = reinterpret_cast<tflite::OneHotOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::OneHotOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_LogicalAndOptions: {
-      auto ptr = reinterpret_cast<tflite::LogicalAndOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::LogicalAndOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_LogicalNotOptions: {
-      auto ptr = reinterpret_cast<tflite::LogicalNotOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::LogicalNotOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_UnpackOptions: {
-      auto ptr = reinterpret_cast<tflite::UnpackOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::UnpackOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_FloorDivOptions: {
-      auto ptr = reinterpret_cast<tflite::FloorDivOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::FloorDivOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_SquareOptions: {
-      auto ptr = reinterpret_cast<tflite::SquareOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::SquareOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_ZerosLikeOptions: {
-      auto ptr = reinterpret_cast<tflite::ZerosLikeOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::ZerosLikeOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_FillOptions: {
-      auto ptr = reinterpret_cast<tflite::FillOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::FillOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_BidirectionalSequenceLSTMOptions: {
-      auto ptr = reinterpret_cast<tflite::BidirectionalSequenceLSTMOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::BidirectionalSequenceLSTMOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_BidirectionalSequenceRNNOptions: {
-      auto ptr = reinterpret_cast<tflite::BidirectionalSequenceRNNOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::BidirectionalSequenceRNNOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_UnidirectionalSequenceLSTMOptions: {
-      auto ptr = reinterpret_cast<tflite::UnidirectionalSequenceLSTMOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::UnidirectionalSequenceLSTMOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_FloorModOptions: {
-      auto ptr = reinterpret_cast<tflite::FloorModOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::FloorModOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_RangeOptions: {
-      auto ptr = reinterpret_cast<tflite::RangeOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::RangeOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_ResizeNearestNeighborOptions: {
-      auto ptr = reinterpret_cast<tflite::ResizeNearestNeighborOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::ResizeNearestNeighborOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_LeakyReluOptions: {
-      auto ptr = reinterpret_cast<tflite::LeakyReluOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::LeakyReluOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_SquaredDifferenceOptions: {
-      auto ptr = reinterpret_cast<tflite::SquaredDifferenceOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::SquaredDifferenceOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_MirrorPadOptions: {
-      auto ptr = reinterpret_cast<tflite::MirrorPadOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::MirrorPadOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_AbsOptions: {
-      auto ptr = reinterpret_cast<tflite::AbsOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::AbsOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_SplitVOptions: {
-      auto ptr = reinterpret_cast<tflite::SplitVOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::SplitVOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_UniqueOptions: {
-      auto ptr = reinterpret_cast<tflite::UniqueOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::UniqueOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_ReverseV2Options: {
-      auto ptr = reinterpret_cast<tflite::ReverseV2OptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::ReverseV2OptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_AddNOptions: {
-      auto ptr = reinterpret_cast<tflite::AddNOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::AddNOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_GatherNdOptions: {
-      auto ptr = reinterpret_cast<tflite::GatherNdOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::GatherNdOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_CosOptions: {
-      auto ptr = reinterpret_cast<tflite::CosOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::CosOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_WhereOptions: {
-      auto ptr = reinterpret_cast<tflite::WhereOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::WhereOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_RankOptions: {
-      auto ptr = reinterpret_cast<tflite::RankOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::RankOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_ReverseSequenceOptions: {
-      auto ptr = reinterpret_cast<tflite::ReverseSequenceOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::ReverseSequenceOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_MatrixDiagOptions: {
-      auto ptr = reinterpret_cast<tflite::MatrixDiagOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::MatrixDiagOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_QuantizeOptions: {
-      auto ptr = reinterpret_cast<tflite::QuantizeOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::QuantizeOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_MatrixSetDiagOptions: {
-      auto ptr = reinterpret_cast<tflite::MatrixSetDiagOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::MatrixSetDiagOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_HardSwishOptions: {
-      auto ptr = reinterpret_cast<tflite::HardSwishOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::HardSwishOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_IfOptions: {
-      auto ptr = reinterpret_cast<tflite::IfOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::IfOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_WhileOptions: {
-      auto ptr = reinterpret_cast<tflite::WhileOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::WhileOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_DepthToSpaceOptions: {
-      auto ptr = reinterpret_cast<tflite::DepthToSpaceOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::DepthToSpaceOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_NonMaxSuppressionV4Options: {
-      auto ptr = reinterpret_cast<tflite::NonMaxSuppressionV4OptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::NonMaxSuppressionV4OptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_NonMaxSuppressionV5Options: {
-      auto ptr = reinterpret_cast<tflite::NonMaxSuppressionV5OptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::NonMaxSuppressionV5OptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_ScatterNdOptions: {
-      auto ptr = reinterpret_cast<tflite::ScatterNdOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::ScatterNdOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_SelectV2Options: {
-      auto ptr = reinterpret_cast<tflite::SelectV2OptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::SelectV2OptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_DensifyOptions: {
-      auto ptr = reinterpret_cast<tflite::DensifyOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::DensifyOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_SegmentSumOptions: {
-      auto ptr = reinterpret_cast<tflite::SegmentSumOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::SegmentSumOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_BatchMatMulOptions: {
-      auto ptr = reinterpret_cast<tflite::BatchMatMulOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::BatchMatMulOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_CumsumOptions: {
-      auto ptr = reinterpret_cast<tflite::CumsumOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::CumsumOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_CallOnceOptions: {
-      auto ptr = reinterpret_cast<tflite::CallOnceOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::CallOnceOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_BroadcastToOptions: {
-      auto ptr = reinterpret_cast<tflite::BroadcastToOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::BroadcastToOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_Rfft2dOptions: {
-      auto ptr = reinterpret_cast<tflite::Rfft2dOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::Rfft2dOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_Conv3DOptions: {
-      auto ptr = reinterpret_cast<tflite::Conv3DOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::Conv3DOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_HashtableOptions: {
-      auto ptr = reinterpret_cast<tflite::HashtableOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::HashtableOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_HashtableFindOptions: {
-      auto ptr = reinterpret_cast<tflite::HashtableFindOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::HashtableFindOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_HashtableImportOptions: {
-      auto ptr = reinterpret_cast<tflite::HashtableImportOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::HashtableImportOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_HashtableSizeOptions: {
-      auto ptr = reinterpret_cast<tflite::HashtableSizeOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::HashtableSizeOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_VarHandleOptions: {
-      auto ptr = reinterpret_cast<tflite::VarHandleOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::VarHandleOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_ReadVariableOptions: {
-      auto ptr = reinterpret_cast<tflite::ReadVariableOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::ReadVariableOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_AssignVariableOptions: {
-      auto ptr = reinterpret_cast<tflite::AssignVariableOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::AssignVariableOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_RandomOptions: {
-      auto ptr = reinterpret_cast<tflite::RandomOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::RandomOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_BucketizeOptions: {
-      auto ptr = reinterpret_cast<tflite::BucketizeOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::BucketizeOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_GeluOptions: {
-      auto ptr = reinterpret_cast<tflite::GeluOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::GeluOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_DynamicUpdateSliceOptions: {
-      auto ptr = reinterpret_cast<tflite::DynamicUpdateSliceOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::DynamicUpdateSliceOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_UnsortedSegmentProdOptions: {
-      auto ptr = reinterpret_cast<tflite::UnsortedSegmentProdOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::UnsortedSegmentProdOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_UnsortedSegmentMaxOptions: {
-      auto ptr = reinterpret_cast<tflite::UnsortedSegmentMaxOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::UnsortedSegmentMaxOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_UnsortedSegmentMinOptions: {
-      auto ptr = reinterpret_cast<tflite::UnsortedSegmentMinOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::UnsortedSegmentMinOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_UnsortedSegmentSumOptions: {
-      auto ptr = reinterpret_cast<tflite::UnsortedSegmentSumOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::UnsortedSegmentSumOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_ATan2Options: {
-      auto ptr = reinterpret_cast<tflite::ATan2OptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::ATan2OptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_SignOptions: {
-      auto ptr = reinterpret_cast<tflite::SignOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::SignOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_BitcastOptions: {
-      auto ptr = reinterpret_cast<tflite::BitcastOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::BitcastOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_BitwiseXorOptions: {
-      auto ptr = reinterpret_cast<tflite::BitwiseXorOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::BitwiseXorOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions_RightShiftOptions: {
-      auto ptr = reinterpret_cast<tflite::RightShiftOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::RightShiftOptionsT *>(value);
       delete ptr;
       break;
     }
@@ -24115,83 +24115,83 @@ inline bool VerifyBuiltinOptions2(flatbuffers::Verifier &verifier, const void *o
       return true;
     }
     case BuiltinOptions2_StablehloConcatenateOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloConcatenateOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloConcatenateOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions2_StablehloBroadcastInDimOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloBroadcastInDimOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloBroadcastInDimOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions2_StablehloSliceOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloSliceOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloSliceOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions2_StablehloConvolutionOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloConvolutionOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloConvolutionOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions2_StablehloCustomCallOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloCustomCallOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloCustomCallOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions2_StablehloReduceOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloReduceOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloReduceOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions2_StablehloScatterOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloScatterOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloScatterOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions2_StablehloCompareOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloCompareOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloCompareOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions2_StablehloDynamicSliceOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloDynamicSliceOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloDynamicSliceOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions2_StablehloPadOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloPadOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloPadOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions2_StablehloIotaOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloIotaOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloIotaOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions2_StablehloDotGeneralOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloDotGeneralOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloDotGeneralOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions2_StablehloReduceWindowOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloReduceWindowOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloReduceWindowOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions2_StablehloSortOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloSortOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloSortOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions2_StablehloWhileOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloWhileOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloWhileOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions2_StablehloGatherOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloGatherOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloGatherOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions2_StablehloTransposeOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloTransposeOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloTransposeOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions2_DilateOptions: {
-      auto ptr = reinterpret_cast<const tflite::DilateOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::DilateOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions2_StablehloRngBitGeneratorOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloRngBitGeneratorOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloRngBitGeneratorOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     case BuiltinOptions2_ReduceWindowOptions: {
-      auto ptr = reinterpret_cast<const tflite::ReduceWindowOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ReduceWindowOptions *>(obj);
       return verifier.VerifyTable(ptr);
     }
     default: return true;
@@ -24214,83 +24214,83 @@ inline void *BuiltinOptions2Union::UnPack(const void *obj, BuiltinOptions2 type,
   (void)resolver;
   switch (type) {
     case BuiltinOptions2_StablehloConcatenateOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloConcatenateOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloConcatenateOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions2_StablehloBroadcastInDimOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloBroadcastInDimOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloBroadcastInDimOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions2_StablehloSliceOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloSliceOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloSliceOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions2_StablehloConvolutionOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloConvolutionOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloConvolutionOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions2_StablehloCustomCallOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloCustomCallOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloCustomCallOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions2_StablehloReduceOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloReduceOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloReduceOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions2_StablehloScatterOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloScatterOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloScatterOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions2_StablehloCompareOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloCompareOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloCompareOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions2_StablehloDynamicSliceOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloDynamicSliceOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloDynamicSliceOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions2_StablehloPadOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloPadOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloPadOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions2_StablehloIotaOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloIotaOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloIotaOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions2_StablehloDotGeneralOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloDotGeneralOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloDotGeneralOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions2_StablehloReduceWindowOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloReduceWindowOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloReduceWindowOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions2_StablehloSortOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloSortOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloSortOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions2_StablehloWhileOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloWhileOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloWhileOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions2_StablehloGatherOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloGatherOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloGatherOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions2_StablehloTransposeOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloTransposeOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloTransposeOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions2_DilateOptions: {
-      auto ptr = reinterpret_cast<const tflite::DilateOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::DilateOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions2_StablehloRngBitGeneratorOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloRngBitGeneratorOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloRngBitGeneratorOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     case BuiltinOptions2_ReduceWindowOptions: {
-      auto ptr = reinterpret_cast<const tflite::ReduceWindowOptions *>(obj);
+      auto ptr = reinterpret_cast<const tflite_micro::ReduceWindowOptions *>(obj);
       return ptr->UnPack(resolver);
     }
     default: return nullptr;
@@ -24301,83 +24301,83 @@ inline flatbuffers::Offset<void> BuiltinOptions2Union::Pack(flatbuffers::FlatBuf
   (void)_rehasher;
   switch (type) {
     case BuiltinOptions2_StablehloConcatenateOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloConcatenateOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloConcatenateOptionsT *>(value);
       return CreateStablehloConcatenateOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions2_StablehloBroadcastInDimOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloBroadcastInDimOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloBroadcastInDimOptionsT *>(value);
       return CreateStablehloBroadcastInDimOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions2_StablehloSliceOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloSliceOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloSliceOptionsT *>(value);
       return CreateStablehloSliceOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions2_StablehloConvolutionOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloConvolutionOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloConvolutionOptionsT *>(value);
       return CreateStablehloConvolutionOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions2_StablehloCustomCallOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloCustomCallOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloCustomCallOptionsT *>(value);
       return CreateStablehloCustomCallOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions2_StablehloReduceOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloReduceOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloReduceOptionsT *>(value);
       return CreateStablehloReduceOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions2_StablehloScatterOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloScatterOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloScatterOptionsT *>(value);
       return CreateStablehloScatterOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions2_StablehloCompareOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloCompareOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloCompareOptionsT *>(value);
       return CreateStablehloCompareOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions2_StablehloDynamicSliceOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloDynamicSliceOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloDynamicSliceOptionsT *>(value);
       return CreateStablehloDynamicSliceOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions2_StablehloPadOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloPadOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloPadOptionsT *>(value);
       return CreateStablehloPadOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions2_StablehloIotaOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloIotaOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloIotaOptionsT *>(value);
       return CreateStablehloIotaOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions2_StablehloDotGeneralOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloDotGeneralOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloDotGeneralOptionsT *>(value);
       return CreateStablehloDotGeneralOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions2_StablehloReduceWindowOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloReduceWindowOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloReduceWindowOptionsT *>(value);
       return CreateStablehloReduceWindowOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions2_StablehloSortOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloSortOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloSortOptionsT *>(value);
       return CreateStablehloSortOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions2_StablehloWhileOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloWhileOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloWhileOptionsT *>(value);
       return CreateStablehloWhileOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions2_StablehloGatherOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloGatherOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloGatherOptionsT *>(value);
       return CreateStablehloGatherOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions2_StablehloTransposeOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloTransposeOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloTransposeOptionsT *>(value);
       return CreateStablehloTransposeOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions2_DilateOptions: {
-      auto ptr = reinterpret_cast<const tflite::DilateOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::DilateOptionsT *>(value);
       return CreateDilateOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions2_StablehloRngBitGeneratorOptions: {
-      auto ptr = reinterpret_cast<const tflite::StablehloRngBitGeneratorOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::StablehloRngBitGeneratorOptionsT *>(value);
       return CreateStablehloRngBitGeneratorOptions(_fbb, ptr, _rehasher).Union();
     }
     case BuiltinOptions2_ReduceWindowOptions: {
-      auto ptr = reinterpret_cast<const tflite::ReduceWindowOptionsT *>(value);
+      auto ptr = reinterpret_cast<const tflite_micro::ReduceWindowOptionsT *>(value);
       return CreateReduceWindowOptions(_fbb, ptr, _rehasher).Union();
     }
     default: return 0;
@@ -24387,83 +24387,83 @@ inline flatbuffers::Offset<void> BuiltinOptions2Union::Pack(flatbuffers::FlatBuf
 inline BuiltinOptions2Union::BuiltinOptions2Union(const BuiltinOptions2Union &u) : type(u.type), value(nullptr) {
   switch (type) {
     case BuiltinOptions2_StablehloConcatenateOptions: {
-      value = new tflite::StablehloConcatenateOptionsT(*reinterpret_cast<tflite::StablehloConcatenateOptionsT *>(u.value));
+      value = new tflite_micro::StablehloConcatenateOptionsT(*reinterpret_cast<tflite_micro::StablehloConcatenateOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions2_StablehloBroadcastInDimOptions: {
-      value = new tflite::StablehloBroadcastInDimOptionsT(*reinterpret_cast<tflite::StablehloBroadcastInDimOptionsT *>(u.value));
+      value = new tflite_micro::StablehloBroadcastInDimOptionsT(*reinterpret_cast<tflite_micro::StablehloBroadcastInDimOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions2_StablehloSliceOptions: {
-      value = new tflite::StablehloSliceOptionsT(*reinterpret_cast<tflite::StablehloSliceOptionsT *>(u.value));
+      value = new tflite_micro::StablehloSliceOptionsT(*reinterpret_cast<tflite_micro::StablehloSliceOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions2_StablehloConvolutionOptions: {
-      value = new tflite::StablehloConvolutionOptionsT(*reinterpret_cast<tflite::StablehloConvolutionOptionsT *>(u.value));
+      value = new tflite_micro::StablehloConvolutionOptionsT(*reinterpret_cast<tflite_micro::StablehloConvolutionOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions2_StablehloCustomCallOptions: {
-      value = new tflite::StablehloCustomCallOptionsT(*reinterpret_cast<tflite::StablehloCustomCallOptionsT *>(u.value));
+      value = new tflite_micro::StablehloCustomCallOptionsT(*reinterpret_cast<tflite_micro::StablehloCustomCallOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions2_StablehloReduceOptions: {
-      value = new tflite::StablehloReduceOptionsT(*reinterpret_cast<tflite::StablehloReduceOptionsT *>(u.value));
+      value = new tflite_micro::StablehloReduceOptionsT(*reinterpret_cast<tflite_micro::StablehloReduceOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions2_StablehloScatterOptions: {
-      value = new tflite::StablehloScatterOptionsT(*reinterpret_cast<tflite::StablehloScatterOptionsT *>(u.value));
+      value = new tflite_micro::StablehloScatterOptionsT(*reinterpret_cast<tflite_micro::StablehloScatterOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions2_StablehloCompareOptions: {
-      value = new tflite::StablehloCompareOptionsT(*reinterpret_cast<tflite::StablehloCompareOptionsT *>(u.value));
+      value = new tflite_micro::StablehloCompareOptionsT(*reinterpret_cast<tflite_micro::StablehloCompareOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions2_StablehloDynamicSliceOptions: {
-      value = new tflite::StablehloDynamicSliceOptionsT(*reinterpret_cast<tflite::StablehloDynamicSliceOptionsT *>(u.value));
+      value = new tflite_micro::StablehloDynamicSliceOptionsT(*reinterpret_cast<tflite_micro::StablehloDynamicSliceOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions2_StablehloPadOptions: {
-      value = new tflite::StablehloPadOptionsT(*reinterpret_cast<tflite::StablehloPadOptionsT *>(u.value));
+      value = new tflite_micro::StablehloPadOptionsT(*reinterpret_cast<tflite_micro::StablehloPadOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions2_StablehloIotaOptions: {
-      value = new tflite::StablehloIotaOptionsT(*reinterpret_cast<tflite::StablehloIotaOptionsT *>(u.value));
+      value = new tflite_micro::StablehloIotaOptionsT(*reinterpret_cast<tflite_micro::StablehloIotaOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions2_StablehloDotGeneralOptions: {
-      value = new tflite::StablehloDotGeneralOptionsT(*reinterpret_cast<tflite::StablehloDotGeneralOptionsT *>(u.value));
+      value = new tflite_micro::StablehloDotGeneralOptionsT(*reinterpret_cast<tflite_micro::StablehloDotGeneralOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions2_StablehloReduceWindowOptions: {
-      value = new tflite::StablehloReduceWindowOptionsT(*reinterpret_cast<tflite::StablehloReduceWindowOptionsT *>(u.value));
+      value = new tflite_micro::StablehloReduceWindowOptionsT(*reinterpret_cast<tflite_micro::StablehloReduceWindowOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions2_StablehloSortOptions: {
-      value = new tflite::StablehloSortOptionsT(*reinterpret_cast<tflite::StablehloSortOptionsT *>(u.value));
+      value = new tflite_micro::StablehloSortOptionsT(*reinterpret_cast<tflite_micro::StablehloSortOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions2_StablehloWhileOptions: {
-      value = new tflite::StablehloWhileOptionsT(*reinterpret_cast<tflite::StablehloWhileOptionsT *>(u.value));
+      value = new tflite_micro::StablehloWhileOptionsT(*reinterpret_cast<tflite_micro::StablehloWhileOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions2_StablehloGatherOptions: {
-      value = new tflite::StablehloGatherOptionsT(*reinterpret_cast<tflite::StablehloGatherOptionsT *>(u.value));
+      value = new tflite_micro::StablehloGatherOptionsT(*reinterpret_cast<tflite_micro::StablehloGatherOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions2_StablehloTransposeOptions: {
-      value = new tflite::StablehloTransposeOptionsT(*reinterpret_cast<tflite::StablehloTransposeOptionsT *>(u.value));
+      value = new tflite_micro::StablehloTransposeOptionsT(*reinterpret_cast<tflite_micro::StablehloTransposeOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions2_DilateOptions: {
-      value = new tflite::DilateOptionsT(*reinterpret_cast<tflite::DilateOptionsT *>(u.value));
+      value = new tflite_micro::DilateOptionsT(*reinterpret_cast<tflite_micro::DilateOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions2_StablehloRngBitGeneratorOptions: {
-      value = new tflite::StablehloRngBitGeneratorOptionsT(*reinterpret_cast<tflite::StablehloRngBitGeneratorOptionsT *>(u.value));
+      value = new tflite_micro::StablehloRngBitGeneratorOptionsT(*reinterpret_cast<tflite_micro::StablehloRngBitGeneratorOptionsT *>(u.value));
       break;
     }
     case BuiltinOptions2_ReduceWindowOptions: {
-      value = new tflite::ReduceWindowOptionsT(*reinterpret_cast<tflite::ReduceWindowOptionsT *>(u.value));
+      value = new tflite_micro::ReduceWindowOptionsT(*reinterpret_cast<tflite_micro::ReduceWindowOptionsT *>(u.value));
       break;
     }
     default:
@@ -24474,102 +24474,102 @@ inline BuiltinOptions2Union::BuiltinOptions2Union(const BuiltinOptions2Union &u)
 inline void BuiltinOptions2Union::Reset() {
   switch (type) {
     case BuiltinOptions2_StablehloConcatenateOptions: {
-      auto ptr = reinterpret_cast<tflite::StablehloConcatenateOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::StablehloConcatenateOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions2_StablehloBroadcastInDimOptions: {
-      auto ptr = reinterpret_cast<tflite::StablehloBroadcastInDimOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::StablehloBroadcastInDimOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions2_StablehloSliceOptions: {
-      auto ptr = reinterpret_cast<tflite::StablehloSliceOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::StablehloSliceOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions2_StablehloConvolutionOptions: {
-      auto ptr = reinterpret_cast<tflite::StablehloConvolutionOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::StablehloConvolutionOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions2_StablehloCustomCallOptions: {
-      auto ptr = reinterpret_cast<tflite::StablehloCustomCallOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::StablehloCustomCallOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions2_StablehloReduceOptions: {
-      auto ptr = reinterpret_cast<tflite::StablehloReduceOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::StablehloReduceOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions2_StablehloScatterOptions: {
-      auto ptr = reinterpret_cast<tflite::StablehloScatterOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::StablehloScatterOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions2_StablehloCompareOptions: {
-      auto ptr = reinterpret_cast<tflite::StablehloCompareOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::StablehloCompareOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions2_StablehloDynamicSliceOptions: {
-      auto ptr = reinterpret_cast<tflite::StablehloDynamicSliceOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::StablehloDynamicSliceOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions2_StablehloPadOptions: {
-      auto ptr = reinterpret_cast<tflite::StablehloPadOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::StablehloPadOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions2_StablehloIotaOptions: {
-      auto ptr = reinterpret_cast<tflite::StablehloIotaOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::StablehloIotaOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions2_StablehloDotGeneralOptions: {
-      auto ptr = reinterpret_cast<tflite::StablehloDotGeneralOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::StablehloDotGeneralOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions2_StablehloReduceWindowOptions: {
-      auto ptr = reinterpret_cast<tflite::StablehloReduceWindowOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::StablehloReduceWindowOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions2_StablehloSortOptions: {
-      auto ptr = reinterpret_cast<tflite::StablehloSortOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::StablehloSortOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions2_StablehloWhileOptions: {
-      auto ptr = reinterpret_cast<tflite::StablehloWhileOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::StablehloWhileOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions2_StablehloGatherOptions: {
-      auto ptr = reinterpret_cast<tflite::StablehloGatherOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::StablehloGatherOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions2_StablehloTransposeOptions: {
-      auto ptr = reinterpret_cast<tflite::StablehloTransposeOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::StablehloTransposeOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions2_DilateOptions: {
-      auto ptr = reinterpret_cast<tflite::DilateOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::DilateOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions2_StablehloRngBitGeneratorOptions: {
-      auto ptr = reinterpret_cast<tflite::StablehloRngBitGeneratorOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::StablehloRngBitGeneratorOptionsT *>(value);
       delete ptr;
       break;
     }
     case BuiltinOptions2_ReduceWindowOptions: {
-      auto ptr = reinterpret_cast<tflite::ReduceWindowOptionsT *>(value);
+      auto ptr = reinterpret_cast<tflite_micro::ReduceWindowOptionsT *>(value);
       delete ptr;
       break;
     }
@@ -24579,12 +24579,12 @@ inline void BuiltinOptions2Union::Reset() {
   type = BuiltinOptions2_NONE;
 }
 
-inline const tflite::Model *GetModel(const void *buf) {
-  return flatbuffers::GetRoot<tflite::Model>(buf);
+inline const tflite_micro::Model *GetModel(const void *buf) {
+  return flatbuffers::GetRoot<tflite_micro::Model>(buf);
 }
 
-inline const tflite::Model *GetSizePrefixedModel(const void *buf) {
-  return flatbuffers::GetSizePrefixedRoot<tflite::Model>(buf);
+inline const tflite_micro::Model *GetSizePrefixedModel(const void *buf) {
+  return flatbuffers::GetSizePrefixedRoot<tflite_micro::Model>(buf);
 }
 
 inline const char *ModelIdentifier() {
@@ -24603,12 +24603,12 @@ inline bool SizePrefixedModelBufferHasIdentifier(const void *buf) {
 
 inline bool VerifyModelBuffer(
     flatbuffers::Verifier &verifier) {
-  return verifier.VerifyBuffer<tflite::Model>(ModelIdentifier());
+  return verifier.VerifyBuffer<tflite_micro::Model>(ModelIdentifier());
 }
 
 inline bool VerifySizePrefixedModelBuffer(
     flatbuffers::Verifier &verifier) {
-  return verifier.VerifySizePrefixedBuffer<tflite::Model>(ModelIdentifier());
+  return verifier.VerifySizePrefixedBuffer<tflite_micro::Model>(ModelIdentifier());
 }
 
 inline const char *ModelExtension() {
@@ -24617,28 +24617,28 @@ inline const char *ModelExtension() {
 
 inline void FinishModelBuffer(
     flatbuffers::FlatBufferBuilder &fbb,
-    flatbuffers::Offset<tflite::Model> root) {
+    flatbuffers::Offset<tflite_micro::Model> root) {
   fbb.Finish(root, ModelIdentifier());
 }
 
 inline void FinishSizePrefixedModelBuffer(
     flatbuffers::FlatBufferBuilder &fbb,
-    flatbuffers::Offset<tflite::Model> root) {
+    flatbuffers::Offset<tflite_micro::Model> root) {
   fbb.FinishSizePrefixed(root, ModelIdentifier());
 }
 
-inline std::unique_ptr<tflite::ModelT> UnPackModel(
+inline std::unique_ptr<tflite_micro::ModelT> UnPackModel(
     const void *buf,
     const flatbuffers::resolver_function_t *res = nullptr) {
-  return std::unique_ptr<tflite::ModelT>(GetModel(buf)->UnPack(res));
+  return std::unique_ptr<tflite_micro::ModelT>(GetModel(buf)->UnPack(res));
 }
 
-inline std::unique_ptr<tflite::ModelT> UnPackSizePrefixedModel(
+inline std::unique_ptr<tflite_micro::ModelT> UnPackSizePrefixedModel(
     const void *buf,
     const flatbuffers::resolver_function_t *res = nullptr) {
-  return std::unique_ptr<tflite::ModelT>(GetSizePrefixedModel(buf)->UnPack(res));
+  return std::unique_ptr<tflite_micro::ModelT>(GetSizePrefixedModel(buf)->UnPack(res));
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // FLATBUFFERS_GENERATED_SCHEMA_TFLITE_H_
diff --git a/tensorflow/lite/schema/schema_utils.cc b/tensorflow/lite/schema/schema_utils.cc
index 285873de..bb59247c 100644
--- a/tensorflow/lite/schema/schema_utils.cc
+++ b/tensorflow/lite/schema/schema_utils.cc
@@ -18,7 +18,7 @@ limitations under the License.
 
 #include "tensorflow/lite/kernels/internal/compatibility.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // The following GetBuiltinCode methods are the utility methods for reading
 // builtin operator code, ensuring compatibility issues between v3 and v3a
@@ -59,4 +59,4 @@ BuiltinOperator GetBuiltinCode(const OperatorCodeT* op_code) {
                                              op_code->deprecated_builtin_code));
 }
 
-}  // namespace tflite
+}  // namespace tflite_micro
diff --git a/tensorflow/lite/schema/schema_utils.h b/tensorflow/lite/schema/schema_utils.h
index 9cca36c7..36b02e0b 100644
--- a/tensorflow/lite/schema/schema_utils.h
+++ b/tensorflow/lite/schema/schema_utils.h
@@ -18,7 +18,7 @@ limitations under the License.
 #include "flatbuffers/flatbuffers.h"
 #include "tensorflow/lite/schema/schema_generated.h"
 
-namespace tflite {
+namespace tflite_micro {
 
 // The following methods are introduced to resolve op builtin code shortage
 // problem. The new builtin operator will be assigned to the extended builtin
@@ -28,6 +28,6 @@ BuiltinOperator GetBuiltinCode(const OperatorCode *op_code);
 
 BuiltinOperator GetBuiltinCode(const OperatorCodeT *op_code);
 
-}  // namespace tflite
+}  // namespace tflite_micro
 
 #endif  // TENSORFLOW_LITE_SCHEMA_SCHEMA_UTILS_H_
